{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "# set seed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import utils as ut\n",
    "import experiment as exp\n",
    "from evaluation import *\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import random\n",
    "#define fixed_hyperparams and create a config gen\n",
    "from configurations import RandomConfigGen, Configuration\n",
    "from torch import nn\n",
    "from deep_net import RandomNet\n",
    "from experiment import run_experiment\n",
    "import regex as re\n",
    "from pathlib import *\n",
    "from plot import *\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from sk_models import setup_pls_models_exh, StandardScaler, PLSRegression\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed + 1)\n",
    "np.random.seed(seed + 2)\n",
    "random_state = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\workspace\\lazydeep\\experiments\\2.01\\A_C_OF_ALPHA\n"
     ]
    }
   ],
   "source": [
    "#setup input and output formats, load data\n",
    "\n",
    "file_name = \"A_C_OF_ALPHA.csv\"\n",
    "id_cols =[\"sample_id\"]#[\"db_id\", \"sample_id\"]#[\"sample_id\"]\n",
    "\n",
    "data_path = Path('D:/workspace/lazydeep/data/soil_data/')\n",
    "model_path = Path('D:/workspace/lazydeep/experiments/2.00/')\n",
    "log_path = Path(\"D:/workspace/lazydeep/experiments/2.01\")\n",
    "\n",
    "data_file = data_path / file_name\n",
    "log_dir = log_path / re.sub(r'\\.(?=csv$)[^.]+$', '',file_name)\n",
    "model_dir = model_path / re.sub(r'\\.(?=csv$)[^.]+$', '',file_name)\n",
    "\n",
    "if not log_dir.exists():\n",
    "    log_dir.mkdir()\n",
    "print(log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% load data\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7329, 1703)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(data_file)\n",
    "data = data.sample(frac=1)\n",
    "nrow, ncol = data.shape\n",
    "data = ut.sample_data(data,random_state)\n",
    "n_features = ncol - 1-len(id_cols)\n",
    "\n",
    "dataset = TabularDataset(data,id_cols = id_cols, cat_cols=None, output_cols=None, ignore_cols= None)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% load models\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 models\n"
     ]
    }
   ],
   "source": [
    "n_models = 100\n",
    "model_names = [f\"random_{i}\" for i in range(0,n_models)]\n",
    "deep_models = {name:torch.load(model_dir/\"models\"/name/\"_model\") for name in model_names}\n",
    "configs =  None\n",
    "#for each model, load state\n",
    "print(f\"Loaded {len(deep_models)} models\")\n",
    "#print(deep_models)\n",
    "for name in model_names:\n",
    "    sub_path = log_dir / name\n",
    "    if not sub_path.exists():\n",
    "        sub_path.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% setup logging and tensorboard outputs\n"
    }
   },
   "outputs": [],
   "source": [
    "# set logging, in this case the root logger\n",
    "ut.setup_logger(logger_name=\"\",file_name=log_dir/\"log.txt\")\n",
    "ut.setup_logger(logger_name=\"summary\",file_name=log_dir/\"summary.txt\")\n",
    "summary_logger = logging.getLogger(\"summary\")\n",
    "tb = SummaryWriter(log_dir/\"tb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% setup experiment\n"
    }
   },
   "outputs": [],
   "source": [
    "fixed_hyperparams = {'bs': 32,'loss': nn.MSELoss(),'epochs': 100}\n",
    "preprocessing = StandardScaler()\n",
    "eval = CrossValEvaluation(preprocessing=preprocessing,tensorboard=None,time=True,random_state=random_state)\n",
    "scores={} #->model->knn:{fold_0:number,...,fold_n:number,mean:number,median:number\n",
    "preds={} #model-> foldsxknn_models\n",
    "deep_scores_dict={}\n",
    "deep_preds_dict={}\n",
    "actual_y = None\n",
    "\n",
    "device = \"cpu\" #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "load_fun_cv = lambda name,model, fold : model.load_state(model_dir/'models'/name/f\"_fold_{fold}\")\n",
    "load_fun_pp_cv = lambda fold : preprocessing.from_state(preprocessing.load_state(model_dir/'preprocessing'/f\"_fold_{fold}\"))\n",
    "load_fun_build = lambda name,model : model.load_state(model_dir/'models'/name/f\"_final\")\n",
    "load_fun_pp_build = lambda : preprocessing.from_state(preprocessing.load_state(model_dir/'preprocessing'/f\"_final\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% deep experiment\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Tested (test) on 1222 instances with mean losses of: random_0:12.3517,random_1:10.4208,random_2:603.8857,random_3:11.1601,random_4:12.3001,random_5:8.3633,random_6:607.3073,random_7:607.7903,random_8:604.4302,random_9:8.9701,random_10:10.3307,random_11:11.0018,random_12:11.4681,random_13:16.1366,random_14:108.9007,random_15:9.9472,random_16:15.5362,random_17:16.5022,random_18:14.4068,random_19:9.6157,random_20:15.0358,random_21:38.3918,random_22:94.0412,random_23:19.2689,random_24:13.7357,random_25:12.0343,random_26:11.434,random_27:11.8311,random_28:16.3709,random_29:9.5527,random_30:24.8279,random_31:11.6792,random_32:611.1023,random_33:599.2791,random_34:19.1817,random_35:12.6499,random_36:11.8897,random_37:9.9449,random_38:11.5293,random_39:604.3834,random_40:15.1597,random_41:8.9979,random_42:13.3156,random_43:606.8097,random_44:9.523,random_45:617.8652,random_46:10.208,random_47:8.9079,random_48:15.0799,random_49:10.3779,random_50:13.4244,random_51:10.5702,random_52:18.436,random_53:25.1514,random_54:23.6359,random_55:13.8,random_56:10.9313,random_57:11.418,random_58:616.0418,random_59:86.3769,random_60:9.7339,random_61:14.1301,random_62:611.8407,random_63:24.8045,random_64:15.7416,random_65:14.6494,random_66:14.1717,random_67:14.455,random_68:20.1299,random_69:11.6369,random_70:13.0141,random_71:605.2509,random_72:11.2496,random_73:11.7799,random_74:8.7745,random_75:9.8537,random_76:10.462,random_77:10.13,random_78:16.9064,random_79:10.9809,random_80:9.3567,random_81:15.7936,random_82:9.5457,random_83:609.4607,random_84:11.1921,random_85:608.5262,random_86:16.7189,random_87:9.8758,random_88:610.0345,random_89:12.5012,random_90:72.1204,random_91:32.9044,random_92:13.921,random_93:9.8578,random_94:15.3387,random_95:15.4931,random_96:16.9682,random_97:24.5389,random_98:10.6261,random_99:9.3494'\n",
      "Testing (test) took 0:00:10.076001'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Tested (test) on 1222 instances with mean losses of: random_0:21.0659,random_1:9.3206,random_2:509.3131,random_3:9.2362,random_4:14.3732,random_5:11.9329,random_6:512.558,random_7:513.0073,random_8:509.846,random_9:8.4368,random_10:11.3829,random_11:12.5301,random_12:15.3684,random_13:11.4454,random_14:520.6075,random_15:15.1142,random_16:9.5999,random_17:11.2001,random_18:11.5912,random_19:29.7032,random_20:15.9234,random_21:525.2234,random_22:8.1222,random_23:10.0348,random_24:24.5472,random_25:45.6595,random_26:9.7043,random_27:38.4541,random_28:22.897,random_29:8.9298,random_30:48.9319,random_31:10.7605,random_32:516.1393,random_33:509.8002,random_34:9.1635,random_35:15.2037,random_36:8.1478,random_37:12.5409,random_38:12.9326,random_39:509.8178,random_40:11.2929,random_41:40.2555,random_42:14.7706,random_43:512.1679,random_44:8.8982,random_45:516.3615,random_46:32.6123,random_47:13.1109,random_48:13.8939,random_49:14.778,random_50:13.7306,random_51:13.0584,random_52:15.0834,random_53:20.5979,random_54:13.1668,random_55:12.9027,random_56:12.7321,random_57:10.9375,random_58:520.7895,random_59:41.7875,random_60:13.4585,random_61:13.6447,random_62:516.8065,random_63:518.5152,random_64:11.6741,random_65:10.4999,random_66:13.7541,random_67:9.2444,random_68:12.2654,random_69:14.5613,random_70:38.1349,random_71:510.648,random_72:16.4375,random_73:15.1375,random_74:514.5138,random_75:11.4454,random_76:10.7881,random_77:10.9867,random_78:12.9331,random_79:8.9827,random_80:8.2858,random_81:45.3572,random_82:10.1446,random_83:514.5929,random_84:14.1979,random_85:513.7074,random_86:10.7155,random_87:9.7821,random_88:515.1848,random_89:14.5336,random_90:10.8987,random_91:12.6425,random_92:10.8329,random_93:20.9394,random_94:13.2106,random_95:509.0416,random_96:11.4657,random_97:10.5719,random_98:9.3322,random_99:12.6068'\n",
      "Testing (test) took 0:00:10.002001'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Tested (test) on 1221 instances with mean losses of: random_0:16.0508,random_1:39.8558,random_2:445.9146,random_3:8.1668,random_4:12.7511,random_5:6.4537,random_6:449.0653,random_7:449.5335,random_8:446.4224,random_9:6.6779,random_10:7.9544,random_11:7.1328,random_12:8.2522,random_13:7.4863,random_14:47.187,random_15:10.736,random_16:7.2277,random_17:7.5645,random_18:10.028,random_19:8.089,random_20:22.3351,random_21:35.4951,random_22:7.0507,random_23:8.4231,random_24:32.5394,random_25:42.9304,random_26:7.863,random_27:40.103,random_28:8.8968,random_29:8.3254,random_30:11.8153,random_31:7.318,random_32:452.5736,random_33:446.5214,random_34:7.9542,random_35:8.9894,random_36:8.2137,random_37:11.1442,random_38:10.3427,random_39:446.4063,random_40:7.2996,random_41:38.9261,random_42:8.3036,random_43:448.7172,random_44:7.3095,random_45:458.787,random_46:7.4382,random_47:9.5073,random_48:7.1138,random_49:13.0714,random_50:16.6307,random_51:14.6122,random_52:9.4075,random_53:9.0627,random_54:8.0276,random_55:7.9511,random_56:7.7649,random_57:7.8534,random_58:457.1386,random_59:33.256,random_60:16.4261,random_61:12.6357,random_62:453.2052,random_63:39.0081,random_64:10.1955,random_65:7.1525,random_66:8.1038,random_67:7.6916,random_68:8.6016,random_69:8.719,random_70:33.2713,random_71:447.2935,random_72:12.5494,random_73:8.2022,random_74:6.3785,random_75:8.8575,random_76:6.8401,random_77:6.5165,random_78:7.5824,random_79:7.7545,random_80:6.7168,random_81:12.7099,random_82:7.308,random_83:451.0706,random_84:7.4206,random_85:450.2103,random_86:7.7035,random_87:8.0835,random_88:41.0761,random_89:10.7128,random_90:8.8591,random_91:13.7554,random_92:8.4465,random_93:11.4699,random_94:6.8275,random_95:17.9772,random_96:7.5257,random_97:9.3649,random_98:22.0084,random_99:13.5555'\n",
      "Testing (test) took 0:00:10.075996'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Tested (test) on 1221 instances with mean losses of: random_0:28.7444,random_1:12.5004,random_2:556.2338,random_3:550.8917,random_4:35.0079,random_5:12.0813,random_6:559.5202,random_7:559.9679,random_8:556.8033,random_9:8.8892,random_10:9.8348,random_11:10.5858,random_12:14.2811,random_13:10.2951,random_14:17.3477,random_15:17.1782,random_16:9.9943,random_17:13.3424,random_18:13.8874,random_19:12.3874,random_20:20.2878,random_21:572.2249,random_22:13.0151,random_23:10.9444,random_24:24.794,random_25:565.3695,random_26:12.4882,random_27:47.3903,random_28:13.864,random_29:10.7054,random_30:15.1194,random_31:10.7188,random_32:563.1902,random_33:556.8462,random_34:14.6971,random_35:15.052,random_36:13.2959,random_37:15.994,random_38:11.6926,random_39:556.7505,random_40:12.8615,random_41:18.1813,random_42:15.7796,random_43:559.1137,random_44:10.9196,random_45:569.6291,random_46:11.6287,random_47:9.7948,random_48:13.1774,random_49:15.6292,random_50:14.3089,random_51:13.3869,random_52:12.2806,random_53:14.3363,random_54:14.5976,random_55:14.4229,random_56:13.2318,random_57:11.2911,random_58:567.8816,random_59:32.4687,random_60:15.1891,random_61:14.3519,random_62:563.7953,random_63:19.8602,random_64:13.0342,random_65:12.7536,random_66:14.6466,random_67:17.0075,random_68:11.7873,random_69:12.8758,random_70:13.3136,random_71:557.4834,random_72:16.1973,random_73:28.0107,random_74:8.6525,random_75:14.2313,random_76:21.9543,random_77:10.8331,random_78:13.6803,random_79:10.5972,random_80:8.608,random_81:15.4907,random_82:11.5718,random_83:561.5904,random_84:13.0796,random_85:560.6917,random_86:14.0712,random_87:12.9932,random_88:562.1514,random_89:21.5988,random_90:9.7963,random_91:29.611,random_92:11.0722,random_93:9.0952,random_94:12.448,random_95:18.9139,random_96:13.3982,random_97:13.7815,random_98:9.5958,random_99:18.8935'\n",
      "Testing (test) took 0:00:10.225998'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Tested (test) on 1221 instances with mean losses of: random_0:27.0847,random_1:24.1005,random_2:668.3282,random_3:27.1243,random_4:35.935,random_5:15.691,random_6:671.7107,random_7:672.1812,random_8:668.9159,random_9:669.9141,random_10:16.0334,random_11:27.3959,random_12:22.462,random_13:23.7874,random_14:45.541,random_15:32.9078,random_16:27.0674,random_17:33.4857,random_18:22.3382,random_19:22.0827,random_20:56.9269,random_21:684.6708,random_22:18.8626,random_23:39.6379,random_24:37.6067,random_25:20.5192,random_26:26.6134,random_27:19.3591,random_28:48.8473,random_29:27.8402,random_30:36.5822,random_31:31.7709,random_32:675.5403,random_33:668.6645,random_34:31.4237,random_35:24.5419,random_36:25.3631,random_37:33.0359,random_38:28.659,random_39:668.8236,random_40:32.2722,random_41:20.9325,random_42:38.318,random_43:671.2834,random_44:22.1972,random_45:682.2374,random_46:19.3294,random_47:20.1259,random_48:24.6905,random_49:28.2502,random_50:57.9322,random_51:30.2735,random_52:32.5728,random_53:24.6954,random_54:672.8607,random_55:26.3026,random_56:30.2381,random_57:21.273,random_58:680.3671,random_59:668.298,random_60:28.0302,random_61:27.8094,random_62:676.183,random_63:48.1153,random_64:27.403,random_65:26.7643,random_66:30.7113,random_67:18.714,random_68:34.4998,random_69:24.0634,random_70:25.1083,random_71:669.5646,random_72:30.6506,random_73:666.9692,random_74:15.1221,random_75:24.8435,random_76:23.7154,random_77:16.2861,random_78:31.8629,random_79:19.8661,random_80:15.8545,random_81:38.8735,random_82:17.8475,random_83:673.862,random_84:32.9104,random_85:672.9353,random_86:21.9162,random_87:25.8825,random_88:674.3817,random_89:247.5174,random_90:26.2309,random_91:32.993,random_92:30.8529,random_93:18.5399,random_94:30.205,random_95:667.5593,random_96:29.4602,random_97:32.2729,random_98:31.601,random_99:28.6695'\n",
      "Testing (test) took 0:00:10.031002'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Tested (test) on 1222 instances with mean losses of: random_0:18.2858,random_1:15.3046,random_2:564.2495,random_3:558.8751,random_4:16.0113,random_5:15.9893,random_6:567.5805,random_7:568.0457,random_8:564.7946,random_9:9.365,random_10:13.6578,random_11:11.4543,random_12:12.4308,random_13:14.752,random_14:44.8902,random_15:13.1488,random_16:15.0986,random_17:17.7622,random_18:13.614,random_19:13.2643,random_20:15.478,random_21:27.8257,random_22:11.4795,random_23:20.0402,random_24:25.2701,random_25:573.5144,random_26:15.2297,random_27:47.7062,random_28:13.3985,random_29:11.9748,random_30:16.9527,random_31:14.4388,random_32:571.2955,random_33:564.8735,random_34:10.6443,random_35:11.3812,random_36:15.1233,random_37:22.5311,random_38:12.6796,random_39:564.7417,random_40:13.3159,random_41:13.6647,random_42:16.828,random_43:567.1732,random_44:12.2218,random_45:577.8441,random_46:11.6994,random_47:8.9199,random_48:12.253,random_49:17.3233,random_50:15.5185,random_51:14.5308,random_52:15.9747,random_53:14.0455,random_54:10.5391,random_55:14.9586,random_56:12.7078,random_57:11.856,random_58:576.0702,random_59:564.4845,random_60:18.3817,random_61:14.574,random_62:571.942,random_63:81.698,random_64:14.2619,random_65:11.7227,random_66:14.6843,random_67:9.3723,random_68:12.8464,random_69:12.0322,random_70:11.9148,random_71:565.5699,random_72:16.3398,random_73:15.399,random_74:9.3301,random_75:13.2148,random_76:11.1923,random_77:10.4101,random_78:12.7599,random_79:13.619,random_80:10.6442,random_81:23.5142,random_82:14.8844,random_83:569.6737,random_84:17.1971,random_85:568.7631,random_86:12.8861,random_87:15.4594,random_88:41.6624,random_89:15.012,random_90:11.5569,random_91:15.3228,random_92:11.6987,random_93:11.2097,random_94:11.4755,random_95:24.066,random_96:14.3643,random_97:14.0343,random_98:12.8468,random_99:11.9809'\n",
      "Testing (test) took 0:00:10.016999'\n"
     ]
    }
   ],
   "source": [
    "deep_scheme = DeepScheme(configs, fixed_hyperparams=fixed_hyperparams,loss_eval=loss_target,device=device,tensorboard=tb,adaptive_lr=False,update=False)\n",
    "deep_scores, deep_preds, _ , _, _,_ = eval.evaluate(deep_models,dataset,deep_scheme,logger_name=\"log\",load_fun=load_fun_cv,load_fun_pp=load_fun_pp_cv)\n",
    "deep_scores_final, deep_preds_final, _ ,_, _,_ = eval.build(deep_models,dataset,deep_scheme,logger_name=\"test_log\",load_fun=load_fun_build,load_fun_pp=load_fun_pp_build)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = []\n",
    "for k,v in ut.flip_dicts(deep_scores).items():\n",
    "    dict1 = {'model_num':k,\"predictor\":\"deep\"}\n",
    "    all_scores.append({**dict1,**v})\n",
    "\n",
    "all_scores_final = []\n",
    "for k,v in ut.flip_dicts(deep_scores_final).items():\n",
    "    dict1 = {'model_num':k,\"predictor\":\"deep\"}\n",
    "    all_scores_final.append({**dict1,**v})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - random_80 - deep - 9.35667500285197 - 8.285823323722752 - 6.716800460456142 - 8.608006351995819 - 15.8545239938272 - 9.764056871816873 - 0.9585970043533256\n",
      "1 - random_5 - deep - 8.363282239573676 - 11.932893898990462 - 6.45366010267553 - 12.08134427146771 - 15.690954698879732 - 10.904179259704094 - 0.9537624890609554\n",
      "2 - random_77 - deep - 10.129997791908377 - 10.986665495093638 - 6.516540925684374 - 10.833087069006067 - 16.286100628139735 - 10.95034995834599 - 0.9535667092473011\n",
      "3 - random_10 - deep - 10.33067052610182 - 11.382877679191127 - 7.954366040366483 - 9.834830644577863 - 16.03341594980351 - 11.107150273958244 - 0.9529018213969005\n",
      "4 - random_82 - deep - 9.545721391218969 - 10.144600848714576 - 7.3080058531327685 - 11.571789434168986 - 17.847468303521083 - 11.283046063033243 - 0.9521559621004065\n",
      "5 - random_79 - deep - 10.980880799738358 - 8.982656263485673 - 7.754505259008509 - 10.597175655942975 - 19.866092637364343 - 11.635720435904203 - 0.9506605001508949\n",
      "6 - random_44 - deep - 9.52298825384162 - 8.898159126993482 - 7.309500273283537 - 10.91958726848568 - 22.1971662101152 - 11.7686421942461 - 0.9500968656847886\n",
      "7 - random_47 - deep - 8.907901188745827 - 13.110862716325176 - 9.507269719504217 - 9.794840686151378 - 20.125946021099544 - 12.28894504136583 - 0.9478906007278085\n",
      "8 - random_57 - deep - 11.417966859820627 - 10.93752107799736 - 7.853435932662427 - 11.291140320845845 - 21.273006048678962 - 12.554163113164224 - 0.9467659838993482\n",
      "9 - random_29 - deep - 9.552723580998796 - 8.929788728392456 - 8.325419445490857 - 10.705373998551364 - 27.84022232511803 - 13.069451673932274 - 0.9445809812597342\n",
      "10 - random_87 - deep - 9.875811473827705 - 9.782050730553859 - 8.083531816312274 - 12.99321495090519 - 25.88254552586561 - 13.322286563830856 - 0.9435088734275878\n",
      "11 - random_36 - deep - 11.889654159545898 - 8.147820039778802 - 8.213650376166017 - 13.295923132275481 - 25.363136249326665 - 13.380935344917832 - 0.9432601822063152\n",
      "12 - random_67 - deep - 14.454988715301363 - 9.244372554379478 - 7.691564077241415 - 17.007515916660317 - 18.7140467438319 - 13.421982786027003 - 0.9430861267857189\n",
      "13 - random_26 - deep - 11.434023209367368 - 9.70428475693673 - 7.863040157071301 - 12.488153873556076 - 26.613444107268112 - 13.619589872117398 - 0.9422482040418677\n",
      "14 - random_11 - deep - 11.001792139780697 - 12.530085692429113 - 7.132792166663817 - 10.585779569853925 - 27.395862474683657 - 13.728619616867492 - 0.9417858800195368\n",
      "15 - random_13 - deep - 16.136629760167796 - 11.44541146478169 - 7.486267835756094 - 10.295060170663369 - 23.78743164115612 - 13.830147224221394 - 0.9413553676679113\n",
      "16 - random_75 - deep - 9.853691032397181 - 11.445362464105633 - 8.857515161687678 - 14.231261677082008 - 24.843453543973105 - 13.845209825905277 - 0.9412914970002029\n",
      "17 - random_16 - deep - 15.536192911151193 - 9.59987766691979 - 7.227671823181352 - 9.994330274970878 - 27.06740295916283 - 13.884663785670856 - 0.9411241984944831\n",
      "18 - random_93 - deep - 9.857819746051794 - 20.939418599180232 - 11.469891884793618 - 9.095197556157467 - 18.539932850049617 - 13.98091660610008 - 0.9407160530732173\n",
      "19 - random_86 - deep - 16.718913182884503 - 10.715471840529123 - 7.703454439411585 - 14.071203446212506 - 21.91623016891667 - 14.224888249551285 - 0.9396815284873478\n",
      "20 - random_65 - deep - 14.649402707376183 - 10.49988449341732 - 7.15253514173478 - 12.753595994399474 - 26.76430199597333 - 14.363358477775192 - 0.9390943665799959\n",
      "21 - random_12 - deep - 11.468095334579044 - 15.36842699487939 - 8.252218548730198 - 14.28105270657551 - 22.461967538557122 - 14.366041573888802 - 0.9390829893196859\n",
      "22 - random_69 - deep - 11.636907154526531 - 14.561334305621209 - 8.718981585865818 - 12.875821527645883 - 24.06340261954543 - 14.370872671754105 - 0.9390625037851882\n",
      "23 - random_31 - deep - 11.679184419628836 - 10.76052265003348 - 7.318017936162925 - 10.718760060834455 - 31.770921804785825 - 14.44842343065235 - 0.938733661606654\n",
      "24 - random_18 - deep - 14.40680645725731 - 11.591185887784693 - 10.027969419712125 - 13.88743562096948 - 22.338173798319747 - 14.449839021241074 - 0.9387276590104225\n",
      "25 - random_76 - deep - 10.462007857383565 - 10.78806992794605 - 6.840141374492723 - 21.95431542689443 - 23.71539915711249 - 14.75063508791374 - 0.937452179114875\n",
      "26 - random_48 - deep - 15.079861428655697 - 13.893923826186434 - 7.113819763174221 - 13.177420286933092 - 24.69054599124618 - 14.791014710955526 - 0.9372809554750539\n",
      "27 - random_56 - deep - 10.931306915470504 - 12.732127769879156 - 7.764929458218261 - 13.231821710803683 - 30.238090083210512 - 14.978624154431387 - 0.9364854262116052\n",
      "28 - random_92 - deep - 13.921040107302503 - 10.83285432556452 - 8.446470366844283 - 11.072180727092483 - 30.852914202515947 - 15.02422504349736 - 0.9362920625886493\n",
      "29 - random_38 - deep - 11.529250602285133 - 12.932572183749485 - 10.342650770457624 - 11.69257495268557 - 28.658987877117035 - 15.03029024646719 - 0.9362663440194672\n",
      "30 - random_55 - deep - 13.800012263065469 - 12.902726078969952 - 7.951137500547367 - 14.422855849933859 - 26.30258644300831 - 15.075298715301956 - 0.9360754924642485\n",
      "31 - random_35 - deep - 12.649850740760517 - 15.203695824805726 - 8.989437774983124 - 15.051950790958264 - 24.541922770397864 - 15.286926026637543 - 0.935178119091175\n",
      "32 - random_94 - deep - 15.338658040947298 - 13.21058543848328 - 6.827452146450483 - 12.447995132349437 - 30.204974585337094 - 15.605497025587841 - 0.9338272673032498\n",
      "33 - random_64 - deep - 15.741566656459177 - 11.674125251520286 - 10.195453817194158 - 13.034237646060728 - 27.40299413010881 - 15.609052508285721 - 0.9338121908205342\n",
      "34 - random_84 - deep - 11.192078306319866 - 14.197927281431207 - 7.420606145304212 - 13.079616214094544 - 32.91040286111792 - 15.759122479227138 - 0.933175841971367\n",
      "35 - random_96 - deep - 16.968235543433657 - 11.465680585945492 - 7.5256727001493235 - 13.398236512161493 - 29.46022990646175 - 15.763104175069634 - 0.9331589581967437\n",
      "36 - random_40 - deep - 15.159725609075418 - 11.292915824977543 - 7.299582702424271 - 12.861496915395488 - 32.27218217951269 - 15.77634520698399 - 0.9331028116181336\n",
      "37 - random_46 - deep - 10.208048437308953 - 32.61233824715872 - 7.438219751518931 - 11.628706392160113 - 19.329425896122064 - 16.245039982247764 - 0.931115382827558\n",
      "38 - random_66 - deep - 14.171684880108373 - 13.754087944467797 - 8.103796359069225 - 14.646562896333299 - 30.711338444677754 - 16.27673621484644 - 0.9309809797820282\n",
      "39 - random_19 - deep - 9.615690685528195 - 29.70321013455305 - 8.08904249642737 - 12.387385853287824 - 22.08273310352797 - 16.376687857173916 - 0.9305571500700047\n",
      "40 - random_51 - deep - 10.570224875123918 - 13.058366315064376 - 14.61215201837913 - 13.386902080414043 - 30.273465335417928 - 16.378726945899267 - 0.9305485036249126\n",
      "41 - random_17 - deep - 16.50222815936599 - 11.200119385352501 - 7.5645210951884 - 13.342448192771393 - 33.48573398980618 - 16.418169374779023 - 0.9303812540141574\n",
      "42 - random_49 - deep - 10.377915966530283 - 14.778031381959806 - 13.071431190434486 - 15.62921530452544 - 28.2502326262378 - 16.420106418826823 - 0.9303730402739743\n",
      "43 - random_34 - deep - 19.181682446194163 - 9.163497075316558 - 7.954170517605118 - 14.69710676812618 - 31.423702619585416 - 16.48327483449336 - 0.9301051842308169\n",
      "44 - random_61 - deep - 14.130083480560252 - 13.644701883562654 - 12.635681103136967 - 14.351879069494673 - 27.809366158244064 - 16.51348205886225 - 0.9299770950978404\n",
      "45 - random_37 - deep - 9.944884663127644 - 12.540853509731262 - 11.144245150829319 - 15.993997383273888 - 33.0358691422496 - 16.530238588463614 - 0.9299060415868617\n",
      "46 - random_60 - deep - 9.733871631653532 - 13.458541759297422 - 16.426127278345906 - 15.189095729887242 - 28.030186258701885 - 16.565936550907917 - 0.9297546698155703\n",
      "47 - random_78 - deep - 16.90639553132502 - 12.933087179196447 - 7.582394868599207 - 13.68034257056965 - 31.8628598774778 - 16.592468255971145 - 0.9296421661622577\n",
      "48 - random_99 - deep - 9.349355251231092 - 12.606784400690207 - 13.555476473746586 - 18.89352816699666 - 28.669504833455754 - 16.61308361834116 - 0.9295547498588026\n",
      "49 - random_98 - deep - 10.626133879741163 - 9.332177869231728 - 22.008431144858072 - 9.59582238837796 - 31.6009893425169 - 16.630531878324437 - 0.929480763170522\n",
      "50 - random_15 - deep - 9.947233520045803 - 15.114234921194722 - 10.735981396824293 - 17.178219596252003 - 32.90775129738448 - 17.17516220526167 - 0.9271713412415781\n",
      "51 - random_72 - deep - 11.249637002835687 - 16.437458047695127 - 12.549408613512695 - 16.197317018653408 - 30.65058709753038 - 17.415711110303977 - 0.9261513302564899\n",
      "52 - random_68 - deep - 20.12994358387399 - 12.265366342375986 - 8.601593823632093 - 11.787320688061788 - 34.49983888293367 - 17.456400223776413 - 0.9259787942696472\n",
      "53 - random_52 - deep - 18.436032955463116 - 15.08344180143016 - 9.407539641046798 - 12.280595329528358 - 32.57277027336327 - 17.55581580292709 - 0.9255572376518577\n",
      "54 - random_23 - deep - 19.26891012550766 - 10.034756993701142 - 8.42310145133641 - 10.944351558700925 - 39.637948869975446 - 17.660828044028896 - 0.9251119492416955\n",
      "55 - random_42 - deep - 13.315588565582924 - 14.770604459821886 - 8.303626344791697 - 15.779567514649187 - 38.31796463037689 - 18.096141933776416 - 0.9232660670106961\n",
      "56 - random_97 - deep - 24.538915904180897 - 10.571947153188203 - 9.364918678339928 - 13.781495291907508 - 32.27294563134121 - 18.10586462052603 - 0.9232248394387501\n",
      "57 - random_53 - deep - 25.151440440925562 - 20.597928710537925 - 9.062706146740112 - 14.336263321541452 - 24.695426687854514 - 18.770097822241674 - 0.9204082597403678\n",
      "58 - random_1 - deep - 10.42079413971222 - 9.320572140173905 - 39.85576488110591 - 12.500365526631267 - 24.100519917618534 - 19.236535228776788 - 0.9184304029779844\n",
      "59 - random_0 - deep - 12.351709666306766 - 21.065932760847375 - 16.050840541830226 - 28.744383124227312 - 27.084733941143014 - 21.05809527544463 - 0.9107063551080838\n",
      "60 - random_4 - deep - 12.300096499354087 - 14.373173228262294 - 12.751088063507956 - 35.00793709313645 - 35.93496138457877 - 22.07058987808808 - 0.9064130259954147\n",
      "61 - random_28 - deep - 16.370855161289146 - 22.897015462335315 - 8.896804178473795 - 13.86403295273277 - 48.84732393394427 - 22.174374068820768 - 0.9059729449457538\n",
      "62 - random_50 - deep - 13.424381921022107 - 13.730584115716713 - 16.630698773433302 - 14.308945983086913 - 57.93223357766891 - 23.202214949254945 - 0.9016145422800622\n",
      "63 - random_91 - deep - 32.90436477473832 - 12.642543864523331 - 13.755440533112347 - 29.610969808626916 - 32.99302929605645 - 24.38074310216457 - 0.896617173192092\n",
      "64 - random_70 - deep - 13.014082232785888 - 38.13488360746793 - 33.271337688799086 - 13.313619193046627 - 25.108273235917775 - 24.56876825570979 - 0.8958198811734263\n",
      "65 - random_41 - deep - 8.997948544310274 - 40.255459696493446 - 38.92610016931382 - 18.18127491901782 - 20.93248532035134 - 25.458381395447415 - 0.892047612183669\n",
      "66 - random_90 - deep - 72.12042071191067 - 10.898658430127583 - 8.859052091031462 - 9.79632442276757 - 26.230885725154142 - 25.586284466788534 - 0.8915052586952109\n",
      "67 - random_81 - deep - 15.7935793856357 - 45.35724001820466 - 12.70990668975555 - 15.490688867397136 - 38.87353833204015 - 25.64660506099066 - 0.8912494784051183\n",
      "68 - random_20 - deep - 15.035809351457512 - 15.923429989385527 - 22.335115399934736 - 20.28782128428554 - 56.926887680916 - 26.09833433864849 - 0.8893339892225094\n",
      "69 - random_24 - deep - 13.735732617042654 - 24.54719817072592 - 32.539442181099055 - 24.79397809671438 - 37.60670615687515 - 26.64215479493159 - 0.8870280014266936\n",
      "70 - random_30 - deep - 24.827889980934255 - 48.93192047768451 - 11.815314663911236 - 15.119366940272625 - 36.58216141209458 - 27.458416999856954 - 0.8835667659012617\n",
      "71 - random_22 - deep - 94.0411822011545 - 8.122244499318924 - 7.050653488884003 - 13.01507129520788 - 18.86260481685229 - 28.22583953881743 - 0.8803126275388099\n",
      "72 - random_27 - deep - 11.831145174959874 - 38.454081705471104 - 40.10302887462769 - 47.39033277809962 - 19.359050818684647 - 31.425469576897438 - 0.8667450838142366\n",
      "73 - random_89 - deep - 12.501188296147923 - 14.533573157658553 - 10.712831489381783 - 21.598839316184552 - 247.51741835135695 - 61.35709731769617 - 0.7398245763531232\n",
      "74 - random_74 - deep - 8.774464610360454 - 514.5137824575171 - 6.378507316551865 - 8.652528414269337 - 15.122101706427497 - 110.73771437453959 - 0.5304335926795358\n",
      "75 - random_3 - deep - 11.160089843987247 - 9.23615572222321 - 8.166752889447286 - 550.8917464147719 - 27.124250406519884 - 121.27940849209266 - 0.4857331447625003\n",
      "76 - random_63 - deep - 24.80448056359923 - 518.5152334160189 - 39.008080066177904 - 19.860228914984127 - 48.115320669638145 - 130.10704091698548 - 0.448300914322176\n",
      "77 - random_25 - deep - 12.034310921514484 - 45.659476448783316 - 42.93041640398055 - 565.3695050488722 - 20.51916150632791 - 137.26705495391565 - 0.41793996560046043\n",
      "78 - random_9 - deep - 8.970106681318018 - 8.43681700686191 - 6.677912561352579 - 8.889165362796268 - 669.9140729099682 - 140.53442711668472 - 0.40408517171640757\n",
      "79 - random_73 - deep - 11.779905051529505 - 15.137531865833239 - 8.202169985384554 - 28.01074727102931 - 666.9692107004577 - 145.97649932509938 - 0.381008893596373\n",
      "80 - random_54 - deep - 23.63588054363544 - 13.16677041841997 - 8.02760528639435 - 14.597599453070826 - 672.860661899526 - 146.4157667152887 - 0.37914624714905165\n",
      "81 - random_14 - deep - 108.90070903047602 - 520.6075016902043 - 47.18695702580133 - 17.34769628323267 - 45.540950147955265 - 147.9714012795998 - 0.3725498157743127\n",
      "82 - random_59 - deep - 86.37688812842735 - 41.78747891053045 - 33.256041636923904 - 32.46872290346488 - 668.2979835429023 - 172.40193733197336 - 0.26895584954681684\n",
      "83 - random_95 - deep - 15.493063097889802 - 509.0416056981063 - 17.9772229206357 - 18.91385168017763 - 667.5593415681307 - 245.80241153980515 - -0.042287679038367276\n",
      "84 - random_21 - deep - 38.39181450658071 - 525.223448308517 - 35.49507341064653 - 572.2249045532127 - 684.6707978119721 - 371.1719298180405 - -0.5738980217107754\n",
      "85 - random_88 - deep - 610.0345145817084 - 515.1847636071437 - 41.07610371657613 - 562.1513830086523 - 674.3816928972874 - 480.59256096991146 - -1.0378795382783133\n",
      "86 - random_33 - deep - 599.2790810542879 - 509.8001558144627 - 446.52143603675586 - 556.8461625382707 - 668.6645477944763 - 556.2217234299386 - -1.358573479864448\n",
      "87 - random_2 - deep - 603.885742936704 - 509.31309107204504 - 445.91457600011677 - 556.233758437448 - 668.328187997179 - 556.7350240618762 - -1.3607500529947099\n",
      "88 - random_39 - deep - 604.3833718557405 - 509.8178211468136 - 446.40633872842125 - 556.7504983538004 - 668.8236327175231 - 557.2362895805823 - -1.362875593060819\n",
      "89 - random_8 - deep - 604.4301979077428 - 509.8460132954749 - 446.4224228417649 - 556.8033151462564 - 668.915894679414 - 557.2835165602726 - -1.3630758518733388\n",
      "90 - random_71 - deep - 605.2509016920424 - 510.6479761900956 - 447.29350746898353 - 557.4834060231551 - 669.564556887265 - 558.0480402647704 - -1.3663176981698788\n",
      "91 - random_43 - deep - 606.8097021224651 - 512.16791380837 - 448.7172339437049 - 559.1137257044087 - 671.2834083750831 - 559.6183522771216 - -1.372976367026976\n",
      "92 - random_6 - deep - 607.3073171562532 - 512.5579690137198 - 449.0653329610239 - 559.5201507243438 - 671.7107109425988 - 560.0322601582429 - -1.374731480339523\n",
      "93 - random_7 - deep - 607.790277659015 - 513.0072701761649 - 449.5335260214716 - 559.9678828233973 - 672.1811779375256 - 560.4959919579811 - -1.3766978643884733\n",
      "94 - random_85 - deep - 608.5262012138304 - 513.7074003578598 - 450.2103412450092 - 560.6917255473469 - 672.935266768122 - 561.2141561590064 - -1.379743130273238\n",
      "95 - random_83 - deep - 609.4607305698426 - 514.5929121900892 - 451.0706498679521 - 561.5904272330969 - 673.8619531729884 - 562.115301770133 - -1.3835643009509906\n",
      "96 - random_32 - deep - 611.1023203448656 - 516.1392868216807 - 452.5736128223616 - 563.1901695757592 - 675.540324280635 - 563.7091190346403 - -1.3903226402489168\n",
      "97 - random_62 - deep - 611.8407029871464 - 516.8064551220783 - 453.2051820497255 - 563.7953229774519 - 676.1829888221184 - 564.3661128033094 - -1.393108522234367\n",
      "98 - random_58 - deep - 616.0417689746413 - 520.7895382945159 - 457.1386498803491 - 567.8816313247618 - 680.3670535200839 - 568.4437106396988 - -1.4103989546524955\n",
      "99 - random_45 - deep - 617.8651608531096 - 516.3615196215541 - 458.78701054903445 - 569.6291013275571 - 682.2373902666872 - 568.9754325448326 - -1.412653640385333\n"
     ]
    }
   ],
   "source": [
    "scores_df_sorted = pd.DataFrame(all_scores).sort_values(by='MSE')\n",
    "for i,(index,row) in enumerate(scores_df_sorted.iterrows()):\n",
    "    s = f\"{i} - \" + \" - \".join([f\"{i}\" for i in row.tolist()])\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - random_47 - deep - 8.919891087940544 - 0.9624491752872761\n",
      "1 - random_74 - deep - 9.330148315122123 - 0.9607220805197345\n",
      "2 - random_9 - deep - 9.365010362338474 - 0.96057531879235\n",
      "3 - random_67 - deep - 9.372335621272377 - 0.9605444810263403\n",
      "4 - random_77 - deep - 10.410104090572132 - 0.9561756987734098\n",
      "5 - random_54 - deep - 10.539051099389242 - 0.9556328595753093\n",
      "6 - random_80 - deep - 10.644197250402254 - 0.9551902168740721\n",
      "7 - random_34 - deep - 10.644277164193543 - 0.9551898804541833\n",
      "8 - random_76 - deep - 11.192283060293786 - 0.9528828933908748\n",
      "9 - random_93 - deep - 11.209716252752152 - 0.9528095033967893\n",
      "10 - random_35 - deep - 11.381222868815062 - 0.952087497397685\n",
      "11 - random_11 - deep - 11.45432152691222 - 0.9517797677550379\n",
      "12 - random_94 - deep - 11.475549558651355 - 0.9516904023030416\n",
      "13 - random_22 - deep - 11.479450694018842 - 0.9516739793614473\n",
      "14 - random_90 - deep - 11.556877536235833 - 0.9513480289937257\n",
      "15 - random_92 - deep - 11.698672771713959 - 0.9507511015222976\n",
      "16 - random_46 - deep - 11.699411074214877 - 0.9507479934274202\n",
      "17 - random_65 - deep - 11.72270542205018 - 0.9506499292286832\n",
      "18 - random_57 - deep - 11.855992150852744 - 0.9500888207419929\n",
      "19 - random_70 - deep - 11.914767043629078 - 0.9498413910733574\n",
      "20 - random_29 - deep - 11.974758525688715 - 0.9495888398252683\n",
      "21 - random_99 - deep - 11.980928999704734 - 0.9495628634556148\n",
      "22 - random_69 - deep - 12.032244609668727 - 0.9493468357647173\n",
      "23 - random_44 - deep - 12.221778662509552 - 0.9485489381306375\n",
      "24 - random_48 - deep - 12.252952140302533 - 0.9484177044879002\n",
      "25 - random_12 - deep - 12.4307895743834 - 0.9476690471053498\n",
      "26 - random_38 - deep - 12.67964024039086 - 0.9466214392762\n",
      "27 - random_56 - deep - 12.707791979695429 - 0.946502926503167\n",
      "28 - random_78 - deep - 12.759905873240264 - 0.9462835382099348\n",
      "29 - random_68 - deep - 12.846443527830138 - 0.9459192332799176\n",
      "30 - random_98 - deep - 12.846837635551434 - 0.9459175741710995\n",
      "31 - random_86 - deep - 12.886116321455667 - 0.9457522193439166\n",
      "32 - random_15 - deep - 13.14880941295322 - 0.944646337878009\n",
      "33 - random_75 - deep - 13.214845911391734 - 0.9443683384099595\n",
      "34 - random_19 - deep - 13.26432621524267 - 0.9441600369634149\n",
      "35 - random_40 - deep - 13.31593925312266 - 0.9439427571649044\n",
      "36 - random_28 - deep - 13.398498398857365 - 0.943595200902238\n",
      "37 - random_18 - deep - 13.613996618646041 - 0.9426879997046663\n",
      "38 - random_79 - deep - 13.618987223178559 - 0.9426669903319999\n",
      "39 - random_10 - deep - 13.657757048032929 - 0.9425037777004893\n",
      "40 - random_41 - deep - 13.66473035464276 - 0.942474421578134\n",
      "41 - random_97 - deep - 14.034276809509624 - 0.9409187104138268\n",
      "42 - random_53 - deep - 14.045500791084745 - 0.9408714598632818\n",
      "43 - random_64 - deep - 14.261918079467403 - 0.9399603895844253\n",
      "44 - random_96 - deep - 14.364270466293332 - 0.9395295080300725\n",
      "45 - random_31 - deep - 14.438795708690698 - 0.9392157727740759\n",
      "46 - random_51 - deep - 14.530783810029632 - 0.9388285226344745\n",
      "47 - random_61 - deep - 14.573957845826941 - 0.9386467692213013\n",
      "48 - random_66 - deep - 14.684299720726186 - 0.938182253638996\n",
      "49 - random_13 - deep - 14.751956585088491 - 0.9378974327786027\n",
      "50 - random_82 - deep - 14.88444148715171 - 0.937339699810183\n",
      "51 - random_55 - deep - 14.958608742531089 - 0.9370274716026054\n",
      "52 - random_89 - deep - 15.012017687319478 - 0.9368026314219273\n",
      "53 - random_16 - deep - 15.098621903153362 - 0.9364380462833759\n",
      "54 - random_36 - deep - 15.12331817672224 - 0.9363340802785559\n",
      "55 - random_26 - deep - 15.229659065169454 - 0.9358864079894542\n",
      "56 - random_1 - deep - 15.304631965043257 - 0.9355707881916775\n",
      "57 - random_91 - deep - 15.322766305298963 - 0.9354944465160326\n",
      "58 - random_73 - deep - 15.399049761121377 - 0.9351733095593335\n",
      "59 - random_87 - deep - 15.459350831885951 - 0.9349194550093233\n",
      "60 - random_20 - deep - 15.478028052851986 - 0.9348408279225459\n",
      "61 - random_50 - deep - 15.518470578524354 - 0.9346705735800331\n",
      "62 - random_52 - deep - 15.974750024322141 - 0.9327497351616879\n",
      "63 - random_5 - deep - 15.98929776867421 - 0.932688492284085\n",
      "64 - random_4 - deep - 16.011334207479056 - 0.932595723612058\n",
      "65 - random_72 - deep - 16.339825005934628 - 0.9312128479389206\n",
      "66 - random_42 - deep - 16.828038221863622 - 0.929157575210471\n",
      "67 - random_30 - deep - 16.952672978725477 - 0.9286328896664557\n",
      "68 - random_84 - deep - 17.197131451434867 - 0.9276037720272661\n",
      "69 - random_49 - deep - 17.323268007429913 - 0.9270727642316171\n",
      "70 - random_17 - deep - 17.762154783799485 - 0.9252251452141083\n",
      "71 - random_0 - deep - 18.285768013415662 - 0.9230208460350339\n",
      "72 - random_60 - deep - 18.381726048242175 - 0.9226168833285363\n",
      "73 - random_23 - deep - 20.040208122053112 - 0.9156350301838195\n",
      "74 - random_37 - deep - 22.531130447401214 - 0.9051487824606141\n",
      "75 - random_81 - deep - 23.514165945669998 - 0.901010414254327\n",
      "76 - random_95 - deep - 24.065972608751725 - 0.8986874267787615\n",
      "77 - random_24 - deep - 25.270077596524683 - 0.8936184035265933\n",
      "78 - random_21 - deep - 27.825660718149788 - 0.8828599477458179\n",
      "79 - random_88 - deep - 41.66239446381989 - 0.8246102720089977\n",
      "80 - random_14 - deep - 44.890167361308094 - 0.811022041716598\n",
      "81 - random_27 - deep - 47.70616246092445 - 0.7991673074675952\n",
      "82 - random_63 - deep - 81.69801260478886 - 0.6560689227643204\n",
      "83 - random_3 - deep - 558.8751375632 - -1.3527442342102098\n",
      "84 - random_2 - deep - 564.2494830442619 - -1.3753690738091842\n",
      "85 - random_59 - deep - 564.4844619474037 - -1.3763582844975293\n",
      "86 - random_39 - deep - 564.7416758833091 - -1.3774410998957922\n",
      "87 - random_8 - deep - 564.7945931975814 - -1.377663870417643\n",
      "88 - random_33 - deep - 564.8735081467712 - -1.377996085395945\n",
      "89 - random_71 - deep - 565.5699345408448 - -1.3809278908621043\n",
      "90 - random_43 - deep - 567.1732089573608 - -1.3876773316330104\n",
      "91 - random_6 - deep - 567.5805415307693 - -1.3893921142366374\n",
      "92 - random_7 - deep - 568.0456626906858 - -1.3913501743714543\n",
      "93 - random_85 - deep - 568.7630478060235 - -1.3943702115503815\n",
      "94 - random_83 - deep - 569.673716456463 - -1.3982039308779926\n",
      "95 - random_32 - deep - 571.2955347342802 - -1.4050314373201442\n",
      "96 - random_62 - deep - 571.9420005199768 - -1.4077529193609157\n",
      "97 - random_25 - deep - 573.5143697291247 - -1.4143722558499254\n",
      "98 - random_58 - deep - 576.0701980803275 - -1.4251317439945264\n",
      "99 - random_45 - deep - 577.8440855923603 - -1.4325994292350432\n"
     ]
    }
   ],
   "source": [
    "scores_df_sorted_final = pd.DataFrame(all_scores_final).sort_values(by='MSE')\n",
    "\n",
    "for i,(index,row) in enumerate(scores_df_sorted_final.iterrows()):\n",
    "    s = f\"{i} - \" + \" - \".join([f\"{i}\" for i in row.tolist()])\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% lwr part\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2320eb619a1f4d17a0e7991116f3f83f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_0'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.5466,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0045,lwr_k=40:0.0993,lwr_k=50:0.6652,lwr_k=100:3.4246,lwr_k=200:5.0124,lwr_k=300:5.5289,lwr_k=400:5.7965,lwr_k=500:6.0979,lwr_k=600:6.27,lwr_k=700:6.4826,lwr_k=800:6.64,lwr_k=900:6.7334,lwr_k=1000:6.8203'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.1349,lwr_k=10:28.6436,lwr_k=20:82.2975,lwr_k=30:821.9476,lwr_k=40:430.3186,lwr_k=50:214.9094,lwr_k=100:31.6357,lwr_k=200:11.0418,lwr_k=300:10.525,lwr_k=400:10.3349,lwr_k=500:10.1522,lwr_k=600:10.2411,lwr_k=700:10.017,lwr_k=800:10.0641,lwr_k=900:10.0018,lwr_k=1000:10.1132'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:26.8615,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0354,lwr_k=40:0.2322,lwr_k=50:1.0036,lwr_k=100:5.4825,lwr_k=200:9.4912,lwr_k=300:11.2929,lwr_k=400:12.3237,lwr_k=500:12.9424,lwr_k=600:13.5536,lwr_k=700:14.127,lwr_k=800:14.8097,lwr_k=900:15.2167,lwr_k=1000:15.9184'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:18.1802,lwr_k=10:29.617,lwr_k=20:132.6816,lwr_k=30:202.586,lwr_k=40:2275.3901,lwr_k=50:693.7789,lwr_k=100:44.6778,lwr_k=200:15.5787,lwr_k=300:10.5293,lwr_k=400:10.0342,lwr_k=500:9.6604,lwr_k=600:9.7451,lwr_k=700:9.7926,lwr_k=800:10.0157,lwr_k=900:10.0841,lwr_k=1000:10.2927'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:21.9808,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0431,lwr_k=40:0.1697,lwr_k=50:1.125,lwr_k=100:5.1692,lwr_k=200:8.6932,lwr_k=300:10.1943,lwr_k=400:11.1318,lwr_k=500:11.8696,lwr_k=600:12.5293,lwr_k=700:12.9979,lwr_k=800:13.5023,lwr_k=900:13.8243,lwr_k=1000:14.1053'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:14.8949,lwr_k=10:30.021,lwr_k=20:95.1993,lwr_k=30:160.4342,lwr_k=40:447.4709,lwr_k=50:215.6169,lwr_k=100:83.4038,lwr_k=200:33.065,lwr_k=300:10.2856,lwr_k=400:9.785,lwr_k=500:9.6446,lwr_k=600:9.4701,lwr_k=700:9.4007,lwr_k=800:9.3266,lwr_k=900:9.5442,lwr_k=1000:9.5547'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:27.7862,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0237,lwr_k=40:0.2396,lwr_k=50:1.2286,lwr_k=100:6.463,lwr_k=200:10.0542,lwr_k=300:11.5044,lwr_k=400:12.2523,lwr_k=500:12.9321,lwr_k=600:13.7781,lwr_k=700:14.2913,lwr_k=800:14.8207,lwr_k=900:15.3101,lwr_k=1000:15.6208'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:22.2022,lwr_k=10:43.4174,lwr_k=20:130.0661,lwr_k=30:214.0463,lwr_k=40:764.7403,lwr_k=50:253.7179,lwr_k=100:22.7992,lwr_k=200:12.1094,lwr_k=300:11.5586,lwr_k=400:11.5471,lwr_k=500:11.3917,lwr_k=600:11.5303,lwr_k=700:11.6772,lwr_k=800:11.991,lwr_k=900:12.2148,lwr_k=1000:12.2942'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.5874,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0083,lwr_k=40:0.1203,lwr_k=50:0.7213,lwr_k=100:2.9054,lwr_k=200:4.1782,lwr_k=300:4.7014,lwr_k=400:4.9188,lwr_k=500:5.0873,lwr_k=600:5.2205,lwr_k=700:5.3145,lwr_k=800:5.3907,lwr_k=900:5.4704,lwr_k=1000:5.5654'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:26.1711,lwr_k=10:29.8931,lwr_k=20:94.4627,lwr_k=30:294.8757,lwr_k=40:208.106,lwr_k=50:2209.9291,lwr_k=100:212.1618,lwr_k=200:33.3781,lwr_k=300:22.8861,lwr_k=400:22.232,lwr_k=500:28.2588,lwr_k=600:22.3727,lwr_k=700:22.5407,lwr_k=800:22.6195,lwr_k=900:22.8588,lwr_k=1000:23.231'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:17.0477,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0106,lwr_k=40:0.1834,lwr_k=50:0.9972,lwr_k=100:4.5373,lwr_k=200:7.1362,lwr_k=300:8.388,lwr_k=400:9.1285,lwr_k=500:9.5863,lwr_k=600:9.9733,lwr_k=700:10.3004,lwr_k=800:10.5841,lwr_k=900:10.9373,lwr_k=1000:11.1967'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:16.9309,lwr_k=10:33.3712,lwr_k=20:121.8174,lwr_k=30:248.3298,lwr_k=40:795.5212,lwr_k=50:499.6831,lwr_k=100:18.7997,lwr_k=200:13.8244,lwr_k=300:13.432,lwr_k=400:13.7189,lwr_k=500:13.8519,lwr_k=600:14.2703,lwr_k=700:13.8745,lwr_k=800:13.9632,lwr_k=900:14.3823,lwr_k=1000:14.4203'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_1'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.2834,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.5239,lwr_k=40:1.1526,lwr_k=50:1.5352,lwr_k=100:2.434,lwr_k=200:2.8117,lwr_k=300:3.0473,lwr_k=400:3.1741,lwr_k=500:3.2596,lwr_k=600:3.3196,lwr_k=700:3.3782,lwr_k=800:3.4372,lwr_k=900:3.4774,lwr_k=1000:3.5032'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.0898,lwr_k=10:49.3281,lwr_k=20:40.0128,lwr_k=30:121.4726,lwr_k=40:26.2092,lwr_k=50:14.2794,lwr_k=100:9.1943,lwr_k=200:8.3467,lwr_k=300:8.0147,lwr_k=400:7.944,lwr_k=500:8.042,lwr_k=600:8.0853,lwr_k=700:8.2009,lwr_k=800:8.1785,lwr_k=900:8.1806,lwr_k=1000:8.1802'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.9705,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.4645,lwr_k=40:1.4695,lwr_k=50:2.1585,lwr_k=100:3.4061,lwr_k=200:4.2153,lwr_k=300:4.4146,lwr_k=400:4.6388,lwr_k=500:4.758,lwr_k=600:4.8542,lwr_k=700:4.923,lwr_k=800:5.0496,lwr_k=900:5.0979,lwr_k=1000:5.1226'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.6676,lwr_k=10:93.735,lwr_k=20:110.7102,lwr_k=30:1125.6642,lwr_k=40:89.7183,lwr_k=50:41.5904,lwr_k=100:8.7522,lwr_k=200:7.5882,lwr_k=300:7.0695,lwr_k=400:7.0603,lwr_k=500:6.9816,lwr_k=600:6.9218,lwr_k=700:6.9031,lwr_k=800:6.8695,lwr_k=900:6.9227,lwr_k=1000:6.929'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:45.0528,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:3.367,lwr_k=40:4.9755,lwr_k=50:6.9523,lwr_k=100:13.1743,lwr_k=200:17.5137,lwr_k=300:20.1976,lwr_k=400:21.5612,lwr_k=500:22.066,lwr_k=600:23.4499,lwr_k=700:24.1618,lwr_k=800:24.8153,lwr_k=900:26.0063,lwr_k=1000:26.748'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:32.5112,lwr_k=10:144.9847,lwr_k=20:119.4527,lwr_k=30:215.2455,lwr_k=40:53.2074,lwr_k=50:34.5612,lwr_k=100:22.9288,lwr_k=200:18.6042,lwr_k=300:17.7911,lwr_k=400:17.5148,lwr_k=500:17.279,lwr_k=600:17.5966,lwr_k=700:17.7425,lwr_k=800:17.7666,lwr_k=900:18.6652,lwr_k=1000:19.1044'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:14.1117,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.0772,lwr_k=40:2.6437,lwr_k=50:3.5382,lwr_k=100:6.617,lwr_k=200:9.2472,lwr_k=300:10.2579,lwr_k=400:10.6015,lwr_k=500:10.8615,lwr_k=600:11.1739,lwr_k=700:11.3846,lwr_k=800:11.5941,lwr_k=900:11.783,lwr_k=1000:11.8989'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:12.3176,lwr_k=10:267.4381,lwr_k=20:192.5119,lwr_k=30:440.091,lwr_k=40:42.9085,lwr_k=50:56.9096,lwr_k=100:12.7834,lwr_k=200:10.9839,lwr_k=300:11.9213,lwr_k=400:11.5132,lwr_k=500:11.054,lwr_k=600:12.1514,lwr_k=700:11.716,lwr_k=800:11.9187,lwr_k=900:10.6052,lwr_k=1000:10.7455'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.825,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.8464,lwr_k=40:1.6029,lwr_k=50:2.3154,lwr_k=100:3.6989,lwr_k=200:4.4716,lwr_k=300:4.809,lwr_k=400:5.0098,lwr_k=500:5.1074,lwr_k=600:5.1476,lwr_k=700:5.1967,lwr_k=800:5.2678,lwr_k=900:5.3096,lwr_k=1000:5.3291'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:24.9011,lwr_k=10:333.1684,lwr_k=20:70.6519,lwr_k=30:288.3707,lwr_k=40:37.7418,lwr_k=50:33.6719,lwr_k=100:27.3526,lwr_k=200:26.5405,lwr_k=300:26.0195,lwr_k=400:25.578,lwr_k=500:25.5268,lwr_k=600:25.515,lwr_k=700:25.3623,lwr_k=800:25.5886,lwr_k=900:25.2798,lwr_k=1000:25.2495'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:9.895,lwr_k=10:0.0,lwr_k=20:0.0003,lwr_k=30:0.8758,lwr_k=40:2.7171,lwr_k=50:3.1752,lwr_k=100:5.203,lwr_k=200:6.5958,lwr_k=300:7.1498,lwr_k=400:7.433,lwr_k=500:7.7463,lwr_k=600:7.927,lwr_k=700:8.0835,lwr_k=800:8.1784,lwr_k=900:8.2896,lwr_k=1000:8.3692'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:14.8192,lwr_k=10:35.0001,lwr_k=20:1013.5737,lwr_k=30:1000.5368,lwr_k=40:37.2551,lwr_k=50:22.9373,lwr_k=100:14.1976,lwr_k=200:12.2049,lwr_k=300:12.3667,lwr_k=400:12.5189,lwr_k=500:12.8156,lwr_k=600:12.7979,lwr_k=700:12.8294,lwr_k=800:13.1007,lwr_k=900:13.2061,lwr_k=1000:13.4372'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_2'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:77.4869,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:2.8646,lwr_k=40:5.5042,lwr_k=50:7.7433,lwr_k=100:13.1933,lwr_k=200:18.5769,lwr_k=300:22.3981,lwr_k=400:25.2391,lwr_k=500:28.0524,lwr_k=600:30.5104,lwr_k=700:32.6701,lwr_k=800:34.4941,lwr_k=900:36.2721,lwr_k=1000:37.8307'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:88.2221,lwr_k=10:57.865,lwr_k=20:165.2443,lwr_k=30:51.4878,lwr_k=40:32.4867,lwr_k=50:24.6083,lwr_k=100:23.6235,lwr_k=200:26.544,lwr_k=300:29.6663,lwr_k=400:32.0073,lwr_k=500:34.6492,lwr_k=600:36.9157,lwr_k=700:38.9506,lwr_k=800:40.448,lwr_k=900:41.956,lwr_k=1000:43.3745'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:99.0271,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:4.106,lwr_k=40:6.9401,lwr_k=50:8.4883,lwr_k=100:15.8597,lwr_k=200:23.5834,lwr_k=300:29.0239,lwr_k=400:32.9609,lwr_k=500:36.6272,lwr_k=600:40.5731,lwr_k=700:43.5594,lwr_k=800:46.1905,lwr_k=900:48.3967,lwr_k=1000:50.3'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:83.8969,lwr_k=10:47.9573,lwr_k=20:224.667,lwr_k=30:54.8676,lwr_k=40:36.2291,lwr_k=50:28.3394,lwr_k=100:22.2444,lwr_k=200:23.9067,lwr_k=300:26.0588,lwr_k=400:27.84,lwr_k=500:30.4691,lwr_k=600:32.2835,lwr_k=700:34.9668,lwr_k=800:37.3237,lwr_k=900:38.767,lwr_k=1000:41.3391'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:106.4007,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:4.2961,lwr_k=40:8.1656,lwr_k=50:9.6613,lwr_k=100:16.1183,lwr_k=200:23.9911,lwr_k=300:28.936,lwr_k=400:33.0597,lwr_k=500:36.5462,lwr_k=600:40.6973,lwr_k=700:44.1878,lwr_k=800:46.9898,lwr_k=900:49.4066,lwr_k=1000:51.4823'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:65.2332,lwr_k=10:53.0728,lwr_k=20:202.4447,lwr_k=30:50.5226,lwr_k=40:28.4655,lwr_k=50:22.8198,lwr_k=100:18.1208,lwr_k=200:19.6816,lwr_k=300:22.8026,lwr_k=400:24.3863,lwr_k=500:26.3854,lwr_k=600:28.4185,lwr_k=700:29.9413,lwr_k=800:31.1145,lwr_k=900:32.2548,lwr_k=1000:33.4445'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:104.6715,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:3.4484,lwr_k=40:7.1624,lwr_k=50:9.7163,lwr_k=100:17.457,lwr_k=200:25.2093,lwr_k=300:30.9302,lwr_k=400:35.0368,lwr_k=500:38.6597,lwr_k=600:42.5242,lwr_k=700:45.4246,lwr_k=800:47.6768,lwr_k=900:49.9275,lwr_k=1000:51.442'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:91.2813,lwr_k=10:57.9964,lwr_k=20:204.0256,lwr_k=30:58.3623,lwr_k=40:35.5811,lwr_k=50:28.5765,lwr_k=100:23.3628,lwr_k=200:24.4457,lwr_k=300:27.862,lwr_k=400:29.7672,lwr_k=500:31.7063,lwr_k=600:34.731,lwr_k=700:36.7986,lwr_k=800:39.2668,lwr_k=900:41.0025,lwr_k=1000:42.9265'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:76.5935,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:3.7762,lwr_k=40:6.3475,lwr_k=50:8.7681,lwr_k=100:13.5873,lwr_k=200:19.3616,lwr_k=300:23.0959,lwr_k=400:26.2927,lwr_k=500:28.8026,lwr_k=600:31.3253,lwr_k=700:33.1343,lwr_k=800:34.6952,lwr_k=900:36.1354,lwr_k=1000:37.5512'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:150.2473,lwr_k=10:82.596,lwr_k=20:206.0491,lwr_k=30:60.5377,lwr_k=40:51.4231,lwr_k=50:48.3863,lwr_k=100:48.3345,lwr_k=200:58.291,lwr_k=300:63.3475,lwr_k=400:67.2307,lwr_k=500:72.3535,lwr_k=600:77.29,lwr_k=700:80.3582,lwr_k=800:84.1338,lwr_k=900:86.4721,lwr_k=1000:88.4437'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:94.4186,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:3.3183,lwr_k=40:6.6844,lwr_k=50:8.6855,lwr_k=100:15.3262,lwr_k=200:21.3556,lwr_k=300:26.0035,lwr_k=400:29.5404,lwr_k=500:32.3202,lwr_k=600:34.963,lwr_k=700:37.6188,lwr_k=800:40.003,lwr_k=900:41.7852,lwr_k=1000:43.6725'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:86.239,lwr_k=10:59.3538,lwr_k=20:172.0966,lwr_k=30:60.4255,lwr_k=40:38.5834,lwr_k=50:36.2071,lwr_k=100:30.2495,lwr_k=200:31.1561,lwr_k=300:31.683,lwr_k=400:33.8886,lwr_k=500:34.3238,lwr_k=600:34.4744,lwr_k=700:36.3605,lwr_k=800:37.032,lwr_k=900:38.241,lwr_k=1000:37.8293'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_3'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.4291,lwr_k=10:0.3259,lwr_k=20:2.7476,lwr_k=30:3.6781,lwr_k=40:4.1788,lwr_k=50:4.602,lwr_k=100:5.2651,lwr_k=200:5.7431,lwr_k=300:5.9245,lwr_k=400:6.0182,lwr_k=500:6.0914,lwr_k=600:6.1162,lwr_k=700:6.1304,lwr_k=800:6.157,lwr_k=900:6.1884,lwr_k=1000:6.2034'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.1178,lwr_k=10:491.0014,lwr_k=20:12.8056,lwr_k=30:9.5123,lwr_k=40:9.4069,lwr_k=50:9.4391,lwr_k=100:8.9944,lwr_k=200:8.8107,lwr_k=300:8.7988,lwr_k=400:8.8294,lwr_k=500:8.8709,lwr_k=600:8.9198,lwr_k=700:8.9201,lwr_k=800:8.8997,lwr_k=900:8.9442,lwr_k=1000:8.9428'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.5876,lwr_k=10:0.5657,lwr_k=20:3.1146,lwr_k=30:4.2989,lwr_k=40:5.4591,lwr_k=50:5.9001,lwr_k=100:6.8891,lwr_k=200:7.594,lwr_k=300:7.8366,lwr_k=400:7.9219,lwr_k=500:8.0057,lwr_k=600:8.058,lwr_k=700:8.1144,lwr_k=800:8.1558,lwr_k=900:8.2269,lwr_k=1000:8.2375'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.4261,lwr_k=10:267.1727,lwr_k=20:39.0982,lwr_k=30:26.7674,lwr_k=40:15.0384,lwr_k=50:14.3038,lwr_k=100:6.4818,lwr_k=200:6.5864,lwr_k=300:6.6049,lwr_k=400:6.7525,lwr_k=500:6.8275,lwr_k=600:6.8616,lwr_k=700:6.8909,lwr_k=800:6.9612,lwr_k=900:6.9903,lwr_k=1000:7.103'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.3998,lwr_k=10:0.1797,lwr_k=20:3.6079,lwr_k=30:4.939,lwr_k=40:5.7617,lwr_k=50:6.4103,lwr_k=100:8.4847,lwr_k=200:10.1426,lwr_k=300:10.5591,lwr_k=400:10.8834,lwr_k=500:11.0045,lwr_k=600:11.1292,lwr_k=700:11.2468,lwr_k=800:11.3003,lwr_k=900:11.3385,lwr_k=1000:11.3872'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.1615,lwr_k=10:105389.732,lwr_k=20:15.3659,lwr_k=30:10.973,lwr_k=40:10.1374,lwr_k=50:9.358,lwr_k=100:7.7649,lwr_k=200:7.4428,lwr_k=300:7.5394,lwr_k=400:7.5284,lwr_k=500:7.4953,lwr_k=600:7.4615,lwr_k=700:7.4258,lwr_k=800:7.4046,lwr_k=900:7.381,lwr_k=1000:7.3946'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:132.9141,lwr_k=10:14.4277,lwr_k=20:31.4569,lwr_k=30:38.7508,lwr_k=40:44.0208,lwr_k=50:46.0211,lwr_k=100:53.7428,lwr_k=200:62.898,lwr_k=300:68.8425,lwr_k=400:73.0119,lwr_k=500:76.4781,lwr_k=600:79.2588,lwr_k=700:82.2973,lwr_k=800:85.3857,lwr_k=900:87.8016,lwr_k=1000:91.1497'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:140.339,lwr_k=10:446.5202,lwr_k=20:107.0984,lwr_k=30:119.6969,lwr_k=40:66.6335,lwr_k=50:67.3402,lwr_k=100:66.9572,lwr_k=200:83.7792,lwr_k=300:86.4999,lwr_k=400:88.6459,lwr_k=500:93.0717,lwr_k=600:96.4911,lwr_k=700:99.0722,lwr_k=800:101.2989,lwr_k=900:104.0606,lwr_k=1000:106.0113'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.7614,lwr_k=10:0.586,lwr_k=20:2.5919,lwr_k=30:3.098,lwr_k=40:3.3721,lwr_k=50:3.5533,lwr_k=100:3.8832,lwr_k=200:4.2103,lwr_k=300:4.365,lwr_k=400:4.4105,lwr_k=500:4.4672,lwr_k=600:4.4776,lwr_k=700:4.5065,lwr_k=800:4.5314,lwr_k=900:4.5468,lwr_k=1000:4.5608'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:18.2835,lwr_k=10:137.7403,lwr_k=20:25.0638,lwr_k=30:19.9087,lwr_k=40:19.4065,lwr_k=50:18.4247,lwr_k=100:17.8058,lwr_k=200:17.6279,lwr_k=300:17.6989,lwr_k=400:17.6704,lwr_k=500:17.7,lwr_k=600:17.6827,lwr_k=700:17.7531,lwr_k=800:17.8284,lwr_k=900:17.8746,lwr_k=1000:17.9083'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:121.39,lwr_k=10:14.4913,lwr_k=20:31.1795,lwr_k=30:37.585,lwr_k=40:41.2259,lwr_k=50:44.0358,lwr_k=100:50.4055,lwr_k=200:57.6053,lwr_k=300:61.9194,lwr_k=400:65.6034,lwr_k=500:68.3626,lwr_k=600:70.9107,lwr_k=700:73.3497,lwr_k=800:75.481,lwr_k=900:77.4,lwr_k=1000:79.2313'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:135.6246,lwr_k=10:490.0827,lwr_k=20:65.7541,lwr_k=30:60.4816,lwr_k=40:58.3072,lwr_k=50:55.8652,lwr_k=100:72.363,lwr_k=200:75.4318,lwr_k=300:77.6198,lwr_k=400:81.2115,lwr_k=500:81.5673,lwr_k=600:85.12,lwr_k=700:87.7333,lwr_k=800:90.182,lwr_k=900:92.3518,lwr_k=1000:95.2345'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_4'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.836,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.4501,lwr_k=40:1.6546,lwr_k=50:2.5021,lwr_k=100:4.4311,lwr_k=200:5.7494,lwr_k=300:6.2157,lwr_k=400:6.4147,lwr_k=500:6.6024,lwr_k=600:6.7334,lwr_k=700:6.853,lwr_k=800:6.95,lwr_k=900:7.0318,lwr_k=1000:7.0916'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.0988,lwr_k=10:37.3305,lwr_k=20:72.6575,lwr_k=30:180.0615,lwr_k=40:24.9449,lwr_k=50:15.8628,lwr_k=100:9.4488,lwr_k=200:9.4301,lwr_k=300:9.3113,lwr_k=400:9.2778,lwr_k=500:9.3111,lwr_k=600:9.45,lwr_k=700:9.6295,lwr_k=800:9.7318,lwr_k=900:9.8154,lwr_k=1000:9.8687'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:15.6553,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.4602,lwr_k=40:2.3334,lwr_k=50:3.6896,lwr_k=100:6.9061,lwr_k=200:9.2478,lwr_k=300:10.1029,lwr_k=400:10.6942,lwr_k=500:11.0722,lwr_k=600:11.331,lwr_k=700:11.5445,lwr_k=800:11.733,lwr_k=900:11.9732,lwr_k=1000:12.1403'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:14.6598,lwr_k=10:53.0209,lwr_k=20:105.3486,lwr_k=30:129.6308,lwr_k=40:27.2709,lwr_k=50:18.1907,lwr_k=100:11.0037,lwr_k=200:9.6706,lwr_k=300:9.763,lwr_k=400:10.0841,lwr_k=500:10.2477,lwr_k=600:10.4187,lwr_k=700:10.3627,lwr_k=800:10.6911,lwr_k=900:10.7977,lwr_k=1000:10.8581'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:17.5375,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.3943,lwr_k=40:2.3184,lwr_k=50:3.6957,lwr_k=100:6.8186,lwr_k=200:9.8323,lwr_k=300:10.7293,lwr_k=400:11.3936,lwr_k=500:11.9094,lwr_k=600:12.0956,lwr_k=700:12.3464,lwr_k=800:12.5223,lwr_k=900:12.7195,lwr_k=1000:12.8941'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:11.553,lwr_k=10:38.539,lwr_k=20:163.6043,lwr_k=30:277.1001,lwr_k=40:43.3878,lwr_k=50:18.0903,lwr_k=100:10.2629,lwr_k=200:8.3469,lwr_k=300:8.2156,lwr_k=400:8.0679,lwr_k=500:7.9489,lwr_k=600:7.8869,lwr_k=700:7.94,lwr_k=800:8.0698,lwr_k=900:8.0935,lwr_k=1000:8.0726'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:31.7382,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.6418,lwr_k=40:3.275,lwr_k=50:4.7571,lwr_k=100:8.8783,lwr_k=200:12.1445,lwr_k=300:13.7539,lwr_k=400:14.9413,lwr_k=500:16.5176,lwr_k=600:17.4667,lwr_k=700:18.4139,lwr_k=800:19.0531,lwr_k=900:19.6078,lwr_k=1000:20.3143'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:23.8527,lwr_k=10:78.8362,lwr_k=20:119.554,lwr_k=30:286.6255,lwr_k=40:47.1341,lwr_k=50:25.3579,lwr_k=100:13.599,lwr_k=200:14.4243,lwr_k=300:13.6074,lwr_k=400:13.9086,lwr_k=500:14.0262,lwr_k=600:14.5598,lwr_k=700:14.9477,lwr_k=800:15.2334,lwr_k=900:15.2862,lwr_k=1000:15.6953'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.2472,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.4923,lwr_k=40:1.9187,lwr_k=50:2.7407,lwr_k=100:4.5189,lwr_k=200:5.829,lwr_k=300:6.3228,lwr_k=400:6.7361,lwr_k=500:6.9788,lwr_k=600:7.2036,lwr_k=700:7.3576,lwr_k=800:7.5224,lwr_k=900:7.6566,lwr_k=1000:7.7777'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:33.7709,lwr_k=10:45.0761,lwr_k=20:178.5455,lwr_k=30:167.5747,lwr_k=40:31.28,lwr_k=50:24.6784,lwr_k=100:21.15,lwr_k=200:23.2684,lwr_k=300:23.4271,lwr_k=400:24.5265,lwr_k=500:25.1598,lwr_k=600:25.8072,lwr_k=700:26.3884,lwr_k=800:26.946,lwr_k=900:27.4844,lwr_k=1000:27.8981'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:14.3948,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.4162,lwr_k=40:1.9957,lwr_k=50:3.1111,lwr_k=100:5.6992,lwr_k=200:7.5936,lwr_k=300:8.4748,lwr_k=400:8.9869,lwr_k=500:9.392,lwr_k=600:9.6887,lwr_k=700:9.9628,lwr_k=800:10.2326,lwr_k=900:10.4383,lwr_k=1000:10.5963'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:16.1204,lwr_k=10:62.2447,lwr_k=20:139.5466,lwr_k=30:177.2238,lwr_k=40:27.377,lwr_k=50:29.0683,lwr_k=100:10.5786,lwr_k=200:10.8291,lwr_k=300:11.6204,lwr_k=400:11.7146,lwr_k=500:12.0839,lwr_k=600:12.257,lwr_k=700:12.5717,lwr_k=800:12.619,lwr_k=900:12.7313,lwr_k=1000:12.861'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_5'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.7172,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0418,lwr_k=40:0.7083,lwr_k=50:5.6498,lwr_k=100:4.0891,lwr_k=200:4.2232,lwr_k=300:4.5349,lwr_k=400:4.2584,lwr_k=500:4.7202,lwr_k=600:4.7057,lwr_k=700:4.7216,lwr_k=800:4.6224,lwr_k=900:4.6087,lwr_k=1000:4.6622'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.2198,lwr_k=10:34.0258,lwr_k=20:324.6431,lwr_k=30:735.085,lwr_k=40:117.7115,lwr_k=50:82.7734,lwr_k=100:12.9054,lwr_k=200:10.4873,lwr_k=300:7.9155,lwr_k=400:10.2121,lwr_k=500:8.4087,lwr_k=600:7.7266,lwr_k=700:8.649,lwr_k=800:8.5247,lwr_k=900:8.1896,lwr_k=1000:7.8975'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.4831,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.4769,lwr_k=40:1.2498,lwr_k=50:4.6245,lwr_k=100:5.1233,lwr_k=200:5.4713,lwr_k=300:5.5361,lwr_k=400:5.6952,lwr_k=500:5.6227,lwr_k=600:5.7489,lwr_k=700:5.7843,lwr_k=800:5.8351,lwr_k=900:5.9957,lwr_k=1000:5.9674'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.1635,lwr_k=10:48.9946,lwr_k=20:281.4452,lwr_k=30:928.9922,lwr_k=40:103.7036,lwr_k=50:124.682,lwr_k=100:12.9836,lwr_k=200:8.8005,lwr_k=300:7.1794,lwr_k=400:7.0535,lwr_k=500:7.2409,lwr_k=600:6.7214,lwr_k=700:6.6011,lwr_k=800:6.5387,lwr_k=900:6.6289,lwr_k=1000:6.6441'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:13.0277,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.5657,lwr_k=40:1.91,lwr_k=50:9.4534,lwr_k=100:8.3861,lwr_k=200:8.8427,lwr_k=300:7.4673,lwr_k=400:7.7065,lwr_k=500:7.4492,lwr_k=600:7.0747,lwr_k=700:7.4848,lwr_k=800:7.1462,lwr_k=900:7.4729,lwr_k=1000:7.9388'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:11.9719,lwr_k=10:52.4063,lwr_k=20:411.1325,lwr_k=30:1199.8261,lwr_k=40:64.8304,lwr_k=50:57.6511,lwr_k=100:14.6841,lwr_k=200:15.0842,lwr_k=300:12.1115,lwr_k=400:11.9021,lwr_k=500:8.4345,lwr_k=600:8.4914,lwr_k=700:7.3517,lwr_k=800:7.3657,lwr_k=900:6.8313,lwr_k=1000:6.6675'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.9044,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.4907,lwr_k=40:1.6977,lwr_k=50:37.3607,lwr_k=100:15.2575,lwr_k=200:10.5453,lwr_k=300:9.6197,lwr_k=400:7.4869,lwr_k=500:7.0702,lwr_k=600:6.9145,lwr_k=700:6.9781,lwr_k=800:6.5894,lwr_k=900:6.8045,lwr_k=1000:6.8636'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:11.2429,lwr_k=10:117.6427,lwr_k=20:429.2077,lwr_k=30:1823.1311,lwr_k=40:181.3396,lwr_k=50:534.4837,lwr_k=100:67.1019,lwr_k=200:14.9545,lwr_k=300:11.5967,lwr_k=400:9.9736,lwr_k=500:9.4377,lwr_k=600:9.2902,lwr_k=700:9.3817,lwr_k=800:9.7251,lwr_k=900:9.5558,lwr_k=1000:9.214'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.8171,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0345,lwr_k=40:0.4262,lwr_k=50:6.9349,lwr_k=100:3.7004,lwr_k=200:4.5021,lwr_k=300:4.7536,lwr_k=400:4.9167,lwr_k=500:5.0483,lwr_k=600:5.1177,lwr_k=700:5.1788,lwr_k=800:5.2477,lwr_k=900:5.2911,lwr_k=1000:5.3482'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:17.512,lwr_k=10:47.2934,lwr_k=20:95.9396,lwr_k=30:406.5062,lwr_k=40:4253.9189,lwr_k=50:693.9668,lwr_k=100:58.0019,lwr_k=200:19.5245,lwr_k=300:17.3148,lwr_k=400:17.1284,lwr_k=500:17.0786,lwr_k=600:17.071,lwr_k=700:17.3683,lwr_k=800:17.2711,lwr_k=900:17.348,lwr_k=1000:17.4979'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:7.755,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.1935,lwr_k=40:1.5107,lwr_k=50:8.7832,lwr_k=100:6.0551,lwr_k=200:6.2618,lwr_k=300:6.0966,lwr_k=400:6.5019,lwr_k=500:6.3857,lwr_k=600:6.4078,lwr_k=700:6.5735,lwr_k=800:6.2811,lwr_k=900:6.7266,lwr_k=1000:7.0593'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:15.1095,lwr_k=10:95.1573,lwr_k=20:253.7585,lwr_k=30:6528.6134,lwr_k=40:140.906,lwr_k=50:58.6519,lwr_k=100:19.4962,lwr_k=200:13.375,lwr_k=300:11.3962,lwr_k=400:10.297,lwr_k=500:10.8474,lwr_k=600:10.222,lwr_k=700:9.3123,lwr_k=800:8.8266,lwr_k=900:9.9707,lwr_k=1000:12.1718'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_6'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:62.596,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:2.8283,lwr_k=50:5.1308,lwr_k=100:12.7922,lwr_k=200:20.6818,lwr_k=300:24.7984,lwr_k=400:28.3294,lwr_k=500:31.663,lwr_k=600:34.028,lwr_k=700:35.9997,lwr_k=800:37.8965,lwr_k=900:39.4527,lwr_k=1000:40.8817'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:69.8484,lwr_k=10:95.769,lwr_k=20:299.8591,lwr_k=30:1150.2771,lwr_k=40:94.5569,lwr_k=50:47.0646,lwr_k=100:30.5255,lwr_k=200:32.2495,lwr_k=300:35.6577,lwr_k=400:38.9645,lwr_k=500:41.1686,lwr_k=600:42.7797,lwr_k=700:44.4666,lwr_k=800:45.8604,lwr_k=900:47.1098,lwr_k=1000:48.1282'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:78.0678,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0016,lwr_k=40:3.4803,lwr_k=50:6.3333,lwr_k=100:16.6558,lwr_k=200:27.2984,lwr_k=300:32.945,lwr_k=400:36.6968,lwr_k=500:40.3597,lwr_k=600:43.2084,lwr_k=700:45.558,lwr_k=800:48.0225,lwr_k=900:49.6638,lwr_k=1000:51.6047'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:70.2658,lwr_k=10:117.0519,lwr_k=20:425.9713,lwr_k=30:2907.6419,lwr_k=40:157.5636,lwr_k=50:131.2218,lwr_k=100:29.9375,lwr_k=200:30.1051,lwr_k=300:32.5729,lwr_k=400:34.8557,lwr_k=500:36.0848,lwr_k=600:38.2593,lwr_k=700:40.2489,lwr_k=800:42.8728,lwr_k=900:43.879,lwr_k=1000:44.7718'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:84.1892,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0211,lwr_k=40:3.7493,lwr_k=50:6.9686,lwr_k=100:17.9504,lwr_k=200:27.4059,lwr_k=300:34.4908,lwr_k=400:39.0001,lwr_k=500:42.4718,lwr_k=600:45.6033,lwr_k=700:48.2581,lwr_k=800:50.3997,lwr_k=900:52.5873,lwr_k=1000:54.6213'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:52.5839,lwr_k=10:60.3543,lwr_k=20:279.7102,lwr_k=30:6054.6197,lwr_k=40:297.7152,lwr_k=50:146.7919,lwr_k=100:167.6771,lwr_k=200:25.2027,lwr_k=300:26.6937,lwr_k=400:28.7064,lwr_k=500:30.5779,lwr_k=600:31.5481,lwr_k=700:32.2351,lwr_k=800:33.2482,lwr_k=900:34.5473,lwr_k=1000:35.0678'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:82.2777,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:3.3389,lwr_k=50:7.0192,lwr_k=100:17.5129,lwr_k=200:27.3734,lwr_k=300:34.6452,lwr_k=400:39.4805,lwr_k=500:42.3706,lwr_k=600:44.8781,lwr_k=700:47.6239,lwr_k=800:50.2392,lwr_k=900:52.077,lwr_k=1000:53.4506'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:77.3298,lwr_k=10:67.0109,lwr_k=20:332.0331,lwr_k=30:1898.0061,lwr_k=40:1092.3111,lwr_k=50:1329.7317,lwr_k=100:519.1434,lwr_k=200:274.389,lwr_k=300:261.3819,lwr_k=400:268.6421,lwr_k=500:257.852,lwr_k=600:256.9519,lwr_k=700:237.7993,lwr_k=800:169.7898,lwr_k=900:154.6371,lwr_k=1000:152.2953'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:61.1151,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0001,lwr_k=40:3.1103,lwr_k=50:5.3617,lwr_k=100:13.4307,lwr_k=200:21.0188,lwr_k=300:26.1199,lwr_k=400:29.5134,lwr_k=500:32.0188,lwr_k=600:34.1024,lwr_k=700:35.9427,lwr_k=800:37.4653,lwr_k=900:38.8135,lwr_k=1000:40.0667'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:193.4522,lwr_k=10:96.2132,lwr_k=20:410.6548,lwr_k=30:1897.0529,lwr_k=40:242.8757,lwr_k=50:235.211,lwr_k=100:75.1042,lwr_k=200:64.2291,lwr_k=300:69.3567,lwr_k=400:73.2027,lwr_k=500:73.7534,lwr_k=600:76.2628,lwr_k=700:80.0244,lwr_k=800:83.4511,lwr_k=900:83.6388,lwr_k=1000:85.5245'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:74.7135,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:2.9377,lwr_k=50:5.5051,lwr_k=100:15.0613,lwr_k=200:23.5767,lwr_k=300:29.3743,lwr_k=400:33.097,lwr_k=500:35.8808,lwr_k=600:38.1311,lwr_k=700:40.2883,lwr_k=800:41.9544,lwr_k=900:43.8161,lwr_k=1000:45.4771'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:77.0113,lwr_k=10:84.9962,lwr_k=20:401.8912,lwr_k=30:3101.9349,lwr_k=40:612.9256,lwr_k=50:583.0965,lwr_k=100:70.2843,lwr_k=200:73.7954,lwr_k=300:70.5309,lwr_k=400:67.7451,lwr_k=500:62.678,lwr_k=600:60.7515,lwr_k=700:64.3183,lwr_k=800:67.8118,lwr_k=900:69.5439,lwr_k=1000:73.4569'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_7'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:106.4276,lwr_k=10:0.0,lwr_k=20:12.1941,lwr_k=30:20.0017,lwr_k=40:23.5394,lwr_k=50:25.948,lwr_k=100:34.923,lwr_k=200:43.6174,lwr_k=300:49.5401,lwr_k=400:53.2114,lwr_k=500:56.2118,lwr_k=600:58.6176,lwr_k=700:60.7867,lwr_k=800:63.2942,lwr_k=900:65.5972,lwr_k=1000:67.296'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:141.629,lwr_k=10:939.6524,lwr_k=20:260.942,lwr_k=30:60.0871,lwr_k=40:55.8957,lwr_k=50:54.2057,lwr_k=100:54.4557,lwr_k=200:56.6268,lwr_k=300:60.6384,lwr_k=400:65.2746,lwr_k=500:68.8381,lwr_k=600:72.43,lwr_k=700:75.6502,lwr_k=800:78.9173,lwr_k=900:82.0521,lwr_k=1000:84.7015'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:131.447,lwr_k=10:0.0,lwr_k=20:12.2225,lwr_k=30:20.6327,lwr_k=40:24.9879,lwr_k=50:27.6626,lwr_k=100:38.0274,lwr_k=200:50.2475,lwr_k=300:57.7519,lwr_k=400:62.8054,lwr_k=500:66.5563,lwr_k=600:69.9887,lwr_k=700:72.8666,lwr_k=800:76.5442,lwr_k=900:79.9092,lwr_k=1000:82.3423'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:109.5394,lwr_k=10:1740.9675,lwr_k=20:95.6435,lwr_k=30:59.9512,lwr_k=40:51.7344,lwr_k=50:48.6243,lwr_k=100:46.1081,lwr_k=200:50.7018,lwr_k=300:53.4688,lwr_k=400:55.6707,lwr_k=500:58.0221,lwr_k=600:60.5069,lwr_k=700:62.9085,lwr_k=800:65.5879,lwr_k=900:67.9714,lwr_k=1000:69.5284'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:150.0622,lwr_k=10:0.0,lwr_k=20:13.5598,lwr_k=30:21.4884,lwr_k=40:25.8751,lwr_k=50:28.9354,lwr_k=100:40.1194,lwr_k=200:52.6558,lwr_k=300:60.1558,lwr_k=400:65.8989,lwr_k=500:70.5054,lwr_k=600:74.9642,lwr_k=700:79.0584,lwr_k=800:83.1267,lwr_k=900:86.8066,lwr_k=1000:89.2245'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:86.8008,lwr_k=10:833.8186,lwr_k=20:71.5086,lwr_k=30:54.3727,lwr_k=40:48.1879,lwr_k=50:43.2873,lwr_k=100:40.1942,lwr_k=200:43.3082,lwr_k=300:45.1269,lwr_k=400:46.9459,lwr_k=500:48.5348,lwr_k=600:49.7981,lwr_k=700:51.1202,lwr_k=800:52.3091,lwr_k=900:53.8585,lwr_k=1000:54.9648'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:143.3574,lwr_k=10:0.0,lwr_k=20:14.3724,lwr_k=30:21.8002,lwr_k=40:25.658,lwr_k=50:29.3304,lwr_k=100:39.5346,lwr_k=200:52.772,lwr_k=300:60.2448,lwr_k=400:65.2823,lwr_k=500:69.984,lwr_k=600:74.1061,lwr_k=700:77.9898,lwr_k=800:81.532,lwr_k=900:84.678,lwr_k=1000:87.3102'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:130.1834,lwr_k=10:2722.5654,lwr_k=20:124.3983,lwr_k=30:64.4437,lwr_k=40:48.7597,lwr_k=50:46.2991,lwr_k=100:48.9536,lwr_k=200:55.8608,lwr_k=300:60.0544,lwr_k=400:63.8401,lwr_k=500:66.5267,lwr_k=600:70.2495,lwr_k=700:73.4589,lwr_k=800:76.8639,lwr_k=900:78.9504,lwr_k=1000:81.587'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:110.1866,lwr_k=10:0.0,lwr_k=20:13.5865,lwr_k=30:21.136,lwr_k=40:24.5486,lwr_k=50:27.8017,lwr_k=100:35.5633,lwr_k=200:44.417,lwr_k=300:49.2573,lwr_k=400:53.0278,lwr_k=500:55.8091,lwr_k=600:58.8037,lwr_k=700:61.1949,lwr_k=800:63.7303,lwr_k=900:65.907,lwr_k=1000:67.9707'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:184.3549,lwr_k=10:1186.3781,lwr_k=20:87.416,lwr_k=30:86.6228,lwr_k=40:78.6979,lwr_k=50:75.7936,lwr_k=100:75.0864,lwr_k=200:83.1869,lwr_k=300:90.6279,lwr_k=400:97.1529,lwr_k=500:105.2592,lwr_k=600:109.2855,lwr_k=700:112.7908,lwr_k=800:116.0924,lwr_k=900:118.9565,lwr_k=1000:121.3836'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:128.5858,lwr_k=10:0.0,lwr_k=20:13.8449,lwr_k=30:21.1703,lwr_k=40:25.4375,lwr_k=50:28.2813,lwr_k=100:36.7127,lwr_k=200:45.5507,lwr_k=300:52.6709,lwr_k=400:57.2553,lwr_k=500:60.1123,lwr_k=600:63.3976,lwr_k=700:66.1264,lwr_k=800:68.6602,lwr_k=900:70.9698,lwr_k=1000:73.2356'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:128.1876,lwr_k=10:3189.1729,lwr_k=20:97.7425,lwr_k=30:51.6362,lwr_k=40:51.3418,lwr_k=50:51.784,lwr_k=100:55.5342,lwr_k=200:59.0858,lwr_k=300:63.2973,lwr_k=400:66.2235,lwr_k=500:68.4284,lwr_k=600:69.5133,lwr_k=700:71.0071,lwr_k=800:73.6228,lwr_k=900:75.6722,lwr_k=1000:77.3982'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_8'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:143.6131,lwr_k=10:0.0,lwr_k=20:5.5929,lwr_k=30:12.5155,lwr_k=40:16.6011,lwr_k=50:19.1934,lwr_k=100:28.6691,lwr_k=200:38.9415,lwr_k=300:47.0438,lwr_k=400:53.8711,lwr_k=500:58.7764,lwr_k=600:63.5634,lwr_k=700:67.8508,lwr_k=800:71.9521,lwr_k=900:75.7487,lwr_k=1000:78.7747'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:186.2555,lwr_k=10:645.7634,lwr_k=20:259.0923,lwr_k=30:123.8418,lwr_k=40:53.1282,lwr_k=50:51.2485,lwr_k=100:52.9944,lwr_k=200:60.9023,lwr_k=300:67.7045,lwr_k=400:74.8902,lwr_k=500:79.6666,lwr_k=600:85.2485,lwr_k=700:91.6351,lwr_k=800:96.9325,lwr_k=900:100.9114,lwr_k=1000:104.6841'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:168.5198,lwr_k=10:0.0,lwr_k=20:6.0227,lwr_k=30:12.655,lwr_k=40:17.9389,lwr_k=50:21.5152,lwr_k=100:33.122,lwr_k=200:45.9683,lwr_k=300:53.5879,lwr_k=400:61.9303,lwr_k=500:68.0433,lwr_k=600:73.3661,lwr_k=700:78.9587,lwr_k=800:83.5641,lwr_k=900:87.7028,lwr_k=1000:91.1004'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:148.6055,lwr_k=10:458.2702,lwr_k=20:85.1245,lwr_k=30:91.542,lwr_k=40:45.2778,lwr_k=50:42.9159,lwr_k=100:39.0032,lwr_k=200:44.7974,lwr_k=300:50.0795,lwr_k=400:55.7402,lwr_k=500:60.3966,lwr_k=600:64.8912,lwr_k=700:70.2727,lwr_k=800:74.5066,lwr_k=900:77.8258,lwr_k=1000:80.3528'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:187.3684,lwr_k=10:0.0,lwr_k=20:7.5035,lwr_k=30:14.8925,lwr_k=40:19.9478,lwr_k=50:23.6956,lwr_k=100:35.1161,lwr_k=200:49.1423,lwr_k=300:58.2979,lwr_k=400:66.8993,lwr_k=500:73.3525,lwr_k=600:79.2859,lwr_k=700:85.6086,lwr_k=800:90.0697,lwr_k=900:94.9042,lwr_k=1000:98.4139'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:127.4283,lwr_k=10:709.3045,lwr_k=20:185.5931,lwr_k=30:49.6213,lwr_k=40:38.509,lwr_k=50:34.6673,lwr_k=100:36.8738,lwr_k=200:42.0329,lwr_k=300:46.9663,lwr_k=400:52.6939,lwr_k=500:57.5413,lwr_k=600:62.0824,lwr_k=700:65.4882,lwr_k=800:69.1588,lwr_k=900:72.3898,lwr_k=1000:74.5481'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:180.6738,lwr_k=10:0.0,lwr_k=20:6.3521,lwr_k=30:14.7584,lwr_k=40:19.6587,lwr_k=50:22.5802,lwr_k=100:34.8793,lwr_k=200:48.8939,lwr_k=300:57.9784,lwr_k=400:66.149,lwr_k=500:72.5306,lwr_k=600:78.5869,lwr_k=700:84.1118,lwr_k=800:88.7472,lwr_k=900:93.2005,lwr_k=1000:97.2409'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:168.2766,lwr_k=10:507.8244,lwr_k=20:250.8989,lwr_k=30:56.2907,lwr_k=40:48.7521,lwr_k=50:47.1396,lwr_k=100:43.4938,lwr_k=200:49.6336,lwr_k=300:57.926,lwr_k=400:63.6896,lwr_k=500:69.3258,lwr_k=600:74.2871,lwr_k=700:79.3539,lwr_k=800:83.4471,lwr_k=900:87.1113,lwr_k=1000:89.9058'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:149.7283,lwr_k=10:0.0,lwr_k=20:5.2741,lwr_k=30:12.0315,lwr_k=40:16.3684,lwr_k=50:19.5248,lwr_k=100:29.3058,lwr_k=200:40.8192,lwr_k=300:49.5709,lwr_k=400:57.1801,lwr_k=500:62.7813,lwr_k=600:67.9278,lwr_k=700:72.9783,lwr_k=800:77.1176,lwr_k=900:80.8161,lwr_k=1000:83.5689'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:224.0067,lwr_k=10:738.7713,lwr_k=20:357.4684,lwr_k=30:134.8225,lwr_k=40:126.3317,lwr_k=50:134.664,lwr_k=100:67.4127,lwr_k=200:76.299,lwr_k=300:84.5285,lwr_k=400:94.2413,lwr_k=500:101.5777,lwr_k=600:109.6196,lwr_k=700:116.9835,lwr_k=800:123.3531,lwr_k=900:127.3644,lwr_k=1000:131.7364'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:167.1022,lwr_k=10:0.0,lwr_k=20:5.3416,lwr_k=30:12.2062,lwr_k=40:17.2334,lwr_k=50:19.9093,lwr_k=100:30.3407,lwr_k=200:40.9677,lwr_k=300:48.7029,lwr_k=400:55.1148,lwr_k=500:61.1542,lwr_k=600:65.8257,lwr_k=700:70.8162,lwr_k=800:74.6332,lwr_k=900:79.107,lwr_k=1000:82.7462'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:174.849,lwr_k=10:541.5008,lwr_k=20:121.5234,lwr_k=30:69.3198,lwr_k=40:61.5165,lwr_k=50:59.0376,lwr_k=100:57.5734,lwr_k=200:60.4461,lwr_k=300:65.4969,lwr_k=400:68.1246,lwr_k=500:72.1714,lwr_k=600:76.4922,lwr_k=700:80.3013,lwr_k=800:83.9407,lwr_k=900:87.2677,lwr_k=1000:89.6965'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_9'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.8959,lwr_k=10:0.0,lwr_k=20:0.0004,lwr_k=30:0.0026,lwr_k=40:0.0222,lwr_k=50:0.3445,lwr_k=100:1.9776,lwr_k=200:3.0238,lwr_k=300:3.4382,lwr_k=400:3.6894,lwr_k=500:3.8558,lwr_k=600:4.0059,lwr_k=700:4.1022,lwr_k=800:4.1674,lwr_k=900:4.2258,lwr_k=1000:4.2539'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.2549,lwr_k=10:17.3256,lwr_k=20:21.1719,lwr_k=30:603.6876,lwr_k=40:215.8897,lwr_k=50:101.5521,lwr_k=100:9.7458,lwr_k=200:7.9098,lwr_k=300:7.4002,lwr_k=400:7.2843,lwr_k=500:7.3839,lwr_k=600:7.3923,lwr_k=700:7.3564,lwr_k=800:7.4589,lwr_k=900:7.441,lwr_k=1000:7.4631'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.5491,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0012,lwr_k=40:0.0236,lwr_k=50:0.4176,lwr_k=100:2.2464,lwr_k=200:4.0534,lwr_k=300:4.9726,lwr_k=400:5.3429,lwr_k=500:5.6007,lwr_k=600:5.8477,lwr_k=700:5.9904,lwr_k=800:6.1362,lwr_k=900:6.2561,lwr_k=1000:6.3676'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.9434,lwr_k=10:20.3797,lwr_k=20:14.8701,lwr_k=30:27.2234,lwr_k=40:239821.8124,lwr_k=50:312.6308,lwr_k=100:7.7495,lwr_k=200:6.3421,lwr_k=300:6.1413,lwr_k=400:5.9568,lwr_k=500:5.95,lwr_k=600:6.052,lwr_k=700:6.2088,lwr_k=800:6.2995,lwr_k=900:6.4473,lwr_k=1000:6.5449'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.4141,lwr_k=10:0.0,lwr_k=20:0.0002,lwr_k=30:0.0008,lwr_k=40:0.0264,lwr_k=50:0.4032,lwr_k=100:2.6093,lwr_k=200:4.3319,lwr_k=300:4.9595,lwr_k=400:5.4297,lwr_k=500:5.683,lwr_k=600:5.8737,lwr_k=700:6.0425,lwr_k=800:6.1865,lwr_k=900:6.2759,lwr_k=1000:6.3571'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.1,lwr_k=10:14.2436,lwr_k=20:13.0656,lwr_k=30:25.3702,lwr_k=40:235.8563,lwr_k=50:179.2102,lwr_k=100:8.8085,lwr_k=200:5.6956,lwr_k=300:5.2991,lwr_k=400:5.1842,lwr_k=500:5.1757,lwr_k=600:5.1899,lwr_k=700:5.2363,lwr_k=800:5.3024,lwr_k=900:5.3167,lwr_k=1000:5.3312'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.393,lwr_k=10:0.0021,lwr_k=20:0.0377,lwr_k=30:0.1495,lwr_k=40:0.3286,lwr_k=50:0.6839,lwr_k=100:2.7084,lwr_k=200:4.0565,lwr_k=300:4.5812,lwr_k=400:4.8049,lwr_k=500:4.9944,lwr_k=600:5.0957,lwr_k=700:5.2218,lwr_k=800:5.3379,lwr_k=900:5.4299,lwr_k=1000:5.5051'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.3246,lwr_k=10:22.8199,lwr_k=20:60.6128,lwr_k=30:867.5647,lwr_k=40:10413432.8732,lwr_k=50:2023195.9892,lwr_k=100:12.593,lwr_k=200:7.5927,lwr_k=300:7.7108,lwr_k=400:7.7198,lwr_k=500:7.7269,lwr_k=600:7.8063,lwr_k=700:7.8667,lwr_k=800:7.8904,lwr_k=900:7.9076,lwr_k=1000:7.9663'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:52.2781,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0114,lwr_k=40:0.7267,lwr_k=50:1.9001,lwr_k=100:7.5254,lwr_k=200:13.5143,lwr_k=300:16.753,lwr_k=400:19.5896,lwr_k=500:21.9968,lwr_k=600:23.9154,lwr_k=700:25.6366,lwr_k=800:27.0215,lwr_k=900:28.1987,lwr_k=1000:29.4769'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:102.426,lwr_k=10:45.632,lwr_k=20:55.6292,lwr_k=30:147.6372,lwr_k=40:504.2211,lwr_k=50:470.2081,lwr_k=100:48.3176,lwr_k=200:46.3481,lwr_k=300:53.2924,lwr_k=400:57.9067,lwr_k=500:61.3349,lwr_k=600:64.547,lwr_k=700:66.7638,lwr_k=800:68.4128,lwr_k=900:70.4864,lwr_k=1000:72.4248'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:5.638,lwr_k=10:0.0002,lwr_k=20:0.0042,lwr_k=30:0.0125,lwr_k=40:0.1235,lwr_k=50:0.4847,lwr_k=100:2.2594,lwr_k=200:3.4269,lwr_k=300:3.8843,lwr_k=400:4.1374,lwr_k=500:4.3094,lwr_k=600:4.4662,lwr_k=700:4.551,lwr_k=800:4.5947,lwr_k=900:4.6683,lwr_k=1000:4.7307'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.0102,lwr_k=10:17.7942,lwr_k=20:48.7561,lwr_k=30:11227.2888,lwr_k=40:524250.4624,lwr_k=50:4794.6211,lwr_k=100:7.8568,lwr_k=200:6.7469,lwr_k=300:6.7634,lwr_k=400:7.1008,lwr_k=500:7.2815,lwr_k=600:7.4704,lwr_k=700:7.5377,lwr_k=800:7.6149,lwr_k=900:7.7192,lwr_k=1000:7.8163'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_10'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.0842,lwr_k=10:0.0,lwr_k=20:2.3212,lwr_k=30:2.8654,lwr_k=40:3.3291,lwr_k=50:3.5482,lwr_k=100:4.1269,lwr_k=200:4.3948,lwr_k=300:4.4998,lwr_k=400:4.5601,lwr_k=500:4.6228,lwr_k=600:4.6821,lwr_k=700:4.7173,lwr_k=800:4.7595,lwr_k=900:4.7961,lwr_k=1000:4.8164'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.277,lwr_k=10:721014.244,lwr_k=20:23.4179,lwr_k=30:16.8353,lwr_k=40:13.4661,lwr_k=50:9.8985,lwr_k=100:8.0711,lwr_k=200:8.2043,lwr_k=300:8.4671,lwr_k=400:8.5954,lwr_k=500:8.6662,lwr_k=600:8.7306,lwr_k=700:8.7566,lwr_k=800:8.7494,lwr_k=900:8.7769,lwr_k=1000:8.8057'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.4902,lwr_k=10:0.0,lwr_k=20:2.9787,lwr_k=30:4.0179,lwr_k=40:4.705,lwr_k=50:4.9728,lwr_k=100:5.7852,lwr_k=200:6.3575,lwr_k=300:6.6165,lwr_k=400:6.8095,lwr_k=500:6.9429,lwr_k=600:7.0504,lwr_k=700:7.1367,lwr_k=800:7.2305,lwr_k=900:7.3098,lwr_k=1000:7.3904'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.206,lwr_k=10:198164.8338,lwr_k=20:61.2508,lwr_k=30:13.321,lwr_k=40:13.3377,lwr_k=50:14.5755,lwr_k=100:23.501,lwr_k=200:8.5361,lwr_k=300:7.3201,lwr_k=400:7.5689,lwr_k=500:7.6507,lwr_k=600:7.6835,lwr_k=700:7.7744,lwr_k=800:7.8535,lwr_k=900:7.9358,lwr_k=1000:7.9769'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.0388,lwr_k=10:0.0,lwr_k=20:2.9711,lwr_k=30:3.9297,lwr_k=40:4.6827,lwr_k=50:5.4729,lwr_k=100:7.3039,lwr_k=200:8.2524,lwr_k=300:8.5703,lwr_k=400:8.7373,lwr_k=500:8.8221,lwr_k=600:8.8924,lwr_k=700:8.9392,lwr_k=800:9.0036,lwr_k=900:9.0679,lwr_k=1000:9.1209'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.3206,lwr_k=10:128215.5214,lwr_k=20:12.1206,lwr_k=30:7.8727,lwr_k=40:12.9074,lwr_k=50:6.5496,lwr_k=100:6.0599,lwr_k=200:5.9773,lwr_k=300:6.0106,lwr_k=400:6.0619,lwr_k=500:6.1667,lwr_k=600:6.2354,lwr_k=700:6.2528,lwr_k=800:6.2609,lwr_k=900:6.2594,lwr_k=1000:6.3216'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.2175,lwr_k=10:0.0001,lwr_k=20:2.9312,lwr_k=30:3.7304,lwr_k=40:4.1443,lwr_k=50:4.3492,lwr_k=100:5.3055,lwr_k=200:6.0609,lwr_k=300:6.284,lwr_k=400:6.4258,lwr_k=500:6.5308,lwr_k=600:6.5651,lwr_k=700:6.5948,lwr_k=800:6.6322,lwr_k=900:6.6791,lwr_k=1000:6.7242'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.9629,lwr_k=10:23040.104,lwr_k=20:98.072,lwr_k=30:24.9139,lwr_k=40:14.6291,lwr_k=50:10.1474,lwr_k=100:7.956,lwr_k=200:7.8532,lwr_k=300:8.0641,lwr_k=400:8.0899,lwr_k=500:8.217,lwr_k=600:8.3026,lwr_k=700:8.3707,lwr_k=800:8.4826,lwr_k=900:8.4367,lwr_k=1000:8.441'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.38,lwr_k=10:0.0,lwr_k=20:2.1949,lwr_k=30:2.9258,lwr_k=40:3.299,lwr_k=50:3.5347,lwr_k=100:3.9899,lwr_k=200:4.2708,lwr_k=300:4.3995,lwr_k=400:4.4889,lwr_k=500:4.5798,lwr_k=600:4.641,lwr_k=700:4.7044,lwr_k=800:4.7583,lwr_k=900:4.8132,lwr_k=1000:4.8495'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:15.7512,lwr_k=10:16319.9592,lwr_k=20:20.9963,lwr_k=30:16.1397,lwr_k=40:16.5291,lwr_k=50:16.347,lwr_k=100:17.0338,lwr_k=200:15.6289,lwr_k=300:15.5497,lwr_k=400:15.512,lwr_k=500:15.4752,lwr_k=600:15.426,lwr_k=700:15.3732,lwr_k=800:15.3534,lwr_k=900:15.3492,lwr_k=1000:15.3249'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:5.9898,lwr_k=10:0.0,lwr_k=20:2.2284,lwr_k=30:3.0122,lwr_k=40:3.3719,lwr_k=50:3.7348,lwr_k=100:4.4305,lwr_k=200:4.8122,lwr_k=300:5.0,lwr_k=400:5.1177,lwr_k=500:5.1944,lwr_k=600:5.2589,lwr_k=700:5.3161,lwr_k=800:5.3569,lwr_k=900:5.3898,lwr_k=1000:5.424'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.8586,lwr_k=10:2628.7302,lwr_k=20:30.6022,lwr_k=30:12.3414,lwr_k=40:10.3468,lwr_k=50:8.9849,lwr_k=100:7.8573,lwr_k=200:8.1731,lwr_k=300:8.1066,lwr_k=400:7.9693,lwr_k=500:8.2928,lwr_k=600:8.3922,lwr_k=700:8.5298,lwr_k=800:8.5771,lwr_k=900:8.6792,lwr_k=1000:8.8243'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_11'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.4991,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:0.0131,lwr_k=100:1.9434,lwr_k=200:2.9465,lwr_k=300:3.265,lwr_k=400:3.4473,lwr_k=500:3.5542,lwr_k=600:3.6321,lwr_k=700:3.6971,lwr_k=800:3.7315,lwr_k=900:3.7671,lwr_k=1000:3.8081'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.038,lwr_k=10:25.0228,lwr_k=20:32.1913,lwr_k=30:58.2323,lwr_k=40:73.0033,lwr_k=50:5049.7444,lwr_k=100:11.4689,lwr_k=200:7.477,lwr_k=300:7.3943,lwr_k=400:7.2916,lwr_k=500:7.3223,lwr_k=600:7.3545,lwr_k=700:7.3153,lwr_k=800:7.294,lwr_k=900:7.3108,lwr_k=1000:7.3356'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.3312,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0001,lwr_k=40:0.0134,lwr_k=50:1.0739,lwr_k=100:2.5522,lwr_k=200:4.7283,lwr_k=300:5.3626,lwr_k=400:6.1099,lwr_k=500:6.4164,lwr_k=600:6.676,lwr_k=700:6.9118,lwr_k=800:7.0974,lwr_k=900:7.1964,lwr_k=1000:7.3181'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.4281,lwr_k=10:36.0949,lwr_k=20:57.3386,lwr_k=30:107.2266,lwr_k=40:799.453,lwr_k=50:23164.2195,lwr_k=100:123.8884,lwr_k=200:21.1939,lwr_k=300:8.1669,lwr_k=400:7.0744,lwr_k=500:7.093,lwr_k=600:6.7631,lwr_k=700:6.9349,lwr_k=800:6.913,lwr_k=900:6.5941,lwr_k=1000:6.6936'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.7359,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0008,lwr_k=40:0.0319,lwr_k=50:2.3849,lwr_k=100:2.6229,lwr_k=200:3.9343,lwr_k=300:4.5705,lwr_k=400:4.8178,lwr_k=500:5.0342,lwr_k=600:5.1825,lwr_k=700:5.3087,lwr_k=800:5.434,lwr_k=900:5.5773,lwr_k=1000:5.6961'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.3842,lwr_k=10:33.5419,lwr_k=20:91.4276,lwr_k=30:182.0235,lwr_k=40:391.226,lwr_k=50:1231733.629,lwr_k=100:350.2276,lwr_k=200:6.8636,lwr_k=300:6.0827,lwr_k=400:5.9389,lwr_k=500:5.803,lwr_k=600:5.8142,lwr_k=700:5.8023,lwr_k=800:5.718,lwr_k=900:5.6565,lwr_k=1000:5.7352'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.6859,lwr_k=10:0.0011,lwr_k=20:0.0684,lwr_k=30:1.3495,lwr_k=40:1.4617,lwr_k=50:5.9813,lwr_k=100:4.6657,lwr_k=200:5.765,lwr_k=300:6.5507,lwr_k=400:6.7295,lwr_k=500:6.6645,lwr_k=600:7.0626,lwr_k=700:7.3445,lwr_k=800:8.0344,lwr_k=900:8.913,lwr_k=1000:9.2341'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.287,lwr_k=10:136.2208,lwr_k=20:221.9388,lwr_k=30:391.2681,lwr_k=40:975.0624,lwr_k=50:915.977,lwr_k=100:32.9911,lwr_k=200:9.6394,lwr_k=300:9.5112,lwr_k=400:9.7961,lwr_k=500:8.7844,lwr_k=600:8.9873,lwr_k=700:8.901,lwr_k=800:8.4225,lwr_k=900:8.1318,lwr_k=1000:8.2546'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.8954,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0001,lwr_k=50:0.0961,lwr_k=100:2.3633,lwr_k=200:3.4345,lwr_k=300:3.7357,lwr_k=400:3.9268,lwr_k=500:4.0389,lwr_k=600:4.1581,lwr_k=700:4.2199,lwr_k=800:4.2873,lwr_k=900:4.3302,lwr_k=1000:4.3794'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:19.6041,lwr_k=10:38.2874,lwr_k=20:141.0365,lwr_k=30:170.1247,lwr_k=40:237.6343,lwr_k=50:8734.4576,lwr_k=100:25.3087,lwr_k=200:18.0821,lwr_k=300:17.6566,lwr_k=400:17.5904,lwr_k=500:17.766,lwr_k=600:17.9242,lwr_k=700:18.3474,lwr_k=800:18.5488,lwr_k=900:18.6397,lwr_k=1000:18.6896'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:7.6942,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0532,lwr_k=40:0.208,lwr_k=50:4.5783,lwr_k=100:2.7195,lwr_k=200:4.3714,lwr_k=300:4.9311,lwr_k=400:5.3042,lwr_k=500:5.5992,lwr_k=600:5.7951,lwr_k=700:5.9205,lwr_k=800:6.1147,lwr_k=900:6.2331,lwr_k=1000:6.3397'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.9295,lwr_k=10:56.9009,lwr_k=20:120.2992,lwr_k=30:924.9973,lwr_k=40:2293.7739,lwr_k=50:6138.2983,lwr_k=100:32.3852,lwr_k=200:39.1973,lwr_k=300:14.7731,lwr_k=400:14.3312,lwr_k=500:8.7719,lwr_k=600:8.5503,lwr_k=700:10.7566,lwr_k=800:8.9461,lwr_k=900:8.9842,lwr_k=1000:8.9923'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_12'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.0571,lwr_k=10:6.4146,lwr_k=20:6.9303,lwr_k=30:7.0066,lwr_k=40:7.1264,lwr_k=50:7.203,lwr_k=100:7.5226,lwr_k=200:7.9784,lwr_k=300:8.1458,lwr_k=400:8.2173,lwr_k=500:8.2909,lwr_k=600:8.3366,lwr_k=700:8.3724,lwr_k=800:8.4341,lwr_k=900:8.504,lwr_k=1000:8.5702'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.3827,lwr_k=10:11.7954,lwr_k=20:10.1391,lwr_k=30:10.0613,lwr_k=40:9.598,lwr_k=50:9.735,lwr_k=100:9.8771,lwr_k=200:10.036,lwr_k=300:10.1972,lwr_k=400:10.2228,lwr_k=500:10.2924,lwr_k=600:10.3205,lwr_k=700:10.3812,lwr_k=800:10.4795,lwr_k=900:10.6028,lwr_k=1000:10.7417'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:13.0485,lwr_k=10:9.6597,lwr_k=20:10.3535,lwr_k=30:10.5432,lwr_k=40:10.8845,lwr_k=50:11.0438,lwr_k=100:11.4168,lwr_k=200:11.7472,lwr_k=300:11.8571,lwr_k=400:11.9157,lwr_k=500:11.96,lwr_k=600:11.9731,lwr_k=700:12.015,lwr_k=800:12.0372,lwr_k=900:12.06,lwr_k=1000:12.0714'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.5089,lwr_k=10:10.8735,lwr_k=20:10.3968,lwr_k=30:9.9146,lwr_k=40:9.6866,lwr_k=50:9.6598,lwr_k=100:9.6715,lwr_k=200:9.6999,lwr_k=300:9.6592,lwr_k=400:9.8306,lwr_k=500:9.9106,lwr_k=600:10.0312,lwr_k=700:10.4124,lwr_k=800:10.3886,lwr_k=900:10.4299,lwr_k=1000:10.4531'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.9282,lwr_k=10:6.8842,lwr_k=20:7.3879,lwr_k=30:7.7617,lwr_k=40:8.0246,lwr_k=50:8.1151,lwr_k=100:8.5911,lwr_k=200:8.9103,lwr_k=300:9.0551,lwr_k=400:9.2054,lwr_k=500:9.2846,lwr_k=600:9.3238,lwr_k=700:9.3575,lwr_k=800:9.4003,lwr_k=900:9.4567,lwr_k=1000:9.5297'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.1273,lwr_k=10:11.1631,lwr_k=20:46.5257,lwr_k=30:37.5501,lwr_k=40:31.516,lwr_k=50:8.9359,lwr_k=100:8.1837,lwr_k=200:7.7324,lwr_k=300:7.6491,lwr_k=400:7.6439,lwr_k=500:7.6496,lwr_k=600:7.6427,lwr_k=700:7.6643,lwr_k=800:7.684,lwr_k=900:7.7149,lwr_k=1000:7.7538'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.2768,lwr_k=10:7.5529,lwr_k=20:8.4459,lwr_k=30:8.9258,lwr_k=40:9.253,lwr_k=50:9.4213,lwr_k=100:10.5403,lwr_k=200:10.9352,lwr_k=300:11.0045,lwr_k=400:11.0487,lwr_k=500:11.1,lwr_k=600:11.092,lwr_k=700:11.1435,lwr_k=800:11.187,lwr_k=900:11.2683,lwr_k=1000:11.3405'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:12.9171,lwr_k=10:13.4724,lwr_k=20:11.9731,lwr_k=30:11.7797,lwr_k=40:12.0016,lwr_k=50:11.7613,lwr_k=100:11.8927,lwr_k=200:11.9798,lwr_k=300:11.9148,lwr_k=400:11.9217,lwr_k=500:11.9434,lwr_k=600:11.9992,lwr_k=700:12.0625,lwr_k=800:12.1904,lwr_k=900:12.261,lwr_k=1000:12.3051'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.8868,lwr_k=10:6.4827,lwr_k=20:7.1329,lwr_k=30:7.1955,lwr_k=40:7.3082,lwr_k=50:7.429,lwr_k=100:7.5278,lwr_k=200:7.7052,lwr_k=300:7.759,lwr_k=400:7.8177,lwr_k=500:7.8708,lwr_k=600:7.9268,lwr_k=700:7.9697,lwr_k=800:8.0127,lwr_k=900:8.0869,lwr_k=1000:8.1647'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:23.3269,lwr_k=10:20.9059,lwr_k=20:17.0315,lwr_k=30:18.4219,lwr_k=40:19.5656,lwr_k=50:20.5317,lwr_k=100:21.8311,lwr_k=200:22.4677,lwr_k=300:22.605,lwr_k=400:22.534,lwr_k=500:22.6251,lwr_k=600:22.6695,lwr_k=700:22.6735,lwr_k=800:22.5731,lwr_k=900:22.6332,lwr_k=1000:22.6632'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:9.6128,lwr_k=10:6.6619,lwr_k=20:7.2499,lwr_k=30:7.5077,lwr_k=40:7.7548,lwr_k=50:7.834,lwr_k=100:8.3428,lwr_k=200:8.5743,lwr_k=300:8.6665,lwr_k=400:8.7228,lwr_k=500:8.7874,lwr_k=600:8.8085,lwr_k=700:8.8978,lwr_k=800:8.9347,lwr_k=900:8.9999,lwr_k=1000:9.0425'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.4183,lwr_k=10:11.5417,lwr_k=20:11.2302,lwr_k=30:11.1932,lwr_k=40:10.9552,lwr_k=50:10.9382,lwr_k=100:11.1494,lwr_k=200:10.8358,lwr_k=300:10.8383,lwr_k=400:11.0075,lwr_k=500:11.0568,lwr_k=600:11.1541,lwr_k=700:11.2347,lwr_k=800:11.306,lwr_k=900:11.3306,lwr_k=1000:11.3616'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_13'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.3246,lwr_k=10:0.0008,lwr_k=20:2.8156,lwr_k=30:3.6952,lwr_k=40:4.3528,lwr_k=50:4.6594,lwr_k=100:5.2275,lwr_k=200:5.6011,lwr_k=300:5.691,lwr_k=400:5.7281,lwr_k=500:5.753,lwr_k=600:5.7921,lwr_k=700:5.8456,lwr_k=800:5.8786,lwr_k=900:5.914,lwr_k=1000:5.9564'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.0937,lwr_k=10:15537.5585,lwr_k=20:13.6064,lwr_k=30:9.9313,lwr_k=40:9.1072,lwr_k=50:8.3509,lwr_k=100:8.0887,lwr_k=200:8.1387,lwr_k=300:8.1763,lwr_k=400:8.2364,lwr_k=500:8.2684,lwr_k=600:8.3388,lwr_k=700:8.3897,lwr_k=800:8.4211,lwr_k=900:8.4444,lwr_k=1000:8.5089'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.4344,lwr_k=10:0.0001,lwr_k=20:2.711,lwr_k=30:3.7196,lwr_k=40:4.2429,lwr_k=50:4.5344,lwr_k=100:5.5694,lwr_k=200:6.1746,lwr_k=300:6.7287,lwr_k=400:6.9688,lwr_k=500:7.1317,lwr_k=600:7.2374,lwr_k=700:7.3779,lwr_k=800:7.4916,lwr_k=900:7.5784,lwr_k=1000:7.6355'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.4097,lwr_k=10:2189.3745,lwr_k=20:10.8848,lwr_k=30:6.7284,lwr_k=40:6.4294,lwr_k=50:6.2875,lwr_k=100:6.3002,lwr_k=200:6.2876,lwr_k=300:6.3343,lwr_k=400:6.3594,lwr_k=500:6.4081,lwr_k=600:6.4203,lwr_k=700:6.4746,lwr_k=800:6.4815,lwr_k=900:6.4964,lwr_k=1000:6.514'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.8277,lwr_k=10:0.0,lwr_k=20:2.6837,lwr_k=30:3.5948,lwr_k=40:4.2985,lwr_k=50:4.6234,lwr_k=100:5.8214,lwr_k=200:6.5644,lwr_k=300:6.9933,lwr_k=400:7.2983,lwr_k=500:7.5036,lwr_k=600:7.6461,lwr_k=700:7.8814,lwr_k=800:8.0681,lwr_k=900:8.1805,lwr_k=1000:8.3482'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.9666,lwr_k=10:56640.5745,lwr_k=20:8.1754,lwr_k=30:6.7382,lwr_k=40:6.1809,lwr_k=50:6.1268,lwr_k=100:5.4942,lwr_k=200:5.2648,lwr_k=300:5.2986,lwr_k=400:5.3067,lwr_k=500:5.2995,lwr_k=600:5.3068,lwr_k=700:5.292,lwr_k=800:5.3234,lwr_k=900:5.3607,lwr_k=1000:5.3685'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.1697,lwr_k=10:0.0,lwr_k=20:2.8009,lwr_k=30:3.8937,lwr_k=40:4.7233,lwr_k=50:5.188,lwr_k=100:6.2619,lwr_k=200:6.8241,lwr_k=300:7.1599,lwr_k=400:7.4303,lwr_k=500:7.5551,lwr_k=600:7.6628,lwr_k=700:7.7691,lwr_k=800:7.8506,lwr_k=900:7.9481,lwr_k=1000:8.0463'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.6989,lwr_k=10:2596.5578,lwr_k=20:10.2997,lwr_k=30:8.0677,lwr_k=40:7.7453,lwr_k=50:7.3035,lwr_k=100:6.8995,lwr_k=200:7.2892,lwr_k=300:7.2052,lwr_k=400:7.1338,lwr_k=500:7.1299,lwr_k=600:7.1614,lwr_k=700:7.1434,lwr_k=800:7.1719,lwr_k=900:7.1704,lwr_k=1000:7.1945'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.4135,lwr_k=10:0.0,lwr_k=20:2.8009,lwr_k=30:3.3842,lwr_k=40:3.8987,lwr_k=50:4.0636,lwr_k=100:4.4775,lwr_k=200:4.7182,lwr_k=300:4.8172,lwr_k=400:4.888,lwr_k=500:4.9176,lwr_k=600:4.9373,lwr_k=700:4.9615,lwr_k=800:4.9865,lwr_k=900:5.01,lwr_k=1000:5.0384'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:19.8932,lwr_k=10:126637.483,lwr_k=20:14.8468,lwr_k=30:17.7211,lwr_k=40:17.4927,lwr_k=50:17.4123,lwr_k=100:18.6565,lwr_k=200:19.2362,lwr_k=300:19.2289,lwr_k=400:19.1827,lwr_k=500:19.2351,lwr_k=600:19.2104,lwr_k=700:19.1172,lwr_k=800:19.1235,lwr_k=900:19.1414,lwr_k=1000:19.1865'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:9.0099,lwr_k=10:0.0,lwr_k=20:2.8719,lwr_k=30:3.8415,lwr_k=40:4.2388,lwr_k=50:4.5808,lwr_k=100:5.7612,lwr_k=200:6.5738,lwr_k=300:6.8552,lwr_k=400:6.9764,lwr_k=500:7.0377,lwr_k=600:7.1144,lwr_k=700:7.1833,lwr_k=800:7.2345,lwr_k=900:7.2994,lwr_k=1000:7.3625'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.863,lwr_k=10:13234.667,lwr_k=20:13.5746,lwr_k=30:9.3251,lwr_k=40:7.7655,lwr_k=50:7.3425,lwr_k=100:7.1137,lwr_k=200:7.6534,lwr_k=300:7.8614,lwr_k=400:7.8769,lwr_k=500:7.8751,lwr_k=600:7.849,lwr_k=700:7.8886,lwr_k=800:7.9613,lwr_k=900:7.9914,lwr_k=1000:8.037'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_14'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:54.084,lwr_k=10:45.4625,lwr_k=20:46.8018,lwr_k=30:46.0773,lwr_k=40:46.1291,lwr_k=50:46.4224,lwr_k=100:46.403,lwr_k=200:46.7002,lwr_k=300:47.2934,lwr_k=400:48.2813,lwr_k=500:48.7317,lwr_k=600:48.8826,lwr_k=700:48.9932,lwr_k=800:49.1404,lwr_k=900:49.3812,lwr_k=1000:49.5782'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:85.2198,lwr_k=10:71.5143,lwr_k=20:74.7992,lwr_k=30:71.1496,lwr_k=40:71.2966,lwr_k=50:69.2294,lwr_k=100:71.4913,lwr_k=200:72.2593,lwr_k=300:72.507,lwr_k=400:74.4666,lwr_k=500:75.4074,lwr_k=600:75.8896,lwr_k=700:76.2789,lwr_k=800:76.5783,lwr_k=900:76.9026,lwr_k=1000:77.2272'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:233.0272,lwr_k=10:215.7284,lwr_k=20:231.6988,lwr_k=30:226.182,lwr_k=40:228.1956,lwr_k=50:225.6936,lwr_k=100:229.8587,lwr_k=200:230.0001,lwr_k=300:229.38,lwr_k=400:229.7132,lwr_k=500:230.003,lwr_k=600:230.0787,lwr_k=700:230.3375,lwr_k=800:230.2145,lwr_k=900:230.0876,lwr_k=1000:230.1776'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:197.783,lwr_k=10:218.3368,lwr_k=20:207.3408,lwr_k=30:202.6823,lwr_k=40:202.6257,lwr_k=50:201.0194,lwr_k=100:198.5066,lwr_k=200:197.1093,lwr_k=300:197.1745,lwr_k=400:197.0049,lwr_k=500:196.724,lwr_k=600:197.0813,lwr_k=700:196.9087,lwr_k=800:197.0405,lwr_k=900:197.0524,lwr_k=1000:197.0073'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:53.7655,lwr_k=10:24.919,lwr_k=20:29.1845,lwr_k=30:29.7565,lwr_k=40:30.4758,lwr_k=50:31.0758,lwr_k=100:32.595,lwr_k=200:34.5725,lwr_k=300:35.9788,lwr_k=400:36.9359,lwr_k=500:37.6431,lwr_k=600:38.2362,lwr_k=700:38.8999,lwr_k=800:39.4703,lwr_k=900:40.0904,lwr_k=1000:40.6661'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:38.3867,lwr_k=10:34.1963,lwr_k=20:30.6156,lwr_k=30:29.9575,lwr_k=40:29.4745,lwr_k=50:29.1784,lwr_k=100:29.5653,lwr_k=200:29.4956,lwr_k=300:29.8051,lwr_k=400:29.9674,lwr_k=500:29.946,lwr_k=600:30.1594,lwr_k=700:30.2537,lwr_k=800:30.3593,lwr_k=900:30.4206,lwr_k=1000:30.4512'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:18.7512,lwr_k=10:9.2529,lwr_k=20:11.9577,lwr_k=30:13.0953,lwr_k=40:13.9008,lwr_k=50:14.3492,lwr_k=100:15.3698,lwr_k=200:15.7919,lwr_k=300:16.1662,lwr_k=400:16.4039,lwr_k=500:16.5597,lwr_k=600:16.6456,lwr_k=700:16.6833,lwr_k=800:16.735,lwr_k=900:16.8463,lwr_k=1000:16.9634'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:15.6472,lwr_k=10:19.4741,lwr_k=20:16.083,lwr_k=30:15.9339,lwr_k=40:15.2699,lwr_k=50:14.7946,lwr_k=100:13.9602,lwr_k=200:13.8254,lwr_k=300:13.9033,lwr_k=400:13.9218,lwr_k=500:13.9987,lwr_k=600:14.0721,lwr_k=700:14.1276,lwr_k=800:14.1075,lwr_k=900:14.127,lwr_k=1000:14.151'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.3863,lwr_k=10:7.0467,lwr_k=20:8.6694,lwr_k=30:8.9443,lwr_k=40:9.2093,lwr_k=50:9.3157,lwr_k=100:9.6661,lwr_k=200:9.951,lwr_k=300:10.1303,lwr_k=400:10.2842,lwr_k=500:10.4301,lwr_k=600:10.524,lwr_k=700:10.6378,lwr_k=800:10.7418,lwr_k=900:10.8379,lwr_k=1000:10.9086'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:37.7655,lwr_k=10:31.9226,lwr_k=20:22.8442,lwr_k=30:23.4153,lwr_k=40:25.6031,lwr_k=50:25.8131,lwr_k=100:27.2452,lwr_k=200:29.5302,lwr_k=300:30.6304,lwr_k=400:31.423,lwr_k=500:32.2315,lwr_k=600:32.7308,lwr_k=700:33.1871,lwr_k=800:33.5494,lwr_k=900:33.9038,lwr_k=1000:34.185'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:41.4221,lwr_k=10:19.1401,lwr_k=20:21.1699,lwr_k=30:22.1968,lwr_k=40:22.8991,lwr_k=50:23.341,lwr_k=100:24.5144,lwr_k=200:25.9554,lwr_k=300:26.8792,lwr_k=400:27.5642,lwr_k=500:28.2314,lwr_k=600:28.6735,lwr_k=700:29.1594,lwr_k=800:29.5578,lwr_k=900:29.9563,lwr_k=1000:30.3045'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:35.9271,lwr_k=10:32.4721,lwr_k=20:29.0663,lwr_k=30:28.1128,lwr_k=40:27.7535,lwr_k=50:26.893,lwr_k=100:24.5889,lwr_k=200:23.3893,lwr_k=300:23.0693,lwr_k=400:23.2342,lwr_k=500:23.4561,lwr_k=600:23.6889,lwr_k=700:23.8135,lwr_k=800:24.0771,lwr_k=900:24.3173,lwr_k=1000:24.6184'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_15'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.523,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0011,lwr_k=40:1.0147,lwr_k=50:1.7186,lwr_k=100:3.2101,lwr_k=200:4.2681,lwr_k=300:4.6876,lwr_k=400:4.9315,lwr_k=500:5.1398,lwr_k=600:5.2838,lwr_k=700:5.3994,lwr_k=800:5.4692,lwr_k=900:5.54,lwr_k=1000:5.6018'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.3374,lwr_k=10:30.7632,lwr_k=20:56.5211,lwr_k=30:7796.3019,lwr_k=40:17.5462,lwr_k=50:10.2543,lwr_k=100:7.8153,lwr_k=200:7.8164,lwr_k=300:8.1007,lwr_k=400:8.3174,lwr_k=500:8.4341,lwr_k=600:8.5031,lwr_k=700:8.5705,lwr_k=800:8.7338,lwr_k=900:8.8559,lwr_k=1000:8.9946'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:18.9347,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0029,lwr_k=40:1.5506,lwr_k=50:2.4451,lwr_k=100:5.3927,lwr_k=200:8.3154,lwr_k=300:9.4138,lwr_k=400:10.2096,lwr_k=500:10.676,lwr_k=600:11.219,lwr_k=700:11.6438,lwr_k=800:12.0629,lwr_k=900:12.3935,lwr_k=1000:12.7165'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:15.5411,lwr_k=10:40.8879,lwr_k=20:62.6975,lwr_k=30:16575.5716,lwr_k=40:28.6322,lwr_k=50:17.4316,lwr_k=100:8.7603,lwr_k=200:7.5024,lwr_k=300:7.7093,lwr_k=400:8.0114,lwr_k=500:8.2531,lwr_k=600:8.5023,lwr_k=700:8.9379,lwr_k=800:9.2075,lwr_k=900:9.3337,lwr_k=1000:9.4086'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:16.2533,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0009,lwr_k=40:1.2996,lwr_k=50:2.5101,lwr_k=100:5.5775,lwr_k=200:7.7758,lwr_k=300:8.8562,lwr_k=400:9.5928,lwr_k=500:10.0854,lwr_k=600:10.5034,lwr_k=700:10.9289,lwr_k=800:11.1427,lwr_k=900:11.4227,lwr_k=1000:11.6416'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.8806,lwr_k=10:37.3845,lwr_k=20:47.7171,lwr_k=30:14728.0715,lwr_k=40:29.2366,lwr_k=50:21.1541,lwr_k=100:8.7968,lwr_k=200:7.4391,lwr_k=300:6.9886,lwr_k=400:7.1151,lwr_k=500:7.0241,lwr_k=600:7.0693,lwr_k=700:7.2858,lwr_k=800:7.4135,lwr_k=900:7.4554,lwr_k=1000:7.5242'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:18.9198,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0009,lwr_k=40:1.5524,lwr_k=50:2.6459,lwr_k=100:5.7074,lwr_k=200:8.502,lwr_k=300:9.7126,lwr_k=400:10.4114,lwr_k=500:10.9303,lwr_k=600:11.4483,lwr_k=700:11.8716,lwr_k=800:12.2638,lwr_k=900:12.5619,lwr_k=1000:12.8486'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:15.5253,lwr_k=10:36.9837,lwr_k=20:79.9553,lwr_k=30:6081.126,lwr_k=40:29.8593,lwr_k=50:18.3234,lwr_k=100:10.6891,lwr_k=200:9.3944,lwr_k=300:9.634,lwr_k=400:9.6603,lwr_k=500:9.7788,lwr_k=600:10.0327,lwr_k=700:10.299,lwr_k=800:10.5129,lwr_k=900:10.5978,lwr_k=1000:10.7153'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.1518,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0098,lwr_k=40:1.3403,lwr_k=50:2.0161,lwr_k=100:4.0152,lwr_k=200:5.3359,lwr_k=300:5.808,lwr_k=400:6.0937,lwr_k=500:6.3406,lwr_k=600:6.5314,lwr_k=700:6.673,lwr_k=800:6.8135,lwr_k=900:6.893,lwr_k=1000:6.9937'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:30.5648,lwr_k=10:35.2817,lwr_k=20:62.2322,lwr_k=30:12512.7957,lwr_k=40:33.4442,lwr_k=50:24.1849,lwr_k=100:18.5528,lwr_k=200:20.1244,lwr_k=300:20.7679,lwr_k=400:21.9527,lwr_k=500:22.1503,lwr_k=600:22.4482,lwr_k=700:23.1409,lwr_k=800:23.6022,lwr_k=900:24.0418,lwr_k=1000:24.2104'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:10.004,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0003,lwr_k=40:1.1018,lwr_k=50:1.9848,lwr_k=100:3.7408,lwr_k=200:5.3936,lwr_k=300:6.0623,lwr_k=400:6.5228,lwr_k=500:6.807,lwr_k=600:7.0307,lwr_k=700:7.2083,lwr_k=800:7.3043,lwr_k=900:7.4501,lwr_k=1000:7.5723'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:13.39,lwr_k=10:33.2501,lwr_k=20:30.9123,lwr_k=30:11481.3665,lwr_k=40:21.0928,lwr_k=50:11.6506,lwr_k=100:9.3556,lwr_k=200:9.1507,lwr_k=300:9.609,lwr_k=400:10.0105,lwr_k=500:10.3224,lwr_k=600:10.7128,lwr_k=700:10.8506,lwr_k=800:11.0333,lwr_k=900:11.1743,lwr_k=1000:11.3574'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_16'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.152,lwr_k=10:6.1766,lwr_k=20:6.942,lwr_k=30:7.1088,lwr_k=40:7.2817,lwr_k=50:7.3896,lwr_k=100:7.484,lwr_k=200:7.5467,lwr_k=300:7.6066,lwr_k=400:7.679,lwr_k=500:7.7232,lwr_k=600:7.7363,lwr_k=700:7.7605,lwr_k=800:7.7693,lwr_k=900:7.7969,lwr_k=1000:7.8076'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.4163,lwr_k=10:11.6758,lwr_k=20:10.1285,lwr_k=30:10.2652,lwr_k=40:9.8567,lwr_k=50:9.3302,lwr_k=100:9.3902,lwr_k=200:9.5243,lwr_k=300:9.6007,lwr_k=400:9.6755,lwr_k=500:9.7445,lwr_k=600:9.7813,lwr_k=700:9.8367,lwr_k=800:9.8847,lwr_k=900:9.9193,lwr_k=1000:9.9421'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.3174,lwr_k=10:5.3106,lwr_k=20:6.4164,lwr_k=30:6.8901,lwr_k=40:7.2216,lwr_k=50:7.4394,lwr_k=100:8.118,lwr_k=200:8.5534,lwr_k=300:8.7909,lwr_k=400:8.9314,lwr_k=500:8.9312,lwr_k=600:8.9883,lwr_k=700:9.0235,lwr_k=800:9.028,lwr_k=900:9.0574,lwr_k=1000:9.0963'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.7346,lwr_k=10:9.6112,lwr_k=20:8.123,lwr_k=30:7.5254,lwr_k=40:7.0483,lwr_k=50:6.9379,lwr_k=100:7.0554,lwr_k=200:7.1561,lwr_k=300:7.2839,lwr_k=400:7.239,lwr_k=500:7.3122,lwr_k=600:7.3244,lwr_k=700:7.2991,lwr_k=800:7.3497,lwr_k=900:7.2739,lwr_k=1000:7.2928'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.4067,lwr_k=10:4.8455,lwr_k=20:5.6747,lwr_k=30:6.0247,lwr_k=40:6.5243,lwr_k=50:6.8505,lwr_k=100:7.7029,lwr_k=200:8.5693,lwr_k=300:8.6219,lwr_k=400:8.763,lwr_k=500:8.8828,lwr_k=600:8.9105,lwr_k=700:8.9286,lwr_k=800:8.9671,lwr_k=900:8.9624,lwr_k=1000:8.9851'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.8478,lwr_k=10:7.7656,lwr_k=20:7.5165,lwr_k=30:6.9901,lwr_k=40:6.6426,lwr_k=50:6.6297,lwr_k=100:6.1353,lwr_k=200:6.075,lwr_k=300:5.9358,lwr_k=400:5.9996,lwr_k=500:6.0075,lwr_k=600:6.0361,lwr_k=700:6.0691,lwr_k=800:6.8947,lwr_k=900:6.4844,lwr_k=1000:7.1926'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.4125,lwr_k=10:5.5783,lwr_k=20:6.4247,lwr_k=30:6.7745,lwr_k=40:6.9813,lwr_k=50:7.0973,lwr_k=100:7.3151,lwr_k=200:7.5969,lwr_k=300:7.6286,lwr_k=400:7.7083,lwr_k=500:7.7594,lwr_k=600:7.8115,lwr_k=700:7.8579,lwr_k=800:7.9041,lwr_k=900:7.946,lwr_k=1000:7.9761'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.1455,lwr_k=10:9.8595,lwr_k=20:9.5673,lwr_k=30:9.31,lwr_k=40:9.3685,lwr_k=50:9.2478,lwr_k=100:9.1063,lwr_k=200:9.6032,lwr_k=300:9.7108,lwr_k=400:9.7548,lwr_k=500:9.7496,lwr_k=600:9.8069,lwr_k=700:9.8373,lwr_k=800:9.8585,lwr_k=900:9.8645,lwr_k=1000:9.8894'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.5708,lwr_k=10:6.1253,lwr_k=20:6.9487,lwr_k=30:7.2372,lwr_k=40:7.4463,lwr_k=50:7.5214,lwr_k=100:7.7039,lwr_k=200:7.8964,lwr_k=300:7.9504,lwr_k=400:7.9643,lwr_k=500:8.0069,lwr_k=600:8.141,lwr_k=700:8.4846,lwr_k=800:8.8333,lwr_k=900:8.9965,lwr_k=1000:9.1337'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:18.2566,lwr_k=10:23.2577,lwr_k=20:21.5474,lwr_k=30:23.5678,lwr_k=40:18.8301,lwr_k=50:18.6523,lwr_k=100:17.8058,lwr_k=200:18.1135,lwr_k=300:18.1681,lwr_k=400:18.1308,lwr_k=500:18.06,lwr_k=600:17.795,lwr_k=700:17.5147,lwr_k=800:17.7599,lwr_k=900:17.7513,lwr_k=1000:17.8155'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:8.4667,lwr_k=10:6.1595,lwr_k=20:6.7954,lwr_k=30:6.9255,lwr_k=40:6.9594,lwr_k=50:7.0344,lwr_k=100:7.1972,lwr_k=200:7.4595,lwr_k=300:7.6798,lwr_k=400:7.7015,lwr_k=500:7.712,lwr_k=600:7.7504,lwr_k=700:7.7638,lwr_k=800:7.8029,lwr_k=900:7.8146,lwr_k=1000:7.8503'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:13.6631,lwr_k=10:12.5466,lwr_k=20:11.8523,lwr_k=30:11.6532,lwr_k=40:11.6797,lwr_k=50:11.7068,lwr_k=100:11.8133,lwr_k=200:12.0337,lwr_k=300:12.492,lwr_k=400:12.6613,lwr_k=500:12.751,lwr_k=600:12.7981,lwr_k=700:12.8537,lwr_k=800:12.8381,lwr_k=900:12.8656,lwr_k=1000:12.8505'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_17'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.5933,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.006,lwr_k=40:0.6715,lwr_k=50:1.3877,lwr_k=100:2.8208,lwr_k=200:3.5761,lwr_k=300:3.7722,lwr_k=400:3.8902,lwr_k=500:3.9473,lwr_k=600:4.0261,lwr_k=700:4.0664,lwr_k=800:4.1023,lwr_k=900:4.1427,lwr_k=1000:4.1779'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:6.4908,lwr_k=10:51.8431,lwr_k=20:60.9652,lwr_k=30:192.1818,lwr_k=40:79.7361,lwr_k=50:22.3189,lwr_k=100:8.8236,lwr_k=200:7.002,lwr_k=300:6.4462,lwr_k=400:6.2757,lwr_k=500:6.1549,lwr_k=600:6.1231,lwr_k=700:6.1171,lwr_k=800:6.1005,lwr_k=900:6.1012,lwr_k=1000:6.0975'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.3187,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0001,lwr_k=40:0.8492,lwr_k=50:1.6803,lwr_k=100:4.0363,lwr_k=200:5.7999,lwr_k=300:6.6579,lwr_k=400:7.003,lwr_k=500:7.2157,lwr_k=600:7.4394,lwr_k=700:7.5564,lwr_k=800:7.6932,lwr_k=900:7.7875,lwr_k=1000:7.9011'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.8566,lwr_k=10:25.9887,lwr_k=20:53.1671,lwr_k=30:284.7901,lwr_k=40:60.7792,lwr_k=50:41.97,lwr_k=100:8.9062,lwr_k=200:6.8434,lwr_k=300:6.6877,lwr_k=400:6.6568,lwr_k=500:6.7282,lwr_k=600:6.7982,lwr_k=700:6.7618,lwr_k=800:6.8352,lwr_k=900:6.8893,lwr_k=1000:6.9404'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.7031,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0093,lwr_k=40:0.8792,lwr_k=50:1.869,lwr_k=100:4.0449,lwr_k=200:5.3748,lwr_k=300:5.7876,lwr_k=400:6.0817,lwr_k=500:6.3197,lwr_k=600:6.4794,lwr_k=700:6.6613,lwr_k=800:6.7838,lwr_k=900:6.8721,lwr_k=1000:6.9585'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.212,lwr_k=10:34.4778,lwr_k=20:67.1106,lwr_k=30:99.4604,lwr_k=40:47.7814,lwr_k=50:16.9146,lwr_k=100:6.9242,lwr_k=200:5.7839,lwr_k=300:5.7333,lwr_k=400:5.6412,lwr_k=500:5.6318,lwr_k=600:5.6116,lwr_k=700:5.6212,lwr_k=800:5.6437,lwr_k=900:5.6327,lwr_k=1000:5.6359'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.8487,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0031,lwr_k=40:0.6757,lwr_k=50:1.5386,lwr_k=100:3.5613,lwr_k=200:4.8824,lwr_k=300:5.432,lwr_k=400:5.7618,lwr_k=500:6.0087,lwr_k=600:6.3016,lwr_k=700:6.4106,lwr_k=800:6.4979,lwr_k=900:6.6326,lwr_k=1000:6.7846'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.9833,lwr_k=10:17.7381,lwr_k=20:72.0229,lwr_k=30:384.8797,lwr_k=40:94.813,lwr_k=50:17.7091,lwr_k=100:8.7688,lwr_k=200:7.7344,lwr_k=300:7.4239,lwr_k=400:7.438,lwr_k=500:7.2957,lwr_k=600:7.0777,lwr_k=700:7.068,lwr_k=800:7.0438,lwr_k=900:7.0336,lwr_k=1000:7.0537'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.6128,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0001,lwr_k=40:0.6894,lwr_k=50:1.4957,lwr_k=100:2.6299,lwr_k=200:3.2423,lwr_k=300:3.4897,lwr_k=400:3.664,lwr_k=500:3.7572,lwr_k=600:3.8487,lwr_k=700:3.926,lwr_k=800:3.9534,lwr_k=900:3.9922,lwr_k=1000:4.0275'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:18.6873,lwr_k=10:63.632,lwr_k=20:145.5108,lwr_k=30:317.495,lwr_k=40:66.6449,lwr_k=50:20.0456,lwr_k=100:16.1664,lwr_k=200:15.6338,lwr_k=300:15.9932,lwr_k=400:15.9421,lwr_k=500:15.9597,lwr_k=600:16.0006,lwr_k=700:15.9928,lwr_k=800:16.0356,lwr_k=900:16.0545,lwr_k=1000:16.0282'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:9.6678,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0185,lwr_k=40:0.9848,lwr_k=50:1.8389,lwr_k=100:4.7012,lwr_k=200:6.2458,lwr_k=300:6.7416,lwr_k=400:7.0353,lwr_k=500:7.2233,lwr_k=600:7.4114,lwr_k=700:7.5432,lwr_k=800:7.637,lwr_k=900:7.7399,lwr_k=1000:7.8226'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.9466,lwr_k=10:27.2727,lwr_k=20:95.715,lwr_k=30:278.2401,lwr_k=40:75.1592,lwr_k=50:26.8834,lwr_k=100:9.4179,lwr_k=200:8.4899,lwr_k=300:8.4037,lwr_k=400:8.4324,lwr_k=500:8.4602,lwr_k=600:8.621,lwr_k=700:8.7902,lwr_k=800:8.8686,lwr_k=900:8.9964,lwr_k=1000:9.1505'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_18'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.378,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0191,lwr_k=40:2.51,lwr_k=50:1.5167,lwr_k=100:3.7217,lwr_k=200:5.2976,lwr_k=300:5.8292,lwr_k=400:6.126,lwr_k=500:6.3669,lwr_k=600:6.5798,lwr_k=700:6.7722,lwr_k=800:6.9025,lwr_k=900:7.0271,lwr_k=1000:7.1292'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.9798,lwr_k=10:55.5387,lwr_k=20:55.3059,lwr_k=30:115.793,lwr_k=40:109.248,lwr_k=50:31.4735,lwr_k=100:10.4496,lwr_k=200:8.7481,lwr_k=300:9.2105,lwr_k=400:9.25,lwr_k=500:9.2982,lwr_k=600:9.4983,lwr_k=700:9.5935,lwr_k=800:9.7027,lwr_k=900:9.8169,lwr_k=1000:9.9528'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.8232,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0036,lwr_k=40:0.4647,lwr_k=50:1.2983,lwr_k=100:3.7587,lwr_k=200:5.4576,lwr_k=300:6.1641,lwr_k=400:6.5967,lwr_k=500:6.8494,lwr_k=600:7.0637,lwr_k=700:7.1457,lwr_k=800:7.2835,lwr_k=900:7.3839,lwr_k=1000:7.4477'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.5794,lwr_k=10:62.8192,lwr_k=20:45.9765,lwr_k=30:95.9622,lwr_k=40:87.6186,lwr_k=50:27.614,lwr_k=100:11.2823,lwr_k=200:8.2932,lwr_k=300:7.8419,lwr_k=400:7.6988,lwr_k=500:7.8604,lwr_k=600:7.83,lwr_k=700:7.9599,lwr_k=800:8.0489,lwr_k=900:8.1317,lwr_k=1000:8.157'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.5661,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.03,lwr_k=40:0.8391,lwr_k=50:1.8279,lwr_k=100:4.5887,lwr_k=200:6.6683,lwr_k=300:7.5778,lwr_k=400:8.1289,lwr_k=500:8.4783,lwr_k=600:8.7566,lwr_k=700:9.0492,lwr_k=800:9.3229,lwr_k=900:9.4543,lwr_k=1000:9.6017'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.5276,lwr_k=10:42.1676,lwr_k=20:41.8978,lwr_k=30:107.7627,lwr_k=40:155.9848,lwr_k=50:33.7622,lwr_k=100:10.5631,lwr_k=200:8.6037,lwr_k=300:8.2755,lwr_k=400:8.0848,lwr_k=500:7.966,lwr_k=600:8.1334,lwr_k=700:8.1188,lwr_k=800:8.1203,lwr_k=900:8.1761,lwr_k=1000:8.2944'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.1689,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0428,lwr_k=40:1.6764,lwr_k=50:1.7207,lwr_k=100:4.447,lwr_k=200:5.8184,lwr_k=300:6.5448,lwr_k=400:7.0043,lwr_k=500:7.3613,lwr_k=600:7.6268,lwr_k=700:7.7557,lwr_k=800:7.9562,lwr_k=900:8.0906,lwr_k=1000:8.2079'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:12.2558,lwr_k=10:39.8902,lwr_k=20:74.8975,lwr_k=30:132.5745,lwr_k=40:119.2613,lwr_k=50:28.0674,lwr_k=100:10.8184,lwr_k=200:9.5407,lwr_k=300:9.66,lwr_k=400:9.6601,lwr_k=500:9.7018,lwr_k=600:9.8653,lwr_k=700:9.8472,lwr_k=800:10.1036,lwr_k=900:10.2397,lwr_k=1000:10.402'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.9621,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0011,lwr_k=40:0.4897,lwr_k=50:1.4378,lwr_k=100:3.9023,lwr_k=200:5.289,lwr_k=300:5.9119,lwr_k=400:6.1913,lwr_k=500:6.4699,lwr_k=600:6.6927,lwr_k=700:6.8188,lwr_k=800:6.8898,lwr_k=900:6.9511,lwr_k=1000:7.0052'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:20.1725,lwr_k=10:37.747,lwr_k=20:195.0125,lwr_k=30:246.5511,lwr_k=40:241.5222,lwr_k=50:50.5832,lwr_k=100:20.0879,lwr_k=200:17.239,lwr_k=300:17.1004,lwr_k=400:17.7296,lwr_k=500:18.572,lwr_k=600:19.0673,lwr_k=700:19.1832,lwr_k=800:19.3417,lwr_k=900:19.3603,lwr_k=1000:19.3178'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:10.4365,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0095,lwr_k=40:0.4486,lwr_k=50:1.5027,lwr_k=100:3.7172,lwr_k=200:5.3079,lwr_k=300:6.273,lwr_k=400:6.7232,lwr_k=500:7.179,lwr_k=600:7.5335,lwr_k=700:7.809,lwr_k=800:7.963,lwr_k=900:8.1423,lwr_k=1000:8.3457'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.6026,lwr_k=10:41.0439,lwr_k=20:127.1657,lwr_k=30:231.4292,lwr_k=40:143.6337,lwr_k=50:40.9702,lwr_k=100:36.8171,lwr_k=200:12.3122,lwr_k=300:13.5247,lwr_k=400:20.2126,lwr_k=500:9.0905,lwr_k=600:8.941,lwr_k=700:8.7914,lwr_k=800:9.8544,lwr_k=900:10.4084,lwr_k=1000:10.6123'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_19'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.2575,lwr_k=10:6.797,lwr_k=20:7.2224,lwr_k=30:7.3963,lwr_k=40:7.5005,lwr_k=50:7.5733,lwr_k=100:7.7177,lwr_k=200:7.8765,lwr_k=300:7.8919,lwr_k=400:7.9323,lwr_k=500:7.9797,lwr_k=600:7.9806,lwr_k=700:7.9882,lwr_k=800:7.9942,lwr_k=900:8.0018,lwr_k=1000:8.007'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.7108,lwr_k=10:10.5764,lwr_k=20:10.0699,lwr_k=30:9.8918,lwr_k=40:9.8248,lwr_k=50:9.7249,lwr_k=100:9.5867,lwr_k=200:9.6454,lwr_k=300:9.6576,lwr_k=400:9.6229,lwr_k=500:9.629,lwr_k=600:9.5962,lwr_k=700:9.6009,lwr_k=800:9.588,lwr_k=900:9.5887,lwr_k=1000:9.6136'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:28.1426,lwr_k=10:21.0822,lwr_k=20:22.5524,lwr_k=30:23.5377,lwr_k=40:24.0061,lwr_k=50:24.249,lwr_k=100:25.2358,lwr_k=200:25.4308,lwr_k=300:25.5822,lwr_k=400:25.6447,lwr_k=500:25.7775,lwr_k=600:25.8127,lwr_k=700:25.8533,lwr_k=800:25.8601,lwr_k=900:25.8693,lwr_k=1000:25.8573'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:23.3885,lwr_k=10:23.6564,lwr_k=20:46.9561,lwr_k=30:43.6121,lwr_k=40:22.3981,lwr_k=50:22.2352,lwr_k=100:21.8275,lwr_k=200:21.6423,lwr_k=300:21.7092,lwr_k=400:21.713,lwr_k=500:21.6802,lwr_k=600:21.752,lwr_k=700:21.7284,lwr_k=800:21.7587,lwr_k=900:21.7511,lwr_k=1000:21.7563'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.2069,lwr_k=10:7.7761,lwr_k=20:8.0211,lwr_k=30:8.213,lwr_k=40:8.2365,lwr_k=50:8.3213,lwr_k=100:8.3746,lwr_k=200:8.4431,lwr_k=300:8.5859,lwr_k=400:8.6432,lwr_k=500:8.6405,lwr_k=600:8.6554,lwr_k=700:8.6609,lwr_k=800:8.671,lwr_k=900:8.6954,lwr_k=1000:8.7247'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.9194,lwr_k=10:9.4124,lwr_k=20:8.8097,lwr_k=30:8.4733,lwr_k=40:8.5872,lwr_k=50:8.7681,lwr_k=100:8.5362,lwr_k=200:8.7171,lwr_k=300:8.3321,lwr_k=400:8.4333,lwr_k=500:8.4374,lwr_k=600:8.481,lwr_k=700:8.5258,lwr_k=800:8.5196,lwr_k=900:8.5446,lwr_k=1000:8.5605'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.9637,lwr_k=10:7.9761,lwr_k=20:8.3747,lwr_k=30:8.7166,lwr_k=40:9.1149,lwr_k=50:9.5403,lwr_k=100:10.2885,lwr_k=200:10.8692,lwr_k=300:11.0672,lwr_k=400:11.1605,lwr_k=500:11.2486,lwr_k=600:11.2878,lwr_k=700:11.2965,lwr_k=800:11.3209,lwr_k=900:11.3281,lwr_k=1000:11.3509'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.2743,lwr_k=10:10.7459,lwr_k=20:10.4976,lwr_k=30:10.3125,lwr_k=40:10.1411,lwr_k=50:10.1287,lwr_k=100:10.0721,lwr_k=200:9.8524,lwr_k=300:9.79,lwr_k=400:9.9126,lwr_k=500:9.9519,lwr_k=600:9.996,lwr_k=700:10.0037,lwr_k=800:10.0096,lwr_k=900:10.0111,lwr_k=1000:10.0068'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.424,lwr_k=10:9.2034,lwr_k=20:9.6504,lwr_k=30:9.9087,lwr_k=40:9.9927,lwr_k=50:10.069,lwr_k=100:10.2308,lwr_k=200:10.421,lwr_k=300:10.5982,lwr_k=400:10.8192,lwr_k=500:10.879,lwr_k=600:10.9351,lwr_k=700:10.9631,lwr_k=800:10.9738,lwr_k=900:10.9902,lwr_k=1000:11.0034'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:19.866,lwr_k=10:32.1322,lwr_k=20:23.355,lwr_k=30:22.1723,lwr_k=40:22.4025,lwr_k=50:21.8683,lwr_k=100:20.8093,lwr_k=200:19.7139,lwr_k=300:19.2125,lwr_k=400:18.9637,lwr_k=500:18.9139,lwr_k=600:18.9407,lwr_k=700:18.97,lwr_k=800:19.0229,lwr_k=900:19.0291,lwr_k=1000:19.0667'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:8.2882,lwr_k=10:5.998,lwr_k=20:6.2509,lwr_k=30:6.474,lwr_k=40:6.6747,lwr_k=50:6.7471,lwr_k=100:6.9466,lwr_k=200:7.2772,lwr_k=300:7.4536,lwr_k=400:7.5902,lwr_k=500:7.6853,lwr_k=600:7.7358,lwr_k=700:7.7588,lwr_k=800:7.7764,lwr_k=900:7.796,lwr_k=1000:7.8068'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.0952,lwr_k=10:11.8001,lwr_k=20:10.9619,lwr_k=30:11.2018,lwr_k=40:11.1116,lwr_k=50:10.8556,lwr_k=100:10.622,lwr_k=200:10.331,lwr_k=300:10.383,lwr_k=400:10.5342,lwr_k=500:10.6464,lwr_k=600:10.6832,lwr_k=700:10.6912,lwr_k=800:10.703,lwr_k=900:10.7484,lwr_k=1000:10.5284'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_20'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.022,lwr_k=10:2.5776,lwr_k=20:5.5666,lwr_k=30:6.1897,lwr_k=40:6.6447,lwr_k=50:6.9047,lwr_k=100:7.5588,lwr_k=200:7.9709,lwr_k=300:8.2067,lwr_k=400:8.3621,lwr_k=500:8.4938,lwr_k=600:8.5955,lwr_k=700:8.7002,lwr_k=800:8.7677,lwr_k=900:8.8436,lwr_k=1000:8.9356'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:15.0183,lwr_k=10:32.3153,lwr_k=20:13.799,lwr_k=30:12.1282,lwr_k=40:10.9845,lwr_k=50:10.6583,lwr_k=100:9.9912,lwr_k=200:10.2594,lwr_k=300:10.64,lwr_k=400:10.9048,lwr_k=500:11.1466,lwr_k=600:11.3554,lwr_k=700:11.5632,lwr_k=800:11.6927,lwr_k=900:11.8321,lwr_k=1000:11.9998'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:22.0996,lwr_k=10:3.1538,lwr_k=20:6.8058,lwr_k=30:8.3229,lwr_k=40:9.5766,lwr_k=50:10.4009,lwr_k=100:12.4976,lwr_k=200:14.5397,lwr_k=300:15.2453,lwr_k=400:15.7836,lwr_k=500:16.1694,lwr_k=600:16.4952,lwr_k=700:16.8053,lwr_k=800:17.0764,lwr_k=900:17.3107,lwr_k=1000:17.4837'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:15.3228,lwr_k=10:34.065,lwr_k=20:13.9537,lwr_k=30:12.6777,lwr_k=40:12.7494,lwr_k=50:12.1772,lwr_k=100:11.866,lwr_k=200:11.6838,lwr_k=300:11.7633,lwr_k=400:11.829,lwr_k=500:11.9108,lwr_k=600:11.9743,lwr_k=700:12.1157,lwr_k=800:12.1858,lwr_k=900:12.2934,lwr_k=1000:12.3575'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:36.5163,lwr_k=10:4.0737,lwr_k=20:8.9635,lwr_k=30:10.7335,lwr_k=40:12.6497,lwr_k=50:13.9603,lwr_k=100:16.4847,lwr_k=200:18.2141,lwr_k=300:19.4184,lwr_k=400:20.3094,lwr_k=500:20.9367,lwr_k=600:21.5098,lwr_k=700:22.1107,lwr_k=800:22.647,lwr_k=900:23.2229,lwr_k=1000:23.6796'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:21.5659,lwr_k=10:49.6719,lwr_k=20:17.898,lwr_k=30:14.7607,lwr_k=40:13.8583,lwr_k=50:12.8743,lwr_k=100:12.2752,lwr_k=200:12.6021,lwr_k=300:13.0628,lwr_k=400:13.2589,lwr_k=500:13.6776,lwr_k=600:13.6716,lwr_k=700:13.8515,lwr_k=800:13.9877,lwr_k=900:14.1173,lwr_k=1000:14.3168'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:23.6691,lwr_k=10:3.5498,lwr_k=20:7.2431,lwr_k=30:8.815,lwr_k=40:9.8676,lwr_k=50:10.6066,lwr_k=100:12.8474,lwr_k=200:14.8416,lwr_k=300:15.7228,lwr_k=400:16.3398,lwr_k=500:16.7487,lwr_k=600:17.128,lwr_k=700:17.5015,lwr_k=800:17.7508,lwr_k=900:18.0097,lwr_k=1000:18.2305'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:19.5527,lwr_k=10:45.7762,lwr_k=20:16.7114,lwr_k=30:14.3394,lwr_k=40:13.4902,lwr_k=50:13.4298,lwr_k=100:13.2809,lwr_k=200:13.0059,lwr_k=300:13.4112,lwr_k=400:13.7086,lwr_k=500:13.996,lwr_k=600:14.198,lwr_k=700:14.3622,lwr_k=800:14.5323,lwr_k=900:14.6977,lwr_k=1000:14.7988'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:18.4894,lwr_k=10:3.566,lwr_k=20:7.1023,lwr_k=30:8.0591,lwr_k=40:8.5475,lwr_k=50:8.9865,lwr_k=100:9.7237,lwr_k=200:10.59,lwr_k=300:11.0243,lwr_k=400:11.357,lwr_k=500:11.6858,lwr_k=600:12.0016,lwr_k=700:12.3333,lwr_k=800:12.6756,lwr_k=900:12.8955,lwr_k=1000:13.1378'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:55.5578,lwr_k=10:54.2881,lwr_k=20:28.2641,lwr_k=30:25.8663,lwr_k=40:26.2107,lwr_k=50:27.1677,lwr_k=100:30.3457,lwr_k=200:33.8005,lwr_k=300:35.8171,lwr_k=400:37.254,lwr_k=500:38.462,lwr_k=600:39.8695,lwr_k=700:41.2202,lwr_k=800:42.384,lwr_k=900:43.155,lwr_k=1000:44.1271'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:14.8391,lwr_k=10:2.4105,lwr_k=20:5.2568,lwr_k=30:6.3681,lwr_k=40:7.1891,lwr_k=50:7.802,lwr_k=100:9.2742,lwr_k=200:10.2243,lwr_k=300:10.8482,lwr_k=400:11.1839,lwr_k=500:11.4734,lwr_k=600:11.7118,lwr_k=700:11.876,lwr_k=800:12.0053,lwr_k=900:12.1127,lwr_k=1000:12.2342'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:14.8074,lwr_k=10:24.9834,lwr_k=20:13.3643,lwr_k=30:11.0311,lwr_k=40:11.1854,lwr_k=50:10.96,lwr_k=100:11.408,lwr_k=200:11.969,lwr_k=300:12.4457,lwr_k=400:12.5348,lwr_k=500:12.6988,lwr_k=600:12.8294,lwr_k=700:12.9086,lwr_k=800:12.9645,lwr_k=900:13.0792,lwr_k=1000:13.2284'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_21'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:30.0231,lwr_k=10:17.7388,lwr_k=20:18.9812,lwr_k=30:19.4531,lwr_k=40:19.7086,lwr_k=50:19.9787,lwr_k=100:20.642,lwr_k=200:21.4215,lwr_k=300:21.8835,lwr_k=400:22.3568,lwr_k=500:22.6548,lwr_k=600:22.9646,lwr_k=700:23.2281,lwr_k=800:23.5047,lwr_k=900:23.7799,lwr_k=1000:24.1045'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:36.9521,lwr_k=10:307.9613,lwr_k=20:164.1765,lwr_k=30:39.5002,lwr_k=40:24.9612,lwr_k=50:24.9761,lwr_k=100:24.2557,lwr_k=200:24.4192,lwr_k=300:24.8763,lwr_k=400:25.5481,lwr_k=500:25.9684,lwr_k=600:26.4669,lwr_k=700:26.9035,lwr_k=800:27.3462,lwr_k=900:27.8149,lwr_k=1000:28.2806'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:225.3182,lwr_k=10:143.2157,lwr_k=20:170.048,lwr_k=30:176.6544,lwr_k=40:180.0868,lwr_k=50:181.7878,lwr_k=100:184.0285,lwr_k=200:189.0993,lwr_k=300:192.719,lwr_k=400:194.5424,lwr_k=500:195.9487,lwr_k=600:197.5298,lwr_k=700:198.5442,lwr_k=800:199.7749,lwr_k=900:200.7295,lwr_k=1000:201.7101'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:189.3441,lwr_k=10:192.5222,lwr_k=20:174.2545,lwr_k=30:168.0016,lwr_k=40:166.7107,lwr_k=50:162.4024,lwr_k=100:160.1366,lwr_k=200:160.4974,lwr_k=300:160.74,lwr_k=400:161.6915,lwr_k=500:162.9067,lwr_k=600:163.6031,lwr_k=700:164.6342,lwr_k=800:166.024,lwr_k=900:167.3313,lwr_k=1000:168.5112'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:43.7212,lwr_k=10:20.3053,lwr_k=20:23.2447,lwr_k=30:24.3229,lwr_k=40:25.1768,lwr_k=50:25.8673,lwr_k=100:27.6077,lwr_k=200:29.3715,lwr_k=300:30.5593,lwr_k=400:31.2344,lwr_k=500:31.8237,lwr_k=600:32.3036,lwr_k=700:32.78,lwr_k=800:33.2413,lwr_k=900:33.7601,lwr_k=1000:34.1734'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:30.067,lwr_k=10:26.3751,lwr_k=20:24.5784,lwr_k=30:24.0687,lwr_k=40:23.3784,lwr_k=50:23.0047,lwr_k=100:23.0667,lwr_k=200:23.9078,lwr_k=300:24.3475,lwr_k=400:24.5216,lwr_k=500:24.5918,lwr_k=600:24.7076,lwr_k=700:24.7285,lwr_k=800:24.8191,lwr_k=900:24.8119,lwr_k=1000:24.8715'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:242.344,lwr_k=10:144.3755,lwr_k=20:178.7446,lwr_k=30:188.1348,lwr_k=40:190.0582,lwr_k=50:192.4671,lwr_k=100:196.0227,lwr_k=200:201.6998,lwr_k=300:205.2933,lwr_k=400:207.7575,lwr_k=500:209.5033,lwr_k=600:211.4889,lwr_k=700:213.0628,lwr_k=800:214.7568,lwr_k=900:216.1937,lwr_k=1000:217.5854'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:227.3054,lwr_k=10:219.1733,lwr_k=20:196.3452,lwr_k=30:201.5208,lwr_k=40:195.5723,lwr_k=50:192.6532,lwr_k=100:191.5328,lwr_k=200:195.9726,lwr_k=300:197.575,lwr_k=400:198.2655,lwr_k=500:199.145,lwr_k=600:200.0919,lwr_k=700:200.9936,lwr_k=800:201.7259,lwr_k=900:202.5592,lwr_k=1000:203.5336'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:191.8404,lwr_k=10:113.7853,lwr_k=20:133.9047,lwr_k=30:140.689,lwr_k=40:142.6566,lwr_k=50:146.0266,lwr_k=100:150.3835,lwr_k=200:156.1481,lwr_k=300:158.9701,lwr_k=400:161.3475,lwr_k=500:162.9407,lwr_k=600:164.6973,lwr_k=700:166.3847,lwr_k=800:167.5072,lwr_k=900:168.9474,lwr_k=1000:170.0117'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:302.0189,lwr_k=10:299.0659,lwr_k=20:275.6975,lwr_k=30:274.1349,lwr_k=40:273.5668,lwr_k=50:270.8796,lwr_k=100:266.7262,lwr_k=200:266.0257,lwr_k=300:267.8814,lwr_k=400:269.0572,lwr_k=500:270.4318,lwr_k=600:271.2021,lwr_k=700:272.3848,lwr_k=800:272.7519,lwr_k=900:274.2668,lwr_k=1000:275.3588'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:26.6371,lwr_k=10:13.1188,lwr_k=20:15.0863,lwr_k=30:15.9259,lwr_k=40:16.5136,lwr_k=50:16.6779,lwr_k=100:17.8562,lwr_k=200:18.9092,lwr_k=300:19.4865,lwr_k=400:19.8636,lwr_k=500:20.1846,lwr_k=600:20.4241,lwr_k=700:20.6691,lwr_k=800:20.8496,lwr_k=900:21.0446,lwr_k=1000:21.1799'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:25.6877,lwr_k=10:30.4231,lwr_k=20:25.5454,lwr_k=30:23.7858,lwr_k=40:23.9778,lwr_k=50:22.8053,lwr_k=100:21.6038,lwr_k=200:20.9186,lwr_k=300:20.9162,lwr_k=400:21.0251,lwr_k=500:20.8806,lwr_k=600:20.9148,lwr_k=700:20.9186,lwr_k=800:20.8368,lwr_k=900:20.9846,lwr_k=1000:21.0934'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_22'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:51.3163,lwr_k=10:29.9664,lwr_k=20:28.8257,lwr_k=30:29.035,lwr_k=40:28.9109,lwr_k=50:29.3528,lwr_k=100:29.2661,lwr_k=200:33.777,lwr_k=300:36.5106,lwr_k=400:38.4808,lwr_k=500:39.9523,lwr_k=600:41.0772,lwr_k=700:42.0018,lwr_k=800:42.8224,lwr_k=900:43.5084,lwr_k=1000:44.0578'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:81.6508,lwr_k=10:49.0658,lwr_k=20:44.3169,lwr_k=30:41.4608,lwr_k=40:43.6548,lwr_k=50:45.7006,lwr_k=100:42.3739,lwr_k=200:47.6127,lwr_k=300:53.3746,lwr_k=400:57.4722,lwr_k=500:60.2726,lwr_k=600:62.6195,lwr_k=700:64.693,lwr_k=800:66.5569,lwr_k=900:67.9242,lwr_k=1000:69.0896'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.5832,lwr_k=10:2.6203,lwr_k=20:4.3103,lwr_k=30:4.9646,lwr_k=40:5.4446,lwr_k=50:5.6947,lwr_k=100:6.1572,lwr_k=200:6.8033,lwr_k=300:6.969,lwr_k=400:7.0519,lwr_k=500:7.113,lwr_k=600:7.1598,lwr_k=700:7.1865,lwr_k=800:7.2179,lwr_k=900:7.2338,lwr_k=1000:7.2604'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.1307,lwr_k=10:24.8454,lwr_k=20:8.3555,lwr_k=30:7.2929,lwr_k=40:6.9946,lwr_k=50:6.8495,lwr_k=100:6.545,lwr_k=200:6.5607,lwr_k=300:6.6329,lwr_k=400:6.6732,lwr_k=500:6.6984,lwr_k=600:6.7373,lwr_k=700:6.6963,lwr_k=800:6.7183,lwr_k=900:6.7373,lwr_k=1000:6.7497'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.721,lwr_k=10:2.9143,lwr_k=20:4.7222,lwr_k=30:5.39,lwr_k=40:5.9085,lwr_k=50:6.4826,lwr_k=100:7.2901,lwr_k=200:7.8008,lwr_k=300:7.9714,lwr_k=400:8.099,lwr_k=500:8.2251,lwr_k=600:8.2422,lwr_k=700:8.3148,lwr_k=800:8.3587,lwr_k=900:8.3926,lwr_k=1000:8.4144'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.3134,lwr_k=10:24.5801,lwr_k=20:13.0519,lwr_k=30:9.3746,lwr_k=40:8.6242,lwr_k=50:8.106,lwr_k=100:7.0736,lwr_k=200:6.934,lwr_k=300:6.8657,lwr_k=400:6.8823,lwr_k=500:6.9195,lwr_k=600:6.9212,lwr_k=700:6.9121,lwr_k=800:6.9089,lwr_k=900:6.8982,lwr_k=1000:6.9178'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.9176,lwr_k=10:3.0182,lwr_k=20:4.7113,lwr_k=30:5.2933,lwr_k=40:5.7354,lwr_k=50:6.0314,lwr_k=100:6.6095,lwr_k=200:7.0039,lwr_k=300:7.1195,lwr_k=400:7.2056,lwr_k=500:7.3347,lwr_k=600:7.3955,lwr_k=700:7.4531,lwr_k=800:7.4853,lwr_k=900:7.525,lwr_k=1000:7.5494'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.188,lwr_k=10:56.2667,lwr_k=20:10.1419,lwr_k=30:8.6868,lwr_k=40:8.5995,lwr_k=50:8.9668,lwr_k=100:12.2122,lwr_k=200:11.6706,lwr_k=300:11.637,lwr_k=400:11.4498,lwr_k=500:11.8769,lwr_k=600:9.4477,lwr_k=700:9.6739,lwr_k=800:9.74,lwr_k=900:9.7609,lwr_k=1000:9.7245'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.9061,lwr_k=10:2.1078,lwr_k=20:3.3976,lwr_k=30:3.6755,lwr_k=40:3.8754,lwr_k=50:3.9895,lwr_k=100:4.2967,lwr_k=200:4.4691,lwr_k=300:4.5529,lwr_k=400:4.5849,lwr_k=500:4.6151,lwr_k=600:4.6129,lwr_k=700:4.6231,lwr_k=800:4.6522,lwr_k=900:4.6574,lwr_k=1000:4.6686'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:18.3269,lwr_k=10:39.5538,lwr_k=20:16.4722,lwr_k=30:16.5095,lwr_k=40:18.7166,lwr_k=50:18.3746,lwr_k=100:18.816,lwr_k=200:18.8727,lwr_k=300:18.9332,lwr_k=400:18.9167,lwr_k=500:18.8267,lwr_k=600:18.7422,lwr_k=700:18.621,lwr_k=800:18.5597,lwr_k=900:18.5245,lwr_k=1000:18.51'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:8.0105,lwr_k=10:2.2926,lwr_k=20:4.2973,lwr_k=30:4.8434,lwr_k=40:5.1134,lwr_k=50:5.1918,lwr_k=100:5.9383,lwr_k=200:6.7014,lwr_k=300:6.888,lwr_k=400:6.9638,lwr_k=500:7.0112,lwr_k=600:7.0721,lwr_k=700:7.1319,lwr_k=800:7.1814,lwr_k=900:7.242,lwr_k=1000:7.2833'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.8793,lwr_k=10:197.7912,lwr_k=20:9.2736,lwr_k=30:8.5621,lwr_k=40:9.7283,lwr_k=50:10.2789,lwr_k=100:27.6905,lwr_k=200:35.7981,lwr_k=300:38.5485,lwr_k=400:35.3719,lwr_k=500:33.7877,lwr_k=600:33.8838,lwr_k=700:34.4147,lwr_k=800:34.2002,lwr_k=900:33.9199,lwr_k=1000:33.5614'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_23'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.9957,lwr_k=10:0.0,lwr_k=20:0.0005,lwr_k=30:2.0916,lwr_k=40:3.406,lwr_k=50:4.0804,lwr_k=100:5.2382,lwr_k=200:5.8056,lwr_k=300:6.0894,lwr_k=400:6.2577,lwr_k=500:6.348,lwr_k=600:6.4184,lwr_k=700:6.4947,lwr_k=800:6.5267,lwr_k=900:6.5846,lwr_k=1000:6.6124'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.1722,lwr_k=10:114.8674,lwr_k=20:29884.4774,lwr_k=30:155.4296,lwr_k=40:428.5176,lwr_k=50:467.0687,lwr_k=100:22.1631,lwr_k=200:9.7796,lwr_k=300:8.4468,lwr_k=400:8.4528,lwr_k=500:8.4337,lwr_k=600:8.425,lwr_k=700:8.4661,lwr_k=800:8.5359,lwr_k=900:8.5848,lwr_k=1000:8.6548'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.4825,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.368,lwr_k=40:1.9665,lwr_k=50:2.4985,lwr_k=100:3.9781,lwr_k=200:5.3206,lwr_k=300:5.8358,lwr_k=400:6.1774,lwr_k=500:6.3947,lwr_k=600:6.5629,lwr_k=700:6.7661,lwr_k=800:6.951,lwr_k=900:7.0901,lwr_k=1000:7.2021'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.3988,lwr_k=10:25.1671,lwr_k=20:151.9988,lwr_k=30:12.3613,lwr_k=40:8.223,lwr_k=50:7.3033,lwr_k=100:6.5522,lwr_k=200:6.2798,lwr_k=300:6.0144,lwr_k=400:5.9633,lwr_k=500:5.9661,lwr_k=600:5.9884,lwr_k=700:6.1539,lwr_k=800:6.2127,lwr_k=900:6.3359,lwr_k=1000:6.3671'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.0343,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.6178,lwr_k=40:2.3506,lwr_k=50:2.7909,lwr_k=100:3.9243,lwr_k=200:5.0709,lwr_k=300:5.7963,lwr_k=400:6.2589,lwr_k=500:6.6949,lwr_k=600:7.0199,lwr_k=700:7.4267,lwr_k=800:7.7073,lwr_k=900:7.9805,lwr_k=1000:8.246'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.9992,lwr_k=10:44.7403,lwr_k=20:488.5905,lwr_k=30:17.5704,lwr_k=40:11.2684,lwr_k=50:7.9253,lwr_k=100:6.305,lwr_k=200:5.674,lwr_k=300:5.7116,lwr_k=400:5.5959,lwr_k=500:5.6429,lwr_k=600:5.7105,lwr_k=700:5.9634,lwr_k=800:5.9687,lwr_k=900:6.0496,lwr_k=1000:6.0964'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.8402,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.3797,lwr_k=40:2.0025,lwr_k=50:2.5052,lwr_k=100:3.8793,lwr_k=200:4.7458,lwr_k=300:5.1748,lwr_k=400:5.4376,lwr_k=500:5.7632,lwr_k=600:5.9193,lwr_k=700:6.2246,lwr_k=800:6.4218,lwr_k=900:6.5176,lwr_k=1000:6.6155'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.8514,lwr_k=10:21.731,lwr_k=20:136.4866,lwr_k=30:13.5666,lwr_k=40:9.808,lwr_k=50:7.9626,lwr_k=100:7.036,lwr_k=200:7.0344,lwr_k=300:6.9933,lwr_k=400:6.9508,lwr_k=500:6.9501,lwr_k=600:6.99,lwr_k=700:6.9308,lwr_k=800:6.9664,lwr_k=900:7.0083,lwr_k=1000:7.0502'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.7438,lwr_k=10:0.0,lwr_k=20:0.0095,lwr_k=30:1.284,lwr_k=40:1.8582,lwr_k=50:2.1694,lwr_k=100:3.0839,lwr_k=200:3.6221,lwr_k=300:3.8423,lwr_k=400:3.9314,lwr_k=500:4.0287,lwr_k=600:4.0656,lwr_k=700:4.1224,lwr_k=800:4.1601,lwr_k=900:4.1884,lwr_k=1000:4.2254'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:17.3569,lwr_k=10:53.6997,lwr_k=20:358170.1346,lwr_k=30:25.0054,lwr_k=40:20.7086,lwr_k=50:17.9955,lwr_k=100:15.5973,lwr_k=200:16.6284,lwr_k=300:16.2681,lwr_k=400:15.9943,lwr_k=500:15.967,lwr_k=600:15.9525,lwr_k=700:16.0599,lwr_k=800:16.1293,lwr_k=900:16.1781,lwr_k=1000:16.2894'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:6.4412,lwr_k=10:0.0,lwr_k=20:0.003,lwr_k=30:1.1756,lwr_k=40:1.7627,lwr_k=50:2.2662,lwr_k=100:3.3275,lwr_k=200:3.9486,lwr_k=300:4.1745,lwr_k=400:4.3488,lwr_k=500:4.4583,lwr_k=600:4.5322,lwr_k=700:4.6185,lwr_k=800:4.6707,lwr_k=900:4.7267,lwr_k=1000:4.7672'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.1489,lwr_k=10:267.6191,lwr_k=20:229097.3372,lwr_k=30:4289.8219,lwr_k=40:50.4637,lwr_k=50:23.7523,lwr_k=100:11.7478,lwr_k=200:8.7526,lwr_k=300:7.4406,lwr_k=400:7.1394,lwr_k=500:7.0443,lwr_k=600:7.1818,lwr_k=700:7.3234,lwr_k=800:7.4273,lwr_k=900:7.471,lwr_k=1000:7.4032'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_24'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.2693,lwr_k=10:6.6601,lwr_k=20:7.2799,lwr_k=30:7.3683,lwr_k=40:7.6185,lwr_k=50:7.7156,lwr_k=100:7.9949,lwr_k=200:8.3574,lwr_k=300:8.4671,lwr_k=400:8.5798,lwr_k=500:8.6882,lwr_k=600:8.7555,lwr_k=700:8.8394,lwr_k=800:8.9433,lwr_k=900:9.0068,lwr_k=1000:9.0409'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.7049,lwr_k=10:14.5615,lwr_k=20:12.3163,lwr_k=30:11.8599,lwr_k=40:10.8733,lwr_k=50:10.9996,lwr_k=100:10.2057,lwr_k=200:10.5385,lwr_k=300:10.6763,lwr_k=400:10.8771,lwr_k=500:10.9444,lwr_k=600:11.0142,lwr_k=700:11.1113,lwr_k=800:11.2067,lwr_k=900:11.2715,lwr_k=1000:11.3381'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:25.0658,lwr_k=10:12.4344,lwr_k=20:15.0091,lwr_k=30:15.9889,lwr_k=40:16.5865,lwr_k=50:16.9081,lwr_k=100:17.8168,lwr_k=200:18.8388,lwr_k=300:19.39,lwr_k=400:19.7658,lwr_k=500:20.0949,lwr_k=600:20.3195,lwr_k=700:20.5873,lwr_k=800:20.8058,lwr_k=900:20.9999,lwr_k=1000:21.1211'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:19.3586,lwr_k=10:22.2889,lwr_k=20:16.9969,lwr_k=30:15.6858,lwr_k=40:15.1157,lwr_k=50:15.0093,lwr_k=100:14.4856,lwr_k=200:14.4129,lwr_k=300:14.3171,lwr_k=400:14.3972,lwr_k=500:14.5976,lwr_k=600:14.6366,lwr_k=700:14.6585,lwr_k=800:14.7326,lwr_k=900:14.776,lwr_k=1000:14.8476'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:48.0618,lwr_k=10:19.1016,lwr_k=20:21.618,lwr_k=30:23.0782,lwr_k=40:23.78,lwr_k=50:24.3387,lwr_k=100:25.5729,lwr_k=200:27.3988,lwr_k=300:29.0321,lwr_k=400:30.3406,lwr_k=500:31.176,lwr_k=600:32.0043,lwr_k=700:32.7278,lwr_k=800:33.4677,lwr_k=900:34.0613,lwr_k=1000:34.7082'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:31.1376,lwr_k=10:30.2699,lwr_k=20:23.3742,lwr_k=30:22.1488,lwr_k=40:21.5135,lwr_k=50:21.2322,lwr_k=100:20.2765,lwr_k=200:20.7673,lwr_k=300:21.1713,lwr_k=400:21.4225,lwr_k=500:21.6632,lwr_k=600:21.7554,lwr_k=700:21.8354,lwr_k=800:21.8935,lwr_k=900:22.042,lwr_k=1000:22.1041'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:22.3458,lwr_k=10:12.0144,lwr_k=20:13.8327,lwr_k=30:14.4702,lwr_k=40:15.2538,lwr_k=50:15.5446,lwr_k=100:16.3526,lwr_k=200:16.9076,lwr_k=300:17.1588,lwr_k=400:17.4309,lwr_k=500:17.684,lwr_k=600:17.9245,lwr_k=700:18.129,lwr_k=800:18.2983,lwr_k=900:18.4277,lwr_k=1000:18.5254'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:19.7112,lwr_k=10:32.1129,lwr_k=20:43.7795,lwr_k=30:52.3562,lwr_k=40:18.8386,lwr_k=50:17.337,lwr_k=100:15.5868,lwr_k=200:15.5269,lwr_k=300:15.5524,lwr_k=400:15.4849,lwr_k=500:15.5477,lwr_k=600:15.662,lwr_k=700:15.5934,lwr_k=800:15.6936,lwr_k=900:15.7872,lwr_k=1000:15.9255'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.0832,lwr_k=10:6.2593,lwr_k=20:7.2799,lwr_k=30:7.6787,lwr_k=40:7.8036,lwr_k=50:7.9502,lwr_k=100:8.1923,lwr_k=200:8.286,lwr_k=300:8.3639,lwr_k=400:8.4607,lwr_k=500:8.5179,lwr_k=600:8.5488,lwr_k=700:8.5417,lwr_k=800:8.5567,lwr_k=900:8.5968,lwr_k=1000:8.7263'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:29.1156,lwr_k=10:26.0122,lwr_k=20:21.7083,lwr_k=30:23.6321,lwr_k=40:25.0746,lwr_k=50:25.2892,lwr_k=100:24.9319,lwr_k=200:26.3562,lwr_k=300:26.7201,lwr_k=400:27.1658,lwr_k=500:27.3665,lwr_k=600:27.4569,lwr_k=700:27.4878,lwr_k=800:27.6148,lwr_k=900:27.7588,lwr_k=1000:28.074'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:20.6803,lwr_k=10:10.6342,lwr_k=20:12.5351,lwr_k=30:13.0059,lwr_k=40:13.3813,lwr_k=50:13.6576,lwr_k=100:14.6059,lwr_k=200:15.2757,lwr_k=300:15.6791,lwr_k=400:16.0789,lwr_k=500:16.3728,lwr_k=600:16.6021,lwr_k=700:16.879,lwr_k=800:17.0725,lwr_k=900:17.2532,lwr_k=1000:17.4329'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:20.683,lwr_k=10:19.3424,lwr_k=20:17.5993,lwr_k=30:18.4261,lwr_k=40:18.6883,lwr_k=50:17.3982,lwr_k=100:17.1121,lwr_k=200:17.304,lwr_k=300:17.3689,lwr_k=400:17.3236,lwr_k=500:17.2384,lwr_k=600:17.3617,lwr_k=700:17.4563,lwr_k=800:17.6259,lwr_k=900:17.6689,lwr_k=1000:17.8817'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_25'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.5217,lwr_k=10:4.5507,lwr_k=20:6.1783,lwr_k=30:6.5984,lwr_k=40:6.8259,lwr_k=50:7.0974,lwr_k=100:7.5641,lwr_k=200:7.747,lwr_k=300:7.8676,lwr_k=400:7.8743,lwr_k=500:7.9408,lwr_k=600:8.0238,lwr_k=700:7.9857,lwr_k=800:8.0022,lwr_k=900:8.0266,lwr_k=1000:8.06'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.6664,lwr_k=10:20.1283,lwr_k=20:13.2511,lwr_k=30:12.2949,lwr_k=40:11.8499,lwr_k=50:11.5314,lwr_k=100:11.1508,lwr_k=200:11.2655,lwr_k=300:11.3741,lwr_k=400:11.6271,lwr_k=500:11.6088,lwr_k=600:11.5323,lwr_k=700:11.5138,lwr_k=800:11.2993,lwr_k=900:11.5847,lwr_k=1000:11.5893'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:44.5089,lwr_k=10:0.0,lwr_k=20:1.4452,lwr_k=30:6.2971,lwr_k=40:9.2168,lwr_k=50:11.1898,lwr_k=100:17.4598,lwr_k=200:21.1803,lwr_k=300:23.0643,lwr_k=400:24.4165,lwr_k=500:25.5311,lwr_k=600:26.2448,lwr_k=700:27.1891,lwr_k=800:27.7611,lwr_k=900:28.4688,lwr_k=1000:29.2701'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:28.945,lwr_k=10:74.5745,lwr_k=20:433.4457,lwr_k=30:50.3662,lwr_k=40:41.9223,lwr_k=50:37.6345,lwr_k=100:23.5935,lwr_k=200:22.224,lwr_k=300:21.2489,lwr_k=400:20.6988,lwr_k=500:20.5656,lwr_k=600:20.3031,lwr_k=700:20.0414,lwr_k=800:19.8179,lwr_k=900:19.7439,lwr_k=1000:19.7621'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:45.497,lwr_k=10:0.0,lwr_k=20:1.2504,lwr_k=30:6.4724,lwr_k=40:9.5972,lwr_k=50:12.1334,lwr_k=100:17.6469,lwr_k=200:21.8414,lwr_k=300:24.2715,lwr_k=400:25.5153,lwr_k=500:26.3442,lwr_k=600:27.025,lwr_k=700:27.6404,lwr_k=800:28.2887,lwr_k=900:28.7988,lwr_k=1000:29.3009'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:34.6864,lwr_k=10:69.5168,lwr_k=20:649.9055,lwr_k=30:60.0004,lwr_k=40:38.4156,lwr_k=50:31.2976,lwr_k=100:24.2292,lwr_k=200:20.6072,lwr_k=300:20.8747,lwr_k=400:20.6075,lwr_k=500:20.6036,lwr_k=600:21.0124,lwr_k=700:21.381,lwr_k=800:21.7099,lwr_k=900:22.0541,lwr_k=1000:22.3022'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:94.8857,lwr_k=10:0.0,lwr_k=20:0.0164,lwr_k=30:2.9463,lwr_k=40:6.6213,lwr_k=50:9.8923,lwr_k=100:20.7189,lwr_k=200:28.9387,lwr_k=300:34.8721,lwr_k=400:38.4048,lwr_k=500:42.0687,lwr_k=600:44.5857,lwr_k=700:46.931,lwr_k=800:49.2669,lwr_k=900:51.4296,lwr_k=1000:53.3763'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:90.1944,lwr_k=10:54.2253,lwr_k=20:168.2751,lwr_k=30:6566.0489,lwr_k=40:3132.5743,lwr_k=50:43.6679,lwr_k=100:44.044,lwr_k=200:31.8805,lwr_k=300:35.5919,lwr_k=400:37.3736,lwr_k=500:40.4652,lwr_k=600:42.9352,lwr_k=700:45.2837,lwr_k=800:47.572,lwr_k=900:49.6587,lwr_k=1000:51.6002'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.1949,lwr_k=10:0.0,lwr_k=20:0.2197,lwr_k=30:1.6001,lwr_k=40:2.1877,lwr_k=50:2.5975,lwr_k=100:3.2132,lwr_k=200:3.6492,lwr_k=300:3.8259,lwr_k=400:3.9083,lwr_k=500:3.9529,lwr_k=600:3.9785,lwr_k=700:3.9951,lwr_k=800:4.0176,lwr_k=900:4.0272,lwr_k=1000:4.0508'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:20.5699,lwr_k=10:31.9854,lwr_k=20:188.3704,lwr_k=30:51.43,lwr_k=40:24.9982,lwr_k=50:24.1848,lwr_k=100:21.9562,lwr_k=200:21.0121,lwr_k=300:20.6566,lwr_k=400:20.4035,lwr_k=500:20.2039,lwr_k=600:20.2483,lwr_k=700:20.2894,lwr_k=800:20.2742,lwr_k=900:20.2779,lwr_k=1000:20.3217'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:87.593,lwr_k=10:0.0,lwr_k=20:0.0163,lwr_k=30:2.754,lwr_k=40:6.5442,lwr_k=50:10.1518,lwr_k=100:16.6178,lwr_k=200:25.4848,lwr_k=300:29.8737,lwr_k=400:33.5485,lwr_k=500:36.1575,lwr_k=600:38.3656,lwr_k=700:40.2655,lwr_k=800:41.7689,lwr_k=900:43.7342,lwr_k=1000:45.2544'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:90.3297,lwr_k=10:66.5709,lwr_k=20:317.7424,lwr_k=30:1617.7426,lwr_k=40:7443.8557,lwr_k=50:58.433,lwr_k=100:41.7429,lwr_k=200:42.6684,lwr_k=300:43.0832,lwr_k=400:45.3269,lwr_k=500:48.0905,lwr_k=600:49.598,lwr_k=700:51.1148,lwr_k=800:52.3847,lwr_k=900:54.9549,lwr_k=1000:55.9332'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_26'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.4469,lwr_k=10:2.0433,lwr_k=20:3.6056,lwr_k=30:4.3359,lwr_k=40:4.7451,lwr_k=50:4.9514,lwr_k=100:5.652,lwr_k=200:5.9108,lwr_k=300:6.0326,lwr_k=400:6.0909,lwr_k=500:6.1537,lwr_k=600:6.2071,lwr_k=700:6.2135,lwr_k=800:6.2146,lwr_k=900:6.2872,lwr_k=1000:6.2932'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.137,lwr_k=10:90.2233,lwr_k=20:19.5605,lwr_k=30:11.7577,lwr_k=40:734861.7564,lwr_k=50:224332.2277,lwr_k=100:47769.6556,lwr_k=200:14887.5991,lwr_k=300:2138.9249,lwr_k=400:8.1761,lwr_k=500:8.1807,lwr_k=600:8.1372,lwr_k=700:8.2318,lwr_k=800:8.228,lwr_k=900:8.188,lwr_k=1000:8.1428'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.8677,lwr_k=10:5.3126,lwr_k=20:6.3174,lwr_k=30:6.6786,lwr_k=40:6.7529,lwr_k=50:7.1353,lwr_k=100:6.9992,lwr_k=200:7.1252,lwr_k=300:7.2415,lwr_k=400:7.3242,lwr_k=500:6.9368,lwr_k=600:7.3706,lwr_k=700:7.3704,lwr_k=800:7.4464,lwr_k=900:7.4113,lwr_k=1000:7.6778'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.9607,lwr_k=10:9.8759,lwr_k=20:8.8294,lwr_k=30:8.6343,lwr_k=40:8.8029,lwr_k=50:8.4904,lwr_k=100:8.3662,lwr_k=200:8.1906,lwr_k=300:8.3445,lwr_k=400:8.8289,lwr_k=500:8.3789,lwr_k=600:8.4033,lwr_k=700:8.1917,lwr_k=800:8.9553,lwr_k=900:8.9459,lwr_k=1000:8.2984'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.2737,lwr_k=10:3.5025,lwr_k=20:4.827,lwr_k=30:5.331,lwr_k=40:5.6171,lwr_k=50:5.8021,lwr_k=100:6.3397,lwr_k=200:6.6657,lwr_k=300:6.7592,lwr_k=400:6.7982,lwr_k=500:6.8844,lwr_k=600:6.9081,lwr_k=700:6.9695,lwr_k=800:7.0216,lwr_k=900:7.0475,lwr_k=1000:7.0694'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.8716,lwr_k=10:15.4275,lwr_k=20:11.4508,lwr_k=30:10.2463,lwr_k=40:36945131949.5494,lwr_k=50:20398619.3304,lwr_k=100:1252618.4747,lwr_k=200:6.455,lwr_k=300:6.3835,lwr_k=400:6.5331,lwr_k=500:6.5707,lwr_k=600:6.6473,lwr_k=700:6.7196,lwr_k=800:6.7406,lwr_k=900:6.8099,lwr_k=1000:6.8304'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.3156,lwr_k=10:3.2796,lwr_k=20:5.4982,lwr_k=30:6.0695,lwr_k=40:6.5087,lwr_k=50:6.8458,lwr_k=100:7.3699,lwr_k=200:7.67,lwr_k=300:7.8106,lwr_k=400:7.8708,lwr_k=500:7.9722,lwr_k=600:8.0941,lwr_k=700:8.1298,lwr_k=800:8.143,lwr_k=900:8.1805,lwr_k=1000:8.1917'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:11.0088,lwr_k=10:83.9351,lwr_k=20:29.7426,lwr_k=30:12.2535,lwr_k=40:29718.0857,lwr_k=50:327487904.4013,lwr_k=100:10.3808,lwr_k=200:11.4638,lwr_k=300:11.4801,lwr_k=400:10.3733,lwr_k=500:10.5482,lwr_k=600:10.5274,lwr_k=700:10.5386,lwr_k=800:10.6104,lwr_k=900:10.7064,lwr_k=1000:10.7101'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.5358,lwr_k=10:3.8378,lwr_k=20:5.7297,lwr_k=30:6.0346,lwr_k=40:6.5134,lwr_k=50:6.5984,lwr_k=100:6.9331,lwr_k=200:7.0781,lwr_k=300:7.2114,lwr_k=400:7.3113,lwr_k=500:7.3332,lwr_k=600:7.3042,lwr_k=700:7.357,lwr_k=800:7.3645,lwr_k=900:7.4361,lwr_k=1000:7.4074'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:20.9249,lwr_k=10:19788.6282,lwr_k=20:301263.6769,lwr_k=30:19.0995,lwr_k=40:138157938.1342,lwr_k=50:3486997544.7141,lwr_k=100:127.297,lwr_k=200:17.3259,lwr_k=300:70.6351,lwr_k=400:31.2155,lwr_k=500:9.6065,lwr_k=600:71.7463,lwr_k=700:25.1452,lwr_k=800:49.7246,lwr_k=900:19.4225,lwr_k=1000:61.8471'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:12.1237,lwr_k=10:5.7652,lwr_k=20:6.9383,lwr_k=30:7.6931,lwr_k=40:8.6744,lwr_k=50:8.9863,lwr_k=100:9.8826,lwr_k=200:10.3469,lwr_k=300:10.4386,lwr_k=400:10.4168,lwr_k=500:10.4334,lwr_k=600:10.4771,lwr_k=700:10.5033,lwr_k=800:9.8957,lwr_k=900:10.4916,lwr_k=1000:10.6227'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:13.8317,lwr_k=10:152.4486,lwr_k=20:33.3469,lwr_k=30:24.2255,lwr_k=40:26.1227,lwr_k=50:21.6083,lwr_k=100:12.3703,lwr_k=200:12.3071,lwr_k=300:12.2592,lwr_k=400:12.1431,lwr_k=500:12.2524,lwr_k=600:12.2761,lwr_k=700:12.1945,lwr_k=800:12.2273,lwr_k=900:12.1252,lwr_k=1000:12.2295'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_27'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.2609,lwr_k=10:0.0,lwr_k=20:0.6225,lwr_k=30:1.6558,lwr_k=40:2.1055,lwr_k=50:2.3461,lwr_k=100:2.9231,lwr_k=200:3.2573,lwr_k=300:3.3834,lwr_k=400:3.4938,lwr_k=500:3.5666,lwr_k=600:3.6478,lwr_k=700:3.6978,lwr_k=800:3.7355,lwr_k=900:3.7701,lwr_k=1000:3.8431'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.2557,lwr_k=10:22.7516,lwr_k=20:95.5058,lwr_k=30:18.7185,lwr_k=40:12.7962,lwr_k=50:11.4435,lwr_k=100:9.7337,lwr_k=200:9.3699,lwr_k=300:9.2276,lwr_k=400:9.2368,lwr_k=500:9.1982,lwr_k=600:9.1765,lwr_k=700:9.1396,lwr_k=800:9.1325,lwr_k=900:9.1026,lwr_k=1000:9.1591'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:49.4172,lwr_k=10:0.0,lwr_k=20:2.1118,lwr_k=30:8.2116,lwr_k=40:10.5217,lwr_k=50:12.6876,lwr_k=100:19.7992,lwr_k=200:23.6977,lwr_k=300:26.1948,lwr_k=400:27.1146,lwr_k=500:27.8062,lwr_k=600:28.6443,lwr_k=700:29.4477,lwr_k=800:30.2149,lwr_k=900:30.9588,lwr_k=1000:31.6459'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:35.685,lwr_k=10:169.5573,lwr_k=20:3315.4243,lwr_k=30:61.1901,lwr_k=40:44.3438,lwr_k=50:37.8822,lwr_k=100:29.1263,lwr_k=200:25.1215,lwr_k=300:24.605,lwr_k=400:24.2288,lwr_k=500:23.762,lwr_k=600:24.3311,lwr_k=700:24.1304,lwr_k=800:23.9594,lwr_k=900:24.1772,lwr_k=1000:24.1912'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:50.1641,lwr_k=10:1.7292,lwr_k=20:42.1831,lwr_k=30:19.8568,lwr_k=40:18.1304,lwr_k=50:18.767,lwr_k=100:23.73,lwr_k=200:28.3295,lwr_k=300:30.4409,lwr_k=400:32.1126,lwr_k=500:32.9794,lwr_k=600:33.6375,lwr_k=700:34.538,lwr_k=800:36.1051,lwr_k=900:36.697,lwr_k=1000:37.3156'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:38.6579,lwr_k=10:1623137.3816,lwr_k=20:14360964.4361,lwr_k=30:1567729.2064,lwr_k=40:56334.9159,lwr_k=50:27632.9745,lwr_k=100:69163.7517,lwr_k=200:145.1288,lwr_k=300:29.9499,lwr_k=400:29.3506,lwr_k=500:28.9835,lwr_k=600:28.6312,lwr_k=700:28.6772,lwr_k=800:28.836,lwr_k=900:29.0465,lwr_k=1000:29.1919'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:39.7128,lwr_k=10:0.0,lwr_k=20:1.7439,lwr_k=30:7.3054,lwr_k=40:11.1221,lwr_k=50:13.2262,lwr_k=100:18.744,lwr_k=200:22.3242,lwr_k=300:23.9616,lwr_k=400:24.8147,lwr_k=500:25.5977,lwr_k=600:26.527,lwr_k=700:27.1164,lwr_k=800:27.7398,lwr_k=900:28.3268,lwr_k=1000:29.0027'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:31.2963,lwr_k=10:144.6837,lwr_k=20:401.9981,lwr_k=30:88.2722,lwr_k=40:44.0778,lwr_k=50:38.8131,lwr_k=100:24.5202,lwr_k=200:21.7957,lwr_k=300:22.0432,lwr_k=400:21.447,lwr_k=500:21.7098,lwr_k=600:21.8369,lwr_k=700:21.8868,lwr_k=800:22.1212,lwr_k=900:22.3287,lwr_k=1000:22.6145'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.7433,lwr_k=10:0.0,lwr_k=20:0.5452,lwr_k=30:2.1099,lwr_k=40:2.6978,lwr_k=50:3.0343,lwr_k=100:3.7028,lwr_k=200:4.1531,lwr_k=300:4.2822,lwr_k=400:4.3674,lwr_k=500:4.4322,lwr_k=600:4.4528,lwr_k=700:4.48,lwr_k=800:4.4994,lwr_k=900:4.5271,lwr_k=1000:4.5495'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:18.8009,lwr_k=10:67.238,lwr_k=20:417.2525,lwr_k=30:28.622,lwr_k=40:23.5497,lwr_k=50:21.3232,lwr_k=100:19.0132,lwr_k=200:18.8836,lwr_k=300:19.1532,lwr_k=400:19.3427,lwr_k=500:19.3968,lwr_k=600:19.5178,lwr_k=700:19.5226,lwr_k=800:19.5334,lwr_k=900:19.5467,lwr_k=1000:19.5242'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:36.3206,lwr_k=10:0.0,lwr_k=20:2.2913,lwr_k=30:6.579,lwr_k=40:9.0324,lwr_k=50:10.8059,lwr_k=100:15.9681,lwr_k=200:19.1712,lwr_k=300:20.4098,lwr_k=400:21.1933,lwr_k=500:21.971,lwr_k=600:22.5607,lwr_k=700:23.0416,lwr_k=800:23.7122,lwr_k=900:24.1625,lwr_k=1000:24.7422'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:32.3219,lwr_k=10:112.1174,lwr_k=20:117.6956,lwr_k=30:66.9269,lwr_k=40:60.1223,lwr_k=50:43.7509,lwr_k=100:21.3774,lwr_k=200:21.0731,lwr_k=300:21.6148,lwr_k=400:21.9479,lwr_k=500:21.9101,lwr_k=600:22.5301,lwr_k=700:22.9882,lwr_k=800:23.4054,lwr_k=900:23.5061,lwr_k=1000:23.7015'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_28'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.0226,lwr_k=10:7.4072,lwr_k=20:8.0528,lwr_k=30:8.2882,lwr_k=40:8.3948,lwr_k=50:8.4737,lwr_k=100:8.7758,lwr_k=200:9.0101,lwr_k=300:9.2164,lwr_k=400:9.3426,lwr_k=500:9.4416,lwr_k=600:9.5419,lwr_k=700:9.6292,lwr_k=800:9.7164,lwr_k=900:9.7794,lwr_k=1000:9.8548'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:16.0042,lwr_k=10:12.0822,lwr_k=20:11.6717,lwr_k=30:11.1186,lwr_k=40:10.9922,lwr_k=50:10.9466,lwr_k=100:10.8105,lwr_k=200:11.1274,lwr_k=300:11.3374,lwr_k=400:11.5956,lwr_k=500:11.798,lwr_k=600:12.0104,lwr_k=700:12.212,lwr_k=800:12.3414,lwr_k=900:12.4556,lwr_k=1000:12.5599'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:34.8275,lwr_k=10:13.9443,lwr_k=20:16.0612,lwr_k=30:17.3444,lwr_k=40:17.896,lwr_k=50:18.3581,lwr_k=100:19.708,lwr_k=200:20.915,lwr_k=300:21.7999,lwr_k=400:22.5283,lwr_k=500:23.0098,lwr_k=600:23.5113,lwr_k=700:23.9206,lwr_k=800:24.2694,lwr_k=900:24.6046,lwr_k=1000:24.9466'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:22.6957,lwr_k=10:19.2297,lwr_k=20:17.064,lwr_k=30:16.0984,lwr_k=40:15.585,lwr_k=50:15.2737,lwr_k=100:15.1888,lwr_k=200:15.4066,lwr_k=300:15.6202,lwr_k=400:15.7495,lwr_k=500:15.8605,lwr_k=600:15.9569,lwr_k=700:16.0887,lwr_k=800:16.272,lwr_k=900:16.4152,lwr_k=1000:16.5338'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:14.1259,lwr_k=10:7.8565,lwr_k=20:8.9354,lwr_k=30:9.2905,lwr_k=40:9.8657,lwr_k=50:10.2736,lwr_k=100:11.0832,lwr_k=200:11.7075,lwr_k=300:11.9749,lwr_k=400:12.1738,lwr_k=500:12.2707,lwr_k=600:12.3896,lwr_k=700:12.4691,lwr_k=800:12.515,lwr_k=900:12.5947,lwr_k=1000:12.6393'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.8025,lwr_k=10:11.1214,lwr_k=20:9.5839,lwr_k=30:9.2416,lwr_k=40:9.2014,lwr_k=50:9.018,lwr_k=100:8.6732,lwr_k=200:8.4757,lwr_k=300:8.4602,lwr_k=400:8.6322,lwr_k=500:8.6364,lwr_k=600:8.6516,lwr_k=700:8.6775,lwr_k=800:8.7891,lwr_k=900:8.7758,lwr_k=1000:8.8095'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:16.024,lwr_k=10:8.1705,lwr_k=20:9.3236,lwr_k=30:9.6107,lwr_k=40:10.0181,lwr_k=50:10.3426,lwr_k=100:11.1454,lwr_k=200:11.9923,lwr_k=300:12.4695,lwr_k=400:12.8371,lwr_k=500:12.9484,lwr_k=600:13.1631,lwr_k=700:13.3537,lwr_k=800:13.4299,lwr_k=900:13.528,lwr_k=1000:13.6795'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:14.0591,lwr_k=10:13.0725,lwr_k=20:11.4586,lwr_k=30:11.5589,lwr_k=40:11.5368,lwr_k=50:11.6192,lwr_k=100:11.8404,lwr_k=200:11.8711,lwr_k=300:11.7495,lwr_k=400:11.7233,lwr_k=500:11.7053,lwr_k=600:11.7002,lwr_k=700:11.7787,lwr_k=800:11.8248,lwr_k=900:11.8698,lwr_k=1000:11.8916'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:15.3476,lwr_k=10:9.094,lwr_k=20:9.9692,lwr_k=30:10.383,lwr_k=40:10.5443,lwr_k=50:10.7332,lwr_k=100:10.9319,lwr_k=200:11.2725,lwr_k=300:11.5057,lwr_k=400:11.7097,lwr_k=500:11.8678,lwr_k=600:12.0434,lwr_k=700:12.2023,lwr_k=800:12.3487,lwr_k=900:12.4735,lwr_k=1000:12.5654'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:48.1333,lwr_k=10:25.6878,lwr_k=20:25.2966,lwr_k=30:27.5258,lwr_k=40:28.3092,lwr_k=50:29.6378,lwr_k=100:31.126,lwr_k=200:34.1481,lwr_k=300:35.4928,lwr_k=400:36.7576,lwr_k=500:37.7711,lwr_k=600:38.6231,lwr_k=700:39.3693,lwr_k=800:40.0103,lwr_k=900:40.6142,lwr_k=1000:41.0769'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:12.3078,lwr_k=10:7.1114,lwr_k=20:8.0711,lwr_k=30:8.3132,lwr_k=40:8.4687,lwr_k=50:8.7083,lwr_k=100:9.2084,lwr_k=200:9.6261,lwr_k=300:9.9233,lwr_k=400:10.0834,lwr_k=500:10.2116,lwr_k=600:10.3091,lwr_k=700:10.3907,lwr_k=800:10.4525,lwr_k=900:10.5123,lwr_k=1000:10.5733'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:13.3531,lwr_k=10:16.0875,lwr_k=20:12.6031,lwr_k=30:12.3336,lwr_k=40:11.7974,lwr_k=50:11.795,lwr_k=100:11.913,lwr_k=200:11.9468,lwr_k=300:11.9139,lwr_k=400:12.003,lwr_k=500:12.2684,lwr_k=600:12.292,lwr_k=700:12.3985,lwr_k=800:12.4181,lwr_k=900:12.4361,lwr_k=1000:12.4568'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_29'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.8074,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.6905,lwr_k=40:0.9298,lwr_k=50:1.2128,lwr_k=100:3.2901,lwr_k=200:4.245,lwr_k=300:4.5939,lwr_k=400:4.8347,lwr_k=500:4.915,lwr_k=600:5.0032,lwr_k=700:5.0441,lwr_k=800:5.1081,lwr_k=900:5.1533,lwr_k=1000:5.198'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.6583,lwr_k=10:22.2356,lwr_k=20:366.452,lwr_k=30:185.5977,lwr_k=40:29.0399,lwr_k=50:51.5738,lwr_k=100:14.9515,lwr_k=200:12.7558,lwr_k=300:9.2657,lwr_k=400:9.2452,lwr_k=500:9.2476,lwr_k=600:8.3993,lwr_k=700:8.5787,lwr_k=800:8.6055,lwr_k=900:8.896,lwr_k=1000:8.6096'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.2375,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.8267,lwr_k=40:1.6722,lwr_k=50:2.0358,lwr_k=100:3.7779,lwr_k=200:5.417,lwr_k=300:6.1315,lwr_k=400:6.4636,lwr_k=500:6.6492,lwr_k=600:6.7789,lwr_k=700:6.8771,lwr_k=800:6.9835,lwr_k=900:7.05,lwr_k=1000:7.117'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.0984,lwr_k=10:19.5651,lwr_k=20:90.9489,lwr_k=30:152.5351,lwr_k=40:53.5051,lwr_k=50:32.4567,lwr_k=100:15.4821,lwr_k=200:8.9196,lwr_k=300:7.0913,lwr_k=400:7.2068,lwr_k=500:7.2786,lwr_k=600:7.2945,lwr_k=700:7.3018,lwr_k=800:7.3383,lwr_k=900:7.3939,lwr_k=1000:7.4245'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.8228,lwr_k=10:0.0,lwr_k=20:0.0033,lwr_k=30:1.3406,lwr_k=40:1.9607,lwr_k=50:2.747,lwr_k=100:4.9837,lwr_k=200:7.1986,lwr_k=300:7.9615,lwr_k=400:8.4954,lwr_k=500:8.7508,lwr_k=600:8.9968,lwr_k=700:9.0972,lwr_k=800:9.2516,lwr_k=900:9.338,lwr_k=1000:9.4139'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.2164,lwr_k=10:30.9168,lwr_k=20:1230.1723,lwr_k=30:385.1184,lwr_k=40:41.5858,lwr_k=50:356107.9705,lwr_k=100:22.9463,lwr_k=200:6108.9016,lwr_k=300:13.8143,lwr_k=400:7.0915,lwr_k=500:6.9192,lwr_k=600:6.8388,lwr_k=700:6.8288,lwr_k=800:6.8314,lwr_k=900:6.8247,lwr_k=1000:6.7988'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.3125,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.453,lwr_k=40:1.3149,lwr_k=50:1.8478,lwr_k=100:3.7521,lwr_k=200:5.449,lwr_k=300:6.03,lwr_k=400:6.3017,lwr_k=500:6.5703,lwr_k=600:6.7281,lwr_k=700:6.9133,lwr_k=800:6.9924,lwr_k=900:7.0888,lwr_k=1000:7.169'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.9084,lwr_k=10:35.9484,lwr_k=20:90.0781,lwr_k=30:1222394.6244,lwr_k=40:42.4205,lwr_k=50:31.8025,lwr_k=100:15.2293,lwr_k=200:11.658,lwr_k=300:9.6328,lwr_k=400:9.4873,lwr_k=500:9.5299,lwr_k=600:9.8096,lwr_k=700:9.8105,lwr_k=800:9.9342,lwr_k=900:10.0079,lwr_k=1000:10.0074'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.2824,lwr_k=10:0.0,lwr_k=20:0.0068,lwr_k=30:1.0469,lwr_k=40:0.7385,lwr_k=50:1.0594,lwr_k=100:3.1294,lwr_k=200:4.8184,lwr_k=300:5.46,lwr_k=400:5.8427,lwr_k=500:6.0283,lwr_k=600:6.1569,lwr_k=700:6.2725,lwr_k=800:6.3196,lwr_k=900:6.3779,lwr_k=1000:6.4144'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:26.7246,lwr_k=10:33.1916,lwr_k=20:934.1308,lwr_k=30:490.766,lwr_k=40:34495.9904,lwr_k=50:40.2628,lwr_k=100:36.6744,lwr_k=200:26.962,lwr_k=300:22.8009,lwr_k=400:23.3464,lwr_k=500:23.438,lwr_k=600:24.0576,lwr_k=700:24.222,lwr_k=800:24.3278,lwr_k=900:24.4805,lwr_k=1000:24.4994'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:6.8096,lwr_k=10:0.0,lwr_k=20:0.0003,lwr_k=30:0.7856,lwr_k=40:1.2966,lwr_k=50:1.6331,lwr_k=100:3.1844,lwr_k=200:4.6525,lwr_k=300:5.1072,lwr_k=400:5.3583,lwr_k=500:5.5045,lwr_k=600:5.6088,lwr_k=700:5.6878,lwr_k=800:5.756,lwr_k=900:5.7946,lwr_k=1000:5.8637'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.3824,lwr_k=10:36.9528,lwr_k=20:224.9221,lwr_k=30:653.7005,lwr_k=40:573939.9919,lwr_k=50:39.5591,lwr_k=100:18.6112,lwr_k=200:13.5733,lwr_k=300:9.9575,lwr_k=400:9.9924,lwr_k=500:10.4067,lwr_k=600:10.411,lwr_k=700:10.61,lwr_k=800:10.768,lwr_k=900:10.9516,lwr_k=1000:11.068'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_30'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.3403,lwr_k=10:8.4521,lwr_k=20:9.3255,lwr_k=30:9.3876,lwr_k=40:9.3264,lwr_k=50:9.4054,lwr_k=100:9.6695,lwr_k=200:10.0441,lwr_k=300:10.2161,lwr_k=400:10.3159,lwr_k=500:10.365,lwr_k=600:10.3742,lwr_k=700:10.394,lwr_k=800:10.4124,lwr_k=900:10.3915,lwr_k=1000:10.3828'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:14.4202,lwr_k=10:15.0921,lwr_k=20:14.4682,lwr_k=30:14.3447,lwr_k=40:13.8957,lwr_k=50:13.6812,lwr_k=100:13.509,lwr_k=200:13.6378,lwr_k=300:13.9271,lwr_k=400:14.0425,lwr_k=500:14.1325,lwr_k=600:14.1768,lwr_k=700:14.1988,lwr_k=800:14.2168,lwr_k=900:14.1982,lwr_k=1000:14.1683'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:45.8286,lwr_k=10:32.1924,lwr_k=20:35.6598,lwr_k=30:36.7591,lwr_k=40:37.9148,lwr_k=50:38.6783,lwr_k=100:40.7494,lwr_k=200:42.4577,lwr_k=300:43.0498,lwr_k=400:43.5008,lwr_k=500:43.8435,lwr_k=600:44.0969,lwr_k=700:44.3915,lwr_k=800:44.5417,lwr_k=900:44.6531,lwr_k=1000:44.7148'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:41.2297,lwr_k=10:51.14,lwr_k=20:47.2737,lwr_k=30:41.6096,lwr_k=40:40.3533,lwr_k=50:40.7855,lwr_k=100:39.4148,lwr_k=200:38.8312,lwr_k=300:38.6348,lwr_k=400:38.5377,lwr_k=500:38.6389,lwr_k=600:38.8247,lwr_k=700:39.1835,lwr_k=800:39.4199,lwr_k=900:39.7092,lwr_k=1000:39.9392'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.8035,lwr_k=10:10.2128,lwr_k=20:10.8741,lwr_k=30:11.1798,lwr_k=40:11.4921,lwr_k=50:11.4728,lwr_k=100:11.4223,lwr_k=200:11.5571,lwr_k=300:11.5479,lwr_k=400:11.544,lwr_k=500:11.5685,lwr_k=600:11.5769,lwr_k=700:11.5939,lwr_k=800:11.6117,lwr_k=900:11.6162,lwr_k=1000:11.619'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:12.1508,lwr_k=10:12.8138,lwr_k=20:11.8814,lwr_k=30:11.5251,lwr_k=40:11.931,lwr_k=50:12.0495,lwr_k=100:11.9138,lwr_k=200:12.0934,lwr_k=300:12.1267,lwr_k=400:12.0546,lwr_k=500:12.048,lwr_k=600:12.032,lwr_k=700:12.006,lwr_k=800:12.0267,lwr_k=900:12.0254,lwr_k=1000:12.026'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:13.1192,lwr_k=10:8.5494,lwr_k=20:9.0965,lwr_k=30:9.544,lwr_k=40:9.9308,lwr_k=50:10.3017,lwr_k=100:10.9663,lwr_k=200:12.3682,lwr_k=300:12.5646,lwr_k=400:12.6286,lwr_k=500:12.6998,lwr_k=600:12.7168,lwr_k=700:12.7443,lwr_k=800:12.7331,lwr_k=900:12.7751,lwr_k=1000:12.7727'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:12.295,lwr_k=10:12.5806,lwr_k=20:11.9386,lwr_k=30:11.4076,lwr_k=40:11.4423,lwr_k=50:11.1182,lwr_k=100:11.5309,lwr_k=200:12.1456,lwr_k=300:12.165,lwr_k=400:12.1457,lwr_k=500:12.1014,lwr_k=600:12.1278,lwr_k=700:12.1687,lwr_k=800:12.199,lwr_k=900:12.2633,lwr_k=1000:12.2885'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.5674,lwr_k=10:7.0168,lwr_k=20:7.57,lwr_k=30:7.6713,lwr_k=40:7.7677,lwr_k=50:7.8665,lwr_k=100:8.0286,lwr_k=200:8.2135,lwr_k=300:8.3011,lwr_k=400:8.3542,lwr_k=500:8.3717,lwr_k=600:8.4076,lwr_k=700:8.4257,lwr_k=800:8.4369,lwr_k=900:8.4632,lwr_k=1000:8.463'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:23.2038,lwr_k=10:24.9674,lwr_k=20:23.732,lwr_k=30:23.4593,lwr_k=40:23.5041,lwr_k=50:23.3457,lwr_k=100:23.0928,lwr_k=200:22.6815,lwr_k=300:22.7323,lwr_k=400:22.7612,lwr_k=500:22.8087,lwr_k=600:22.8083,lwr_k=700:22.8906,lwr_k=800:22.9051,lwr_k=900:22.96,lwr_k=1000:22.9882'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:14.2522,lwr_k=10:11.1723,lwr_k=20:12.0511,lwr_k=30:12.4795,lwr_k=40:12.7473,lwr_k=50:12.8254,lwr_k=100:13.0593,lwr_k=200:13.283,lwr_k=300:13.3542,lwr_k=400:13.3624,lwr_k=500:13.3745,lwr_k=600:13.3641,lwr_k=700:13.3755,lwr_k=800:13.3815,lwr_k=900:13.402,lwr_k=1000:13.408'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:15.5735,lwr_k=10:16.2548,lwr_k=20:15.2036,lwr_k=30:14.7703,lwr_k=40:14.8328,lwr_k=50:14.6956,lwr_k=100:14.8074,lwr_k=200:14.7917,lwr_k=300:14.8267,lwr_k=400:14.8653,lwr_k=500:14.8537,lwr_k=600:14.817,lwr_k=700:14.8482,lwr_k=800:14.8509,lwr_k=900:14.8674,lwr_k=1000:14.8772'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_31'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.1116,lwr_k=10:0.0001,lwr_k=20:0.0043,lwr_k=30:0.0192,lwr_k=40:0.0522,lwr_k=50:0.5722,lwr_k=100:2.4766,lwr_k=200:3.4399,lwr_k=300:3.8364,lwr_k=400:4.0404,lwr_k=500:4.1582,lwr_k=600:4.2047,lwr_k=700:4.2474,lwr_k=800:4.2882,lwr_k=900:4.3433,lwr_k=1000:4.3696'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.2705,lwr_k=10:17.5167,lwr_k=20:25.5776,lwr_k=30:49.0031,lwr_k=40:955.5511,lwr_k=50:29259.9058,lwr_k=100:9.7849,lwr_k=200:7.9769,lwr_k=300:7.8111,lwr_k=400:7.7854,lwr_k=500:7.894,lwr_k=600:7.9201,lwr_k=700:7.9974,lwr_k=800:8.05,lwr_k=900:8.1044,lwr_k=1000:8.0791'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.697,lwr_k=10:0.0058,lwr_k=20:0.0628,lwr_k=30:0.1278,lwr_k=40:0.3196,lwr_k=50:1.0744,lwr_k=100:3.2361,lwr_k=200:4.5288,lwr_k=300:5.1756,lwr_k=400:5.6432,lwr_k=500:5.9754,lwr_k=600:6.2391,lwr_k=700:6.456,lwr_k=800:6.5592,lwr_k=900:6.6585,lwr_k=1000:6.7895'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.4881,lwr_k=10:26.6828,lwr_k=20:50.0409,lwr_k=30:189.4772,lwr_k=40:104826.7739,lwr_k=50:133.3173,lwr_k=100:9.1559,lwr_k=200:6.962,lwr_k=300:6.4521,lwr_k=400:6.3507,lwr_k=500:6.2708,lwr_k=600:6.2877,lwr_k=700:6.2744,lwr_k=800:6.2898,lwr_k=900:6.3005,lwr_k=1000:6.3376'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.4872,lwr_k=10:0.0119,lwr_k=20:0.0794,lwr_k=30:0.1289,lwr_k=40:0.1914,lwr_k=50:1.1016,lwr_k=100:3.2965,lwr_k=200:4.3193,lwr_k=300:4.7671,lwr_k=400:5.1114,lwr_k=500:5.4267,lwr_k=600:5.5832,lwr_k=700:5.8465,lwr_k=800:6.0763,lwr_k=900:6.1938,lwr_k=1000:6.3871'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.7461,lwr_k=10:32.176,lwr_k=20:36.9184,lwr_k=30:286.9411,lwr_k=40:65554.6903,lwr_k=50:69119.4446,lwr_k=100:8.2266,lwr_k=200:6.7478,lwr_k=300:6.306,lwr_k=400:6.2404,lwr_k=500:6.2665,lwr_k=600:6.0937,lwr_k=700:6.0715,lwr_k=800:6.065,lwr_k=900:6.0297,lwr_k=1000:6.0668'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.7665,lwr_k=10:0.0,lwr_k=20:0.008,lwr_k=30:0.0259,lwr_k=40:0.0404,lwr_k=50:0.6243,lwr_k=100:2.8112,lwr_k=200:4.2751,lwr_k=300:4.8805,lwr_k=400:5.171,lwr_k=500:5.3624,lwr_k=600:5.5052,lwr_k=700:5.6263,lwr_k=800:5.751,lwr_k=900:5.8342,lwr_k=1000:5.8976'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.966,lwr_k=10:24.06,lwr_k=20:49.1318,lwr_k=30:58.8892,lwr_k=40:346.1059,lwr_k=50:96.2691,lwr_k=100:10.6106,lwr_k=200:9.6995,lwr_k=300:8.2471,lwr_k=400:8.2541,lwr_k=500:8.2667,lwr_k=600:8.2487,lwr_k=700:8.267,lwr_k=800:8.3004,lwr_k=900:8.3104,lwr_k=1000:8.3678'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.2151,lwr_k=10:0.0142,lwr_k=20:0.1153,lwr_k=30:0.212,lwr_k=40:0.4505,lwr_k=50:1.2311,lwr_k=100:3.171,lwr_k=200:4.042,lwr_k=300:4.2863,lwr_k=400:4.4417,lwr_k=500:4.5718,lwr_k=600:4.6533,lwr_k=700:4.6969,lwr_k=800:4.7485,lwr_k=900:4.7877,lwr_k=1000:4.8207'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:20.5052,lwr_k=10:59.7746,lwr_k=20:230.7856,lwr_k=30:197.9559,lwr_k=40:821.4578,lwr_k=50:4949.8254,lwr_k=100:20.5253,lwr_k=200:16.4753,lwr_k=300:15.6238,lwr_k=400:16.1545,lwr_k=500:16.7351,lwr_k=600:16.4441,lwr_k=700:16.5745,lwr_k=800:16.8152,lwr_k=900:17.3651,lwr_k=1000:17.3597'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:5.4689,lwr_k=10:0.0477,lwr_k=20:0.1578,lwr_k=30:0.3424,lwr_k=40:0.8961,lwr_k=50:1.0966,lwr_k=100:2.7947,lwr_k=200:3.7075,lwr_k=300:4.2439,lwr_k=400:4.606,lwr_k=500:4.8329,lwr_k=600:4.9707,lwr_k=700:5.0364,lwr_k=800:5.1095,lwr_k=900:5.157,lwr_k=1000:5.1828'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.7311,lwr_k=10:49.8857,lwr_k=20:72.5522,lwr_k=30:158.6142,lwr_k=40:838772.6114,lwr_k=50:119269.9335,lwr_k=100:10.6668,lwr_k=200:9.4797,lwr_k=300:8.7765,lwr_k=400:7.9049,lwr_k=500:7.7043,lwr_k=600:7.6583,lwr_k=700:7.5898,lwr_k=800:7.5993,lwr_k=900:7.5642,lwr_k=1000:7.5401'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_32'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:51.9503,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:1.7685,lwr_k=50:3.1675,lwr_k=100:7.7792,lwr_k=200:12.4101,lwr_k=300:15.9034,lwr_k=400:18.0115,lwr_k=500:20.1808,lwr_k=600:21.6639,lwr_k=700:23.0989,lwr_k=800:24.3629,lwr_k=900:25.7626,lwr_k=1000:27.1061'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:69.5441,lwr_k=10:37.4078,lwr_k=20:49.0049,lwr_k=30:1250.9767,lwr_k=40:46.0598,lwr_k=50:33.8371,lwr_k=100:22.8047,lwr_k=200:19.6124,lwr_k=300:22.0544,lwr_k=400:24.3841,lwr_k=500:26.1652,lwr_k=600:28.548,lwr_k=700:31.0702,lwr_k=800:33.1623,lwr_k=900:34.8113,lwr_k=1000:36.6512'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:65.7333,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:2.1615,lwr_k=50:4.2719,lwr_k=100:10.3143,lwr_k=200:16.076,lwr_k=300:19.3028,lwr_k=400:22.106,lwr_k=500:24.2134,lwr_k=600:26.3362,lwr_k=700:28.3451,lwr_k=800:30.1859,lwr_k=900:32.3691,lwr_k=1000:33.9205'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:56.5212,lwr_k=10:37.141,lwr_k=20:46.4767,lwr_k=30:534.3191,lwr_k=40:41.6068,lwr_k=50:28.3136,lwr_k=100:18.9672,lwr_k=200:20.1733,lwr_k=300:22.1988,lwr_k=400:23.6253,lwr_k=500:24.9024,lwr_k=600:26.3437,lwr_k=700:28.0282,lwr_k=800:29.1306,lwr_k=900:30.1304,lwr_k=1000:31.2513'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:73.0322,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:2.5354,lwr_k=50:4.5494,lwr_k=100:10.2782,lwr_k=200:17.0135,lwr_k=300:21.0058,lwr_k=400:23.9284,lwr_k=500:27.0231,lwr_k=600:29.0108,lwr_k=700:30.9811,lwr_k=800:32.9189,lwr_k=900:35.0847,lwr_k=1000:36.4926'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:47.11,lwr_k=10:32.2036,lwr_k=20:50.3378,lwr_k=30:576.0802,lwr_k=40:59.4587,lwr_k=50:27.8286,lwr_k=100:16.1505,lwr_k=200:16.2495,lwr_k=300:17.4904,lwr_k=400:18.0777,lwr_k=500:18.7842,lwr_k=600:20.1595,lwr_k=700:21.1666,lwr_k=800:21.6853,lwr_k=900:22.6375,lwr_k=1000:23.4256'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:70.8639,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:2.2,lwr_k=50:3.9119,lwr_k=100:8.9737,lwr_k=200:16.3773,lwr_k=300:21.1355,lwr_k=400:24.0228,lwr_k=500:26.9072,lwr_k=600:29.1451,lwr_k=700:31.116,lwr_k=800:33.1128,lwr_k=900:35.1166,lwr_k=1000:36.4048'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:63.4128,lwr_k=10:35.5768,lwr_k=20:58.5207,lwr_k=30:984.1997,lwr_k=40:44.1863,lwr_k=50:29.6869,lwr_k=100:20.4605,lwr_k=200:21.0387,lwr_k=300:22.7723,lwr_k=400:23.3458,lwr_k=500:24.4315,lwr_k=600:25.5737,lwr_k=700:26.9571,lwr_k=800:28.1382,lwr_k=900:29.1784,lwr_k=1000:30.3348'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:53.8343,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:2.0047,lwr_k=50:3.4052,lwr_k=100:7.5276,lwr_k=200:12.3804,lwr_k=300:15.8599,lwr_k=400:18.0427,lwr_k=500:19.8437,lwr_k=600:21.7642,lwr_k=700:23.2879,lwr_k=800:24.7029,lwr_k=900:26.1552,lwr_k=1000:27.5458'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:100.4585,lwr_k=10:54.57,lwr_k=20:54.3969,lwr_k=30:569.4849,lwr_k=40:56.1096,lwr_k=50:47.6463,lwr_k=100:34.4965,lwr_k=200:43.1275,lwr_k=300:47.0897,lwr_k=400:51.4487,lwr_k=500:55.3845,lwr_k=600:57.238,lwr_k=700:59.8317,lwr_k=800:61.5433,lwr_k=900:63.9341,lwr_k=1000:66.7696'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:64.5564,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:2.0311,lwr_k=50:3.8773,lwr_k=100:8.1401,lwr_k=200:13.1764,lwr_k=300:17.4447,lwr_k=400:20.0118,lwr_k=500:21.908,lwr_k=600:23.7582,lwr_k=700:25.4827,lwr_k=800:27.0101,lwr_k=900:28.3702,lwr_k=1000:29.651'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:63.5478,lwr_k=10:35.897,lwr_k=20:50.0315,lwr_k=30:1035.269,lwr_k=40:43.5054,lwr_k=50:25.31,lwr_k=100:21.2628,lwr_k=200:21.3112,lwr_k=300:21.8762,lwr_k=400:23.7786,lwr_k=500:25.0459,lwr_k=600:26.0092,lwr_k=700:27.0961,lwr_k=800:28.1513,lwr_k=900:28.6648,lwr_k=1000:29.6731'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_33'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:57.8736,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0036,lwr_k=50:1.0408,lwr_k=100:8.2455,lwr_k=200:14.7108,lwr_k=300:17.486,lwr_k=400:19.5484,lwr_k=500:21.3653,lwr_k=600:23.4371,lwr_k=700:25.4656,lwr_k=800:27.3012,lwr_k=900:28.7681,lwr_k=1000:30.6475'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:73.9139,lwr_k=10:95.885,lwr_k=20:115.2934,lwr_k=30:152.5029,lwr_k=40:263.1439,lwr_k=50:186.8843,lwr_k=100:33.7782,lwr_k=200:30.636,lwr_k=300:30.0498,lwr_k=400:31.1712,lwr_k=500:31.0309,lwr_k=600:32.063,lwr_k=700:33.9474,lwr_k=800:34.7345,lwr_k=900:35.6719,lwr_k=1000:37.3494'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:71.9798,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:1.0369,lwr_k=100:7.6869,lwr_k=200:14.7133,lwr_k=300:19.0583,lwr_k=400:22.3737,lwr_k=500:24.7611,lwr_k=600:27.3277,lwr_k=700:29.3088,lwr_k=800:31.0542,lwr_k=900:32.9302,lwr_k=1000:34.3874'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:56.7109,lwr_k=10:48.3705,lwr_k=20:58.3923,lwr_k=30:83.0099,lwr_k=40:176.477,lwr_k=50:125.8321,lwr_k=100:25.0372,lwr_k=200:19.3599,lwr_k=300:21.4515,lwr_k=400:22.5296,lwr_k=500:23.8792,lwr_k=600:24.8249,lwr_k=700:25.3145,lwr_k=800:26.8446,lwr_k=900:29.2205,lwr_k=1000:29.465'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:62.2801,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:0.9046,lwr_k=100:7.0162,lwr_k=200:12.7972,lwr_k=300:17.6864,lwr_k=400:19.7704,lwr_k=500:22.5602,lwr_k=600:25.2018,lwr_k=700:27.8746,lwr_k=800:29.6347,lwr_k=900:31.4831,lwr_k=1000:33.1643'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:41.0541,lwr_k=10:35.968,lwr_k=20:42.0006,lwr_k=30:67.0578,lwr_k=40:160.56,lwr_k=50:87.4153,lwr_k=100:19.4292,lwr_k=200:15.6852,lwr_k=300:15.853,lwr_k=400:16.9277,lwr_k=500:17.6915,lwr_k=600:17.848,lwr_k=700:19.0234,lwr_k=800:19.6723,lwr_k=900:20.3169,lwr_k=1000:20.3914'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:60.3111,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:0.819,lwr_k=100:7.2383,lwr_k=200:13.1012,lwr_k=300:16.6357,lwr_k=400:19.3189,lwr_k=500:22.0656,lwr_k=600:24.0983,lwr_k=700:26.4238,lwr_k=800:28.3628,lwr_k=900:30.2488,lwr_k=1000:31.8228'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:58.0351,lwr_k=10:32.3716,lwr_k=20:46.6222,lwr_k=30:70.9547,lwr_k=40:183.6983,lwr_k=50:78.8919,lwr_k=100:22.7608,lwr_k=200:20.0297,lwr_k=300:21.5881,lwr_k=400:22.5846,lwr_k=500:22.8695,lwr_k=600:25.1153,lwr_k=700:26.6276,lwr_k=800:28.0437,lwr_k=900:29.7458,lwr_k=1000:31.2555'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:43.9937,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:0.7072,lwr_k=100:5.6168,lwr_k=200:10.0519,lwr_k=300:12.6495,lwr_k=400:14.9593,lwr_k=500:16.8992,lwr_k=600:18.3187,lwr_k=700:19.9977,lwr_k=800:21.4969,lwr_k=900:22.7317,lwr_k=1000:23.8702'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:77.2767,lwr_k=10:47.5156,lwr_k=20:55.0466,lwr_k=30:81.446,lwr_k=40:181.8934,lwr_k=50:101.403,lwr_k=100:33.8958,lwr_k=200:36.5362,lwr_k=300:36.5744,lwr_k=400:40.1936,lwr_k=500:44.7552,lwr_k=600:45.7244,lwr_k=700:48.1486,lwr_k=800:50.3321,lwr_k=900:51.395,lwr_k=1000:52.5717'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:54.8934,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:0.8219,lwr_k=100:5.9029,lwr_k=200:10.8583,lwr_k=300:13.8206,lwr_k=400:16.1083,lwr_k=500:18.0167,lwr_k=600:19.7498,lwr_k=700:21.2914,lwr_k=800:22.7508,lwr_k=900:24.2124,lwr_k=1000:25.4809'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:59.8975,lwr_k=10:39.1041,lwr_k=20:57.8853,lwr_k=30:68.8808,lwr_k=40:169.6871,lwr_k=50:78.4138,lwr_k=100:17.9632,lwr_k=200:18.1464,lwr_k=300:21.7186,lwr_k=400:22.9415,lwr_k=500:25.1322,lwr_k=600:26.0717,lwr_k=700:27.4731,lwr_k=800:29.2067,lwr_k=900:30.358,lwr_k=1000:31.0043'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_34'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.0734,lwr_k=10:0.0,lwr_k=20:0.0001,lwr_k=30:0.2902,lwr_k=40:0.9074,lwr_k=50:8.5665,lwr_k=100:5.0941,lwr_k=200:6.8053,lwr_k=300:7.5401,lwr_k=400:8.0922,lwr_k=500:8.3329,lwr_k=600:8.4998,lwr_k=700:8.6482,lwr_k=800:8.7748,lwr_k=900:8.9045,lwr_k=1000:9.005'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:13.1533,lwr_k=10:76.3602,lwr_k=20:118.216,lwr_k=30:224.4542,lwr_k=40:500.4217,lwr_k=50:121.3862,lwr_k=100:19.0254,lwr_k=200:12.6032,lwr_k=300:12.3448,lwr_k=400:12.1286,lwr_k=500:11.9968,lwr_k=600:12.0696,lwr_k=700:12.1464,lwr_k=800:12.1821,lwr_k=900:12.2653,lwr_k=1000:12.3707'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.1473,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:0.5509,lwr_k=100:2.3474,lwr_k=200:4.0579,lwr_k=300:5.0451,lwr_k=400:5.4049,lwr_k=500:5.5788,lwr_k=600:5.7269,lwr_k=700:5.8034,lwr_k=800:5.9062,lwr_k=900:6.0131,lwr_k=1000:6.0913'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:6.8641,lwr_k=10:18.9542,lwr_k=20:28.7499,lwr_k=30:38.2296,lwr_k=40:268.0917,lwr_k=50:75.3611,lwr_k=100:8.6021,lwr_k=200:6.6858,lwr_k=300:6.2859,lwr_k=400:6.1486,lwr_k=500:6.1084,lwr_k=600:6.1181,lwr_k=700:6.1098,lwr_k=800:6.1396,lwr_k=900:6.1523,lwr_k=1000:6.1821'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.2133,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0097,lwr_k=40:0.1624,lwr_k=50:1.4576,lwr_k=100:3.0893,lwr_k=200:4.8985,lwr_k=300:5.316,lwr_k=400:5.5306,lwr_k=500:5.7261,lwr_k=600:5.8095,lwr_k=700:5.915,lwr_k=800:6.0355,lwr_k=900:6.1482,lwr_k=1000:6.2279'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.3502,lwr_k=10:81.4346,lwr_k=20:81.2322,lwr_k=30:164.8693,lwr_k=40:478.3864,lwr_k=50:240.6525,lwr_k=100:15.6069,lwr_k=200:6.803,lwr_k=300:6.1842,lwr_k=400:6.0776,lwr_k=500:6.0541,lwr_k=600:5.9807,lwr_k=700:5.8412,lwr_k=800:5.7618,lwr_k=900:5.7083,lwr_k=1000:5.72'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.836,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0247,lwr_k=40:0.4274,lwr_k=50:3.3636,lwr_k=100:3.6248,lwr_k=200:4.8187,lwr_k=300:5.1173,lwr_k=400:5.3492,lwr_k=500:5.4881,lwr_k=600:5.616,lwr_k=700:5.7147,lwr_k=800:5.7856,lwr_k=900:5.8334,lwr_k=1000:6.0044'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.021,lwr_k=10:52.8327,lwr_k=20:165.4866,lwr_k=30:312.3324,lwr_k=40:27722.3862,lwr_k=50:77264.2194,lwr_k=100:376.0008,lwr_k=200:9.2028,lwr_k=300:11.5344,lwr_k=400:7.9671,lwr_k=500:8.032,lwr_k=600:7.9352,lwr_k=700:7.8133,lwr_k=800:7.8971,lwr_k=900:7.9174,lwr_k=1000:12.2177'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.9451,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0004,lwr_k=40:0.0299,lwr_k=50:0.7347,lwr_k=100:2.5176,lwr_k=200:3.6104,lwr_k=300:3.9814,lwr_k=400:4.2106,lwr_k=500:4.3242,lwr_k=600:4.4401,lwr_k=700:4.5476,lwr_k=800:4.6101,lwr_k=900:4.6649,lwr_k=1000:4.6948'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:18.1838,lwr_k=10:61.5466,lwr_k=20:98.1624,lwr_k=30:135.0208,lwr_k=40:3211.5315,lwr_k=50:691.9473,lwr_k=100:51.453,lwr_k=200:16.2705,lwr_k=300:15.7925,lwr_k=400:15.6263,lwr_k=500:15.2642,lwr_k=600:15.5676,lwr_k=700:15.835,lwr_k=800:16.1683,lwr_k=900:16.3338,lwr_k=1000:16.8855'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:6.3272,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0008,lwr_k=50:0.5579,lwr_k=100:2.2732,lwr_k=200:3.2602,lwr_k=300:3.7039,lwr_k=400:3.9605,lwr_k=500:4.1061,lwr_k=600:4.2154,lwr_k=700:4.3558,lwr_k=800:4.4418,lwr_k=900:4.5288,lwr_k=1000:4.6087'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.9876,lwr_k=10:41.8848,lwr_k=20:55.4132,lwr_k=30:131.7361,lwr_k=40:1196.8644,lwr_k=50:187.5163,lwr_k=100:9.325,lwr_k=200:6.8208,lwr_k=300:6.6979,lwr_k=400:6.7735,lwr_k=500:6.758,lwr_k=600:6.7966,lwr_k=700:6.8581,lwr_k=800:6.9578,lwr_k=900:6.9871,lwr_k=1000:7.033'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_35'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.2739,lwr_k=10:3.4597,lwr_k=20:4.6863,lwr_k=30:4.9635,lwr_k=40:5.1424,lwr_k=50:5.2063,lwr_k=100:5.4729,lwr_k=200:5.658,lwr_k=300:5.7729,lwr_k=400:5.8388,lwr_k=500:5.8847,lwr_k=600:5.9236,lwr_k=700:5.9554,lwr_k=800:5.9718,lwr_k=900:5.9859,lwr_k=1000:6.0036'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.4118,lwr_k=10:13.314,lwr_k=20:9.9652,lwr_k=30:9.1347,lwr_k=40:8.8841,lwr_k=50:8.8289,lwr_k=100:8.7049,lwr_k=200:8.8073,lwr_k=300:8.9879,lwr_k=400:9.019,lwr_k=500:9.0244,lwr_k=600:9.0303,lwr_k=700:9.0611,lwr_k=800:9.1038,lwr_k=900:9.1502,lwr_k=1000:9.1817'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.4672,lwr_k=10:6.2304,lwr_k=20:7.8251,lwr_k=30:8.3434,lwr_k=40:8.5414,lwr_k=50:8.6151,lwr_k=100:8.8588,lwr_k=200:9.0615,lwr_k=300:9.1182,lwr_k=400:9.1748,lwr_k=500:9.1946,lwr_k=600:9.1966,lwr_k=700:9.222,lwr_k=800:9.2306,lwr_k=900:9.2389,lwr_k=1000:9.277'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.9506,lwr_k=10:14.8114,lwr_k=20:9.2622,lwr_k=30:8.9638,lwr_k=40:9.2971,lwr_k=50:9.3662,lwr_k=100:9.3041,lwr_k=200:9.3525,lwr_k=300:9.2814,lwr_k=400:9.3197,lwr_k=500:9.3309,lwr_k=600:9.3493,lwr_k=700:9.3852,lwr_k=800:9.3965,lwr_k=900:9.421,lwr_k=1000:9.5798'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.8783,lwr_k=10:3.9322,lwr_k=20:5.2316,lwr_k=30:5.8269,lwr_k=40:6.381,lwr_k=50:6.6045,lwr_k=100:7.62,lwr_k=200:9.0304,lwr_k=300:9.4896,lwr_k=400:9.6779,lwr_k=500:9.819,lwr_k=600:9.9141,lwr_k=700:10.0137,lwr_k=800:10.0823,lwr_k=900:10.1575,lwr_k=1000:10.2114'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.7408,lwr_k=10:13.1109,lwr_k=20:8.6498,lwr_k=30:8.0007,lwr_k=40:7.5345,lwr_k=50:7.2002,lwr_k=100:6.845,lwr_k=200:6.6765,lwr_k=300:6.6998,lwr_k=400:6.6971,lwr_k=500:6.757,lwr_k=600:6.9365,lwr_k=700:7.1099,lwr_k=800:7.1271,lwr_k=900:7.1304,lwr_k=1000:7.1407'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.3395,lwr_k=10:2.8994,lwr_k=20:4.6839,lwr_k=30:5.193,lwr_k=40:6.0976,lwr_k=50:6.5721,lwr_k=100:7.2177,lwr_k=200:7.8217,lwr_k=300:8.1684,lwr_k=400:8.3549,lwr_k=500:8.4329,lwr_k=600:8.4889,lwr_k=700:8.5473,lwr_k=800:8.5681,lwr_k=900:8.5912,lwr_k=1000:8.629'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.4553,lwr_k=10:14.0645,lwr_k=20:9.8814,lwr_k=30:9.1208,lwr_k=40:8.5823,lwr_k=50:8.5351,lwr_k=100:8.6281,lwr_k=200:9.0901,lwr_k=300:9.1064,lwr_k=400:9.3102,lwr_k=500:9.4123,lwr_k=600:9.4834,lwr_k=700:9.4907,lwr_k=800:9.4814,lwr_k=900:9.4714,lwr_k=1000:9.4453'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.7196,lwr_k=10:2.4601,lwr_k=20:4.1263,lwr_k=30:4.5944,lwr_k=40:4.7543,lwr_k=50:4.8429,lwr_k=100:5.1644,lwr_k=200:5.31,lwr_k=300:5.3522,lwr_k=400:5.4042,lwr_k=500:5.4378,lwr_k=600:5.4789,lwr_k=700:5.5105,lwr_k=800:5.5421,lwr_k=900:5.5593,lwr_k=1000:5.571'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:20.7434,lwr_k=10:36.0846,lwr_k=20:16.9677,lwr_k=30:16.3879,lwr_k=40:17.5525,lwr_k=50:18.4948,lwr_k=100:19.7223,lwr_k=200:20.3171,lwr_k=300:20.2148,lwr_k=400:20.3545,lwr_k=500:20.5453,lwr_k=600:20.6762,lwr_k=700:20.8333,lwr_k=800:20.8411,lwr_k=900:20.9065,lwr_k=1000:20.9133'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:7.7266,lwr_k=10:2.9824,lwr_k=20:4.3585,lwr_k=30:4.9911,lwr_k=40:5.1739,lwr_k=50:5.3249,lwr_k=100:5.9426,lwr_k=200:6.5083,lwr_k=300:6.6473,lwr_k=400:6.7737,lwr_k=500:6.8999,lwr_k=600:7.0185,lwr_k=700:7.0799,lwr_k=800:7.1319,lwr_k=900:7.2098,lwr_k=1000:7.2571'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.2424,lwr_k=10:51.1562,lwr_k=20:9.6126,lwr_k=30:9.6603,lwr_k=40:8.2943,lwr_k=50:8.4914,lwr_k=100:7.4545,lwr_k=200:7.8734,lwr_k=300:8.077,lwr_k=400:8.1567,lwr_k=500:8.2235,lwr_k=600:8.3256,lwr_k=700:8.398,lwr_k=800:8.4915,lwr_k=900:8.5627,lwr_k=1000:8.6245'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_36'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.633,lwr_k=10:5.9075,lwr_k=20:6.5512,lwr_k=30:6.7492,lwr_k=40:6.8262,lwr_k=50:6.9035,lwr_k=100:7.1236,lwr_k=200:6.9079,lwr_k=300:7.3283,lwr_k=400:7.3449,lwr_k=500:7.2965,lwr_k=600:7.3793,lwr_k=700:7.3849,lwr_k=800:7.4529,lwr_k=900:7.4485,lwr_k=1000:7.396'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.2941,lwr_k=10:10.3441,lwr_k=20:9.9097,lwr_k=30:10.3158,lwr_k=40:9.5133,lwr_k=50:9.6867,lwr_k=100:9.4672,lwr_k=200:9.7531,lwr_k=300:9.6285,lwr_k=400:9.8002,lwr_k=500:9.9184,lwr_k=600:10.1395,lwr_k=700:9.9781,lwr_k=800:10.2491,lwr_k=900:9.9222,lwr_k=1000:9.9347'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.8825,lwr_k=10:5.192,lwr_k=20:6.1556,lwr_k=30:6.7268,lwr_k=40:7.5849,lwr_k=50:7.3288,lwr_k=100:7.6942,lwr_k=200:8.3585,lwr_k=300:8.3664,lwr_k=400:8.4258,lwr_k=500:8.1725,lwr_k=600:8.4094,lwr_k=700:8.9507,lwr_k=800:8.499,lwr_k=900:8.6144,lwr_k=1000:8.3282'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.7256,lwr_k=10:14.1335,lwr_k=20:7.919,lwr_k=30:7.336,lwr_k=40:142.1868,lwr_k=50:1966247.1052,lwr_k=100:4899313.2895,lwr_k=200:6.8904,lwr_k=300:6.9042,lwr_k=400:6.832,lwr_k=500:7.0207,lwr_k=600:6.7608,lwr_k=700:6.9127,lwr_k=800:6.8219,lwr_k=900:6.7825,lwr_k=1000:6.9821'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.4529,lwr_k=10:5.8815,lwr_k=20:6.2589,lwr_k=30:6.4816,lwr_k=40:6.5717,lwr_k=50:6.7365,lwr_k=100:6.8236,lwr_k=200:7.0557,lwr_k=300:7.0554,lwr_k=400:7.1656,lwr_k=500:7.2107,lwr_k=600:7.2454,lwr_k=700:7.205,lwr_k=800:7.216,lwr_k=900:7.2063,lwr_k=1000:7.2764'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.7515,lwr_k=10:14.9825,lwr_k=20:7.7088,lwr_k=30:7.597,lwr_k=40:5025158998.7036,lwr_k=50:26.6629,lwr_k=100:7.3315,lwr_k=200:7.3666,lwr_k=300:7.4858,lwr_k=400:7.5919,lwr_k=500:7.3286,lwr_k=600:7.4772,lwr_k=700:7.5529,lwr_k=800:7.4819,lwr_k=900:7.6623,lwr_k=1000:7.4688'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.2796,lwr_k=10:6.3138,lwr_k=20:6.871,lwr_k=30:7.0058,lwr_k=40:7.2386,lwr_k=50:7.374,lwr_k=100:7.4154,lwr_k=200:7.7624,lwr_k=300:7.882,lwr_k=400:7.9385,lwr_k=500:7.9804,lwr_k=600:8.0132,lwr_k=700:8.018,lwr_k=800:8.0219,lwr_k=900:8.0277,lwr_k=1000:8.0092'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:11.7529,lwr_k=10:27.8076,lwr_k=20:10.6437,lwr_k=30:10.3916,lwr_k=40:11985168.394,lwr_k=50:10.9041,lwr_k=100:2635047.8772,lwr_k=200:22422.7309,lwr_k=300:11.3297,lwr_k=400:11.393,lwr_k=500:11.452,lwr_k=600:11.5076,lwr_k=700:11.5069,lwr_k=800:11.5424,lwr_k=900:11.5012,lwr_k=1000:11.5316'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.1068,lwr_k=10:5.3881,lwr_k=20:6.4989,lwr_k=30:6.7351,lwr_k=40:7.1113,lwr_k=50:7.2291,lwr_k=100:7.5038,lwr_k=200:7.7409,lwr_k=300:7.8291,lwr_k=400:7.7973,lwr_k=500:7.8307,lwr_k=600:7.9392,lwr_k=700:7.9853,lwr_k=800:7.9212,lwr_k=900:7.9606,lwr_k=1000:7.9646'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:21.4014,lwr_k=10:57.1406,lwr_k=20:20.2021,lwr_k=30:19.6097,lwr_k=40:21.34,lwr_k=50:380822314.5153,lwr_k=100:2183447.0069,lwr_k=200:2181774.2537,lwr_k=300:20.7578,lwr_k=400:21.0982,lwr_k=500:58.8605,lwr_k=600:21.8966,lwr_k=700:21.128,lwr_k=800:21.2279,lwr_k=900:21.1132,lwr_k=1000:21.1297'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:7.6596,lwr_k=10:5.7841,lwr_k=20:6.3509,lwr_k=30:6.4584,lwr_k=40:6.4702,lwr_k=50:6.6092,lwr_k=100:6.8648,lwr_k=200:7.2266,lwr_k=300:7.2411,lwr_k=400:7.2582,lwr_k=500:7.379,lwr_k=600:7.3544,lwr_k=700:7.5032,lwr_k=800:7.3444,lwr_k=900:7.4296,lwr_k=1000:7.4969'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.8939,lwr_k=10:11.7352,lwr_k=20:11.5583,lwr_k=30:10.356,lwr_k=40:2401835239.5528,lwr_k=50:10.1336,lwr_k=100:10.9111,lwr_k=200:10.8582,lwr_k=300:16.1561,lwr_k=400:10.1457,lwr_k=500:10.1165,lwr_k=600:10.0873,lwr_k=700:10.2087,lwr_k=800:10.1149,lwr_k=900:10.1441,lwr_k=1000:10.2151'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_37'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.8217,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.5549,lwr_k=50:1.2786,lwr_k=100:2.9429,lwr_k=200:4.0711,lwr_k=300:4.578,lwr_k=400:4.9099,lwr_k=500:5.1617,lwr_k=600:5.3612,lwr_k=700:5.5102,lwr_k=800:5.6382,lwr_k=900:5.7149,lwr_k=1000:5.788'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.8653,lwr_k=10:29.8799,lwr_k=20:55.5842,lwr_k=30:95.9799,lwr_k=40:37.9538,lwr_k=50:15.286,lwr_k=100:8.5316,lwr_k=200:7.5864,lwr_k=300:7.7113,lwr_k=400:7.8366,lwr_k=500:7.9177,lwr_k=600:8.1241,lwr_k=700:8.259,lwr_k=800:8.3755,lwr_k=900:8.4856,lwr_k=1000:8.5745'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:15.3824,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.5792,lwr_k=50:1.454,lwr_k=100:4.4088,lwr_k=200:6.7503,lwr_k=300:7.7722,lwr_k=400:8.6588,lwr_k=500:9.1413,lwr_k=600:9.4692,lwr_k=700:9.798,lwr_k=800:10.0743,lwr_k=900:10.3557,lwr_k=1000:10.6003'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.7835,lwr_k=10:28.986,lwr_k=20:39.5187,lwr_k=30:108.0241,lwr_k=40:64.0682,lwr_k=50:24.4665,lwr_k=100:9.6502,lwr_k=200:7.6429,lwr_k=300:7.5614,lwr_k=400:7.8092,lwr_k=500:7.7983,lwr_k=600:7.9847,lwr_k=700:8.0623,lwr_k=800:8.3948,lwr_k=900:8.5087,lwr_k=1000:8.6313'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:16.6621,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.725,lwr_k=50:1.6699,lwr_k=100:4.6985,lwr_k=200:7.3788,lwr_k=300:8.7035,lwr_k=400:9.3426,lwr_k=500:9.8942,lwr_k=600:10.2875,lwr_k=700:10.533,lwr_k=800:10.8407,lwr_k=900:11.0792,lwr_k=1000:11.267'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.964,lwr_k=10:27.6771,lwr_k=20:27.2328,lwr_k=30:73.8251,lwr_k=40:44.972,lwr_k=50:21.2082,lwr_k=100:8.1233,lwr_k=200:6.9343,lwr_k=300:6.7675,lwr_k=400:6.8592,lwr_k=500:7.0282,lwr_k=600:7.0214,lwr_k=700:7.054,lwr_k=800:7.0888,lwr_k=900:7.1635,lwr_k=1000:7.2455'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:17.37,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.6646,lwr_k=50:1.9806,lwr_k=100:4.8068,lwr_k=200:7.1822,lwr_k=300:8.6053,lwr_k=400:9.2664,lwr_k=500:9.8277,lwr_k=600:10.3555,lwr_k=700:10.8045,lwr_k=800:11.1855,lwr_k=900:11.4967,lwr_k=1000:11.776'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:14.2485,lwr_k=10:35.5147,lwr_k=20:45.0151,lwr_k=30:93.7685,lwr_k=40:73.7115,lwr_k=50:23.1234,lwr_k=100:10.9431,lwr_k=200:9.7061,lwr_k=300:9.333,lwr_k=400:9.2613,lwr_k=500:9.2988,lwr_k=600:9.4799,lwr_k=700:9.6101,lwr_k=800:9.7841,lwr_k=900:9.972,lwr_k=1000:10.0831'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.9535,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.693,lwr_k=50:1.4737,lwr_k=100:3.3167,lwr_k=200:4.5242,lwr_k=300:5.1731,lwr_k=400:5.5999,lwr_k=500:5.8726,lwr_k=600:6.0859,lwr_k=700:6.2647,lwr_k=800:6.4516,lwr_k=900:6.5936,lwr_k=1000:6.7112'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:30.4181,lwr_k=10:63.5831,lwr_k=20:106.1223,lwr_k=30:284.8212,lwr_k=40:120.683,lwr_k=50:42.8709,lwr_k=100:18.3774,lwr_k=200:20.5565,lwr_k=300:20.582,lwr_k=400:21.6067,lwr_k=500:22.51,lwr_k=600:22.8528,lwr_k=700:23.3624,lwr_k=800:23.9378,lwr_k=900:24.2875,lwr_k=1000:24.5188'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:22.4144,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.8771,lwr_k=50:2.2127,lwr_k=100:5.639,lwr_k=200:8.5144,lwr_k=300:10.0224,lwr_k=400:10.887,lwr_k=500:11.4851,lwr_k=600:12.0239,lwr_k=700:12.4304,lwr_k=800:12.7556,lwr_k=900:13.0814,lwr_k=1000:13.3632'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:19.6657,lwr_k=10:34.4891,lwr_k=20:66.9847,lwr_k=30:114.5259,lwr_k=40:63.0279,lwr_k=50:26.1941,lwr_k=100:12.5168,lwr_k=200:12.4076,lwr_k=300:12.4222,lwr_k=400:12.542,lwr_k=500:12.7801,lwr_k=600:12.9132,lwr_k=700:13.0748,lwr_k=800:13.3565,lwr_k=900:13.398,lwr_k=1000:13.5953'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_38'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.9839,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.1104,lwr_k=40:2.2798,lwr_k=50:2.9499,lwr_k=100:4.549,lwr_k=200:5.4737,lwr_k=300:5.9021,lwr_k=400:6.1151,lwr_k=500:6.2453,lwr_k=600:6.3657,lwr_k=700:6.4846,lwr_k=800:6.5691,lwr_k=900:6.643,lwr_k=1000:6.717'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.9654,lwr_k=10:33.0848,lwr_k=20:88.5076,lwr_k=30:43.2536,lwr_k=40:22.0364,lwr_k=50:16.4112,lwr_k=100:10.8072,lwr_k=200:9.1784,lwr_k=300:9.097,lwr_k=400:9.0442,lwr_k=500:9.1017,lwr_k=600:9.1675,lwr_k=700:9.1952,lwr_k=800:9.3002,lwr_k=900:9.3104,lwr_k=1000:9.3901'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:15.6044,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.4759,lwr_k=40:3.1407,lwr_k=50:4.5601,lwr_k=100:7.2147,lwr_k=200:9.3388,lwr_k=300:10.139,lwr_k=400:10.6181,lwr_k=500:10.8965,lwr_k=600:11.1136,lwr_k=700:11.272,lwr_k=800:11.4409,lwr_k=900:11.6255,lwr_k=1000:11.8116'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.9405,lwr_k=10:33.2268,lwr_k=20:136.2726,lwr_k=30:52.3712,lwr_k=40:21.2634,lwr_k=50:15.2471,lwr_k=100:9.8746,lwr_k=200:8.8137,lwr_k=300:8.6489,lwr_k=400:8.7882,lwr_k=500:8.8869,lwr_k=600:9.0145,lwr_k=700:9.3539,lwr_k=800:9.4195,lwr_k=900:9.4259,lwr_k=1000:9.4959'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:14.7121,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.5159,lwr_k=40:2.994,lwr_k=50:4.223,lwr_k=100:7.102,lwr_k=200:9.2539,lwr_k=300:10.0914,lwr_k=400:10.6146,lwr_k=500:10.9366,lwr_k=600:11.202,lwr_k=700:11.3791,lwr_k=800:11.5633,lwr_k=900:11.67,lwr_k=1000:11.8126'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.2105,lwr_k=10:35.67,lwr_k=20:85.2716,lwr_k=30:42.9167,lwr_k=40:16.7743,lwr_k=50:13.6748,lwr_k=100:9.5766,lwr_k=200:8.4586,lwr_k=300:8.2805,lwr_k=400:8.2857,lwr_k=500:8.1991,lwr_k=600:8.0679,lwr_k=700:8.0142,lwr_k=800:8.0107,lwr_k=900:8.029,lwr_k=1000:8.0127'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.705,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.2151,lwr_k=40:2.5363,lwr_k=50:3.7391,lwr_k=100:6.2494,lwr_k=200:7.6461,lwr_k=300:8.4348,lwr_k=400:8.7703,lwr_k=500:9.0201,lwr_k=600:9.2689,lwr_k=700:9.4772,lwr_k=800:9.5495,lwr_k=900:9.6535,lwr_k=1000:9.7284'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:11.5425,lwr_k=10:39.369,lwr_k=20:254.6712,lwr_k=30:41.0988,lwr_k=40:18.2679,lwr_k=50:14.1143,lwr_k=100:10.3506,lwr_k=200:10.4365,lwr_k=300:10.3525,lwr_k=400:10.3858,lwr_k=500:10.2787,lwr_k=600:10.2105,lwr_k=700:10.2389,lwr_k=800:10.333,lwr_k=900:10.3585,lwr_k=1000:10.3379'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.8757,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.131,lwr_k=40:2.4034,lwr_k=50:2.9407,lwr_k=100:4.4058,lwr_k=200:5.3743,lwr_k=300:5.7198,lwr_k=400:5.9426,lwr_k=500:6.0319,lwr_k=600:6.1605,lwr_k=700:6.2527,lwr_k=800:6.3662,lwr_k=900:6.4383,lwr_k=1000:6.5009'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:27.5792,lwr_k=10:39.5042,lwr_k=20:93.3746,lwr_k=30:48.1169,lwr_k=40:23.0696,lwr_k=50:22.3269,lwr_k=100:21.4651,lwr_k=200:21.7777,lwr_k=300:21.8136,lwr_k=400:22.1369,lwr_k=500:22.5366,lwr_k=600:22.9491,lwr_k=700:23.2051,lwr_k=800:23.4704,lwr_k=900:23.6988,lwr_k=1000:23.915'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:8.5706,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.9629,lwr_k=40:1.9751,lwr_k=50:2.7695,lwr_k=100:4.6263,lwr_k=200:5.8901,lwr_k=300:6.3771,lwr_k=400:6.6202,lwr_k=500:6.8321,lwr_k=600:6.9931,lwr_k=700:7.0961,lwr_k=800:7.1509,lwr_k=900:7.2159,lwr_k=1000:7.2976'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.9264,lwr_k=10:48.8857,lwr_k=20:103.766,lwr_k=30:55.3223,lwr_k=40:17.586,lwr_k=50:15.381,lwr_k=100:10.127,lwr_k=200:10.1201,lwr_k=300:10.4542,lwr_k=400:10.5945,lwr_k=500:10.9288,lwr_k=600:11.116,lwr_k=700:11.2579,lwr_k=800:11.3315,lwr_k=900:11.3847,lwr_k=1000:11.5609'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_39'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:46.1934,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:1.109,lwr_k=50:2.5743,lwr_k=100:6.9319,lwr_k=200:11.4355,lwr_k=300:14.6047,lwr_k=400:17.183,lwr_k=500:19.5549,lwr_k=600:21.4206,lwr_k=700:23.3843,lwr_k=800:25.0822,lwr_k=900:26.3965,lwr_k=1000:27.566'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:59.9724,lwr_k=10:30.5934,lwr_k=20:32.9785,lwr_k=30:107.496,lwr_k=40:64.2204,lwr_k=50:34.7035,lwr_k=100:17.9658,lwr_k=200:17.1132,lwr_k=300:18.7473,lwr_k=400:21.3942,lwr_k=500:24.212,lwr_k=600:26.5618,lwr_k=700:28.8446,lwr_k=800:30.8046,lwr_k=900:32.8062,lwr_k=1000:34.3921'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:59.7501,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:1.305,lwr_k=50:3.2449,lwr_k=100:8.9627,lwr_k=200:14.6932,lwr_k=300:18.1907,lwr_k=400:20.4747,lwr_k=500:24.1292,lwr_k=600:27.1726,lwr_k=700:29.8993,lwr_k=800:32.0928,lwr_k=900:33.8437,lwr_k=1000:35.427'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:47.3623,lwr_k=10:39.3598,lwr_k=20:45.2543,lwr_k=30:113.8005,lwr_k=40:64.9496,lwr_k=50:27.8437,lwr_k=100:16.0118,lwr_k=200:17.26,lwr_k=300:18.7557,lwr_k=400:20.2243,lwr_k=500:22.1826,lwr_k=600:23.3902,lwr_k=700:24.7035,lwr_k=800:25.4703,lwr_k=900:26.8771,lwr_k=1000:27.9157'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:64.8419,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:1.6493,lwr_k=50:3.7578,lwr_k=100:9.6984,lwr_k=200:15.4723,lwr_k=300:19.1974,lwr_k=400:21.6593,lwr_k=500:25.1711,lwr_k=600:28.3861,lwr_k=700:31.4335,lwr_k=800:33.5277,lwr_k=900:35.3082,lwr_k=1000:37.0084'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:44.7534,lwr_k=10:34.1664,lwr_k=20:39.2769,lwr_k=30:108.5771,lwr_k=40:65.3476,lwr_k=50:28.1875,lwr_k=100:15.5116,lwr_k=200:15.8372,lwr_k=300:16.9546,lwr_k=400:18.6979,lwr_k=500:21.1153,lwr_k=600:21.9129,lwr_k=700:23.5981,lwr_k=800:24.8132,lwr_k=900:25.7785,lwr_k=1000:26.3702'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:62.8026,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:1.3398,lwr_k=50:3.0793,lwr_k=100:9.0103,lwr_k=200:15.0318,lwr_k=300:19.4765,lwr_k=400:22.3442,lwr_k=500:25.7344,lwr_k=600:28.7117,lwr_k=700:31.5514,lwr_k=800:33.3294,lwr_k=900:35.1679,lwr_k=1000:36.8219'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:53.9788,lwr_k=10:29.9658,lwr_k=20:43.3684,lwr_k=30:143.4562,lwr_k=40:75.8331,lwr_k=50:37.1619,lwr_k=100:18.1802,lwr_k=200:18.6306,lwr_k=300:19.4233,lwr_k=400:20.8126,lwr_k=500:22.9862,lwr_k=600:25.1185,lwr_k=700:27.4781,lwr_k=800:28.7142,lwr_k=900:29.5675,lwr_k=1000:30.531'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:47.5898,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:1.1996,lwr_k=50:2.6952,lwr_k=100:7.1586,lwr_k=200:11.7745,lwr_k=300:14.6642,lwr_k=400:16.9804,lwr_k=500:19.3168,lwr_k=600:21.1748,lwr_k=700:23.0503,lwr_k=800:24.6356,lwr_k=900:26.1109,lwr_k=1000:27.5251'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:90.03,lwr_k=10:51.0736,lwr_k=20:57.7606,lwr_k=30:103.2,lwr_k=40:72.874,lwr_k=50:48.7918,lwr_k=100:29.8728,lwr_k=200:37.0408,lwr_k=300:43.8191,lwr_k=400:48.2922,lwr_k=500:52.7488,lwr_k=600:56.6965,lwr_k=700:60.0797,lwr_k=800:61.7127,lwr_k=900:63.6039,lwr_k=1000:65.3001'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:57.5192,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:1.1573,lwr_k=50:2.8025,lwr_k=100:7.6515,lwr_k=200:12.4762,lwr_k=300:15.773,lwr_k=400:18.6864,lwr_k=500:20.5894,lwr_k=600:22.8742,lwr_k=700:25.0759,lwr_k=800:27.1074,lwr_k=900:29.0577,lwr_k=1000:30.4591'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:59.4049,lwr_k=10:43.3313,lwr_k=20:57.5074,lwr_k=30:122.6373,lwr_k=40:57.2589,lwr_k=50:33.7478,lwr_k=100:20.9556,lwr_k=200:20.0273,lwr_k=300:20.9251,lwr_k=400:22.679,lwr_k=500:24.2573,lwr_k=600:26.3221,lwr_k=700:28.1219,lwr_k=800:29.8759,lwr_k=900:31.3898,lwr_k=1000:33.2061'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_40'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.4245,lwr_k=10:0.0002,lwr_k=20:0.7084,lwr_k=30:1.9071,lwr_k=40:14.4393,lwr_k=50:12.8434,lwr_k=100:7.0481,lwr_k=200:7.7221,lwr_k=300:7.5405,lwr_k=400:7.1627,lwr_k=500:7.4834,lwr_k=600:7.2175,lwr_k=700:7.0936,lwr_k=800:6.9042,lwr_k=900:7.5142,lwr_k=1000:7.5253'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.9061,lwr_k=10:839.2639,lwr_k=20:1335174.3793,lwr_k=30:120167.9956,lwr_k=40:5850.372,lwr_k=50:208910.1494,lwr_k=100:64306.3572,lwr_k=200:2751.3647,lwr_k=300:54857.735,lwr_k=400:50107.8724,lwr_k=500:43189.3583,lwr_k=600:25.5632,lwr_k=700:20161.8945,lwr_k=800:34796.935,lwr_k=900:2136.7619,lwr_k=1000:1315.5483'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.5367,lwr_k=10:0.0044,lwr_k=20:2.1727,lwr_k=30:3.3725,lwr_k=40:4.2399,lwr_k=50:4.6249,lwr_k=100:5.9927,lwr_k=200:6.7378,lwr_k=300:7.062,lwr_k=400:7.1333,lwr_k=500:7.198,lwr_k=600:7.2925,lwr_k=700:7.3093,lwr_k=800:7.3343,lwr_k=900:7.4193,lwr_k=1000:7.4555'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.1882,lwr_k=10:3791.7395,lwr_k=20:6377.8311,lwr_k=30:18.2481,lwr_k=40:41.0816,lwr_k=50:13.0796,lwr_k=100:8.3379,lwr_k=200:7.7347,lwr_k=300:7.7065,lwr_k=400:7.7624,lwr_k=500:7.7547,lwr_k=600:7.8418,lwr_k=700:7.9155,lwr_k=800:7.8242,lwr_k=900:7.9658,lwr_k=1000:7.8295'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.9795,lwr_k=10:0.061,lwr_k=20:3.0181,lwr_k=30:4.1433,lwr_k=40:4.8342,lwr_k=50:5.2347,lwr_k=100:6.0015,lwr_k=200:6.6589,lwr_k=300:6.8831,lwr_k=400:6.9894,lwr_k=500:7.1117,lwr_k=600:7.1064,lwr_k=700:7.1856,lwr_k=800:7.1944,lwr_k=900:7.2283,lwr_k=1000:7.325'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.3199,lwr_k=10:18806814.216,lwr_k=20:27.2702,lwr_k=30:12.468,lwr_k=40:8.1733,lwr_k=50:7.707,lwr_k=100:6.3771,lwr_k=200:6.0417,lwr_k=300:5.9854,lwr_k=400:5.9166,lwr_k=500:5.9358,lwr_k=600:5.9005,lwr_k=700:5.8749,lwr_k=800:5.9906,lwr_k=900:5.7513,lwr_k=1000:5.7083'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.8332,lwr_k=10:0.0021,lwr_k=20:1.3801,lwr_k=30:2.9659,lwr_k=40:10.3804,lwr_k=50:7.9987,lwr_k=100:8.0378,lwr_k=200:7.1512,lwr_k=300:6.668,lwr_k=400:6.7456,lwr_k=500:6.8935,lwr_k=600:6.8346,lwr_k=700:7.0902,lwr_k=800:7.2798,lwr_k=900:7.3196,lwr_k=1000:7.651'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.9412,lwr_k=10:14383.6593,lwr_k=20:7614839.3274,lwr_k=30:110659691.2656,lwr_k=40:47481491.7003,lwr_k=50:22655348.4058,lwr_k=100:2282890.7606,lwr_k=200:853864.421,lwr_k=300:295867.8615,lwr_k=400:189474.7457,lwr_k=500:33928.1604,lwr_k=600:2526.6731,lwr_k=700:279.2289,lwr_k=800:56.8784,lwr_k=900:863.1595,lwr_k=1000:156.0209'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.4268,lwr_k=10:0.0004,lwr_k=20:0.6963,lwr_k=30:2.0025,lwr_k=40:3.1404,lwr_k=50:4.4846,lwr_k=100:4.7943,lwr_k=200:8.3008,lwr_k=300:6.2052,lwr_k=400:6.9567,lwr_k=500:7.3873,lwr_k=600:6.9207,lwr_k=700:7.1991,lwr_k=800:6.6902,lwr_k=900:6.5493,lwr_k=1000:6.4991'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:41.1863,lwr_k=10:272.7113,lwr_k=20:6072117.2749,lwr_k=30:23907065.1758,lwr_k=40:303290.4642,lwr_k=50:989337.2845,lwr_k=100:167.6772,lwr_k=200:23973.2298,lwr_k=300:12653.729,lwr_k=400:6576.861,lwr_k=500:2521.6282,lwr_k=600:17283.8426,lwr_k=700:5199.9661,lwr_k=800:2613.856,lwr_k=900:1984.1895,lwr_k=1000:1111.965'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:6.2246,lwr_k=10:0.0124,lwr_k=20:1.5252,lwr_k=30:2.5969,lwr_k=40:3.3443,lwr_k=50:3.8249,lwr_k=100:4.6531,lwr_k=200:5.1119,lwr_k=300:5.2552,lwr_k=400:5.3689,lwr_k=500:5.4487,lwr_k=600:5.4809,lwr_k=700:5.518,lwr_k=800:5.5518,lwr_k=900:5.5891,lwr_k=1000:5.6124'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.857,lwr_k=10:34516.7813,lwr_k=20:14708145.0111,lwr_k=30:17823254.7777,lwr_k=40:14674830.2521,lwr_k=50:4466494.0133,lwr_k=100:243603.0967,lwr_k=200:18469.5757,lwr_k=300:46589.7344,lwr_k=400:14369.0389,lwr_k=500:31579.8967,lwr_k=600:10.6424,lwr_k=700:9.2062,lwr_k=800:67.9828,lwr_k=900:47.6338,lwr_k=1000:40.5706'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_41'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.1952,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.5813,lwr_k=40:1.5614,lwr_k=50:2.1603,lwr_k=100:3.2309,lwr_k=200:3.9292,lwr_k=300:4.2626,lwr_k=400:4.4012,lwr_k=500:4.516,lwr_k=600:4.5856,lwr_k=700:4.6469,lwr_k=800:4.7397,lwr_k=900:4.7583,lwr_k=1000:4.7843'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.1876,lwr_k=10:61.1333,lwr_k=20:66.9313,lwr_k=30:63.9497,lwr_k=40:22.0197,lwr_k=50:13.1291,lwr_k=100:9.9404,lwr_k=200:9.2351,lwr_k=300:8.8072,lwr_k=400:8.7478,lwr_k=500:8.7133,lwr_k=600:8.724,lwr_k=700:8.8214,lwr_k=800:8.8363,lwr_k=900:8.8623,lwr_k=1000:8.8336'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:41.3281,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.8476,lwr_k=40:5.1035,lwr_k=50:6.9723,lwr_k=100:13.8223,lwr_k=200:17.2938,lwr_k=300:19.5273,lwr_k=400:20.7493,lwr_k=500:21.7112,lwr_k=600:22.3444,lwr_k=700:23.1283,lwr_k=800:23.8571,lwr_k=900:24.3503,lwr_k=1000:25.1632'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:35.1116,lwr_k=10:239.0223,lwr_k=20:183.642,lwr_k=30:219.3783,lwr_k=40:66.2453,lwr_k=50:53.8006,lwr_k=100:28.3417,lwr_k=200:19.6463,lwr_k=300:19.7285,lwr_k=400:19.144,lwr_k=500:18.9809,lwr_k=600:18.6029,lwr_k=700:18.4893,lwr_k=800:18.5823,lwr_k=900:18.8123,lwr_k=1000:19.1045'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:41.5998,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.585,lwr_k=40:5.2384,lwr_k=50:7.8309,lwr_k=100:13.7688,lwr_k=200:18.5268,lwr_k=300:20.4901,lwr_k=400:21.4677,lwr_k=500:22.1796,lwr_k=600:23.1635,lwr_k=700:23.8363,lwr_k=800:24.4192,lwr_k=900:25.1575,lwr_k=1000:25.6595'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:31.2525,lwr_k=10:206.6032,lwr_k=20:145.0672,lwr_k=30:168.98,lwr_k=40:70.0849,lwr_k=50:45.8214,lwr_k=100:22.0062,lwr_k=200:18.9823,lwr_k=300:18.4488,lwr_k=400:18.6387,lwr_k=500:18.7313,lwr_k=600:18.4286,lwr_k=700:18.3684,lwr_k=800:18.4788,lwr_k=900:18.6703,lwr_k=1000:18.938'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:18.9856,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.4483,lwr_k=40:3.9418,lwr_k=50:5.3598,lwr_k=100:8.758,lwr_k=200:12.0069,lwr_k=300:12.9582,lwr_k=400:13.5576,lwr_k=500:13.9505,lwr_k=600:14.3154,lwr_k=700:14.4942,lwr_k=800:14.7376,lwr_k=900:15.0154,lwr_k=1000:15.2303'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:15.0235,lwr_k=10:530.3876,lwr_k=20:172.1573,lwr_k=30:124.5726,lwr_k=40:36.5046,lwr_k=50:23.6269,lwr_k=100:16.2622,lwr_k=200:13.6118,lwr_k=300:13.533,lwr_k=400:13.4036,lwr_k=500:13.4222,lwr_k=600:13.3839,lwr_k=700:13.3104,lwr_k=800:13.4375,lwr_k=900:13.3926,lwr_k=1000:13.5315'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.9435,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.7575,lwr_k=40:1.7928,lwr_k=50:2.3796,lwr_k=100:3.5491,lwr_k=200:4.1157,lwr_k=300:4.2805,lwr_k=400:4.3966,lwr_k=500:4.4742,lwr_k=600:4.5323,lwr_k=700:4.5699,lwr_k=800:4.625,lwr_k=900:4.6599,lwr_k=1000:4.6918'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:20.8689,lwr_k=10:45.5373,lwr_k=20:106.0314,lwr_k=30:115.1183,lwr_k=40:33.9354,lwr_k=50:29.6528,lwr_k=100:23.0979,lwr_k=200:20.622,lwr_k=300:20.1433,lwr_k=400:20.121,lwr_k=500:20.1225,lwr_k=600:20.0248,lwr_k=700:20.0023,lwr_k=800:20.0246,lwr_k=900:19.9969,lwr_k=1000:20.1085'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:10.0457,lwr_k=10:0.0,lwr_k=20:0.0018,lwr_k=30:0.8959,lwr_k=40:2.5502,lwr_k=50:3.669,lwr_k=100:5.7531,lwr_k=200:7.0651,lwr_k=300:7.5729,lwr_k=400:7.8504,lwr_k=500:8.0224,lwr_k=600:8.1361,lwr_k=700:8.2625,lwr_k=800:8.3645,lwr_k=900:8.4695,lwr_k=1000:8.5242'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:13.899,lwr_k=10:74.0742,lwr_k=20:315.4078,lwr_k=30:196.1873,lwr_k=40:113.0065,lwr_k=50:20.6941,lwr_k=100:13.1425,lwr_k=200:12.1974,lwr_k=300:12.2668,lwr_k=400:12.2218,lwr_k=500:12.4144,lwr_k=600:12.4254,lwr_k=700:12.4781,lwr_k=800:12.5572,lwr_k=900:12.7626,lwr_k=1000:12.8024'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_42'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.0786,lwr_k=10:5.2683,lwr_k=20:5.9742,lwr_k=30:5.9357,lwr_k=40:5.9675,lwr_k=50:6.0364,lwr_k=100:6.1395,lwr_k=200:6.4423,lwr_k=300:6.5126,lwr_k=400:6.5302,lwr_k=500:6.5499,lwr_k=600:6.5661,lwr_k=700:6.5815,lwr_k=800:6.577,lwr_k=900:6.5845,lwr_k=1000:6.5893'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.1696,lwr_k=10:10.4906,lwr_k=20:9.8494,lwr_k=30:9.8276,lwr_k=40:9.7744,lwr_k=50:9.6407,lwr_k=100:9.5617,lwr_k=200:10.98,lwr_k=300:10.9509,lwr_k=400:10.9026,lwr_k=500:10.8964,lwr_k=600:10.8972,lwr_k=700:10.9179,lwr_k=800:10.9222,lwr_k=900:10.972,lwr_k=1000:11.0068'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:14.6418,lwr_k=10:11.5734,lwr_k=20:11.8672,lwr_k=30:12.0086,lwr_k=40:12.1574,lwr_k=50:12.266,lwr_k=100:12.5273,lwr_k=200:12.7267,lwr_k=300:12.9371,lwr_k=400:13.0727,lwr_k=500:13.1553,lwr_k=600:13.1905,lwr_k=700:13.2316,lwr_k=800:13.25,lwr_k=900:13.2768,lwr_k=1000:13.3033'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.2734,lwr_k=10:12.18,lwr_k=20:11.6491,lwr_k=30:11.585,lwr_k=40:11.3766,lwr_k=50:11.3368,lwr_k=100:11.2408,lwr_k=200:11.2449,lwr_k=300:11.2393,lwr_k=400:11.2309,lwr_k=500:11.2617,lwr_k=600:11.2852,lwr_k=700:11.3321,lwr_k=800:11.3696,lwr_k=900:11.3782,lwr_k=1000:11.3914'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.5296,lwr_k=10:7.4434,lwr_k=20:8.4877,lwr_k=30:9.0694,lwr_k=40:9.7083,lwr_k=50:10.239,lwr_k=100:11.051,lwr_k=200:11.5754,lwr_k=300:11.6538,lwr_k=400:11.6692,lwr_k=500:11.7031,lwr_k=600:11.7558,lwr_k=700:11.7728,lwr_k=800:11.7839,lwr_k=900:11.8265,lwr_k=1000:11.8387'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.0591,lwr_k=10:7.797,lwr_k=20:7.8075,lwr_k=30:7.7969,lwr_k=40:7.6662,lwr_k=50:7.5177,lwr_k=100:7.3597,lwr_k=200:7.2877,lwr_k=300:7.2766,lwr_k=400:7.2417,lwr_k=500:7.2684,lwr_k=600:7.4185,lwr_k=700:7.416,lwr_k=800:7.4292,lwr_k=900:7.4524,lwr_k=1000:7.4829'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:16.1631,lwr_k=10:11.2969,lwr_k=20:11.7572,lwr_k=30:11.9506,lwr_k=40:12.2615,lwr_k=50:12.5088,lwr_k=100:13.1365,lwr_k=200:13.7333,lwr_k=300:14.0005,lwr_k=400:14.1034,lwr_k=500:14.1567,lwr_k=600:14.2318,lwr_k=700:14.2743,lwr_k=800:14.3014,lwr_k=900:14.3365,lwr_k=1000:14.3707'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:14.5955,lwr_k=10:14.9256,lwr_k=20:14.0086,lwr_k=30:13.88,lwr_k=40:14.0338,lwr_k=50:14.1657,lwr_k=100:13.7856,lwr_k=200:13.6266,lwr_k=300:13.6643,lwr_k=400:13.6258,lwr_k=500:13.497,lwr_k=600:13.5023,lwr_k=700:13.4896,lwr_k=800:13.4664,lwr_k=900:13.4646,lwr_k=1000:13.4762'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.0343,lwr_k=10:6.8407,lwr_k=20:7.2969,lwr_k=30:7.4073,lwr_k=40:7.5188,lwr_k=50:7.5787,lwr_k=100:7.7526,lwr_k=200:7.7998,lwr_k=300:7.8698,lwr_k=400:7.887,lwr_k=500:7.9352,lwr_k=600:7.9607,lwr_k=700:7.9797,lwr_k=800:8.0078,lwr_k=900:8.0542,lwr_k=1000:8.0977'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:31.0164,lwr_k=10:20.7747,lwr_k=20:18.3023,lwr_k=30:20.6206,lwr_k=40:22.2303,lwr_k=50:22.803,lwr_k=100:24.2683,lwr_k=200:25.1014,lwr_k=300:25.9097,lwr_k=400:26.2235,lwr_k=500:26.6483,lwr_k=600:26.8348,lwr_k=700:27.142,lwr_k=800:27.3854,lwr_k=900:27.4546,lwr_k=1000:27.6266'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:17.1952,lwr_k=10:14.5328,lwr_k=20:14.8499,lwr_k=30:14.883,lwr_k=40:15.2655,lwr_k=50:15.3662,lwr_k=100:15.9478,lwr_k=200:16.2872,lwr_k=300:16.4452,lwr_k=400:16.4659,lwr_k=500:16.4763,lwr_k=600:16.4912,lwr_k=700:16.507,lwr_k=800:16.5062,lwr_k=900:16.4919,lwr_k=1000:16.4917'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:15.5811,lwr_k=10:17.0499,lwr_k=20:16.0761,lwr_k=30:16.9516,lwr_k=40:16.0907,lwr_k=50:15.9099,lwr_k=100:15.6378,lwr_k=200:15.0747,lwr_k=300:14.8477,lwr_k=400:14.7503,lwr_k=500:14.7552,lwr_k=600:14.728,lwr_k=700:14.7228,lwr_k=800:14.7127,lwr_k=900:14.7358,lwr_k=1000:14.7421'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_43'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:46.4643,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:0.0028,lwr_k=100:4.6944,lwr_k=200:10.0981,lwr_k=300:13.5144,lwr_k=400:16.0553,lwr_k=500:18.1516,lwr_k=600:19.6866,lwr_k=700:20.8629,lwr_k=800:22.243,lwr_k=900:23.2476,lwr_k=1000:24.422'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:51.0447,lwr_k=10:55.4355,lwr_k=20:92.5061,lwr_k=30:268.1594,lwr_k=40:372.8149,lwr_k=50:2452.2987,lwr_k=100:30.5502,lwr_k=200:23.7407,lwr_k=300:22.5838,lwr_k=400:24.5699,lwr_k=500:24.9973,lwr_k=600:26.076,lwr_k=700:26.2327,lwr_k=800:26.9944,lwr_k=900:27.7516,lwr_k=1000:28.7293'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:43.9318,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:0.0047,lwr_k=100:5.5089,lwr_k=200:12.1743,lwr_k=300:16.371,lwr_k=400:19.1009,lwr_k=500:21.1167,lwr_k=600:22.8305,lwr_k=700:23.9285,lwr_k=800:25.2219,lwr_k=900:26.6264,lwr_k=1000:27.7406'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:54.9751,lwr_k=10:43.8624,lwr_k=20:81.2457,lwr_k=30:253.6828,lwr_k=40:130.1477,lwr_k=50:712.656,lwr_k=100:28.1142,lwr_k=200:24.1497,lwr_k=300:24.809,lwr_k=400:27.0452,lwr_k=500:28.4505,lwr_k=600:30.4656,lwr_k=700:32.6152,lwr_k=800:33.9229,lwr_k=900:35.045,lwr_k=1000:36.1541'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:44.9758,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:0.0036,lwr_k=100:6.3113,lwr_k=200:12.4612,lwr_k=300:16.2991,lwr_k=400:19.0424,lwr_k=500:21.5849,lwr_k=600:23.4066,lwr_k=700:24.6961,lwr_k=800:25.8718,lwr_k=900:27.0896,lwr_k=1000:28.1991'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:39.756,lwr_k=10:40.1364,lwr_k=20:91.4853,lwr_k=30:529.7495,lwr_k=40:93.3418,lwr_k=50:890.414,lwr_k=100:25.5622,lwr_k=200:22.9271,lwr_k=300:22.2285,lwr_k=400:23.5567,lwr_k=500:24.0679,lwr_k=600:24.7669,lwr_k=700:25.6575,lwr_k=800:26.0453,lwr_k=900:26.785,lwr_k=1000:27.3608'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:47.6297,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:0.0026,lwr_k=100:6.0541,lwr_k=200:12.5505,lwr_k=300:16.5858,lwr_k=400:19.8778,lwr_k=500:22.597,lwr_k=600:24.7796,lwr_k=700:26.1417,lwr_k=800:27.5464,lwr_k=900:29.1533,lwr_k=1000:30.361'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:44.5463,lwr_k=10:42.8538,lwr_k=20:89.7621,lwr_k=30:173.9997,lwr_k=40:98.0114,lwr_k=50:507.6333,lwr_k=100:29.2358,lwr_k=200:24.8835,lwr_k=300:25.2122,lwr_k=400:24.0668,lwr_k=500:24.1604,lwr_k=600:25.1598,lwr_k=700:25.2214,lwr_k=800:26.0271,lwr_k=900:27.274,lwr_k=1000:28.1014'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:40.4729,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:0.0052,lwr_k=100:5.0875,lwr_k=200:10.3198,lwr_k=300:13.6643,lwr_k=400:16.2742,lwr_k=500:18.526,lwr_k=600:20.0758,lwr_k=700:21.2315,lwr_k=800:22.3859,lwr_k=900:23.5962,lwr_k=1000:24.6547'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:184.2981,lwr_k=10:47.6048,lwr_k=20:127.5993,lwr_k=30:239.3061,lwr_k=40:122.1885,lwr_k=50:874.9133,lwr_k=100:241.7842,lwr_k=200:68.9451,lwr_k=300:48.089,lwr_k=400:49.9805,lwr_k=500:48.8548,lwr_k=600:47.5221,lwr_k=700:49.0706,lwr_k=800:49.082,lwr_k=900:50.3046,lwr_k=1000:49.5982'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:45.4249,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:0.0032,lwr_k=100:5.5233,lwr_k=200:10.9812,lwr_k=300:14.6421,lwr_k=400:17.3454,lwr_k=500:19.7992,lwr_k=600:21.6237,lwr_k=700:23.0268,lwr_k=800:24.3759,lwr_k=900:25.2984,lwr_k=1000:26.3323'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:57.2481,lwr_k=10:64.1716,lwr_k=20:106.8336,lwr_k=30:154.9805,lwr_k=40:320.594,lwr_k=50:1115.5334,lwr_k=100:38.8791,lwr_k=200:23.078,lwr_k=300:24.9518,lwr_k=400:27.2422,lwr_k=500:29.8265,lwr_k=600:31.1623,lwr_k=700:32.9684,lwr_k=800:34.0114,lwr_k=900:35.3496,lwr_k=1000:35.9479'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_44'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.7895,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:0.3862,lwr_k=100:2.1836,lwr_k=200:3.3534,lwr_k=300:3.8676,lwr_k=400:4.1347,lwr_k=500:4.2937,lwr_k=600:4.4254,lwr_k=700:4.5378,lwr_k=800:4.6275,lwr_k=900:4.7386,lwr_k=1000:4.8073'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.9522,lwr_k=10:21.3455,lwr_k=20:23.3727,lwr_k=30:51.9203,lwr_k=40:183.0562,lwr_k=50:48.419,lwr_k=100:13.1693,lwr_k=200:7.7092,lwr_k=300:7.6747,lwr_k=400:7.6118,lwr_k=500:7.7099,lwr_k=600:7.8279,lwr_k=700:7.9245,lwr_k=800:8.0066,lwr_k=900:8.0601,lwr_k=1000:8.1009'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.8933,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0018,lwr_k=50:0.3743,lwr_k=100:2.5123,lwr_k=200:4.3141,lwr_k=300:4.9691,lwr_k=400:5.3498,lwr_k=500:5.6628,lwr_k=600:5.8444,lwr_k=700:5.9715,lwr_k=800:6.1685,lwr_k=900:6.2551,lwr_k=1000:6.38'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.2098,lwr_k=10:17.2171,lwr_k=20:32.4324,lwr_k=30:270.7003,lwr_k=40:147.4796,lwr_k=50:94.8439,lwr_k=100:8.6262,lwr_k=200:6.654,lwr_k=300:6.4491,lwr_k=400:6.5275,lwr_k=500:6.7071,lwr_k=600:6.7259,lwr_k=700:6.7095,lwr_k=800:6.6693,lwr_k=900:6.6517,lwr_k=1000:6.7007'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.401,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0017,lwr_k=50:0.4964,lwr_k=100:2.9222,lwr_k=200:5.0558,lwr_k=300:5.6348,lwr_k=400:6.1407,lwr_k=500:6.3736,lwr_k=600:6.5948,lwr_k=700:6.7428,lwr_k=800:6.8792,lwr_k=900:6.963,lwr_k=1000:7.0586'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.953,lwr_k=10:30.0412,lwr_k=20:36.014,lwr_k=30:132.8687,lwr_k=40:2714.8617,lwr_k=50:2490.5747,lwr_k=100:14.9021,lwr_k=200:5.8042,lwr_k=300:5.7665,lwr_k=400:5.5265,lwr_k=500:5.5363,lwr_k=600:5.5867,lwr_k=700:5.7091,lwr_k=800:5.749,lwr_k=900:5.8073,lwr_k=1000:5.8262'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.3463,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0035,lwr_k=50:0.4868,lwr_k=100:3.1222,lwr_k=200:5.0515,lwr_k=300:5.7357,lwr_k=400:6.2278,lwr_k=500:6.6207,lwr_k=600:6.9575,lwr_k=700:7.1758,lwr_k=800:7.3202,lwr_k=900:7.5224,lwr_k=1000:7.6462'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.8801,lwr_k=10:24.4836,lwr_k=20:38.3662,lwr_k=30:107.8308,lwr_k=40:1292.4212,lwr_k=50:4796.5448,lwr_k=100:11.3126,lwr_k=200:8.3763,lwr_k=300:8.2608,lwr_k=400:8.2822,lwr_k=500:8.4497,lwr_k=600:8.5834,lwr_k=700:8.8051,lwr_k=800:8.9552,lwr_k=900:9.1361,lwr_k=1000:9.3207'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.8954,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0001,lwr_k=40:0.0073,lwr_k=50:0.3966,lwr_k=100:2.2144,lwr_k=200:3.2393,lwr_k=300:3.6135,lwr_k=400:3.8872,lwr_k=500:4.0682,lwr_k=600:4.1925,lwr_k=700:4.2815,lwr_k=800:4.348,lwr_k=900:4.4274,lwr_k=1000:4.492'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:21.8427,lwr_k=10:50.4339,lwr_k=20:637.3686,lwr_k=30:389.381,lwr_k=40:129.1217,lwr_k=50:229.2339,lwr_k=100:25.0677,lwr_k=200:21.6424,lwr_k=300:21.6814,lwr_k=400:21.736,lwr_k=500:21.7566,lwr_k=600:21.8093,lwr_k=700:21.8423,lwr_k=800:21.8763,lwr_k=900:21.9496,lwr_k=1000:21.8271'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:7.9484,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0009,lwr_k=50:0.4403,lwr_k=100:2.5146,lwr_k=200:4.1584,lwr_k=300:4.812,lwr_k=400:5.1623,lwr_k=500:5.413,lwr_k=600:5.6163,lwr_k=700:5.8287,lwr_k=800:5.9974,lwr_k=900:6.1321,lwr_k=1000:6.2476'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.3841,lwr_k=10:21.7995,lwr_k=20:41.8457,lwr_k=30:69.8906,lwr_k=40:560.3334,lwr_k=50:77.2455,lwr_k=100:10.7293,lwr_k=200:8.7153,lwr_k=300:8.7604,lwr_k=400:8.8986,lwr_k=500:9.143,lwr_k=600:9.4404,lwr_k=700:9.6042,lwr_k=800:9.7906,lwr_k=900:9.9049,lwr_k=1000:10.1202'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_45'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:118.6794,lwr_k=10:0.0,lwr_k=20:6.0986,lwr_k=30:13.64,lwr_k=40:18.0114,lwr_k=50:21.0731,lwr_k=100:30.151,lwr_k=200:40.8891,lwr_k=300:48.0097,lwr_k=400:53.1613,lwr_k=500:57.9819,lwr_k=600:62.1798,lwr_k=700:65.8866,lwr_k=800:69.2176,lwr_k=900:72.1904,lwr_k=1000:74.481'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:144.3661,lwr_k=10:789.9004,lwr_k=20:91.1895,lwr_k=30:65.1948,lwr_k=40:52.2065,lwr_k=50:48.4528,lwr_k=100:47.351,lwr_k=200:50.3732,lwr_k=300:55.674,lwr_k=400:61.2404,lwr_k=500:66.7294,lwr_k=600:71.0582,lwr_k=700:75.5959,lwr_k=800:79.9508,lwr_k=900:83.3806,lwr_k=1000:86.0067'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:160.0109,lwr_k=10:0.0,lwr_k=20:12.9862,lwr_k=30:29.1387,lwr_k=40:39.5523,lwr_k=50:47.1631,lwr_k=100:61.8288,lwr_k=200:71.5287,lwr_k=300:77.1743,lwr_k=400:81.3972,lwr_k=500:88.2988,lwr_k=600:101.0707,lwr_k=700:106.8372,lwr_k=800:110.3274,lwr_k=900:114.9169,lwr_k=1000:117.8427'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:138.6091,lwr_k=10:1108.9599,lwr_k=20:1399.5836,lwr_k=30:245.5333,lwr_k=40:151.6572,lwr_k=50:141.3373,lwr_k=100:79.4765,lwr_k=200:78.0875,lwr_k=300:79.2088,lwr_k=400:84.2205,lwr_k=500:89.436,lwr_k=600:99.0696,lwr_k=700:101.901,lwr_k=800:104.5672,lwr_k=900:107.7378,lwr_k=1000:110.3046'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:133.2877,lwr_k=10:0.0,lwr_k=20:6.5028,lwr_k=30:14.1785,lwr_k=40:20.2833,lwr_k=50:22.8943,lwr_k=100:32.647,lwr_k=200:43.3901,lwr_k=300:50.4417,lwr_k=400:57.1581,lwr_k=500:62.0026,lwr_k=600:65.753,lwr_k=700:69.712,lwr_k=800:73.0135,lwr_k=900:75.7925,lwr_k=1000:78.5249'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:105.4292,lwr_k=10:550.2085,lwr_k=20:105.0003,lwr_k=30:53.7241,lwr_k=40:46.1532,lwr_k=50:42.3545,lwr_k=100:38.2601,lwr_k=200:42.8141,lwr_k=300:46.7275,lwr_k=400:51.277,lwr_k=500:54.8632,lwr_k=600:57.2489,lwr_k=700:59.5959,lwr_k=800:62.3165,lwr_k=900:64.5578,lwr_k=1000:67.2005'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:139.1276,lwr_k=10:0.0,lwr_k=20:6.7348,lwr_k=30:16.2729,lwr_k=40:20.8658,lwr_k=50:25.3959,lwr_k=100:35.727,lwr_k=200:48.1636,lwr_k=300:56.8818,lwr_k=400:62.8663,lwr_k=500:68.2368,lwr_k=600:72.3456,lwr_k=700:76.2452,lwr_k=800:79.9304,lwr_k=900:82.6099,lwr_k=1000:85.4564'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:124.5098,lwr_k=10:1065.3717,lwr_k=20:1185.4373,lwr_k=30:436.9191,lwr_k=40:260.1785,lwr_k=50:130.6571,lwr_k=100:237.5262,lwr_k=200:153.6472,lwr_k=300:50.9816,lwr_k=400:57.4188,lwr_k=500:61.3178,lwr_k=600:64.3101,lwr_k=700:68.0545,lwr_k=800:70.5317,lwr_k=900:72.8336,lwr_k=1000:75.3602'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:124.5006,lwr_k=10:0.0,lwr_k=20:6.4943,lwr_k=30:14.5654,lwr_k=40:18.7667,lwr_k=50:22.8018,lwr_k=100:33.2588,lwr_k=200:43.0481,lwr_k=300:49.928,lwr_k=400:56.0759,lwr_k=500:60.8667,lwr_k=600:65.5903,lwr_k=700:69.5697,lwr_k=800:73.1287,lwr_k=900:76.3707,lwr_k=1000:78.9861'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:154.5129,lwr_k=10:760.9934,lwr_k=20:180.7814,lwr_k=30:74.5942,lwr_k=40:60.389,lwr_k=50:58.4541,lwr_k=100:58.5964,lwr_k=200:67.0077,lwr_k=300:75.0958,lwr_k=400:77.1557,lwr_k=500:82.9198,lwr_k=600:83.8513,lwr_k=700:86.5602,lwr_k=800:89.5634,lwr_k=900:92.1724,lwr_k=1000:94.4961'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:129.6025,lwr_k=10:0.0,lwr_k=20:6.8239,lwr_k=30:15.2251,lwr_k=40:20.1428,lwr_k=50:22.8469,lwr_k=100:31.9639,lwr_k=200:41.8166,lwr_k=300:48.0375,lwr_k=400:53.2694,lwr_k=500:57.8712,lwr_k=600:61.4541,lwr_k=700:65.1094,lwr_k=800:68.0665,lwr_k=900:71.0645,lwr_k=1000:73.6804'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:145.5744,lwr_k=10:692.4856,lwr_k=20:288.0842,lwr_k=30:115.2695,lwr_k=40:88.8574,lwr_k=50:69.4498,lwr_k=100:69.7759,lwr_k=200:74.5225,lwr_k=300:78.5054,lwr_k=400:84.407,lwr_k=500:87.8231,lwr_k=600:90.7452,lwr_k=700:92.5438,lwr_k=800:94.283,lwr_k=900:95.784,lwr_k=1000:97.3635'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_46'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.2008,lwr_k=10:0.0,lwr_k=20:0.0001,lwr_k=30:0.8716,lwr_k=40:1.5191,lwr_k=50:1.8307,lwr_k=100:2.7044,lwr_k=200:3.2429,lwr_k=300:3.352,lwr_k=400:3.4377,lwr_k=500:3.5043,lwr_k=600:3.5474,lwr_k=700:3.6036,lwr_k=800:3.6603,lwr_k=900:3.6951,lwr_k=1000:3.7229'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.3822,lwr_k=10:53.4949,lwr_k=20:223.6141,lwr_k=30:35.5101,lwr_k=40:18.171,lwr_k=50:14.7034,lwr_k=100:9.8549,lwr_k=200:9.0942,lwr_k=300:8.9327,lwr_k=400:8.9029,lwr_k=500:8.7946,lwr_k=600:8.8808,lwr_k=700:8.9163,lwr_k=800:8.8805,lwr_k=900:8.9175,lwr_k=1000:8.9613'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:39.363,lwr_k=10:0.0,lwr_k=20:0.0035,lwr_k=30:4.0876,lwr_k=40:7.3033,lwr_k=50:9.5374,lwr_k=100:15.7423,lwr_k=200:19.937,lwr_k=300:21.8809,lwr_k=400:23.139,lwr_k=500:23.8348,lwr_k=600:24.6921,lwr_k=700:25.2853,lwr_k=800:25.9508,lwr_k=900:26.5553,lwr_k=1000:27.173'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:30.2748,lwr_k=10:161.1647,lwr_k=20:365.0315,lwr_k=30:146.8969,lwr_k=40:66.5345,lwr_k=50:50.1875,lwr_k=100:39.6799,lwr_k=200:34.942,lwr_k=300:26.0332,lwr_k=400:22.4997,lwr_k=500:21.9359,lwr_k=600:18.5026,lwr_k=700:18.7207,lwr_k=800:19.1086,lwr_k=900:19.106,lwr_k=1000:19.3224'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.876,lwr_k=10:0.0,lwr_k=20:0.0007,lwr_k=30:1.4618,lwr_k=40:2.5814,lwr_k=50:3.2945,lwr_k=100:4.7752,lwr_k=200:5.812,lwr_k=300:6.164,lwr_k=400:6.4151,lwr_k=500:6.5739,lwr_k=600:6.6366,lwr_k=700:6.7296,lwr_k=800:6.831,lwr_k=900:6.9005,lwr_k=1000:6.9264'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.9102,lwr_k=10:79.1399,lwr_k=20:311.0449,lwr_k=30:36.0706,lwr_k=40:14.8044,lwr_k=50:11.416,lwr_k=100:7.6371,lwr_k=200:6.222,lwr_k=300:6.1614,lwr_k=400:6.1013,lwr_k=500:6.0485,lwr_k=600:6.079,lwr_k=700:6.0421,lwr_k=800:6.0771,lwr_k=900:6.155,lwr_k=1000:6.1667'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.9237,lwr_k=10:0.0,lwr_k=20:0.0003,lwr_k=30:1.5846,lwr_k=40:3.3143,lwr_k=50:4.2554,lwr_k=100:6.884,lwr_k=200:8.5386,lwr_k=300:9.1047,lwr_k=400:9.3741,lwr_k=500:9.5757,lwr_k=600:9.816,lwr_k=700:9.9679,lwr_k=800:10.1797,lwr_k=900:10.2951,lwr_k=1000:10.4317'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:11.4942,lwr_k=10:73.6328,lwr_k=20:735.4149,lwr_k=30:202.9186,lwr_k=40:63.5577,lwr_k=50:36.7981,lwr_k=100:20.0874,lwr_k=200:10.2221,lwr_k=300:10.6408,lwr_k=400:10.5666,lwr_k=500:10.5594,lwr_k=600:10.8289,lwr_k=700:10.6759,lwr_k=800:10.6648,lwr_k=900:10.5709,lwr_k=1000:10.5855'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.6016,lwr_k=10:0.0,lwr_k=20:0.0324,lwr_k=30:1.0421,lwr_k=40:1.9015,lwr_k=50:2.4511,lwr_k=100:3.3468,lwr_k=200:3.7712,lwr_k=300:3.9529,lwr_k=400:4.0689,lwr_k=500:4.1319,lwr_k=600:4.1607,lwr_k=700:4.1863,lwr_k=800:4.2206,lwr_k=900:4.2566,lwr_k=1000:4.3201'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:19.4379,lwr_k=10:912.5275,lwr_k=20:8393.4731,lwr_k=30:40169.226,lwr_k=40:37665.2236,lwr_k=50:4747.6013,lwr_k=100:194.6573,lwr_k=200:98.2757,lwr_k=300:21.5135,lwr_k=400:21.1249,lwr_k=500:20.6806,lwr_k=600:20.7473,lwr_k=700:20.7159,lwr_k=800:20.6254,lwr_k=900:20.6055,lwr_k=1000:20.2953'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:6.7405,lwr_k=10:0.0,lwr_k=20:0.0002,lwr_k=30:1.098,lwr_k=40:2.3674,lwr_k=50:3.0456,lwr_k=100:4.436,lwr_k=200:5.3068,lwr_k=300:5.6635,lwr_k=400:5.8899,lwr_k=500:5.9504,lwr_k=600:6.0275,lwr_k=700:6.0941,lwr_k=800:6.13,lwr_k=900:6.1704,lwr_k=1000:6.2122'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.6858,lwr_k=10:83.2834,lwr_k=20:140.8669,lwr_k=30:43.9859,lwr_k=40:17.2316,lwr_k=50:23.0777,lwr_k=100:11.0254,lwr_k=200:10.4997,lwr_k=300:10.7108,lwr_k=400:10.899,lwr_k=500:10.9762,lwr_k=600:11.1838,lwr_k=700:11.2142,lwr_k=800:11.3027,lwr_k=900:11.2443,lwr_k=1000:11.3011'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_47'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.4725,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.4342,lwr_k=40:1.6355,lwr_k=50:0.0666,lwr_k=100:0.3923,lwr_k=200:1.7789,lwr_k=300:2.8459,lwr_k=400:3.5652,lwr_k=500:3.8999,lwr_k=600:4.206,lwr_k=700:4.3814,lwr_k=800:4.6169,lwr_k=900:4.6458,lwr_k=1000:4.7546'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.9085,lwr_k=10:36.0206,lwr_k=20:48.1664,lwr_k=30:86.9463,lwr_k=40:21.8046,lwr_k=50:31.2438,lwr_k=100:23.8904,lwr_k=200:23.7873,lwr_k=300:14.5998,lwr_k=400:11.0981,lwr_k=500:9.6667,lwr_k=600:9.5594,lwr_k=700:9.8508,lwr_k=800:9.3534,lwr_k=900:9.641,lwr_k=1000:8.8577'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:14.3584,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.4149,lwr_k=40:2.3323,lwr_k=50:0.1186,lwr_k=100:0.3818,lwr_k=200:1.1007,lwr_k=300:2.1527,lwr_k=400:2.988,lwr_k=500:3.5571,lwr_k=600:4.638,lwr_k=700:4.8821,lwr_k=800:6.1088,lwr_k=900:6.1415,lwr_k=1000:6.4914'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.3369,lwr_k=10:45.7426,lwr_k=20:71.0975,lwr_k=30:229.7268,lwr_k=40:34.1331,lwr_k=50:38.3634,lwr_k=100:36.3866,lwr_k=200:20.1107,lwr_k=300:19.5668,lwr_k=400:17.2804,lwr_k=500:17.8241,lwr_k=600:17.4053,lwr_k=700:21.0856,lwr_k=800:26.6686,lwr_k=900:29.1689,lwr_k=1000:14.2444'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.7087,lwr_k=10:0.0,lwr_k=20:0.0001,lwr_k=30:1.9163,lwr_k=40:3.2945,lwr_k=50:4.1243,lwr_k=100:6.4691,lwr_k=200:7.9592,lwr_k=300:8.5645,lwr_k=400:8.9061,lwr_k=500:9.08,lwr_k=600:8.8507,lwr_k=700:9.2989,lwr_k=800:9.3292,lwr_k=900:9.3605,lwr_k=1000:9.4687'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.0838,lwr_k=10:39.84,lwr_k=20:138.9333,lwr_k=30:44.069,lwr_k=40:17.0775,lwr_k=50:13.0409,lwr_k=100:9.6291,lwr_k=200:8.9376,lwr_k=300:8.5858,lwr_k=400:8.6732,lwr_k=500:8.6494,lwr_k=600:8.5107,lwr_k=700:8.4443,lwr_k=800:8.4451,lwr_k=900:8.3755,lwr_k=1000:8.4048'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.5758,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.6186,lwr_k=40:1.8355,lwr_k=50:0.0652,lwr_k=100:0.3776,lwr_k=200:1.5162,lwr_k=300:2.4375,lwr_k=400:3.2954,lwr_k=500:3.9737,lwr_k=600:4.6994,lwr_k=700:4.9293,lwr_k=800:5.058,lwr_k=900:5.2553,lwr_k=1000:5.4356'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.4557,lwr_k=10:25.5762,lwr_k=20:52.599,lwr_k=30:117.9485,lwr_k=40:18.2973,lwr_k=50:28.5611,lwr_k=100:27.8648,lwr_k=200:18.2539,lwr_k=300:15.2463,lwr_k=400:13.2105,lwr_k=500:14.4431,lwr_k=600:11.3606,lwr_k=700:10.7672,lwr_k=800:9.6943,lwr_k=900:9.6416,lwr_k=1000:10.1902'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:3.1575,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.474,lwr_k=40:1.2068,lwr_k=50:0.0396,lwr_k=100:0.1912,lwr_k=200:0.9045,lwr_k=300:1.3269,lwr_k=400:1.6512,lwr_k=500:1.952,lwr_k=600:2.3753,lwr_k=700:2.4995,lwr_k=800:2.675,lwr_k=900:2.8152,lwr_k=1000:2.864'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:20.334,lwr_k=10:35.237,lwr_k=20:110.046,lwr_k=30:68.1828,lwr_k=40:23.3909,lwr_k=50:25.7735,lwr_k=100:27.806,lwr_k=200:28.3226,lwr_k=300:24.5468,lwr_k=400:22.4945,lwr_k=500:20.8573,lwr_k=600:21.1608,lwr_k=700:20.5953,lwr_k=800:19.6439,lwr_k=900:19.631,lwr_k=1000:19.6454'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:3.5761,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.171,lwr_k=40:1.119,lwr_k=50:0.0508,lwr_k=100:0.4857,lwr_k=200:1.6132,lwr_k=300:2.2684,lwr_k=400:2.6512,lwr_k=500:2.8839,lwr_k=600:2.9405,lwr_k=700:3.0064,lwr_k=800:3.1864,lwr_k=900:3.2615,lwr_k=1000:3.078'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.5662,lwr_k=10:19.006,lwr_k=20:66.7365,lwr_k=30:765.1777,lwr_k=40:43.5711,lwr_k=50:32.4271,lwr_k=100:19.9859,lwr_k=200:15.0813,lwr_k=300:11.1761,lwr_k=400:8.2965,lwr_k=500:8.1618,lwr_k=600:8.0822,lwr_k=700:7.9706,lwr_k=800:7.1741,lwr_k=900:7.32,lwr_k=1000:11.1747'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_48'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.1988,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0011,lwr_k=40:0.0294,lwr_k=50:0.5407,lwr_k=100:2.4919,lwr_k=200:3.3791,lwr_k=300:3.83,lwr_k=400:4.0897,lwr_k=500:4.2316,lwr_k=600:4.3401,lwr_k=700:4.3684,lwr_k=800:4.411,lwr_k=900:4.4323,lwr_k=1000:4.4799'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.594,lwr_k=10:36.7999,lwr_k=20:96.3896,lwr_k=30:89.9515,lwr_k=40:178.6419,lwr_k=50:170.49,lwr_k=100:10.0939,lwr_k=200:8.0815,lwr_k=300:7.7541,lwr_k=400:7.5952,lwr_k=500:7.6168,lwr_k=600:7.4588,lwr_k=700:7.2732,lwr_k=800:7.1681,lwr_k=900:7.0679,lwr_k=1000:6.9229'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.8118,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.003,lwr_k=40:0.0535,lwr_k=50:0.5991,lwr_k=100:2.4823,lwr_k=200:3.6664,lwr_k=300:4.0094,lwr_k=400:4.1835,lwr_k=500:4.2958,lwr_k=600:4.3475,lwr_k=700:4.3814,lwr_k=800:4.4184,lwr_k=900:4.4326,lwr_k=1000:4.4642'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.8775,lwr_k=10:43.2965,lwr_k=20:71.8131,lwr_k=30:76.2186,lwr_k=40:179.6694,lwr_k=50:77.2997,lwr_k=100:10.8814,lwr_k=200:7.586,lwr_k=300:7.1984,lwr_k=400:7.0898,lwr_k=500:6.9387,lwr_k=600:6.9153,lwr_k=700:6.9572,lwr_k=800:6.9175,lwr_k=900:6.9612,lwr_k=1000:6.912'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.9656,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0004,lwr_k=40:0.0139,lwr_k=50:0.6504,lwr_k=100:2.6706,lwr_k=200:3.8685,lwr_k=300:4.3261,lwr_k=400:4.5132,lwr_k=500:4.7362,lwr_k=600:4.9089,lwr_k=700:5.0545,lwr_k=800:5.1605,lwr_k=900:5.2587,lwr_k=1000:5.3165'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:5.4433,lwr_k=10:20.2176,lwr_k=20:33.3553,lwr_k=30:111.4029,lwr_k=40:2657.1688,lwr_k=50:90.812,lwr_k=100:9.9228,lwr_k=200:5.6772,lwr_k=300:5.124,lwr_k=400:5.0476,lwr_k=500:5.0274,lwr_k=600:5.0499,lwr_k=700:4.9994,lwr_k=800:5.0054,lwr_k=900:5.0447,lwr_k=1000:5.0426'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.737,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0016,lwr_k=40:0.0503,lwr_k=50:0.8548,lwr_k=100:2.8377,lwr_k=200:4.1994,lwr_k=300:4.6877,lwr_k=400:4.9467,lwr_k=500:5.1197,lwr_k=600:5.2454,lwr_k=700:5.3174,lwr_k=800:5.4221,lwr_k=900:5.5169,lwr_k=1000:5.561'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.2305,lwr_k=10:36.1261,lwr_k=20:75.2758,lwr_k=30:72.9345,lwr_k=40:212.7168,lwr_k=50:273.9176,lwr_k=100:10.0115,lwr_k=200:7.0657,lwr_k=300:6.8435,lwr_k=400:6.6932,lwr_k=500:6.7378,lwr_k=600:6.7432,lwr_k=700:6.7597,lwr_k=800:6.835,lwr_k=900:6.8715,lwr_k=1000:6.8512'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:3.9102,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.036,lwr_k=40:0.1595,lwr_k=50:0.635,lwr_k=100:2.1578,lwr_k=200:2.9784,lwr_k=300:3.1927,lwr_k=400:3.3557,lwr_k=500:3.4585,lwr_k=600:3.5156,lwr_k=700:3.5502,lwr_k=800:3.5998,lwr_k=900:3.6051,lwr_k=1000:3.6095'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:14.1927,lwr_k=10:130.3221,lwr_k=20:81.9028,lwr_k=30:159.812,lwr_k=40:390.1872,lwr_k=50:118.0305,lwr_k=100:18.1328,lwr_k=200:16.6306,lwr_k=300:15.3729,lwr_k=400:14.568,lwr_k=500:14.3649,lwr_k=600:14.1953,lwr_k=700:14.1067,lwr_k=800:14.0219,lwr_k=900:13.9315,lwr_k=1000:13.9384'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:5.1688,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0003,lwr_k=40:0.0415,lwr_k=50:0.4796,lwr_k=100:2.2729,lwr_k=200:3.1482,lwr_k=300:3.4898,lwr_k=400:3.6097,lwr_k=500:3.7171,lwr_k=600:3.7524,lwr_k=700:3.8011,lwr_k=800:3.8249,lwr_k=900:3.8398,lwr_k=1000:3.8701'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.6764,lwr_k=10:150.7762,lwr_k=20:71.872,lwr_k=30:266.5154,lwr_k=40:225.6569,lwr_k=50:103.9549,lwr_k=100:330.837,lwr_k=200:7.0813,lwr_k=300:7.3964,lwr_k=400:7.4453,lwr_k=500:7.0156,lwr_k=600:7.0331,lwr_k=700:7.093,lwr_k=800:7.0329,lwr_k=900:7.0459,lwr_k=1000:6.9941'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_49'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.2481,lwr_k=10:4.304,lwr_k=20:5.1622,lwr_k=30:5.5192,lwr_k=40:5.77,lwr_k=50:5.9214,lwr_k=100:6.1084,lwr_k=200:6.2231,lwr_k=300:6.2724,lwr_k=400:6.3121,lwr_k=500:6.3781,lwr_k=600:6.4075,lwr_k=700:6.4461,lwr_k=800:6.4845,lwr_k=900:6.51,lwr_k=1000:6.5459'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.5948,lwr_k=10:11.6403,lwr_k=20:9.8595,lwr_k=30:9.596,lwr_k=40:9.3222,lwr_k=50:9.0373,lwr_k=100:8.8181,lwr_k=200:8.865,lwr_k=300:9.0846,lwr_k=400:9.2094,lwr_k=500:9.3176,lwr_k=600:9.3465,lwr_k=700:9.3845,lwr_k=800:9.4544,lwr_k=900:9.4994,lwr_k=1000:9.5373'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:19.938,lwr_k=10:8.0316,lwr_k=20:9.8403,lwr_k=30:10.7147,lwr_k=40:11.323,lwr_k=50:11.6145,lwr_k=100:12.5159,lwr_k=200:13.4492,lwr_k=300:14.037,lwr_k=400:14.5697,lwr_k=500:14.932,lwr_k=600:15.2526,lwr_k=700:15.4933,lwr_k=800:15.7349,lwr_k=900:15.9826,lwr_k=1000:16.1788'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:14.9902,lwr_k=10:16.7413,lwr_k=20:13.1987,lwr_k=30:11.9718,lwr_k=40:11.7837,lwr_k=50:11.6022,lwr_k=100:11.386,lwr_k=200:11.3391,lwr_k=300:11.4927,lwr_k=400:11.5364,lwr_k=500:11.6631,lwr_k=600:11.857,lwr_k=700:11.9422,lwr_k=800:12.1654,lwr_k=900:12.2365,lwr_k=1000:12.2945'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:20.3169,lwr_k=10:8.2284,lwr_k=20:10.237,lwr_k=30:10.8786,lwr_k=40:11.5452,lwr_k=50:12.1134,lwr_k=100:13.4783,lwr_k=200:14.3752,lwr_k=300:14.8297,lwr_k=400:15.1792,lwr_k=500:15.3794,lwr_k=600:15.5817,lwr_k=700:15.7814,lwr_k=800:15.9499,lwr_k=900:16.093,lwr_k=1000:16.2596'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:13.2521,lwr_k=10:13.5525,lwr_k=20:11.7587,lwr_k=30:11.204,lwr_k=40:11.0891,lwr_k=50:10.5294,lwr_k=100:10.1915,lwr_k=200:10.1958,lwr_k=300:10.1469,lwr_k=400:10.2603,lwr_k=500:10.2634,lwr_k=600:10.3675,lwr_k=700:10.3882,lwr_k=800:10.4413,lwr_k=900:10.4455,lwr_k=1000:10.4751'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:18.1604,lwr_k=10:7.5862,lwr_k=20:9.2446,lwr_k=30:9.86,lwr_k=40:10.4661,lwr_k=50:11.0354,lwr_k=100:12.106,lwr_k=200:12.9807,lwr_k=300:13.4754,lwr_k=400:13.8867,lwr_k=500:14.1262,lwr_k=600:14.3555,lwr_k=700:14.5229,lwr_k=800:14.6815,lwr_k=900:14.8186,lwr_k=1000:14.968'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:15.7273,lwr_k=10:15.2061,lwr_k=20:12.5718,lwr_k=30:12.4746,lwr_k=40:12.0039,lwr_k=50:12.248,lwr_k=100:12.5342,lwr_k=200:12.5146,lwr_k=300:12.413,lwr_k=400:12.5218,lwr_k=500:12.568,lwr_k=600:12.6247,lwr_k=700:12.6917,lwr_k=800:12.7393,lwr_k=900:12.8265,lwr_k=1000:12.8883'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.7587,lwr_k=10:4.7166,lwr_k=20:5.6811,lwr_k=30:5.9505,lwr_k=40:6.2539,lwr_k=50:6.4264,lwr_k=100:6.6625,lwr_k=200:6.8087,lwr_k=300:6.9066,lwr_k=400:6.9662,lwr_k=500:7.0027,lwr_k=600:7.0436,lwr_k=700:7.0943,lwr_k=800:7.1187,lwr_k=900:7.1419,lwr_k=1000:7.1586'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:28.0524,lwr_k=10:18.7855,lwr_k=20:16.4116,lwr_k=30:18.1002,lwr_k=40:20.5075,lwr_k=50:21.0566,lwr_k=100:21.5464,lwr_k=200:22.9232,lwr_k=300:23.5882,lwr_k=400:24.0025,lwr_k=500:24.2536,lwr_k=600:24.6191,lwr_k=700:24.8166,lwr_k=800:25.0652,lwr_k=900:25.2008,lwr_k=1000:25.3284'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:18.3769,lwr_k=10:7.9724,lwr_k=20:9.621,lwr_k=30:10.0195,lwr_k=40:10.2926,lwr_k=50:10.6073,lwr_k=100:11.6628,lwr_k=200:12.3815,lwr_k=300:12.9635,lwr_k=400:13.3145,lwr_k=500:13.638,lwr_k=600:13.8644,lwr_k=700:14.0428,lwr_k=800:14.1673,lwr_k=900:14.3243,lwr_k=1000:14.4555'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:17.2962,lwr_k=10:16.6329,lwr_k=20:15.8724,lwr_k=30:14.6412,lwr_k=40:14.5399,lwr_k=50:14.1975,lwr_k=100:13.6291,lwr_k=200:13.9518,lwr_k=300:14.0563,lwr_k=400:14.0733,lwr_k=500:14.1492,lwr_k=600:14.1777,lwr_k=700:14.2505,lwr_k=800:14.2905,lwr_k=900:14.3278,lwr_k=1000:14.4176'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_50'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.7628,lwr_k=10:0.6229,lwr_k=20:4.6407,lwr_k=30:5.6828,lwr_k=40:6.2427,lwr_k=50:6.615,lwr_k=100:7.6008,lwr_k=200:8.1044,lwr_k=300:8.245,lwr_k=400:8.3743,lwr_k=500:8.5192,lwr_k=600:8.6209,lwr_k=700:8.6881,lwr_k=800:8.7598,lwr_k=900:8.8028,lwr_k=1000:8.8628'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:13.148,lwr_k=10:3536.6971,lwr_k=20:22.1476,lwr_k=30:15.1393,lwr_k=40:13.0668,lwr_k=50:12.341,lwr_k=100:11.3256,lwr_k=200:11.0774,lwr_k=300:11.1402,lwr_k=400:11.177,lwr_k=500:11.2657,lwr_k=600:11.4298,lwr_k=700:11.5574,lwr_k=800:11.6427,lwr_k=900:11.706,lwr_k=1000:11.7657'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:18.0532,lwr_k=10:0.6018,lwr_k=20:5.7683,lwr_k=30:7.7658,lwr_k=40:8.879,lwr_k=50:10.066,lwr_k=100:11.7669,lwr_k=200:13.0657,lwr_k=300:13.6082,lwr_k=400:13.9587,lwr_k=500:14.2334,lwr_k=600:14.417,lwr_k=700:14.5666,lwr_k=800:14.7926,lwr_k=900:15.0113,lwr_k=1000:15.1767'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:14.0664,lwr_k=10:5485779855.0758,lwr_k=20:1491.6575,lwr_k=30:20.1716,lwr_k=40:14.8224,lwr_k=50:13.7821,lwr_k=100:11.8411,lwr_k=200:11.2619,lwr_k=300:11.0979,lwr_k=400:11.171,lwr_k=500:11.335,lwr_k=600:11.3908,lwr_k=700:11.3749,lwr_k=800:11.5134,lwr_k=900:11.5997,lwr_k=1000:11.634'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:29.3615,lwr_k=10:0.7087,lwr_k=20:7.2907,lwr_k=30:9.6796,lwr_k=40:11.2917,lwr_k=50:12.548,lwr_k=100:15.1988,lwr_k=200:16.9491,lwr_k=300:17.8344,lwr_k=400:18.2409,lwr_k=500:18.9116,lwr_k=600:19.298,lwr_k=700:19.8781,lwr_k=800:20.1304,lwr_k=900:20.5675,lwr_k=1000:20.9649'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:17.7002,lwr_k=10:29030181.2076,lwr_k=20:4158351.2142,lwr_k=30:1957230.794,lwr_k=40:229890.7478,lwr_k=50:1568691.4416,lwr_k=100:7528.2841,lwr_k=200:12.1226,lwr_k=300:11.9553,lwr_k=400:12.0055,lwr_k=500:12.0573,lwr_k=600:12.0445,lwr_k=700:12.0126,lwr_k=800:12.153,lwr_k=900:12.0483,lwr_k=1000:12.175'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:14.4748,lwr_k=10:0.7777,lwr_k=20:5.3,lwr_k=30:6.8861,lwr_k=40:8.0853,lwr_k=50:8.848,lwr_k=100:10.1501,lwr_k=200:11.2991,lwr_k=300:11.8111,lwr_k=400:12.3059,lwr_k=500:12.4693,lwr_k=600:12.5976,lwr_k=700:12.7039,lwr_k=800:12.736,lwr_k=900:12.8223,lwr_k=1000:12.8854'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:14.5415,lwr_k=10:193763461.7358,lwr_k=20:28.9972,lwr_k=30:45.421,lwr_k=40:16.7287,lwr_k=50:21.4205,lwr_k=100:13.0368,lwr_k=200:12.5716,lwr_k=300:12.5022,lwr_k=400:12.474,lwr_k=500:12.61,lwr_k=600:12.6951,lwr_k=700:12.7353,lwr_k=800:12.7585,lwr_k=900:12.7488,lwr_k=1000:12.7583'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:18.7018,lwr_k=10:0.7149,lwr_k=20:6.5055,lwr_k=30:8.3508,lwr_k=40:9.3919,lwr_k=50:9.9115,lwr_k=100:11.1587,lwr_k=200:12.0101,lwr_k=300:12.4344,lwr_k=400:12.7603,lwr_k=500:13.1328,lwr_k=600:13.4261,lwr_k=700:13.825,lwr_k=800:14.083,lwr_k=900:14.3082,lwr_k=1000:14.5628'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:54.5677,lwr_k=10:226112.9059,lwr_k=20:41304.9651,lwr_k=30:43.852,lwr_k=40:51.009,lwr_k=50:48.5346,lwr_k=100:153.6166,lwr_k=200:35.5443,lwr_k=300:37.6283,lwr_k=400:39.0751,lwr_k=500:41.0672,lwr_k=600:42.4211,lwr_k=700:43.8244,lwr_k=800:44.7617,lwr_k=900:45.5026,lwr_k=1000:46.4162'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:11.0567,lwr_k=10:0.7963,lwr_k=20:4.7765,lwr_k=30:5.9914,lwr_k=40:6.6526,lwr_k=50:7.4542,lwr_k=100:8.4296,lwr_k=200:9.1835,lwr_k=300:9.539,lwr_k=400:9.6715,lwr_k=500:9.7665,lwr_k=600:9.8083,lwr_k=700:9.8599,lwr_k=800:9.9155,lwr_k=900:9.9667,lwr_k=1000:10.0158'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:15.5213,lwr_k=10:95309.6581,lwr_k=20:291426.455,lwr_k=30:49.1197,lwr_k=40:38.4384,lwr_k=50:17.2027,lwr_k=100:13.8846,lwr_k=200:14.1803,lwr_k=300:14.5737,lwr_k=400:14.5493,lwr_k=500:14.7571,lwr_k=600:14.979,lwr_k=700:15.076,lwr_k=800:14.9864,lwr_k=900:14.9503,lwr_k=1000:15.0677'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_51'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.9224,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0002,lwr_k=40:0.002,lwr_k=50:0.6238,lwr_k=100:2.7044,lwr_k=200:3.9476,lwr_k=300:4.4051,lwr_k=400:4.638,lwr_k=500:4.8349,lwr_k=600:4.9782,lwr_k=700:5.1037,lwr_k=800:5.2306,lwr_k=900:5.321,lwr_k=1000:5.4001'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.3893,lwr_k=10:30.9761,lwr_k=20:36.0171,lwr_k=30:55.6791,lwr_k=40:182.4126,lwr_k=50:48.3372,lwr_k=100:11.439,lwr_k=200:8.7044,lwr_k=300:8.3626,lwr_k=400:8.3275,lwr_k=500:8.2763,lwr_k=600:8.3645,lwr_k=700:8.4681,lwr_k=800:8.6189,lwr_k=900:8.6405,lwr_k=1000:8.6513'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:13.5962,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0098,lwr_k=50:0.8513,lwr_k=100:4.1839,lwr_k=200:6.8655,lwr_k=300:7.9916,lwr_k=400:8.602,lwr_k=500:9.1041,lwr_k=600:9.4158,lwr_k=700:9.6942,lwr_k=800:9.8947,lwr_k=900:10.087,lwr_k=1000:10.2706'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:13.6148,lwr_k=10:38.5296,lwr_k=20:57.2196,lwr_k=30:66.9285,lwr_k=40:185.1435,lwr_k=50:70.6432,lwr_k=100:14.7351,lwr_k=200:9.9376,lwr_k=300:9.3475,lwr_k=400:9.406,lwr_k=500:9.6627,lwr_k=600:9.6496,lwr_k=700:9.7958,lwr_k=800:9.7656,lwr_k=900:9.9121,lwr_k=1000:10.0338'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:21.0614,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0001,lwr_k=40:0.011,lwr_k=50:0.8956,lwr_k=100:5.1208,lwr_k=200:8.0911,lwr_k=300:9.8634,lwr_k=400:10.874,lwr_k=500:11.4327,lwr_k=600:11.9602,lwr_k=700:12.4284,lwr_k=800:12.7984,lwr_k=900:13.0739,lwr_k=1000:13.2219'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:13.2112,lwr_k=10:29.2666,lwr_k=20:60.3862,lwr_k=30:76.9885,lwr_k=40:242.0832,lwr_k=50:56.0852,lwr_k=100:14.3077,lwr_k=200:10.1814,lwr_k=300:8.9679,lwr_k=400:8.7867,lwr_k=500:8.6654,lwr_k=600:8.6896,lwr_k=700:8.7411,lwr_k=800:8.7047,lwr_k=900:8.626,lwr_k=1000:8.6656'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:14.1225,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0003,lwr_k=40:0.0244,lwr_k=50:1.0219,lwr_k=100:4.6152,lwr_k=200:6.8885,lwr_k=300:7.948,lwr_k=400:8.4035,lwr_k=500:8.7596,lwr_k=600:9.0974,lwr_k=700:9.4467,lwr_k=800:9.647,lwr_k=900:9.8107,lwr_k=1000:9.9923'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:13.3609,lwr_k=10:45.8588,lwr_k=20:58.9156,lwr_k=30:73.2367,lwr_k=40:214.8785,lwr_k=50:60.8579,lwr_k=100:15.3885,lwr_k=200:10.4833,lwr_k=300:9.9543,lwr_k=400:9.9629,lwr_k=500:9.8837,lwr_k=600:9.9469,lwr_k=700:9.9338,lwr_k=800:10.0513,lwr_k=900:10.2036,lwr_k=1000:10.0542'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.6656,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0001,lwr_k=40:0.0167,lwr_k=50:0.6382,lwr_k=100:2.8738,lwr_k=200:4.2656,lwr_k=300:4.8092,lwr_k=400:5.0358,lwr_k=500:5.2529,lwr_k=600:5.4643,lwr_k=700:5.6192,lwr_k=800:5.7194,lwr_k=900:5.7857,lwr_k=1000:5.8815'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:27.7232,lwr_k=10:43.4291,lwr_k=20:66.7301,lwr_k=30:70.337,lwr_k=40:348.3595,lwr_k=50:153.2137,lwr_k=100:24.9019,lwr_k=200:23.212,lwr_k=300:23.5295,lwr_k=400:23.454,lwr_k=500:23.6809,lwr_k=600:23.4614,lwr_k=700:23.7713,lwr_k=800:23.6692,lwr_k=900:23.7866,lwr_k=1000:24.104'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:12.1997,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0002,lwr_k=40:0.0124,lwr_k=50:0.7515,lwr_k=100:3.6397,lwr_k=200:5.6129,lwr_k=300:6.6135,lwr_k=400:7.2776,lwr_k=500:7.6419,lwr_k=600:7.9536,lwr_k=700:8.1953,lwr_k=800:8.3597,lwr_k=900:8.5093,lwr_k=1000:8.6651'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:14.3791,lwr_k=10:159.4246,lwr_k=20:58.5212,lwr_k=30:65.0014,lwr_k=40:226.2903,lwr_k=50:130.1829,lwr_k=100:12.4299,lwr_k=200:10.6532,lwr_k=300:10.8039,lwr_k=400:11.1554,lwr_k=500:11.6365,lwr_k=600:11.7912,lwr_k=700:11.9646,lwr_k=800:12.0499,lwr_k=900:12.3258,lwr_k=1000:12.3765'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_52'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.3761,lwr_k=10:0.0,lwr_k=20:0.0076,lwr_k=30:1.4662,lwr_k=40:4.8303,lwr_k=50:4.7402,lwr_k=100:6.1807,lwr_k=200:6.7047,lwr_k=300:7.1179,lwr_k=400:7.0325,lwr_k=500:7.1751,lwr_k=600:7.2492,lwr_k=700:7.2977,lwr_k=800:7.3922,lwr_k=900:7.4607,lwr_k=1000:7.5358'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.8636,lwr_k=10:92.0067,lwr_k=20:248.0041,lwr_k=30:3695.7937,lwr_k=40:194.0028,lwr_k=50:750469.7431,lwr_k=100:15.4227,lwr_k=200:11.3874,lwr_k=300:10.7773,lwr_k=400:10.149,lwr_k=500:9.9438,lwr_k=600:9.9352,lwr_k=700:9.9307,lwr_k=800:10.0275,lwr_k=900:10.068,lwr_k=1000:10.179'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.9092,lwr_k=10:0.0,lwr_k=20:0.0017,lwr_k=30:0.54,lwr_k=40:2.0012,lwr_k=50:2.5885,lwr_k=100:4.1657,lwr_k=200:6.0881,lwr_k=300:7.0891,lwr_k=400:7.5084,lwr_k=500:7.7027,lwr_k=600:7.8048,lwr_k=700:7.869,lwr_k=800:7.9293,lwr_k=900:7.976,lwr_k=1000:8.0254'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.1615,lwr_k=10:49.8252,lwr_k=20:1862.2915,lwr_k=30:64055.2553,lwr_k=40:81681.7523,lwr_k=50:14146.1334,lwr_k=100:611.395,lwr_k=200:9.0994,lwr_k=300:7.0215,lwr_k=400:7.0412,lwr_k=500:7.0615,lwr_k=600:7.0511,lwr_k=700:7.0657,lwr_k=800:7.066,lwr_k=900:7.0316,lwr_k=1000:7.0141'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.3095,lwr_k=10:0.0024,lwr_k=20:0.385,lwr_k=30:2.4998,lwr_k=40:17.6672,lwr_k=50:15.5737,lwr_k=100:13.1655,lwr_k=200:8.5316,lwr_k=300:8.1974,lwr_k=400:8.9116,lwr_k=500:9.4041,lwr_k=600:9.5556,lwr_k=700:10.2288,lwr_k=800:10.3376,lwr_k=900:10.4183,lwr_k=1000:10.5008'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.5428,lwr_k=10:2339.1286,lwr_k=20:70287262.1853,lwr_k=30:19102487.5394,lwr_k=40:569.8357,lwr_k=50:347.8866,lwr_k=100:121.6957,lwr_k=200:11.2584,lwr_k=300:9.6776,lwr_k=400:8.1046,lwr_k=500:7.9005,lwr_k=600:7.3855,lwr_k=700:7.4339,lwr_k=800:7.4088,lwr_k=900:7.459,lwr_k=1000:7.4782'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.1363,lwr_k=10:0.0,lwr_k=20:0.0135,lwr_k=30:0.5307,lwr_k=40:1.5343,lwr_k=50:2.619,lwr_k=100:4.3401,lwr_k=200:5.1879,lwr_k=300:5.5761,lwr_k=400:6.0173,lwr_k=500:6.6711,lwr_k=600:6.961,lwr_k=700:7.1958,lwr_k=800:7.3702,lwr_k=900:7.5443,lwr_k=1000:7.6853'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.0641,lwr_k=10:38.5981,lwr_k=20:143.5924,lwr_k=30:39807256.8606,lwr_k=40:40245.8815,lwr_k=50:213.7494,lwr_k=100:10.5163,lwr_k=200:8.4299,lwr_k=300:8.4483,lwr_k=400:8.4642,lwr_k=500:8.5156,lwr_k=600:8.5889,lwr_k=700:8.672,lwr_k=800:8.6064,lwr_k=900:8.6259,lwr_k=1000:8.6271'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.5621,lwr_k=10:0.0,lwr_k=20:0.0013,lwr_k=30:0.6082,lwr_k=40:1.406,lwr_k=50:2.0611,lwr_k=100:3.5923,lwr_k=200:4.2708,lwr_k=300:4.613,lwr_k=400:4.7747,lwr_k=500:4.8818,lwr_k=600:4.9544,lwr_k=700:5.0027,lwr_k=800:5.074,lwr_k=900:5.1172,lwr_k=1000:5.1534'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:21.3252,lwr_k=10:200.3603,lwr_k=20:557.6237,lwr_k=30:2263117.0454,lwr_k=40:180124.3295,lwr_k=50:61343.3665,lwr_k=100:1948.4713,lwr_k=200:15.1209,lwr_k=300:15.0226,lwr_k=400:15.6632,lwr_k=500:16.4893,lwr_k=600:17.1125,lwr_k=700:17.5144,lwr_k=800:17.9312,lwr_k=900:18.3264,lwr_k=1000:18.5944'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:10.8948,lwr_k=10:0.0004,lwr_k=20:0.3581,lwr_k=30:3.2077,lwr_k=40:18.3575,lwr_k=50:16.7778,lwr_k=100:13.2494,lwr_k=200:10.4692,lwr_k=300:10.9884,lwr_k=400:10.014,lwr_k=500:10.2761,lwr_k=600:10.3323,lwr_k=700:11.4073,lwr_k=800:10.8873,lwr_k=900:12.0005,lwr_k=1000:10.7547'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.5658,lwr_k=10:8886.1485,lwr_k=20:6782574.0258,lwr_k=30:795666.2119,lwr_k=40:64642.9528,lwr_k=50:406940.8949,lwr_k=100:587190.1655,lwr_k=200:4589078.7992,lwr_k=300:4874181.6875,lwr_k=400:68562554.1529,lwr_k=500:96034910.8464,lwr_k=600:4694923.8461,lwr_k=700:1275546.2255,lwr_k=800:6138395.2345,lwr_k=900:11.886,lwr_k=1000:38116.5253'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_53'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.8677,lwr_k=10:0.0,lwr_k=20:0.301,lwr_k=30:2.3812,lwr_k=40:3.5822,lwr_k=50:4.3417,lwr_k=100:6.1953,lwr_k=200:7.0626,lwr_k=300:7.6055,lwr_k=400:7.8631,lwr_k=500:7.9747,lwr_k=600:8.1137,lwr_k=700:8.2269,lwr_k=800:8.3426,lwr_k=900:8.4563,lwr_k=1000:8.5323'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:14.2178,lwr_k=10:153.4389,lwr_k=20:281.2355,lwr_k=30:29.3389,lwr_k=40:15.7581,lwr_k=50:12.4194,lwr_k=100:10.6822,lwr_k=200:10.0899,lwr_k=300:10.3705,lwr_k=400:10.6447,lwr_k=500:10.8494,lwr_k=600:10.8902,lwr_k=700:11.0547,lwr_k=800:11.23,lwr_k=900:11.4009,lwr_k=1000:11.4833'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:17.5846,lwr_k=10:0.0,lwr_k=20:0.4142,lwr_k=30:3.0868,lwr_k=40:5.005,lwr_k=50:6.1499,lwr_k=100:9.6507,lwr_k=200:11.4825,lwr_k=300:12.2954,lwr_k=400:12.7936,lwr_k=500:13.2339,lwr_k=600:13.4704,lwr_k=700:13.7587,lwr_k=800:13.9827,lwr_k=900:14.1827,lwr_k=1000:14.3748'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:14.0526,lwr_k=10:198.5239,lwr_k=20:914.3324,lwr_k=30:42.8145,lwr_k=40:24.022,lwr_k=50:17.2721,lwr_k=100:12.144,lwr_k=200:10.259,lwr_k=300:10.2923,lwr_k=400:10.4718,lwr_k=500:10.5783,lwr_k=600:10.8793,lwr_k=700:10.9721,lwr_k=800:11.0713,lwr_k=900:11.2216,lwr_k=1000:11.3165'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.937,lwr_k=10:0.0,lwr_k=20:0.3854,lwr_k=30:2.8506,lwr_k=40:4.0471,lwr_k=50:5.1801,lwr_k=100:7.5366,lwr_k=200:8.5529,lwr_k=300:9.4057,lwr_k=400:9.8678,lwr_k=500:10.2847,lwr_k=600:10.5001,lwr_k=700:10.673,lwr_k=800:10.8399,lwr_k=900:10.971,lwr_k=1000:11.1499'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.1067,lwr_k=10:85.9343,lwr_k=20:671.6986,lwr_k=30:38.7161,lwr_k=40:18.488,lwr_k=50:17.3828,lwr_k=100:8.7505,lwr_k=200:7.8619,lwr_k=300:7.3602,lwr_k=400:7.3023,lwr_k=500:7.1304,lwr_k=600:7.1559,lwr_k=700:7.1316,lwr_k=800:7.1056,lwr_k=900:7.1271,lwr_k=1000:7.1186'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:13.9912,lwr_k=10:0.0,lwr_k=20:0.4147,lwr_k=30:2.8645,lwr_k=40:4.1301,lwr_k=50:5.1157,lwr_k=100:7.8519,lwr_k=200:9.3972,lwr_k=300:10.3334,lwr_k=400:10.8952,lwr_k=500:11.2059,lwr_k=600:11.4034,lwr_k=700:11.6296,lwr_k=800:11.8292,lwr_k=900:12.0382,lwr_k=1000:12.2421'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:12.1654,lwr_k=10:135.2374,lwr_k=20:5034.9963,lwr_k=30:7438.7827,lwr_k=40:9194.1943,lwr_k=50:2335.3125,lwr_k=100:13.0246,lwr_k=200:11.1182,lwr_k=300:11.0217,lwr_k=400:11.0131,lwr_k=500:11.1728,lwr_k=600:11.2931,lwr_k=700:11.2807,lwr_k=800:11.2978,lwr_k=900:11.436,lwr_k=1000:11.4067'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.6062,lwr_k=10:0.0,lwr_k=20:0.3757,lwr_k=30:2.508,lwr_k=40:3.5989,lwr_k=50:4.3023,lwr_k=100:5.7114,lwr_k=200:6.6301,lwr_k=300:7.0589,lwr_k=400:7.319,lwr_k=500:7.4301,lwr_k=600:7.5328,lwr_k=700:7.6392,lwr_k=800:7.7569,lwr_k=900:7.8464,lwr_k=1000:7.9169'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:22.2894,lwr_k=10:110.2252,lwr_k=20:723.1521,lwr_k=30:41.826,lwr_k=40:25.2147,lwr_k=50:22.6845,lwr_k=100:21.9336,lwr_k=200:21.8381,lwr_k=300:21.3521,lwr_k=400:21.07,lwr_k=500:20.9234,lwr_k=600:20.8882,lwr_k=700:20.933,lwr_k=800:21.0714,lwr_k=900:21.0958,lwr_k=1000:21.1916'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:10.7525,lwr_k=10:0.0,lwr_k=20:0.3486,lwr_k=30:2.8697,lwr_k=40:4.4174,lwr_k=50:5.3287,lwr_k=100:7.4393,lwr_k=200:8.4147,lwr_k=300:8.9228,lwr_k=400:9.2334,lwr_k=500:9.4624,lwr_k=600:9.675,lwr_k=700:9.7856,lwr_k=800:9.8685,lwr_k=900:9.9465,lwr_k=1000:10.0134'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:16.4801,lwr_k=10:79.302,lwr_k=20:342.7836,lwr_k=30:33.989,lwr_k=40:22.8274,lwr_k=50:18.3419,lwr_k=100:16.5712,lwr_k=200:16.2991,lwr_k=300:15.4612,lwr_k=400:15.8378,lwr_k=500:16.0932,lwr_k=600:16.1869,lwr_k=700:16.3543,lwr_k=800:16.3871,lwr_k=900:16.3711,lwr_k=1000:16.346'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_54'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.7261,lwr_k=10:0.005,lwr_k=20:0.6943,lwr_k=30:2.0902,lwr_k=40:2.8118,lwr_k=50:3.3378,lwr_k=100:4.4608,lwr_k=200:5.2755,lwr_k=300:5.594,lwr_k=400:5.823,lwr_k=500:6.021,lwr_k=600:6.1435,lwr_k=700:6.3368,lwr_k=800:6.5034,lwr_k=900:6.6414,lwr_k=1000:6.763'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:15.6178,lwr_k=10:54.0421,lwr_k=20:205.2303,lwr_k=30:189.3241,lwr_k=40:14.321,lwr_k=50:11.0478,lwr_k=100:9.4198,lwr_k=200:9.5053,lwr_k=300:9.7188,lwr_k=400:10.0655,lwr_k=500:10.1949,lwr_k=600:10.451,lwr_k=700:10.8243,lwr_k=800:11.1039,lwr_k=900:11.4069,lwr_k=1000:11.7255'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.1982,lwr_k=10:0.0203,lwr_k=20:0.5527,lwr_k=30:2.2204,lwr_k=40:3.15,lwr_k=50:3.7314,lwr_k=100:5.8326,lwr_k=200:7.2654,lwr_k=300:7.7175,lwr_k=400:7.9625,lwr_k=500:8.1583,lwr_k=600:8.2755,lwr_k=700:8.3794,lwr_k=800:8.4909,lwr_k=900:8.5748,lwr_k=1000:8.6636'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.4112,lwr_k=10:637.8074,lwr_k=20:48342.0835,lwr_k=30:20.5074,lwr_k=40:15.1799,lwr_k=50:11.2849,lwr_k=100:7.1091,lwr_k=200:7.1881,lwr_k=300:7.2329,lwr_k=400:7.3929,lwr_k=500:7.5381,lwr_k=600:7.641,lwr_k=700:7.7212,lwr_k=800:7.7983,lwr_k=900:7.8564,lwr_k=1000:7.912'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.3079,lwr_k=10:1.2172,lwr_k=20:2.3139,lwr_k=30:3.4522,lwr_k=40:4.1007,lwr_k=50:4.5678,lwr_k=100:5.192,lwr_k=200:6.1232,lwr_k=300:6.3763,lwr_k=400:6.4914,lwr_k=500:6.5741,lwr_k=600:6.648,lwr_k=700:6.7038,lwr_k=800:6.7582,lwr_k=900:6.7873,lwr_k=1000:6.8611'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.211,lwr_k=10:91.6454,lwr_k=20:471.5331,lwr_k=30:8.072,lwr_k=40:8.0333,lwr_k=50:7.7774,lwr_k=100:7.3176,lwr_k=200:6.6917,lwr_k=300:6.485,lwr_k=400:6.5309,lwr_k=500:6.5519,lwr_k=600:6.5958,lwr_k=700:6.6072,lwr_k=800:6.6142,lwr_k=900:6.6022,lwr_k=1000:6.633'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.4487,lwr_k=10:0.0686,lwr_k=20:0.7709,lwr_k=30:2.127,lwr_k=40:2.7337,lwr_k=50:3.2548,lwr_k=100:4.1621,lwr_k=200:5.2831,lwr_k=300:6.1142,lwr_k=400:6.6648,lwr_k=500:7.0285,lwr_k=600:7.3052,lwr_k=700:7.4966,lwr_k=800:7.6998,lwr_k=900:7.8299,lwr_k=1000:7.9158'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.4151,lwr_k=10:70.5322,lwr_k=20:1116967.8958,lwr_k=30:18.2321,lwr_k=40:17.0259,lwr_k=50:9.8803,lwr_k=100:8.3909,lwr_k=200:8.2327,lwr_k=300:8.1432,lwr_k=400:8.1906,lwr_k=500:8.237,lwr_k=600:8.1883,lwr_k=700:8.2143,lwr_k=800:8.2672,lwr_k=900:8.2841,lwr_k=1000:8.3119'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:142.8502,lwr_k=10:0.0,lwr_k=20:9.4238,lwr_k=30:19.3074,lwr_k=40:25.71,lwr_k=50:29.5517,lwr_k=100:44.2715,lwr_k=200:59.375,lwr_k=300:68.2898,lwr_k=400:73.551,lwr_k=500:76.7343,lwr_k=600:79.9847,lwr_k=700:83.5476,lwr_k=800:86.6674,lwr_k=900:89.2282,lwr_k=1000:91.1841'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:213.6159,lwr_k=10:1369.8821,lwr_k=20:175.8751,lwr_k=30:253.7761,lwr_k=40:196.7065,lwr_k=50:179.8755,lwr_k=100:87.6694,lwr_k=200:88.4892,lwr_k=300:114.0087,lwr_k=400:121.0962,lwr_k=500:125.1168,lwr_k=600:128.4458,lwr_k=700:135.9871,lwr_k=800:141.5617,lwr_k=900:145.2671,lwr_k=1000:148.6155'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:6.929,lwr_k=10:0.0835,lwr_k=20:0.5786,lwr_k=30:1.7625,lwr_k=40:2.2715,lwr_k=50:2.6719,lwr_k=100:3.7932,lwr_k=200:4.6228,lwr_k=300:4.975,lwr_k=400:5.1234,lwr_k=500:5.2952,lwr_k=600:5.4364,lwr_k=700:5.4988,lwr_k=800:5.5601,lwr_k=900:5.6298,lwr_k=1000:5.7211'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.9233,lwr_k=10:85.3953,lwr_k=20:1021.8973,lwr_k=30:15.6135,lwr_k=40:12.1933,lwr_k=50:10.9001,lwr_k=100:6.8556,lwr_k=200:6.8668,lwr_k=300:6.8076,lwr_k=400:7.0178,lwr_k=500:7.0848,lwr_k=600:7.082,lwr_k=700:7.1339,lwr_k=800:7.1053,lwr_k=900:7.0332,lwr_k=1000:7.048'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_55'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.7252,lwr_k=10:0.0016,lwr_k=20:1.6425,lwr_k=30:3.3275,lwr_k=40:3.8515,lwr_k=50:4.4214,lwr_k=100:5.3011,lwr_k=200:5.7159,lwr_k=300:5.8405,lwr_k=400:5.9193,lwr_k=500:5.9943,lwr_k=600:6.061,lwr_k=700:6.0922,lwr_k=800:6.1234,lwr_k=900:6.169,lwr_k=1000:6.1905'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.4852,lwr_k=10:5571.5067,lwr_k=20:446.2276,lwr_k=30:9668.0807,lwr_k=40:5921.8514,lwr_k=50:1200.2311,lwr_k=100:8.9343,lwr_k=200:8.2991,lwr_k=300:8.2348,lwr_k=400:8.321,lwr_k=500:8.2662,lwr_k=600:8.2884,lwr_k=700:8.3516,lwr_k=800:8.3104,lwr_k=900:8.3236,lwr_k=1000:8.3364'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.4661,lwr_k=10:0.0003,lwr_k=20:0.9589,lwr_k=30:2.6067,lwr_k=40:3.731,lwr_k=50:4.2729,lwr_k=100:5.5011,lwr_k=200:6.3267,lwr_k=300:6.819,lwr_k=400:7.0239,lwr_k=500:7.2296,lwr_k=600:7.3743,lwr_k=700:7.502,lwr_k=800:7.5922,lwr_k=900:7.6985,lwr_k=1000:7.7995'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.4974,lwr_k=10:15769.5421,lwr_k=20:9444.4937,lwr_k=30:13841.2103,lwr_k=40:88665.8661,lwr_k=50:9031.4241,lwr_k=100:9583.5579,lwr_k=200:12.2546,lwr_k=300:8.2094,lwr_k=400:8.2334,lwr_k=500:8.3303,lwr_k=600:8.3738,lwr_k=700:8.5864,lwr_k=800:8.694,lwr_k=900:8.7071,lwr_k=1000:8.818'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.5261,lwr_k=10:0.0007,lwr_k=20:2.362,lwr_k=30:3.6183,lwr_k=40:4.3899,lwr_k=50:4.9126,lwr_k=100:5.7872,lwr_k=200:6.3512,lwr_k=300:6.8834,lwr_k=400:6.9769,lwr_k=500:7.0354,lwr_k=600:7.1036,lwr_k=700:7.1152,lwr_k=800:7.1496,lwr_k=900:7.1563,lwr_k=1000:7.1976'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.9871,lwr_k=10:1367009.7355,lwr_k=20:1955.5599,lwr_k=30:151.8712,lwr_k=40:12.5616,lwr_k=50:8.668,lwr_k=100:8.0133,lwr_k=200:7.4494,lwr_k=300:7.0073,lwr_k=400:6.3807,lwr_k=500:6.3299,lwr_k=600:6.4274,lwr_k=700:6.4567,lwr_k=800:6.4826,lwr_k=900:6.4682,lwr_k=1000:6.3826'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.255,lwr_k=10:0.0002,lwr_k=20:0.5889,lwr_k=30:2.244,lwr_k=40:3.3994,lwr_k=50:4.0297,lwr_k=100:5.6884,lwr_k=200:6.4837,lwr_k=300:6.7922,lwr_k=400:6.8937,lwr_k=500:6.9903,lwr_k=600:7.0755,lwr_k=700:7.1375,lwr_k=800:7.1785,lwr_k=900:7.2008,lwr_k=1000:7.2481'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.7292,lwr_k=10:361.8202,lwr_k=20:437.1993,lwr_k=30:81.3187,lwr_k=40:34.0152,lwr_k=50:17.1633,lwr_k=100:11.252,lwr_k=200:11.0633,lwr_k=300:10.3259,lwr_k=400:9.461,lwr_k=500:9.6599,lwr_k=600:9.6554,lwr_k=700:9.6185,lwr_k=800:9.6236,lwr_k=900:9.7073,lwr_k=1000:9.7719'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.5209,lwr_k=10:0.0024,lwr_k=20:1.9229,lwr_k=30:2.8753,lwr_k=40:3.8822,lwr_k=50:4.4268,lwr_k=100:5.4795,lwr_k=200:6.0682,lwr_k=300:6.3156,lwr_k=400:6.3838,lwr_k=500:6.5089,lwr_k=600:6.5826,lwr_k=700:6.6387,lwr_k=800:6.6997,lwr_k=900:6.7237,lwr_k=1000:6.7696'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:18.8592,lwr_k=10:7573.5953,lwr_k=20:133.0994,lwr_k=30:44034.2752,lwr_k=40:37.6706,lwr_k=50:17117.1058,lwr_k=100:21.3491,lwr_k=200:20.1584,lwr_k=300:18.978,lwr_k=400:18.3939,lwr_k=500:18.3697,lwr_k=600:18.3833,lwr_k=700:18.3924,lwr_k=800:18.6158,lwr_k=900:18.6498,lwr_k=1000:18.6463'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:7.4995,lwr_k=10:0.0005,lwr_k=20:1.4024,lwr_k=30:2.6238,lwr_k=40:3.3459,lwr_k=50:3.8096,lwr_k=100:4.8117,lwr_k=200:5.4027,lwr_k=300:5.6227,lwr_k=400:5.7429,lwr_k=500:5.8913,lwr_k=600:5.9473,lwr_k=700:6.0334,lwr_k=800:6.0821,lwr_k=900:6.1262,lwr_k=1000:6.1946'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.2536,lwr_k=10:1355.0013,lwr_k=20:118.3368,lwr_k=30:51602.2468,lwr_k=40:5631.398,lwr_k=50:60.6382,lwr_k=100:27.4827,lwr_k=200:9.3793,lwr_k=300:8.6337,lwr_k=400:8.4849,lwr_k=500:8.4001,lwr_k=600:8.4744,lwr_k=700:8.4836,lwr_k=800:8.5279,lwr_k=900:8.593,lwr_k=1000:8.5766'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_56'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.0234,lwr_k=10:0.0,lwr_k=20:2.6357,lwr_k=30:3.9316,lwr_k=40:4.8516,lwr_k=50:5.1249,lwr_k=100:5.7369,lwr_k=200:6.1505,lwr_k=300:6.3182,lwr_k=400:6.3685,lwr_k=500:6.4046,lwr_k=600:6.4194,lwr_k=700:6.4466,lwr_k=800:6.4848,lwr_k=900:6.5012,lwr_k=1000:6.5302'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.9398,lwr_k=10:164.5606,lwr_k=20:17.7296,lwr_k=30:11.1413,lwr_k=40:9.8696,lwr_k=50:8.7434,lwr_k=100:8.2548,lwr_k=200:8.3124,lwr_k=300:8.2669,lwr_k=400:8.2383,lwr_k=500:8.2995,lwr_k=600:8.386,lwr_k=700:8.4478,lwr_k=800:8.5448,lwr_k=900:8.554,lwr_k=1000:8.574'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.5135,lwr_k=10:0.0128,lwr_k=20:3.2325,lwr_k=30:5.0184,lwr_k=40:6.1307,lwr_k=50:6.7377,lwr_k=100:8.0011,lwr_k=200:9.0776,lwr_k=300:9.4691,lwr_k=400:9.696,lwr_k=500:9.802,lwr_k=600:9.9155,lwr_k=700:10.0216,lwr_k=800:10.1129,lwr_k=900:10.169,lwr_k=1000:10.2174'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.9997,lwr_k=10:2931.8809,lwr_k=20:23.9413,lwr_k=30:11.4723,lwr_k=40:9.7956,lwr_k=50:9.3305,lwr_k=100:7.9229,lwr_k=200:7.4952,lwr_k=300:7.6089,lwr_k=400:7.6715,lwr_k=500:7.7689,lwr_k=600:8.0895,lwr_k=700:8.1441,lwr_k=800:8.2141,lwr_k=900:8.198,lwr_k=1000:8.2336'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.4895,lwr_k=10:0.0003,lwr_k=20:3.177,lwr_k=30:4.6041,lwr_k=40:5.3014,lwr_k=50:5.9031,lwr_k=100:7.0953,lwr_k=200:8.6657,lwr_k=300:9.0645,lwr_k=400:9.2041,lwr_k=500:9.3439,lwr_k=600:9.3753,lwr_k=700:9.4908,lwr_k=800:9.5582,lwr_k=900:9.656,lwr_k=1000:9.8129'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.5296,lwr_k=10:994.9143,lwr_k=20:40.3078,lwr_k=30:11.2015,lwr_k=40:9.8417,lwr_k=50:8.2039,lwr_k=100:7.7442,lwr_k=200:7.5323,lwr_k=300:7.4086,lwr_k=400:7.3929,lwr_k=500:7.3488,lwr_k=600:7.3754,lwr_k=700:7.3452,lwr_k=800:7.3262,lwr_k=900:7.2972,lwr_k=1000:7.2644'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.6534,lwr_k=10:0.0055,lwr_k=20:3.339,lwr_k=30:4.8214,lwr_k=40:5.7282,lwr_k=50:6.066,lwr_k=100:7.1403,lwr_k=200:7.78,lwr_k=300:8.0573,lwr_k=400:8.2743,lwr_k=500:8.5034,lwr_k=600:8.5749,lwr_k=700:8.6026,lwr_k=800:8.653,lwr_k=900:8.7343,lwr_k=1000:8.817'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.4249,lwr_k=10:1939.9322,lwr_k=20:38.023,lwr_k=30:28.3928,lwr_k=40:12.1207,lwr_k=50:11.2316,lwr_k=100:10.2674,lwr_k=200:9.777,lwr_k=300:9.5859,lwr_k=400:9.6569,lwr_k=500:9.9872,lwr_k=600:9.9683,lwr_k=700:9.9858,lwr_k=800:10.0424,lwr_k=900:10.0435,lwr_k=1000:10.0743'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.5093,lwr_k=10:0.0066,lwr_k=20:2.7465,lwr_k=30:4.0717,lwr_k=40:4.6841,lwr_k=50:5.1022,lwr_k=100:5.937,lwr_k=200:6.5641,lwr_k=300:6.7832,lwr_k=400:6.8736,lwr_k=500:6.9289,lwr_k=600:7.0118,lwr_k=700:7.0549,lwr_k=800:7.0799,lwr_k=900:7.1088,lwr_k=1000:7.1322'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:24.9764,lwr_k=10:753.7602,lwr_k=20:27.354,lwr_k=30:19.3671,lwr_k=40:17.0568,lwr_k=50:17.6755,lwr_k=100:19.3103,lwr_k=200:21.6232,lwr_k=300:22.9032,lwr_k=400:22.6669,lwr_k=500:22.7657,lwr_k=600:22.7484,lwr_k=700:22.8354,lwr_k=800:22.88,lwr_k=900:23.0746,lwr_k=1000:23.1268'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:8.4132,lwr_k=10:0.0006,lwr_k=20:2.4597,lwr_k=30:3.8126,lwr_k=40:4.5607,lwr_k=50:4.9321,lwr_k=100:6.0856,lwr_k=200:6.8688,lwr_k=300:7.3232,lwr_k=400:7.5083,lwr_k=500:7.5638,lwr_k=600:7.5905,lwr_k=700:7.6491,lwr_k=800:7.7001,lwr_k=900:7.7751,lwr_k=1000:7.8007'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.4006,lwr_k=10:3769.9668,lwr_k=20:14777.3134,lwr_k=30:136.5736,lwr_k=40:163.491,lwr_k=50:5437.5923,lwr_k=100:729.2946,lwr_k=200:10.4982,lwr_k=300:10.3715,lwr_k=400:10.5236,lwr_k=500:10.3675,lwr_k=600:10.2969,lwr_k=700:10.2766,lwr_k=800:10.1823,lwr_k=900:10.1259,lwr_k=1000:10.0855'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_57'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.911,lwr_k=10:0.0718,lwr_k=20:3.5808,lwr_k=30:4.4461,lwr_k=40:4.8612,lwr_k=50:5.1809,lwr_k=100:5.7675,lwr_k=200:6.1864,lwr_k=300:6.2866,lwr_k=400:6.3559,lwr_k=500:6.4093,lwr_k=600:6.4497,lwr_k=700:6.4697,lwr_k=800:6.5013,lwr_k=900:6.5248,lwr_k=1000:6.5629'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.1513,lwr_k=10:1314.3135,lwr_k=20:15.6291,lwr_k=30:11.5895,lwr_k=40:9.9782,lwr_k=50:9.3874,lwr_k=100:8.544,lwr_k=200:8.4797,lwr_k=300:8.4961,lwr_k=400:8.5733,lwr_k=500:8.6231,lwr_k=600:8.6419,lwr_k=700:8.7289,lwr_k=800:8.7341,lwr_k=900:8.8024,lwr_k=1000:8.8491'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.9809,lwr_k=10:0.8808,lwr_k=20:4.1755,lwr_k=30:5.2017,lwr_k=40:5.9362,lwr_k=50:6.4389,lwr_k=100:7.5796,lwr_k=200:8.6559,lwr_k=300:8.956,lwr_k=400:9.1296,lwr_k=500:9.205,lwr_k=600:9.275,lwr_k=700:9.3495,lwr_k=800:9.4106,lwr_k=900:9.4452,lwr_k=1000:9.4933'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.9818,lwr_k=10:224.3328,lwr_k=20:11.6151,lwr_k=30:11.3613,lwr_k=40:10.5002,lwr_k=50:8.3776,lwr_k=100:6.8853,lwr_k=200:6.9615,lwr_k=300:7.0249,lwr_k=400:7.0637,lwr_k=500:7.1077,lwr_k=600:7.1674,lwr_k=700:7.2309,lwr_k=800:7.2648,lwr_k=900:7.3013,lwr_k=1000:7.3554'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.6615,lwr_k=10:0.0,lwr_k=20:3.9734,lwr_k=30:4.9898,lwr_k=40:5.5233,lwr_k=50:6.0444,lwr_k=100:7.4609,lwr_k=200:8.1726,lwr_k=300:8.5746,lwr_k=400:8.7201,lwr_k=500:8.8802,lwr_k=600:8.951,lwr_k=700:9.008,lwr_k=800:9.0618,lwr_k=900:9.1065,lwr_k=1000:9.157'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.9899,lwr_k=10:13633.8135,lwr_k=20:14.2761,lwr_k=30:9.8403,lwr_k=40:8.9082,lwr_k=50:8.3189,lwr_k=100:7.392,lwr_k=200:6.8869,lwr_k=300:6.7422,lwr_k=400:6.7016,lwr_k=500:6.6701,lwr_k=600:6.6791,lwr_k=700:6.6988,lwr_k=800:6.6998,lwr_k=900:6.7083,lwr_k=1000:6.7061'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.4197,lwr_k=10:0.5029,lwr_k=20:4.4936,lwr_k=30:5.8636,lwr_k=40:6.3899,lwr_k=50:6.8394,lwr_k=100:8.0287,lwr_k=200:8.7714,lwr_k=300:9.1385,lwr_k=400:9.3126,lwr_k=500:9.366,lwr_k=600:9.4356,lwr_k=700:9.5175,lwr_k=800:9.5491,lwr_k=900:9.6354,lwr_k=1000:9.6859'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.5487,lwr_k=10:261.9964,lwr_k=20:17.851,lwr_k=30:11.6407,lwr_k=40:11.7218,lwr_k=50:10.9965,lwr_k=100:9.7594,lwr_k=200:9.6426,lwr_k=300:9.6221,lwr_k=400:9.7098,lwr_k=500:9.7067,lwr_k=600:9.7036,lwr_k=700:9.7895,lwr_k=800:9.8591,lwr_k=900:9.9466,lwr_k=1000:9.9676'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.536,lwr_k=10:0.0284,lwr_k=20:3.1285,lwr_k=30:4.2662,lwr_k=40:4.9076,lwr_k=50:5.1791,lwr_k=100:5.9346,lwr_k=200:6.3981,lwr_k=300:6.5936,lwr_k=400:6.6619,lwr_k=500:6.7548,lwr_k=600:6.8115,lwr_k=700:6.8764,lwr_k=800:6.9154,lwr_k=900:6.9301,lwr_k=1000:6.9653'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:21.0067,lwr_k=10:62864.2988,lwr_k=20:22.6906,lwr_k=30:14.5255,lwr_k=40:19.0014,lwr_k=50:22.4387,lwr_k=100:20.7699,lwr_k=200:19.7117,lwr_k=300:20.3356,lwr_k=400:20.3621,lwr_k=500:20.4703,lwr_k=600:20.5827,lwr_k=700:20.6784,lwr_k=800:20.669,lwr_k=900:20.669,lwr_k=1000:20.695'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:7.9667,lwr_k=10:0.0,lwr_k=20:3.0449,lwr_k=30:4.3115,lwr_k=40:4.8725,lwr_k=50:5.2554,lwr_k=100:6.0369,lwr_k=200:6.6426,lwr_k=300:6.8659,lwr_k=400:6.9845,lwr_k=500:7.078,lwr_k=600:7.1646,lwr_k=700:7.2165,lwr_k=800:7.2582,lwr_k=900:7.298,lwr_k=1000:7.3235'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.4376,lwr_k=10:27218.6034,lwr_k=20:15.6007,lwr_k=30:12.1594,lwr_k=40:9.7818,lwr_k=50:9.3453,lwr_k=100:8.8921,lwr_k=200:9.4939,lwr_k=300:9.4342,lwr_k=400:9.5044,lwr_k=500:9.5905,lwr_k=600:9.5977,lwr_k=700:9.5498,lwr_k=800:9.5215,lwr_k=900:9.497,lwr_k=1000:9.4897'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_58'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:97.9205,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:4.2542,lwr_k=40:8.7688,lwr_k=50:12.4759,lwr_k=100:21.7321,lwr_k=200:31.8156,lwr_k=300:38.2639,lwr_k=400:42.5416,lwr_k=500:46.1742,lwr_k=600:48.6354,lwr_k=700:51.2909,lwr_k=800:53.3532,lwr_k=900:55.122,lwr_k=1000:57.4158'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:126.748,lwr_k=10:115.6601,lwr_k=20:410.0897,lwr_k=30:338.4247,lwr_k=40:45.5553,lwr_k=50:35.6898,lwr_k=100:35.006,lwr_k=200:39.954,lwr_k=300:45.4203,lwr_k=400:51.6365,lwr_k=500:55.274,lwr_k=600:59.4652,lwr_k=700:62.8656,lwr_k=800:66.3116,lwr_k=900:69.47,lwr_k=1000:72.1433'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:122.308,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:4.6538,lwr_k=40:9.7361,lwr_k=50:12.3168,lwr_k=100:23.4747,lwr_k=200:34.3452,lwr_k=300:40.5473,lwr_k=400:46.3096,lwr_k=500:50.8664,lwr_k=600:55.1201,lwr_k=700:58.249,lwr_k=800:61.7554,lwr_k=900:65.1621,lwr_k=1000:68.3928'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:99.3874,lwr_k=10:128.6094,lwr_k=20:422.5841,lwr_k=30:91.0478,lwr_k=40:97.2738,lwr_k=50:103.6178,lwr_k=100:39.5028,lwr_k=200:42.7711,lwr_k=300:47.7346,lwr_k=400:51.091,lwr_k=500:52.5221,lwr_k=600:55.4413,lwr_k=700:56.0915,lwr_k=800:57.4654,lwr_k=900:58.8521,lwr_k=1000:60.5256'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:132.7482,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:4.7246,lwr_k=40:9.2147,lwr_k=50:12.4863,lwr_k=100:24.0673,lwr_k=200:34.4988,lwr_k=300:42.2341,lwr_k=400:48.3181,lwr_k=500:53.452,lwr_k=600:57.9537,lwr_k=700:61.5361,lwr_k=800:65.2804,lwr_k=900:69.0217,lwr_k=1000:72.5462'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:97.7648,lwr_k=10:134.5129,lwr_k=20:593.9476,lwr_k=30:442.5228,lwr_k=40:168.8339,lwr_k=50:70.086,lwr_k=100:34.5375,lwr_k=200:36.4551,lwr_k=300:38.1824,lwr_k=400:40.9954,lwr_k=500:42.1526,lwr_k=600:44.9569,lwr_k=700:47.7319,lwr_k=800:48.145,lwr_k=900:48.495,lwr_k=1000:49.3576'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:128.1002,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:4.0244,lwr_k=40:8.3028,lwr_k=50:11.4415,lwr_k=100:23.7847,lwr_k=200:34.6242,lwr_k=300:42.0261,lwr_k=400:49.7328,lwr_k=500:54.3837,lwr_k=600:57.9121,lwr_k=700:61.2673,lwr_k=800:64.3387,lwr_k=900:67.5152,lwr_k=1000:71.2598'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:118.4971,lwr_k=10:197.6923,lwr_k=20:243.6298,lwr_k=30:321.5307,lwr_k=40:86.8988,lwr_k=50:73.32,lwr_k=100:49.6571,lwr_k=200:45.3528,lwr_k=300:50.0324,lwr_k=400:54.0245,lwr_k=500:57.1728,lwr_k=600:59.6118,lwr_k=700:62.2302,lwr_k=800:64.638,lwr_k=900:66.4487,lwr_k=1000:68.9687'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:101.1872,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:3.6335,lwr_k=40:7.3987,lwr_k=50:10.6326,lwr_k=100:20.8684,lwr_k=200:30.5222,lwr_k=300:37.3371,lwr_k=400:42.2263,lwr_k=500:45.7727,lwr_k=600:48.1511,lwr_k=700:50.2393,lwr_k=800:52.3957,lwr_k=900:54.3878,lwr_k=1000:56.1721'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:177.404,lwr_k=10:223.7852,lwr_k=20:289.9081,lwr_k=30:122.3164,lwr_k=40:81.3011,lwr_k=50:64.1326,lwr_k=100:53.4517,lwr_k=200:60.8225,lwr_k=300:73.6228,lwr_k=400:78.7898,lwr_k=500:83.3412,lwr_k=600:88.3026,lwr_k=700:92.6559,lwr_k=800:96.2422,lwr_k=900:99.1914,lwr_k=1000:104.2818'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:118.0652,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:3.5138,lwr_k=40:7.6352,lwr_k=50:10.6716,lwr_k=100:19.9268,lwr_k=200:29.3398,lwr_k=300:35.4978,lwr_k=400:40.8216,lwr_k=500:45.79,lwr_k=600:49.4575,lwr_k=700:52.3711,lwr_k=800:55.0102,lwr_k=900:57.2104,lwr_k=1000:59.2478'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:111.3464,lwr_k=10:138.1267,lwr_k=20:426.3189,lwr_k=30:169.3094,lwr_k=40:133.021,lwr_k=50:125.9709,lwr_k=100:45.523,lwr_k=200:53.1274,lwr_k=300:54.1584,lwr_k=400:56.2242,lwr_k=500:58.43,lwr_k=600:61.3196,lwr_k=700:62.9603,lwr_k=800:63.5577,lwr_k=900:64.7735,lwr_k=1000:66.3307'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_59'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:37.4305,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:3.4513,lwr_k=40:5.7848,lwr_k=50:7.6772,lwr_k=100:11.5935,lwr_k=200:14.8027,lwr_k=300:16.6868,lwr_k=400:17.7392,lwr_k=500:18.7017,lwr_k=600:19.4993,lwr_k=700:20.1867,lwr_k=800:20.8768,lwr_k=900:21.5269,lwr_k=1000:22.3858'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:46.9907,lwr_k=10:79.4239,lwr_k=20:490.3966,lwr_k=30:45.9056,lwr_k=40:27.6373,lwr_k=50:22.6493,lwr_k=100:18.5651,lwr_k=200:19.8561,lwr_k=300:20.898,lwr_k=400:22.0488,lwr_k=500:23.2894,lwr_k=600:24.3722,lwr_k=700:25.4527,lwr_k=800:26.3368,lwr_k=900:27.2568,lwr_k=1000:28.388'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:39.9444,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:3.2258,lwr_k=40:5.5958,lwr_k=50:6.7119,lwr_k=100:11.416,lwr_k=200:15.9077,lwr_k=300:18.3544,lwr_k=400:20.2752,lwr_k=500:21.737,lwr_k=600:22.7246,lwr_k=700:23.6765,lwr_k=800:24.4119,lwr_k=900:25.1991,lwr_k=1000:25.9425'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:25.9598,lwr_k=10:71.3729,lwr_k=20:758.8276,lwr_k=30:47.4749,lwr_k=40:26.1809,lwr_k=50:22.1715,lwr_k=100:17.9805,lwr_k=200:14.1771,lwr_k=300:14.5145,lwr_k=400:14.8704,lwr_k=500:15.1257,lwr_k=600:15.5021,lwr_k=700:15.7716,lwr_k=800:16.2088,lwr_k=900:16.398,lwr_k=1000:16.7053'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:39.7232,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:2.7938,lwr_k=40:4.9129,lwr_k=50:7.1501,lwr_k=100:11.7513,lwr_k=200:15.6276,lwr_k=300:18.5211,lwr_k=400:19.7887,lwr_k=500:20.8581,lwr_k=600:21.8514,lwr_k=700:22.6919,lwr_k=800:23.2506,lwr_k=900:23.8147,lwr_k=1000:24.3837'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:23.689,lwr_k=10:52.5281,lwr_k=20:380.4219,lwr_k=30:27.5614,lwr_k=40:18.5987,lwr_k=50:12.6754,lwr_k=100:12.1051,lwr_k=200:11.8645,lwr_k=300:12.4214,lwr_k=400:12.5243,lwr_k=500:12.8626,lwr_k=600:13.1563,lwr_k=700:13.6606,lwr_k=800:13.887,lwr_k=900:14.0856,lwr_k=1000:14.2453'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:33.491,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:2.727,lwr_k=40:4.5281,lwr_k=50:5.5925,lwr_k=100:9.3377,lwr_k=200:13.3089,lwr_k=300:15.56,lwr_k=400:16.8273,lwr_k=500:17.6951,lwr_k=600:18.5169,lwr_k=700:19.0465,lwr_k=800:19.5601,lwr_k=900:20.0603,lwr_k=1000:20.412'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:26.6292,lwr_k=10:60.9599,lwr_k=20:704.4629,lwr_k=30:48.5101,lwr_k=40:25.615,lwr_k=50:17.6677,lwr_k=100:13.7255,lwr_k=200:12.7757,lwr_k=300:13.2858,lwr_k=400:13.7252,lwr_k=500:14.1033,lwr_k=600:14.4793,lwr_k=700:14.6856,lwr_k=800:14.9673,lwr_k=900:15.3719,lwr_k=1000:15.5802'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:45.95,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:3.0109,lwr_k=40:5.1026,lwr_k=50:6.4797,lwr_k=100:10.6445,lwr_k=200:15.6949,lwr_k=300:19.3613,lwr_k=400:22.0337,lwr_k=500:24.4063,lwr_k=600:26.2994,lwr_k=700:27.9752,lwr_k=800:29.2803,lwr_k=900:30.434,lwr_k=1000:31.4035'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:95.864,lwr_k=10:79.7557,lwr_k=20:497.6746,lwr_k=30:56.279,lwr_k=40:53.5028,lwr_k=50:49.1943,lwr_k=100:50.2614,lwr_k=200:56.4072,lwr_k=300:60.901,lwr_k=400:66.1198,lwr_k=500:68.9111,lwr_k=600:71.7193,lwr_k=700:74.2519,lwr_k=800:76.185,lwr_k=900:78.2666,lwr_k=1000:80.0651'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:57.8765,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:3.2312,lwr_k=40:5.4596,lwr_k=50:8.061,lwr_k=100:14.0551,lwr_k=200:20.4642,lwr_k=300:24.3083,lwr_k=400:27.4381,lwr_k=500:30.1921,lwr_k=600:32.4593,lwr_k=700:34.0393,lwr_k=800:35.5192,lwr_k=900:37.0694,lwr_k=1000:38.4626'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:58.5919,lwr_k=10:66.42,lwr_k=20:578.7314,lwr_k=30:56.0918,lwr_k=40:40.827,lwr_k=50:30.1111,lwr_k=100:27.1772,lwr_k=200:26.0785,lwr_k=300:28.7377,lwr_k=400:30.0049,lwr_k=500:32.1489,lwr_k=600:33.6262,lwr_k=700:35.096,lwr_k=800:36.1516,lwr_k=900:37.4245,lwr_k=1000:38.8923'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_60'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.5902,lwr_k=10:2.1244,lwr_k=20:4.2177,lwr_k=30:4.7295,lwr_k=40:5.1561,lwr_k=50:5.3612,lwr_k=100:5.7165,lwr_k=200:5.9667,lwr_k=300:6.0913,lwr_k=400:6.1518,lwr_k=500:6.209,lwr_k=600:6.2301,lwr_k=700:6.2526,lwr_k=800:6.2705,lwr_k=900:6.3036,lwr_k=1000:6.3122'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.9385,lwr_k=10:37.6037,lwr_k=20:13.5595,lwr_k=30:10.9532,lwr_k=40:10.4,lwr_k=50:10.1055,lwr_k=100:9.2587,lwr_k=200:9.3136,lwr_k=300:9.3274,lwr_k=400:9.3754,lwr_k=500:9.4433,lwr_k=600:9.4544,lwr_k=700:9.5054,lwr_k=800:9.4802,lwr_k=900:9.5082,lwr_k=1000:9.539'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:18.2269,lwr_k=10:3.2466,lwr_k=20:6.9978,lwr_k=30:8.46,lwr_k=40:9.7282,lwr_k=50:10.4656,lwr_k=100:12.0523,lwr_k=200:13.2819,lwr_k=300:13.7473,lwr_k=400:14.1079,lwr_k=500:14.3827,lwr_k=600:14.599,lwr_k=700:14.7653,lwr_k=800:14.9278,lwr_k=900:15.151,lwr_k=1000:15.3155'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:13.92,lwr_k=10:49.4426,lwr_k=20:19.9101,lwr_k=30:14.0592,lwr_k=40:12.8795,lwr_k=50:12.7095,lwr_k=100:11.057,lwr_k=200:10.5049,lwr_k=300:10.6852,lwr_k=400:10.9202,lwr_k=500:11.0385,lwr_k=600:11.1754,lwr_k=700:11.2965,lwr_k=800:11.4045,lwr_k=900:11.5227,lwr_k=1000:11.5654'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:27.1373,lwr_k=10:3.9601,lwr_k=20:8.9393,lwr_k=30:10.9512,lwr_k=40:12.521,lwr_k=50:13.1967,lwr_k=100:15.5569,lwr_k=200:16.7945,lwr_k=300:17.478,lwr_k=400:17.9289,lwr_k=500:18.3619,lwr_k=600:18.7809,lwr_k=700:19.0731,lwr_k=800:19.3517,lwr_k=900:19.5862,lwr_k=1000:19.878'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:16.7326,lwr_k=10:56.9538,lwr_k=20:19.8846,lwr_k=30:16.2844,lwr_k=40:14.5572,lwr_k=50:13.8132,lwr_k=100:12.8428,lwr_k=200:12.5577,lwr_k=300:12.4703,lwr_k=400:12.5098,lwr_k=500:12.3076,lwr_k=600:12.2728,lwr_k=700:12.3059,lwr_k=800:12.2598,lwr_k=900:12.3199,lwr_k=1000:12.3443'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:17.9495,lwr_k=10:3.7135,lwr_k=20:8.1268,lwr_k=30:9.1802,lwr_k=40:10.2665,lwr_k=50:11.308,lwr_k=100:13.0351,lwr_k=200:13.8854,lwr_k=300:14.3391,lwr_k=400:14.5839,lwr_k=500:14.7013,lwr_k=600:14.9043,lwr_k=700:15.0592,lwr_k=800:15.2183,lwr_k=900:15.2957,lwr_k=1000:15.4291'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:15.1169,lwr_k=10:44.7215,lwr_k=20:16.569,lwr_k=30:14.69,lwr_k=40:14.4889,lwr_k=50:13.9575,lwr_k=100:13.0709,lwr_k=200:12.964,lwr_k=300:12.957,lwr_k=400:13.0199,lwr_k=500:13.1312,lwr_k=600:13.1742,lwr_k=700:13.2231,lwr_k=800:13.2141,lwr_k=900:13.2194,lwr_k=1000:13.2103'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.5109,lwr_k=10:2.2656,lwr_k=20:4.7194,lwr_k=30:5.3794,lwr_k=40:5.6851,lwr_k=50:5.9025,lwr_k=100:6.4164,lwr_k=200:6.5996,lwr_k=300:6.6512,lwr_k=400:6.6842,lwr_k=500:6.7323,lwr_k=600:6.7662,lwr_k=700:6.8271,lwr_k=800:6.8801,lwr_k=900:6.9113,lwr_k=1000:6.9465'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:27.1786,lwr_k=10:29.0832,lwr_k=20:17.3202,lwr_k=30:18.6464,lwr_k=40:22.1126,lwr_k=50:21.5545,lwr_k=100:21.9831,lwr_k=200:23.1612,lwr_k=300:23.4328,lwr_k=400:23.7153,lwr_k=500:23.915,lwr_k=600:24.1171,lwr_k=700:24.3775,lwr_k=800:24.6682,lwr_k=900:24.8934,lwr_k=1000:25.1345'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:19.0279,lwr_k=10:3.5292,lwr_k=20:7.55,lwr_k=30:8.7188,lwr_k=40:9.4703,lwr_k=50:10.1907,lwr_k=100:12.068,lwr_k=200:12.95,lwr_k=300:13.6082,lwr_k=400:14.0122,lwr_k=500:14.204,lwr_k=600:14.3776,lwr_k=700:14.528,lwr_k=800:14.7107,lwr_k=900:14.837,lwr_k=1000:14.9652'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:17.8449,lwr_k=10:50.0462,lwr_k=20:21.9465,lwr_k=30:17.2294,lwr_k=40:17.1536,lwr_k=50:16.1592,lwr_k=100:15.7584,lwr_k=200:15.6068,lwr_k=300:15.6566,lwr_k=400:15.6389,lwr_k=500:15.6226,lwr_k=600:15.5753,lwr_k=700:15.6255,lwr_k=800:15.5489,lwr_k=900:15.5614,lwr_k=1000:15.5159'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_61'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.3447,lwr_k=10:0.0,lwr_k=20:2.4033,lwr_k=30:3.7557,lwr_k=40:4.4172,lwr_k=50:4.8818,lwr_k=100:6.032,lwr_k=200:6.6566,lwr_k=300:7.0062,lwr_k=400:7.2125,lwr_k=500:7.4202,lwr_k=600:7.5332,lwr_k=700:7.6709,lwr_k=800:7.7848,lwr_k=900:7.8947,lwr_k=1000:7.9924'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.6648,lwr_k=10:304.7824,lwr_k=20:2736.3602,lwr_k=30:13.5682,lwr_k=40:30.7895,lwr_k=50:52.9183,lwr_k=100:9.2033,lwr_k=200:9.2607,lwr_k=300:9.6087,lwr_k=400:9.9422,lwr_k=500:10.1129,lwr_k=600:10.3346,lwr_k=700:10.5336,lwr_k=800:10.7483,lwr_k=900:10.8732,lwr_k=1000:11.045'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:15.6128,lwr_k=10:0.0319,lwr_k=20:2.7533,lwr_k=30:4.0653,lwr_k=40:5.3557,lwr_k=50:6.928,lwr_k=100:8.5443,lwr_k=200:9.8093,lwr_k=300:10.5736,lwr_k=400:10.865,lwr_k=500:11.2436,lwr_k=600:11.5275,lwr_k=700:11.6819,lwr_k=800:11.9094,lwr_k=900:12.1327,lwr_k=1000:12.3272'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:13.2888,lwr_k=10:722.7484,lwr_k=20:186.5875,lwr_k=30:82.3068,lwr_k=40:30.5366,lwr_k=50:30.048,lwr_k=100:9.4857,lwr_k=200:9.2886,lwr_k=300:9.1962,lwr_k=400:9.3121,lwr_k=500:9.3514,lwr_k=600:9.2887,lwr_k=700:9.4072,lwr_k=800:9.5345,lwr_k=900:9.5446,lwr_k=1000:9.5927'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:17.1914,lwr_k=10:0.0037,lwr_k=20:3.2988,lwr_k=30:5.5628,lwr_k=40:6.9523,lwr_k=50:7.9655,lwr_k=100:10.5401,lwr_k=200:11.6683,lwr_k=300:12.3792,lwr_k=400:12.7983,lwr_k=500:13.0304,lwr_k=600:13.2789,lwr_k=700:13.4126,lwr_k=800:13.6373,lwr_k=900:13.7883,lwr_k=1000:13.8845'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:11.2752,lwr_k=10:7788.9548,lwr_k=20:16.5561,lwr_k=30:11.5709,lwr_k=40:9.3204,lwr_k=50:8.9725,lwr_k=100:8.2621,lwr_k=200:8.0975,lwr_k=300:8.0843,lwr_k=400:8.3512,lwr_k=500:8.4621,lwr_k=600:8.5265,lwr_k=700:8.6535,lwr_k=800:8.7468,lwr_k=900:8.7687,lwr_k=1000:8.8555'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:14.4298,lwr_k=10:0.0271,lwr_k=20:3.343,lwr_k=30:5.2192,lwr_k=40:6.5565,lwr_k=50:6.9598,lwr_k=100:9.2581,lwr_k=200:10.8309,lwr_k=300:11.3866,lwr_k=400:11.7451,lwr_k=500:11.8978,lwr_k=600:12.0058,lwr_k=700:12.1914,lwr_k=800:12.3111,lwr_k=900:12.4414,lwr_k=1000:12.5663'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:11.4238,lwr_k=10:139097.2045,lwr_k=20:664.4668,lwr_k=30:917.3401,lwr_k=40:326.3804,lwr_k=50:357.1472,lwr_k=100:10.5961,lwr_k=200:10.0928,lwr_k=300:10.0782,lwr_k=400:10.0794,lwr_k=500:10.0113,lwr_k=600:10.0494,lwr_k=700:10.0287,lwr_k=800:10.1032,lwr_k=900:10.2124,lwr_k=1000:10.2564'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.3305,lwr_k=10:0.1134,lwr_k=20:3.0916,lwr_k=30:4.215,lwr_k=40:4.8473,lwr_k=50:5.3811,lwr_k=100:6.4717,lwr_k=200:7.2242,lwr_k=300:7.5434,lwr_k=400:7.6555,lwr_k=500:7.7917,lwr_k=600:7.8757,lwr_k=700:7.9251,lwr_k=800:8.0227,lwr_k=900:8.0657,lwr_k=1000:8.1143'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:26.236,lwr_k=10:21509.7849,lwr_k=20:621.9593,lwr_k=30:23.4479,lwr_k=40:20.8696,lwr_k=50:19.7347,lwr_k=100:21.6426,lwr_k=200:22.2418,lwr_k=300:23.0725,lwr_k=400:22.9636,lwr_k=500:23.07,lwr_k=600:23.7553,lwr_k=700:23.8768,lwr_k=800:23.9369,lwr_k=900:24.1541,lwr_k=1000:24.2681'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:13.3629,lwr_k=10:0.0658,lwr_k=20:2.9897,lwr_k=30:4.3331,lwr_k=40:5.0075,lwr_k=50:5.8931,lwr_k=100:8.4381,lwr_k=200:9.6139,lwr_k=300:9.9796,lwr_k=400:10.2041,lwr_k=500:10.2967,lwr_k=600:10.4252,lwr_k=700:10.4914,lwr_k=800:10.5927,lwr_k=900:10.7036,lwr_k=1000:10.7974'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:13.6912,lwr_k=10:20814.7623,lwr_k=20:16245.7088,lwr_k=30:813.7364,lwr_k=40:321.476,lwr_k=50:388.8076,lwr_k=100:48.8434,lwr_k=200:35.1386,lwr_k=300:21.5354,lwr_k=400:22.0974,lwr_k=500:15.7827,lwr_k=600:15.0612,lwr_k=700:14.6222,lwr_k=800:14.5852,lwr_k=900:14.8337,lwr_k=1000:14.6446'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_62'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:55.8132,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.475,lwr_k=40:4.2328,lwr_k=50:6.157,lwr_k=100:12.029,lwr_k=200:17.242,lwr_k=300:20.7447,lwr_k=400:23.6278,lwr_k=500:26.039,lwr_k=600:28.4459,lwr_k=700:30.0262,lwr_k=800:31.4211,lwr_k=900:32.5089,lwr_k=1000:33.5922'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:68.5185,lwr_k=10:64.2566,lwr_k=20:357.3893,lwr_k=30:155.1152,lwr_k=40:234.7606,lwr_k=50:225.7665,lwr_k=100:129.2766,lwr_k=200:47.5649,lwr_k=300:24.8004,lwr_k=400:27.1803,lwr_k=500:30.0556,lwr_k=600:33.0545,lwr_k=700:35.2767,lwr_k=800:36.5998,lwr_k=900:38.6499,lwr_k=1000:40.0063'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:67.7317,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.5871,lwr_k=40:4.7004,lwr_k=50:6.9542,lwr_k=100:12.6941,lwr_k=200:19.4802,lwr_k=300:24.4531,lwr_k=400:27.8736,lwr_k=500:31.1425,lwr_k=600:33.3299,lwr_k=700:35.611,lwr_k=800:37.3201,lwr_k=900:38.7259,lwr_k=1000:39.8161'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:56.9722,lwr_k=10:47.7936,lwr_k=20:383.6493,lwr_k=30:466.9932,lwr_k=40:229.3221,lwr_k=50:44.6086,lwr_k=100:26.4737,lwr_k=200:25.3667,lwr_k=300:27.4225,lwr_k=400:29.1767,lwr_k=500:32.1377,lwr_k=600:32.2799,lwr_k=700:33.3802,lwr_k=800:34.589,lwr_k=900:34.9713,lwr_k=1000:35.6744'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:71.5348,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.9361,lwr_k=40:5.3684,lwr_k=50:7.8385,lwr_k=100:13.2231,lwr_k=200:19.6474,lwr_k=300:23.88,lwr_k=400:26.706,lwr_k=500:29.4879,lwr_k=600:33.2182,lwr_k=700:35.6101,lwr_k=800:37.646,lwr_k=900:39.2181,lwr_k=1000:40.125'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:56.8513,lwr_k=10:55.3625,lwr_k=20:254.7833,lwr_k=30:842.2116,lwr_k=40:100.6742,lwr_k=50:34.7409,lwr_k=100:20.5619,lwr_k=200:22.513,lwr_k=300:26.2144,lwr_k=400:27.5713,lwr_k=500:28.033,lwr_k=600:29.3813,lwr_k=700:30.9256,lwr_k=800:32.0535,lwr_k=900:32.4421,lwr_k=1000:33.2871'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:70.4939,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.5948,lwr_k=40:5.0731,lwr_k=50:7.5127,lwr_k=100:13.8363,lwr_k=200:20.4255,lwr_k=300:24.7215,lwr_k=400:27.8885,lwr_k=500:31.4584,lwr_k=600:34.3298,lwr_k=700:36.1119,lwr_k=800:37.758,lwr_k=900:38.7723,lwr_k=1000:39.7956'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:62.6533,lwr_k=10:66.9438,lwr_k=20:184.4995,lwr_k=30:124.3411,lwr_k=40:39.3627,lwr_k=50:30.7446,lwr_k=100:23.2499,lwr_k=200:24.2174,lwr_k=300:24.7851,lwr_k=400:26.6722,lwr_k=500:28.4331,lwr_k=600:30.9831,lwr_k=700:32.6403,lwr_k=800:33.9741,lwr_k=900:35.3809,lwr_k=1000:36.2043'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:58.2975,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.533,lwr_k=40:4.6815,lwr_k=50:6.7335,lwr_k=100:12.094,lwr_k=200:17.4815,lwr_k=300:21.1856,lwr_k=400:23.6361,lwr_k=500:26.1248,lwr_k=600:28.3773,lwr_k=700:30.4031,lwr_k=800:31.6914,lwr_k=900:32.7811,lwr_k=1000:34.0482'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:94.4487,lwr_k=10:78.1198,lwr_k=20:321.3675,lwr_k=30:252.712,lwr_k=40:106.7395,lwr_k=50:70.053,lwr_k=100:43.6564,lwr_k=200:43.4659,lwr_k=300:46.3676,lwr_k=400:49.4111,lwr_k=500:51.1702,lwr_k=600:55.9601,lwr_k=700:58.6471,lwr_k=800:60.2675,lwr_k=900:61.4542,lwr_k=1000:62.3779'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:66.6308,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.5321,lwr_k=40:4.1477,lwr_k=50:6.4232,lwr_k=100:12.0507,lwr_k=200:18.1115,lwr_k=300:21.8196,lwr_k=400:24.9052,lwr_k=500:27.2167,lwr_k=600:28.9896,lwr_k=700:31.2174,lwr_k=800:33.4198,lwr_k=900:34.9526,lwr_k=1000:36.0719'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:68.4656,lwr_k=10:87.0291,lwr_k=20:204.5523,lwr_k=30:248.5149,lwr_k=40:104.4954,lwr_k=50:53.2295,lwr_k=100:38.3971,lwr_k=200:28.1101,lwr_k=300:28.9733,lwr_k=400:30.9321,lwr_k=500:31.6314,lwr_k=600:32.3843,lwr_k=700:33.3414,lwr_k=800:34.8592,lwr_k=900:35.8919,lwr_k=1000:36.9617'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_63'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:17.462,lwr_k=10:10.5626,lwr_k=20:10.7388,lwr_k=30:11.54,lwr_k=40:12.3762,lwr_k=50:12.7328,lwr_k=100:13.6449,lwr_k=200:14.1704,lwr_k=300:14.4204,lwr_k=400:14.677,lwr_k=500:14.8879,lwr_k=600:15.0837,lwr_k=700:15.3865,lwr_k=800:15.6385,lwr_k=900:15.8732,lwr_k=1000:16.0244'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:24.872,lwr_k=10:451018305.8361,lwr_k=20:1395.8831,lwr_k=30:1622.6055,lwr_k=40:38.4289,lwr_k=50:23.8822,lwr_k=100:5184698.8386,lwr_k=200:21.027,lwr_k=300:21.7937,lwr_k=400:22.2821,lwr_k=500:22.9097,lwr_k=600:23.5247,lwr_k=700:23.976,lwr_k=800:24.275,lwr_k=900:24.5646,lwr_k=1000:25.0822'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:181.7912,lwr_k=10:71.4055,lwr_k=20:72.4942,lwr_k=30:78.1636,lwr_k=40:83.2094,lwr_k=50:87.0215,lwr_k=100:96.9596,lwr_k=200:110.5066,lwr_k=300:115.1598,lwr_k=400:119.9525,lwr_k=500:122.7861,lwr_k=600:125.7385,lwr_k=700:127.7511,lwr_k=800:129.8844,lwr_k=900:131.6645,lwr_k=1000:133.2037'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:156.3493,lwr_k=10:6427473.0236,lwr_k=20:443914.0117,lwr_k=30:100.7625,lwr_k=40:101.1295,lwr_k=50:95.7453,lwr_k=100:100.0582,lwr_k=200:102.1727,lwr_k=300:105.36,lwr_k=400:108.1623,lwr_k=500:110.9294,lwr_k=600:112.3618,lwr_k=700:113.9293,lwr_k=800:116.2009,lwr_k=900:117.6252,lwr_k=1000:118.5452'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:42.6332,lwr_k=10:36.6863,lwr_k=20:25.8578,lwr_k=30:27.4278,lwr_k=40:26.3769,lwr_k=50:26.495,lwr_k=100:28.2406,lwr_k=200:29.6399,lwr_k=300:30.6136,lwr_k=400:31.369,lwr_k=500:31.8971,lwr_k=600:32.2359,lwr_k=700:33.4426,lwr_k=800:33.9113,lwr_k=900:34.4449,lwr_k=1000:35.2267'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:34.671,lwr_k=10:83840776.972,lwr_k=20:4849643.4962,lwr_k=30:3531041.5124,lwr_k=40:27.8788,lwr_k=50:26.1914,lwr_k=100:26.2066,lwr_k=200:25.439,lwr_k=300:25.5596,lwr_k=400:25.846,lwr_k=500:26.2162,lwr_k=600:27.567,lwr_k=700:28.272,lwr_k=800:26.6911,lwr_k=900:26.8957,lwr_k=1000:26.9715'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:21.7268,lwr_k=10:10.8258,lwr_k=20:12.2773,lwr_k=30:12.484,lwr_k=40:13.1867,lwr_k=50:13.6855,lwr_k=100:14.8091,lwr_k=200:15.9705,lwr_k=300:16.5756,lwr_k=400:17.56,lwr_k=500:17.3567,lwr_k=600:18.4757,lwr_k=700:18.0252,lwr_k=800:18.0235,lwr_k=900:18.3291,lwr_k=1000:22.8577'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:17.0163,lwr_k=10:224095048.3279,lwr_k=20:701959.8985,lwr_k=30:209760.4469,lwr_k=40:14980.1435,lwr_k=50:360.4339,lwr_k=100:14.4234,lwr_k=200:13.5477,lwr_k=300:13.6185,lwr_k=400:13.727,lwr_k=500:13.9231,lwr_k=600:13.9978,lwr_k=700:13.9099,lwr_k=800:14.0594,lwr_k=900:14.066,lwr_k=1000:14.117'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:13.1154,lwr_k=10:5.8813,lwr_k=20:8.0155,lwr_k=30:8.9489,lwr_k=40:9.3435,lwr_k=50:9.7191,lwr_k=100:10.5154,lwr_k=200:11.0037,lwr_k=300:11.2092,lwr_k=400:11.332,lwr_k=500:11.4515,lwr_k=600:11.5682,lwr_k=700:11.6279,lwr_k=800:11.7085,lwr_k=900:11.7805,lwr_k=1000:11.8525'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:38.5897,lwr_k=10:5467240.0563,lwr_k=20:88.9174,lwr_k=30:26.4044,lwr_k=40:26.805,lwr_k=50:27.4417,lwr_k=100:28.7765,lwr_k=200:31.1601,lwr_k=300:32.9595,lwr_k=400:33.9459,lwr_k=500:34.8913,lwr_k=600:35.572,lwr_k=700:35.9402,lwr_k=800:36.2307,lwr_k=900:36.3799,lwr_k=1000:36.5494'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:61.2246,lwr_k=10:55.1272,lwr_k=20:184384604677132.47,lwr_k=30:60.0843,lwr_k=40:59.222,lwr_k=50:57.8615,lwr_k=100:55.339,lwr_k=200:57.3826,lwr_k=300:57.8034,lwr_k=400:58.0655,lwr_k=500:58.157,lwr_k=600:58.3071,lwr_k=700:58.4456,lwr_k=800:58.5828,lwr_k=900:58.6792,lwr_k=1000:58.7655'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:55.5407,lwr_k=10:14244042.1959,lwr_k=20:250609199785530.88,lwr_k=30:59.0406,lwr_k=40:58.04,lwr_k=50:56.3487,lwr_k=100:51.4033,lwr_k=200:52.2113,lwr_k=300:52.5137,lwr_k=400:52.7579,lwr_k=500:52.7146,lwr_k=600:52.9222,lwr_k=700:53.1332,lwr_k=800:53.3145,lwr_k=900:53.4285,lwr_k=1000:53.5556'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_64'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.4428,lwr_k=10:5.6052,lwr_k=20:6.1049,lwr_k=30:6.3465,lwr_k=40:6.4292,lwr_k=50:6.5027,lwr_k=100:6.6777,lwr_k=200:6.9139,lwr_k=300:6.9518,lwr_k=400:6.9728,lwr_k=500:7.004,lwr_k=600:7.027,lwr_k=700:7.0287,lwr_k=800:7.0421,lwr_k=900:7.0539,lwr_k=1000:7.0722'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.8128,lwr_k=10:13.5337,lwr_k=20:11.2243,lwr_k=30:11.9445,lwr_k=40:11.4988,lwr_k=50:11.6578,lwr_k=100:11.739,lwr_k=200:11.6256,lwr_k=300:11.4504,lwr_k=400:11.4009,lwr_k=500:11.4844,lwr_k=600:11.4424,lwr_k=700:11.4729,lwr_k=800:11.4926,lwr_k=900:11.4759,lwr_k=1000:11.4907'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.6933,lwr_k=10:6.4865,lwr_k=20:6.9688,lwr_k=30:7.1847,lwr_k=40:7.3869,lwr_k=50:7.5299,lwr_k=100:7.6189,lwr_k=200:7.7382,lwr_k=300:7.7627,lwr_k=400:7.8676,lwr_k=500:7.9027,lwr_k=600:7.9643,lwr_k=700:8.0117,lwr_k=800:8.0352,lwr_k=900:8.052,lwr_k=1000:8.0922'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.9848,lwr_k=10:9.9143,lwr_k=20:8.5785,lwr_k=30:8.3341,lwr_k=40:8.3875,lwr_k=50:8.3104,lwr_k=100:8.1907,lwr_k=200:8.231,lwr_k=300:8.2281,lwr_k=400:8.3727,lwr_k=500:8.3799,lwr_k=600:8.3989,lwr_k=700:8.4632,lwr_k=800:8.4894,lwr_k=900:8.4915,lwr_k=1000:8.5207'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:13.4443,lwr_k=10:7.7759,lwr_k=20:8.7182,lwr_k=30:9.2534,lwr_k=40:9.844,lwr_k=50:10.4919,lwr_k=100:11.7656,lwr_k=200:12.1884,lwr_k=300:12.3515,lwr_k=400:12.5935,lwr_k=500:12.6868,lwr_k=600:12.7242,lwr_k=700:12.7603,lwr_k=800:12.8188,lwr_k=900:12.8757,lwr_k=1000:12.9132'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.7222,lwr_k=10:12.8491,lwr_k=20:10.6474,lwr_k=30:9.9098,lwr_k=40:9.5017,lwr_k=50:9.021,lwr_k=100:8.681,lwr_k=200:9.1411,lwr_k=300:8.6982,lwr_k=400:8.7343,lwr_k=500:8.7916,lwr_k=600:8.8973,lwr_k=700:9.0834,lwr_k=800:9.189,lwr_k=900:9.2513,lwr_k=1000:9.285'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.9598,lwr_k=10:6.2841,lwr_k=20:7.2312,lwr_k=30:7.6042,lwr_k=40:7.8722,lwr_k=50:8.0906,lwr_k=100:8.7261,lwr_k=200:9.0704,lwr_k=300:9.3218,lwr_k=400:9.4311,lwr_k=500:9.4898,lwr_k=600:9.4878,lwr_k=700:9.5148,lwr_k=800:9.5394,lwr_k=900:9.5643,lwr_k=1000:9.5694'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.5061,lwr_k=10:12.8663,lwr_k=20:11.7442,lwr_k=30:11.8775,lwr_k=40:10.5613,lwr_k=50:9.9163,lwr_k=100:9.9183,lwr_k=200:9.8715,lwr_k=300:9.7758,lwr_k=400:9.678,lwr_k=500:9.6306,lwr_k=600:9.6162,lwr_k=700:9.6195,lwr_k=800:9.6214,lwr_k=900:9.6272,lwr_k=1000:9.6344'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.0827,lwr_k=10:4.5838,lwr_k=20:5.7118,lwr_k=30:6.0678,lwr_k=40:6.2963,lwr_k=50:6.4001,lwr_k=100:6.6482,lwr_k=200:6.7824,lwr_k=300:6.7931,lwr_k=400:6.8146,lwr_k=500:6.833,lwr_k=600:6.854,lwr_k=700:6.8747,lwr_k=800:6.8886,lwr_k=900:6.9038,lwr_k=1000:6.9138'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:21.5869,lwr_k=10:26.0171,lwr_k=20:19.2507,lwr_k=30:18.0676,lwr_k=40:19.6671,lwr_k=50:19.3085,lwr_k=100:20.7605,lwr_k=200:23.3124,lwr_k=300:21.1572,lwr_k=400:21.2487,lwr_k=500:21.2607,lwr_k=600:21.2748,lwr_k=700:21.2077,lwr_k=800:21.1918,lwr_k=900:21.1713,lwr_k=1000:21.1904'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:12.9561,lwr_k=10:8.6001,lwr_k=20:9.687,lwr_k=30:9.9278,lwr_k=40:10.1361,lwr_k=50:10.4492,lwr_k=100:11.3211,lwr_k=200:11.8732,lwr_k=300:12.0343,lwr_k=400:12.0379,lwr_k=500:12.1038,lwr_k=600:12.133,lwr_k=700:12.1608,lwr_k=800:12.2165,lwr_k=900:12.2315,lwr_k=1000:12.2479'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.6479,lwr_k=10:13.7017,lwr_k=20:12.7065,lwr_k=30:12.4771,lwr_k=40:12.5143,lwr_k=50:12.3808,lwr_k=100:12.275,lwr_k=200:12.2638,lwr_k=300:12.4844,lwr_k=400:12.4956,lwr_k=500:12.5031,lwr_k=600:12.4916,lwr_k=700:12.4665,lwr_k=800:12.4665,lwr_k=900:12.4623,lwr_k=1000:12.4407'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_65'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.5663,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0228,lwr_k=40:0.163,lwr_k=50:1.4112,lwr_k=100:3.6301,lwr_k=200:4.8796,lwr_k=300:5.3191,lwr_k=400:5.5049,lwr_k=500:5.6733,lwr_k=600:5.7342,lwr_k=700:5.8179,lwr_k=800:5.8668,lwr_k=900:5.91,lwr_k=1000:5.9679'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.7116,lwr_k=10:51.1806,lwr_k=20:85.7227,lwr_k=30:588.7117,lwr_k=40:38798.798,lwr_k=50:165.2959,lwr_k=100:10.9427,lwr_k=200:8.6713,lwr_k=300:8.3107,lwr_k=400:8.2186,lwr_k=500:8.0988,lwr_k=600:8.1129,lwr_k=700:8.0979,lwr_k=800:8.0629,lwr_k=900:8.0343,lwr_k=1000:7.9721'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.1917,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0081,lwr_k=40:0.2485,lwr_k=50:1.2518,lwr_k=100:3.1691,lwr_k=200:4.6152,lwr_k=300:5.0001,lwr_k=400:5.26,lwr_k=500:5.3469,lwr_k=600:5.4332,lwr_k=700:5.4713,lwr_k=800:5.5456,lwr_k=900:5.6007,lwr_k=1000:5.6512'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.7892,lwr_k=10:46.2546,lwr_k=20:118.9638,lwr_k=30:593.7541,lwr_k=40:4044.3835,lwr_k=50:4608.6563,lwr_k=100:12.6758,lwr_k=200:9.1517,lwr_k=300:8.3398,lwr_k=400:8.3323,lwr_k=500:8.0384,lwr_k=600:7.7895,lwr_k=700:7.5236,lwr_k=800:7.488,lwr_k=900:7.211,lwr_k=1000:7.1859'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.0709,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.027,lwr_k=40:0.1462,lwr_k=50:1.4462,lwr_k=100:3.6654,lwr_k=200:5.0774,lwr_k=300:5.454,lwr_k=400:5.8152,lwr_k=500:6.0094,lwr_k=600:6.1617,lwr_k=700:6.2921,lwr_k=800:6.4095,lwr_k=900:6.4873,lwr_k=1000:6.5844'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.439,lwr_k=10:24.2491,lwr_k=20:42.2355,lwr_k=30:148.9525,lwr_k=40:417.6092,lwr_k=50:193.3237,lwr_k=100:9.2699,lwr_k=200:6.319,lwr_k=300:5.9589,lwr_k=400:5.8912,lwr_k=500:5.8926,lwr_k=600:5.8119,lwr_k=700:5.7598,lwr_k=800:5.7795,lwr_k=900:5.7882,lwr_k=1000:5.7761'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.7936,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0041,lwr_k=40:0.0495,lwr_k=50:0.9899,lwr_k=100:3.4117,lwr_k=200:4.8788,lwr_k=300:5.4151,lwr_k=400:5.8691,lwr_k=500:6.0961,lwr_k=600:6.2795,lwr_k=700:6.4231,lwr_k=800:6.5151,lwr_k=900:6.5814,lwr_k=1000:6.734'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.7406,lwr_k=10:30.5611,lwr_k=20:58.1283,lwr_k=30:110.6636,lwr_k=40:2309.8985,lwr_k=50:428.4432,lwr_k=100:11.9361,lwr_k=200:10.2423,lwr_k=300:8.2972,lwr_k=400:8.3779,lwr_k=500:8.2822,lwr_k=600:8.2359,lwr_k=700:8.1791,lwr_k=800:8.2245,lwr_k=900:8.2345,lwr_k=1000:8.2333'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.8462,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.026,lwr_k=50:0.6701,lwr_k=100:2.9746,lwr_k=200:4.1394,lwr_k=300:4.4982,lwr_k=400:4.6783,lwr_k=500:4.8224,lwr_k=600:4.9347,lwr_k=700:4.9894,lwr_k=800:5.0498,lwr_k=900:5.1007,lwr_k=1000:5.1544'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:21.4542,lwr_k=10:44.9788,lwr_k=20:47.4956,lwr_k=30:97.6213,lwr_k=40:254.3656,lwr_k=50:80.1703,lwr_k=100:15.9714,lwr_k=200:18.2129,lwr_k=300:17.7982,lwr_k=400:17.775,lwr_k=500:17.8148,lwr_k=600:18.1032,lwr_k=700:18.3079,lwr_k=800:18.4022,lwr_k=900:18.4161,lwr_k=1000:18.5668'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:7.7488,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0017,lwr_k=40:0.1148,lwr_k=50:0.7502,lwr_k=100:3.1122,lwr_k=200:4.4684,lwr_k=300:5.0649,lwr_k=400:5.3851,lwr_k=500:5.5186,lwr_k=600:5.69,lwr_k=700:5.7873,lwr_k=800:5.8597,lwr_k=900:5.9466,lwr_k=1000:5.9808'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.8912,lwr_k=10:29.3439,lwr_k=20:146.7336,lwr_k=30:4798.3833,lwr_k=40:13168.7095,lwr_k=50:533.2358,lwr_k=100:9.5801,lwr_k=200:7.2858,lwr_k=300:7.3427,lwr_k=400:7.2183,lwr_k=500:7.3245,lwr_k=600:7.3921,lwr_k=700:7.3695,lwr_k=800:7.4052,lwr_k=900:7.4528,lwr_k=1000:7.5062'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_66'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.8334,lwr_k=10:0.0,lwr_k=20:2.2671,lwr_k=30:3.1082,lwr_k=40:3.5493,lwr_k=50:3.8417,lwr_k=100:4.5388,lwr_k=200:4.83,lwr_k=300:5.0834,lwr_k=400:5.2623,lwr_k=500:5.4445,lwr_k=600:5.5586,lwr_k=700:5.6689,lwr_k=800:5.7473,lwr_k=900:5.8144,lwr_k=1000:5.8604'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.0505,lwr_k=10:1895.6677,lwr_k=20:12.4838,lwr_k=30:9.935,lwr_k=40:9.3536,lwr_k=50:8.6424,lwr_k=100:8.5034,lwr_k=200:8.7443,lwr_k=300:9.0457,lwr_k=400:9.2124,lwr_k=500:9.3725,lwr_k=600:9.505,lwr_k=700:9.6197,lwr_k=800:9.6855,lwr_k=900:9.7535,lwr_k=1000:9.8352'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.7748,lwr_k=10:0.0006,lwr_k=20:2.5361,lwr_k=30:3.2178,lwr_k=40:3.5667,lwr_k=50:3.8287,lwr_k=100:4.6676,lwr_k=200:5.6954,lwr_k=300:6.1445,lwr_k=400:6.5559,lwr_k=500:6.6982,lwr_k=600:6.7533,lwr_k=700:6.8859,lwr_k=800:6.9875,lwr_k=900:7.0634,lwr_k=1000:7.1347'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.0077,lwr_k=10:8565.0731,lwr_k=20:10.6004,lwr_k=30:7.1622,lwr_k=40:6.1894,lwr_k=50:5.8554,lwr_k=100:5.6299,lwr_k=200:5.6916,lwr_k=300:5.7227,lwr_k=400:5.7441,lwr_k=500:5.8223,lwr_k=600:5.8751,lwr_k=700:5.9038,lwr_k=800:5.9366,lwr_k=900:5.9688,lwr_k=1000:6.0267'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.6335,lwr_k=10:0.0,lwr_k=20:2.7571,lwr_k=30:3.6897,lwr_k=40:4.1483,lwr_k=50:4.4243,lwr_k=100:5.0319,lwr_k=200:5.5678,lwr_k=300:5.8762,lwr_k=400:6.0968,lwr_k=500:6.2943,lwr_k=600:6.5417,lwr_k=700:6.7866,lwr_k=800:7.155,lwr_k=900:7.3867,lwr_k=1000:7.6294'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.9337,lwr_k=10:6447.0394,lwr_k=20:7.8719,lwr_k=30:6.8118,lwr_k=40:6.1759,lwr_k=50:5.7642,lwr_k=100:5.4751,lwr_k=200:5.5759,lwr_k=300:5.6437,lwr_k=400:5.629,lwr_k=500:5.645,lwr_k=600:5.6532,lwr_k=700:5.6688,lwr_k=800:5.6749,lwr_k=900:5.6825,lwr_k=1000:5.6935'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.4451,lwr_k=10:0.0001,lwr_k=20:2.648,lwr_k=30:3.6652,lwr_k=40:4.2105,lwr_k=50:4.8936,lwr_k=100:6.4236,lwr_k=200:7.4605,lwr_k=300:7.6622,lwr_k=400:7.9186,lwr_k=500:8.0484,lwr_k=600:8.168,lwr_k=700:8.3148,lwr_k=800:8.4685,lwr_k=900:8.5749,lwr_k=1000:8.6734'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.6962,lwr_k=10:12470.9711,lwr_k=20:9.7236,lwr_k=30:7.5243,lwr_k=40:6.8129,lwr_k=50:6.6848,lwr_k=100:6.8574,lwr_k=200:7.0199,lwr_k=300:6.9569,lwr_k=400:7.0248,lwr_k=500:7.1107,lwr_k=600:7.2016,lwr_k=700:7.1983,lwr_k=800:7.1709,lwr_k=900:7.2383,lwr_k=1000:7.285'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.2659,lwr_k=10:0.0,lwr_k=20:2.1049,lwr_k=30:2.6498,lwr_k=40:3.0548,lwr_k=50:3.2354,lwr_k=100:3.6069,lwr_k=200:3.8307,lwr_k=300:3.9268,lwr_k=400:3.9846,lwr_k=500:4.0119,lwr_k=600:4.0514,lwr_k=700:4.0679,lwr_k=800:4.0976,lwr_k=900:4.1101,lwr_k=1000:4.1196'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:16.627,lwr_k=10:4254.9818,lwr_k=20:15.237,lwr_k=30:14.7887,lwr_k=40:14.1556,lwr_k=50:11.4519,lwr_k=100:13.0527,lwr_k=200:14.9299,lwr_k=300:15.1929,lwr_k=400:15.5285,lwr_k=500:15.7847,lwr_k=600:15.8505,lwr_k=700:16.0114,lwr_k=800:16.1925,lwr_k=900:16.2927,lwr_k=1000:16.378'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:6.2916,lwr_k=10:0.0001,lwr_k=20:2.2258,lwr_k=30:2.9128,lwr_k=40:3.2405,lwr_k=50:3.4793,lwr_k=100:4.0501,lwr_k=200:4.8312,lwr_k=300:4.9624,lwr_k=400:5.0544,lwr_k=500:5.1167,lwr_k=600:5.1545,lwr_k=700:5.1821,lwr_k=800:5.2187,lwr_k=900:5.2553,lwr_k=1000:5.2958'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.7148,lwr_k=10:1062.3211,lwr_k=20:12.3896,lwr_k=30:7.492,lwr_k=40:6.4222,lwr_k=50:6.2162,lwr_k=100:6.0152,lwr_k=200:6.1913,lwr_k=300:6.3131,lwr_k=400:6.3889,lwr_k=500:6.4118,lwr_k=600:6.4481,lwr_k=700:6.4789,lwr_k=800:6.5278,lwr_k=900:6.5494,lwr_k=1000:6.5764'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_67'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.6795,lwr_k=10:0.0,lwr_k=20:2.9475,lwr_k=30:4.8876,lwr_k=40:5.7308,lwr_k=50:6.3524,lwr_k=100:7.5594,lwr_k=200:8.4998,lwr_k=300:8.8315,lwr_k=400:9.0326,lwr_k=500:9.1787,lwr_k=600:9.2655,lwr_k=700:9.3343,lwr_k=800:9.3981,lwr_k=900:9.4669,lwr_k=1000:9.5252'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:13.9081,lwr_k=10:145.9789,lwr_k=20:31.0456,lwr_k=30:18.0338,lwr_k=40:15.4724,lwr_k=50:14.1081,lwr_k=100:11.9769,lwr_k=200:11.6566,lwr_k=300:11.799,lwr_k=400:11.8223,lwr_k=500:11.917,lwr_k=600:12.0468,lwr_k=700:12.1338,lwr_k=800:12.2427,lwr_k=900:12.3196,lwr_k=1000:12.3655'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.3953,lwr_k=10:0.0,lwr_k=20:2.2394,lwr_k=30:4.2471,lwr_k=40:5.4969,lwr_k=50:6.176,lwr_k=100:8.0039,lwr_k=200:8.8106,lwr_k=300:9.203,lwr_k=400:9.3462,lwr_k=500:9.4658,lwr_k=600:9.5526,lwr_k=700:9.6104,lwr_k=800:9.7145,lwr_k=900:9.7594,lwr_k=1000:9.8098'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.5298,lwr_k=10:72.2448,lwr_k=20:29.0293,lwr_k=30:15.7317,lwr_k=40:11.6122,lwr_k=50:10.6784,lwr_k=100:8.8349,lwr_k=200:8.3344,lwr_k=300:8.2809,lwr_k=400:8.3634,lwr_k=500:8.2937,lwr_k=600:8.4,lwr_k=700:8.4698,lwr_k=800:8.4868,lwr_k=900:8.4589,lwr_k=1000:8.4717'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.0874,lwr_k=10:0.0,lwr_k=20:2.3503,lwr_k=30:3.8887,lwr_k=40:4.8567,lwr_k=50:5.4859,lwr_k=100:7.1921,lwr_k=200:8.3627,lwr_k=300:8.7838,lwr_k=400:9.0549,lwr_k=500:9.2297,lwr_k=600:9.377,lwr_k=700:9.4182,lwr_k=800:9.4445,lwr_k=900:9.496,lwr_k=1000:9.5206'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.5493,lwr_k=10:57.2569,lwr_k=20:19.394,lwr_k=30:10.7692,lwr_k=40:9.5826,lwr_k=50:8.3884,lwr_k=100:7.7613,lwr_k=200:7.4989,lwr_k=300:7.3856,lwr_k=400:7.3182,lwr_k=500:7.2761,lwr_k=600:7.2094,lwr_k=700:7.1711,lwr_k=800:7.1503,lwr_k=900:7.1368,lwr_k=1000:7.1333'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:19.1941,lwr_k=10:0.0,lwr_k=20:2.0906,lwr_k=30:5.1544,lwr_k=40:6.53,lwr_k=50:7.3119,lwr_k=100:10.7802,lwr_k=200:12.7481,lwr_k=300:13.818,lwr_k=400:14.4769,lwr_k=500:14.763,lwr_k=600:15.1344,lwr_k=700:15.4321,lwr_k=800:15.7726,lwr_k=900:15.9608,lwr_k=1000:16.0536'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:14.4476,lwr_k=10:73.1607,lwr_k=20:75.7187,lwr_k=30:25.9736,lwr_k=40:19.898,lwr_k=50:16.7707,lwr_k=100:14.0854,lwr_k=200:12.708,lwr_k=300:12.4075,lwr_k=400:12.468,lwr_k=500:12.3988,lwr_k=600:12.5839,lwr_k=700:12.5203,lwr_k=800:12.5945,lwr_k=900:12.7203,lwr_k=1000:12.6771'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:3.0148,lwr_k=10:0.0,lwr_k=20:1.0847,lwr_k=30:1.7955,lwr_k=40:2.0745,lwr_k=50:2.2513,lwr_k=100:2.5687,lwr_k=200:2.7269,lwr_k=300:2.7994,lwr_k=400:2.841,lwr_k=500:2.8501,lwr_k=600:2.8705,lwr_k=700:2.8717,lwr_k=800:2.8814,lwr_k=900:2.8935,lwr_k=1000:2.9039'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:16.6084,lwr_k=10:46.5964,lwr_k=20:33.5319,lwr_k=30:18.6113,lwr_k=40:17.7816,lwr_k=50:16.9616,lwr_k=100:16.314,lwr_k=200:16.1044,lwr_k=300:15.963,lwr_k=400:16.0022,lwr_k=500:15.9696,lwr_k=600:16.085,lwr_k=700:16.0736,lwr_k=800:16.0789,lwr_k=900:16.0829,lwr_k=1000:16.0707'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:3.3547,lwr_k=10:0.0,lwr_k=20:1.2094,lwr_k=30:2.0262,lwr_k=40:2.3437,lwr_k=50:2.5469,lwr_k=100:2.9178,lwr_k=200:3.0948,lwr_k=300:3.1691,lwr_k=400:3.206,lwr_k=500:3.2197,lwr_k=600:3.2396,lwr_k=700:3.2576,lwr_k=800:3.2662,lwr_k=900:3.2817,lwr_k=1000:3.2884'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.8801,lwr_k=10:35.9774,lwr_k=20:17.7232,lwr_k=30:9.5829,lwr_k=40:9.0053,lwr_k=50:9.1899,lwr_k=100:8.8614,lwr_k=200:8.6399,lwr_k=300:8.6563,lwr_k=400:8.62,lwr_k=500:8.6593,lwr_k=600:8.6909,lwr_k=700:8.6785,lwr_k=800:8.6834,lwr_k=900:8.6851,lwr_k=1000:8.6915'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_68'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.1205,lwr_k=10:0.0004,lwr_k=20:0.4055,lwr_k=30:1.9729,lwr_k=40:3.3533,lwr_k=50:13.48,lwr_k=100:9.7981,lwr_k=200:11.5591,lwr_k=300:13.8817,lwr_k=400:13.2085,lwr_k=500:12.009,lwr_k=600:12.0521,lwr_k=700:12.2541,lwr_k=800:11.2629,lwr_k=900:10.7669,lwr_k=1000:11.6135'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:14.575,lwr_k=10:188.3788,lwr_k=20:9721.2115,lwr_k=30:4378.7218,lwr_k=40:756.4001,lwr_k=50:2886.5624,lwr_k=100:98.6395,lwr_k=200:19.9099,lwr_k=300:19.2996,lwr_k=400:20.3243,lwr_k=500:19.7529,lwr_k=600:17.9438,lwr_k=700:18.5202,lwr_k=800:17.2991,lwr_k=900:14.6768,lwr_k=1000:14.7094'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.9316,lwr_k=10:0.2319,lwr_k=20:21.4372,lwr_k=30:17.2247,lwr_k=40:11.7485,lwr_k=50:11.2623,lwr_k=100:8.2814,lwr_k=200:7.6043,lwr_k=300:7.6648,lwr_k=400:7.7733,lwr_k=500:7.9448,lwr_k=600:8.0391,lwr_k=700:8.2577,lwr_k=800:8.1791,lwr_k=900:8.1004,lwr_k=1000:8.2702'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.3351,lwr_k=10:641.4994,lwr_k=20:5927.8433,lwr_k=30:76.4697,lwr_k=40:66.1721,lwr_k=50:23.8416,lwr_k=100:11.7293,lwr_k=200:8.6544,lwr_k=300:7.9676,lwr_k=400:7.7234,lwr_k=500:7.7313,lwr_k=600:7.6337,lwr_k=700:7.7632,lwr_k=800:7.9603,lwr_k=900:7.8764,lwr_k=1000:8.0149'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.2038,lwr_k=10:0.0006,lwr_k=20:0.83,lwr_k=30:2.4366,lwr_k=40:3.2805,lwr_k=50:4.183,lwr_k=100:5.3117,lwr_k=200:6.0288,lwr_k=300:6.2544,lwr_k=400:6.4996,lwr_k=500:6.64,lwr_k=600:6.6929,lwr_k=700:6.7168,lwr_k=800:6.8244,lwr_k=900:6.9216,lwr_k=1000:6.9477'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.5816,lwr_k=10:619.6767,lwr_k=20:2868371.3801,lwr_k=30:34645960.9929,lwr_k=40:26304724.1276,lwr_k=50:3221679.1715,lwr_k=100:149432.2864,lwr_k=200:50705.9913,lwr_k=300:2812.1157,lwr_k=400:7.5886,lwr_k=500:7.2474,lwr_k=600:1026.9848,lwr_k=700:6.8918,lwr_k=800:6.9753,lwr_k=900:6.5755,lwr_k=1000:621.2964'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.0656,lwr_k=10:0.0004,lwr_k=20:0.9046,lwr_k=30:2.4106,lwr_k=40:3.4824,lwr_k=50:5.9469,lwr_k=100:7.3045,lwr_k=200:7.5378,lwr_k=300:8.2744,lwr_k=400:9.3545,lwr_k=500:8.1546,lwr_k=600:7.5968,lwr_k=700:7.6099,lwr_k=800:8.0523,lwr_k=900:8.1785,lwr_k=1000:8.5154'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:11.4676,lwr_k=10:637.4259,lwr_k=20:507.4852,lwr_k=30:2033.26,lwr_k=40:443.4281,lwr_k=50:204.6699,lwr_k=100:16.8847,lwr_k=200:13.3669,lwr_k=300:11.8953,lwr_k=400:13.21,lwr_k=500:12.8845,lwr_k=600:13.8976,lwr_k=700:13.9579,lwr_k=800:13.672,lwr_k=900:13.8972,lwr_k=1000:13.5326'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.701,lwr_k=10:0.0021,lwr_k=20:6.9034,lwr_k=30:11.8601,lwr_k=40:9.6299,lwr_k=50:11.562,lwr_k=100:8.7835,lwr_k=200:8.4789,lwr_k=300:7.6922,lwr_k=400:7.4218,lwr_k=500:7.8246,lwr_k=600:7.983,lwr_k=700:7.8791,lwr_k=800:7.6443,lwr_k=900:7.7836,lwr_k=1000:8.24'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:82.6346,lwr_k=10:398.3176,lwr_k=20:1501.5461,lwr_k=30:127139669.5673,lwr_k=40:7454334.326,lwr_k=50:17382667.0737,lwr_k=100:561263.0951,lwr_k=200:111.8012,lwr_k=300:611942.2692,lwr_k=400:96.2005,lwr_k=500:92.5825,lwr_k=600:67.058,lwr_k=700:76.1559,lwr_k=800:77.1025,lwr_k=900:89.8286,lwr_k=1000:91.2648'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:8.4078,lwr_k=10:0.1376,lwr_k=20:7.4671,lwr_k=30:6.8902,lwr_k=40:6.4647,lwr_k=50:8.0529,lwr_k=100:6.8061,lwr_k=200:7.7408,lwr_k=300:8.9938,lwr_k=400:9.1668,lwr_k=500:9.9883,lwr_k=600:9.6736,lwr_k=700:9.1008,lwr_k=800:9.0454,lwr_k=900:10.4085,lwr_k=1000:9.4865'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.3206,lwr_k=10:57935734.5636,lwr_k=20:90916.5323,lwr_k=30:366230.0566,lwr_k=40:476378.8997,lwr_k=50:3161.3867,lwr_k=100:12.901,lwr_k=200:11.3299,lwr_k=300:3674.9272,lwr_k=400:266.9183,lwr_k=500:12.0837,lwr_k=600:1286.5169,lwr_k=700:13.1476,lwr_k=800:11.9335,lwr_k=900:16.0738,lwr_k=1000:14.4542'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_69'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.7786,lwr_k=10:4.7449,lwr_k=20:5.6115,lwr_k=30:6.0672,lwr_k=40:6.2708,lwr_k=50:6.3991,lwr_k=100:6.6901,lwr_k=200:6.9652,lwr_k=300:7.0887,lwr_k=400:7.1133,lwr_k=500:7.1562,lwr_k=600:7.1637,lwr_k=700:7.1689,lwr_k=800:7.184,lwr_k=900:7.199,lwr_k=1000:7.2159'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.3719,lwr_k=10:13.5281,lwr_k=20:10.0726,lwr_k=30:9.4487,lwr_k=40:9.1788,lwr_k=50:9.1122,lwr_k=100:8.8844,lwr_k=200:9.1396,lwr_k=300:16.9164,lwr_k=400:9.4647,lwr_k=500:9.5219,lwr_k=600:9.4882,lwr_k=700:9.5217,lwr_k=800:9.5005,lwr_k=900:9.5346,lwr_k=1000:9.56'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.4753,lwr_k=10:7.487,lwr_k=20:8.6211,lwr_k=30:8.8853,lwr_k=40:9.2179,lwr_k=50:9.3466,lwr_k=100:9.9376,lwr_k=200:10.2523,lwr_k=300:10.7093,lwr_k=400:10.8222,lwr_k=500:10.9584,lwr_k=600:11.0593,lwr_k=700:11.1971,lwr_k=800:11.3571,lwr_k=900:11.4845,lwr_k=1000:11.5661'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.8511,lwr_k=10:18.6083,lwr_k=20:9.4803,lwr_k=30:8.6265,lwr_k=40:8.5357,lwr_k=50:8.1246,lwr_k=100:8.6381,lwr_k=200:8.3774,lwr_k=300:8.4068,lwr_k=400:8.5008,lwr_k=500:8.7126,lwr_k=600:8.8595,lwr_k=700:8.9866,lwr_k=800:9.2111,lwr_k=900:9.3664,lwr_k=1000:9.4763'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.8081,lwr_k=10:5.9818,lwr_k=20:6.6923,lwr_k=30:7.1406,lwr_k=40:7.5133,lwr_k=50:7.822,lwr_k=100:8.5571,lwr_k=200:9.0266,lwr_k=300:9.1674,lwr_k=400:9.2102,lwr_k=500:9.2729,lwr_k=600:9.2844,lwr_k=700:9.327,lwr_k=800:9.3819,lwr_k=900:9.3984,lwr_k=1000:9.4047'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.9264,lwr_k=10:10.0458,lwr_k=20:8.8029,lwr_k=30:7.9967,lwr_k=40:7.8778,lwr_k=50:7.8117,lwr_k=100:8.2433,lwr_k=200:8.0843,lwr_k=300:8.08,lwr_k=400:8.107,lwr_k=500:8.123,lwr_k=600:8.1249,lwr_k=700:8.1112,lwr_k=800:8.1328,lwr_k=900:8.1381,lwr_k=1000:8.1716'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.9617,lwr_k=10:6.939,lwr_k=20:7.6639,lwr_k=30:7.9128,lwr_k=40:8.3755,lwr_k=50:8.8518,lwr_k=100:9.5691,lwr_k=200:9.981,lwr_k=300:10.142,lwr_k=400:10.188,lwr_k=500:10.3052,lwr_k=600:10.3006,lwr_k=700:10.3785,lwr_k=800:10.4171,lwr_k=900:10.4499,lwr_k=1000:10.4818'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:11.0149,lwr_k=10:14.3845,lwr_k=20:11.5211,lwr_k=30:10.4941,lwr_k=40:10.3251,lwr_k=50:10.3367,lwr_k=100:10.8434,lwr_k=200:10.624,lwr_k=300:10.6024,lwr_k=400:10.7796,lwr_k=500:10.6673,lwr_k=600:10.7297,lwr_k=700:10.7986,lwr_k=800:10.8729,lwr_k=900:10.9638,lwr_k=1000:11.0328'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.1253,lwr_k=10:5.6803,lwr_k=20:6.5445,lwr_k=30:6.7539,lwr_k=40:6.8904,lwr_k=50:6.9583,lwr_k=100:7.0329,lwr_k=200:7.1912,lwr_k=300:7.2606,lwr_k=400:7.2707,lwr_k=500:7.293,lwr_k=600:7.3153,lwr_k=700:7.3387,lwr_k=800:7.3645,lwr_k=900:7.4298,lwr_k=1000:7.4912'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:23.5601,lwr_k=10:20.7513,lwr_k=20:32.5297,lwr_k=30:23.6913,lwr_k=40:23.5121,lwr_k=50:23.1843,lwr_k=100:23.4237,lwr_k=200:23.6835,lwr_k=300:23.1207,lwr_k=400:23.2012,lwr_k=500:23.1269,lwr_k=600:23.0092,lwr_k=700:22.9482,lwr_k=800:22.9623,lwr_k=900:23.0243,lwr_k=1000:23.1055'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:8.8285,lwr_k=10:5.958,lwr_k=20:6.5822,lwr_k=30:6.9129,lwr_k=40:7.105,lwr_k=50:7.2225,lwr_k=100:7.7137,lwr_k=200:7.9862,lwr_k=300:8.0494,lwr_k=400:8.0876,lwr_k=500:8.1453,lwr_k=600:8.1905,lwr_k=700:8.2432,lwr_k=800:8.2685,lwr_k=900:8.3206,lwr_k=1000:8.3595'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.6307,lwr_k=10:12.0266,lwr_k=20:11.2561,lwr_k=30:12.7347,lwr_k=40:12.5486,lwr_k=50:12.3226,lwr_k=100:10.724,lwr_k=200:10.799,lwr_k=300:11.0268,lwr_k=400:11.0243,lwr_k=500:11.146,lwr_k=600:11.2068,lwr_k=700:11.3144,lwr_k=800:11.3604,lwr_k=900:11.4896,lwr_k=1000:11.5132'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_70'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.9867,lwr_k=10:4.769,lwr_k=20:5.277,lwr_k=30:5.4807,lwr_k=40:5.537,lwr_k=50:5.5426,lwr_k=100:5.5617,lwr_k=200:5.6168,lwr_k=300:5.6219,lwr_k=400:5.6401,lwr_k=500:5.6669,lwr_k=600:5.6677,lwr_k=700:5.644,lwr_k=800:5.6453,lwr_k=900:5.6478,lwr_k=1000:5.637'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.042,lwr_k=10:9.9965,lwr_k=20:9.8132,lwr_k=30:9.7108,lwr_k=40:9.5224,lwr_k=50:9.5061,lwr_k=100:9.4828,lwr_k=200:9.345,lwr_k=300:9.322,lwr_k=400:9.3851,lwr_k=500:9.3819,lwr_k=600:9.3866,lwr_k=700:9.3964,lwr_k=800:9.3757,lwr_k=900:9.3741,lwr_k=1000:9.3909'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:53.2901,lwr_k=10:9.275,lwr_k=20:9.8931,lwr_k=30:10.3874,lwr_k=40:10.6043,lwr_k=50:10.7809,lwr_k=100:35.6773,lwr_k=200:45.8017,lwr_k=300:46.4706,lwr_k=400:46.8295,lwr_k=500:47.1694,lwr_k=600:47.4899,lwr_k=700:47.7562,lwr_k=800:48.0006,lwr_k=900:48.1497,lwr_k=1000:48.2821'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:32.3185,lwr_k=10:11.7102,lwr_k=20:11.3429,lwr_k=30:10.883,lwr_k=40:10.7781,lwr_k=50:10.9514,lwr_k=100:24.8034,lwr_k=200:30.1083,lwr_k=300:30.382,lwr_k=400:30.4011,lwr_k=500:30.3744,lwr_k=600:30.3922,lwr_k=700:30.351,lwr_k=800:30.2317,lwr_k=900:30.1543,lwr_k=1000:30.0953'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:34.4008,lwr_k=10:18.3172,lwr_k=20:19.9346,lwr_k=30:20.3103,lwr_k=40:21.0421,lwr_k=50:21.4504,lwr_k=100:22.6702,lwr_k=200:23.0498,lwr_k=300:23.1145,lwr_k=400:23.1828,lwr_k=500:23.25,lwr_k=600:23.2967,lwr_k=700:23.348,lwr_k=800:23.4166,lwr_k=900:23.4647,lwr_k=1000:23.5238'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:28.0329,lwr_k=10:19.0308,lwr_k=20:18.8152,lwr_k=30:18.2269,lwr_k=40:17.8771,lwr_k=50:17.424,lwr_k=100:17.0502,lwr_k=200:16.8863,lwr_k=300:16.8696,lwr_k=400:16.919,lwr_k=500:16.9412,lwr_k=600:16.9863,lwr_k=700:17.0359,lwr_k=800:17.0517,lwr_k=900:17.0422,lwr_k=1000:17.0779'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.3006,lwr_k=10:5.6435,lwr_k=20:6.269,lwr_k=30:6.6384,lwr_k=40:6.9766,lwr_k=50:7.3944,lwr_k=100:8.1328,lwr_k=200:8.8149,lwr_k=300:9.2013,lwr_k=400:9.3747,lwr_k=500:9.475,lwr_k=600:9.5231,lwr_k=700:9.5934,lwr_k=800:9.6128,lwr_k=900:9.6577,lwr_k=1000:9.6782'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.9845,lwr_k=10:9.0799,lwr_k=20:8.6522,lwr_k=30:8.5555,lwr_k=40:8.7993,lwr_k=50:8.7941,lwr_k=100:8.6544,lwr_k=200:8.5622,lwr_k=300:8.5372,lwr_k=400:8.5426,lwr_k=500:8.554,lwr_k=600:8.5409,lwr_k=700:8.5374,lwr_k=800:8.5393,lwr_k=900:8.5498,lwr_k=1000:8.5723'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.0411,lwr_k=10:5.3449,lwr_k=20:5.7045,lwr_k=30:5.7703,lwr_k=40:5.8513,lwr_k=50:5.8742,lwr_k=100:5.9436,lwr_k=200:5.9735,lwr_k=300:5.9812,lwr_k=400:6.0113,lwr_k=500:6.0167,lwr_k=600:6.0295,lwr_k=700:6.028,lwr_k=800:6.0354,lwr_k=900:6.0412,lwr_k=1000:6.0451'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:23.9257,lwr_k=10:20.5292,lwr_k=20:20.0159,lwr_k=30:20.0912,lwr_k=40:21.1615,lwr_k=50:21.2311,lwr_k=100:22.3239,lwr_k=200:22.747,lwr_k=300:22.9152,lwr_k=400:22.9109,lwr_k=500:22.8649,lwr_k=600:22.8639,lwr_k=700:22.8244,lwr_k=800:22.7563,lwr_k=900:22.7383,lwr_k=1000:22.685'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:9.8892,lwr_k=10:6.6358,lwr_k=20:7.1138,lwr_k=30:7.2425,lwr_k=40:7.539,lwr_k=50:7.751,lwr_k=100:8.3495,lwr_k=200:8.6417,lwr_k=300:8.7728,lwr_k=400:8.9315,lwr_k=500:9.0356,lwr_k=600:9.0766,lwr_k=700:9.1075,lwr_k=800:9.1359,lwr_k=900:9.188,lwr_k=1000:9.2361'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.689,lwr_k=10:12.2433,lwr_k=20:11.815,lwr_k=30:11.6766,lwr_k=40:11.4547,lwr_k=50:11.8875,lwr_k=100:11.2438,lwr_k=200:11.4141,lwr_k=300:11.3856,lwr_k=400:11.4343,lwr_k=500:11.4446,lwr_k=600:11.4384,lwr_k=700:11.4313,lwr_k=800:11.4391,lwr_k=900:11.4492,lwr_k=1000:11.4416'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_71'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:43.6243,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:0.8032,lwr_k=100:5.2344,lwr_k=200:9.7458,lwr_k=300:12.2104,lwr_k=400:14.4533,lwr_k=500:16.3888,lwr_k=600:18.0447,lwr_k=700:19.5866,lwr_k=800:20.9003,lwr_k=900:22.1116,lwr_k=1000:23.2324'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:51.1579,lwr_k=10:30.9534,lwr_k=20:39.3896,lwr_k=30:45.876,lwr_k=40:96.8915,lwr_k=50:68.8478,lwr_k=100:21.925,lwr_k=200:17.9285,lwr_k=300:18.9903,lwr_k=400:21.5948,lwr_k=500:22.9126,lwr_k=600:24.7163,lwr_k=700:26.2591,lwr_k=800:27.6652,lwr_k=900:28.9481,lwr_k=1000:29.76'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:55.9441,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:1.0744,lwr_k=100:6.4246,lwr_k=200:11.251,lwr_k=300:14.7428,lwr_k=400:17.9247,lwr_k=500:20.5805,lwr_k=600:22.9471,lwr_k=700:24.8377,lwr_k=800:26.4105,lwr_k=900:27.9919,lwr_k=1000:29.1134'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:48.7986,lwr_k=10:29.744,lwr_k=20:36.2284,lwr_k=30:43.4886,lwr_k=40:132.8132,lwr_k=50:72.0056,lwr_k=100:19.4536,lwr_k=200:18.252,lwr_k=300:18.3682,lwr_k=400:18.5343,lwr_k=500:19.7058,lwr_k=600:21.3109,lwr_k=700:22.6224,lwr_k=800:23.9452,lwr_k=900:25.1571,lwr_k=1000:26.2004'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:59.3617,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:1.274,lwr_k=100:6.6904,lwr_k=200:11.8954,lwr_k=300:16.0711,lwr_k=400:18.7887,lwr_k=500:21.5372,lwr_k=600:24.056,lwr_k=700:26.2393,lwr_k=800:28.0741,lwr_k=900:29.8211,lwr_k=1000:31.2889'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:41.4482,lwr_k=10:27.4284,lwr_k=20:33.2713,lwr_k=30:49.2847,lwr_k=40:142.7135,lwr_k=50:79.6728,lwr_k=100:19.9131,lwr_k=200:15.3732,lwr_k=300:14.9202,lwr_k=400:16.186,lwr_k=500:17.1687,lwr_k=600:18.3898,lwr_k=700:18.9848,lwr_k=800:19.8395,lwr_k=900:20.4858,lwr_k=1000:21.0272'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:59.5846,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:0.9764,lwr_k=100:6.7383,lwr_k=200:11.886,lwr_k=300:15.1596,lwr_k=400:18.2821,lwr_k=500:21.0263,lwr_k=600:23.45,lwr_k=700:25.6398,lwr_k=800:27.7295,lwr_k=900:29.5886,lwr_k=1000:31.0211'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:49.8419,lwr_k=10:33.3401,lwr_k=20:37.039,lwr_k=30:71.8397,lwr_k=40:149.9437,lwr_k=50:74.4762,lwr_k=100:23.2517,lwr_k=200:17.5287,lwr_k=300:17.8746,lwr_k=400:19.4228,lwr_k=500:21.1603,lwr_k=600:22.4891,lwr_k=700:23.906,lwr_k=800:25.3397,lwr_k=900:26.7042,lwr_k=1000:27.7817'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:44.3642,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:0.9624,lwr_k=100:5.5544,lwr_k=200:9.891,lwr_k=300:12.0916,lwr_k=400:14.4629,lwr_k=500:16.4844,lwr_k=600:18.1118,lwr_k=700:19.5722,lwr_k=800:21.0955,lwr_k=900:22.3998,lwr_k=1000:23.434'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:88.4981,lwr_k=10:38.7022,lwr_k=20:45.3526,lwr_k=30:79.7967,lwr_k=40:149.3188,lwr_k=50:92.9251,lwr_k=100:34.5648,lwr_k=200:36.157,lwr_k=300:39.1557,lwr_k=400:44.0148,lwr_k=500:45.5537,lwr_k=600:48.788,lwr_k=700:51.1377,lwr_k=800:53.7802,lwr_k=900:55.5153,lwr_k=1000:57.1218'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:54.5937,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:0.9801,lwr_k=100:5.7035,lwr_k=200:10.2685,lwr_k=300:13.0036,lwr_k=400:15.1067,lwr_k=500:17.3455,lwr_k=600:19.4129,lwr_k=700:21.1755,lwr_k=800:22.6931,lwr_k=900:23.9635,lwr_k=1000:25.2895'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:51.8622,lwr_k=10:33.6453,lwr_k=20:37.6196,lwr_k=30:55.0974,lwr_k=40:110.93,lwr_k=50:81.4221,lwr_k=100:21.8828,lwr_k=200:19.3767,lwr_k=300:20.3306,lwr_k=400:21.9092,lwr_k=500:22.5378,lwr_k=600:24.4495,lwr_k=700:26.1259,lwr_k=800:26.5297,lwr_k=900:27.9152,lwr_k=1000:29.1087'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_72'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.126,lwr_k=10:5.757,lwr_k=20:6.1255,lwr_k=30:6.3556,lwr_k=40:6.4408,lwr_k=50:6.499,lwr_k=100:6.7972,lwr_k=200:7.1227,lwr_k=300:7.2544,lwr_k=400:7.3509,lwr_k=500:7.4442,lwr_k=600:7.5012,lwr_k=700:7.5686,lwr_k=800:7.6404,lwr_k=900:7.7078,lwr_k=1000:7.7633'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.4189,lwr_k=10:10.9838,lwr_k=20:9.6579,lwr_k=30:9.5451,lwr_k=40:10.134,lwr_k=50:10.0104,lwr_k=100:10.2059,lwr_k=200:10.4392,lwr_k=300:10.5446,lwr_k=400:10.5894,lwr_k=500:10.6949,lwr_k=600:10.7248,lwr_k=700:10.811,lwr_k=800:10.8664,lwr_k=900:10.9336,lwr_k=1000:10.9922'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.3076,lwr_k=10:9.6168,lwr_k=20:9.7067,lwr_k=30:9.5913,lwr_k=40:9.4312,lwr_k=50:9.4259,lwr_k=100:9.463,lwr_k=200:9.7197,lwr_k=300:9.8607,lwr_k=400:9.8877,lwr_k=500:9.9109,lwr_k=600:9.9639,lwr_k=700:10.032,lwr_k=800:10.0988,lwr_k=900:10.1503,lwr_k=1000:10.2176'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:13.1119,lwr_k=10:11.934,lwr_k=20:11.3264,lwr_k=30:11.0682,lwr_k=40:11.0105,lwr_k=50:10.901,lwr_k=100:10.8826,lwr_k=200:11.2848,lwr_k=300:11.4886,lwr_k=400:11.5455,lwr_k=500:11.4671,lwr_k=600:11.5674,lwr_k=700:11.6042,lwr_k=800:11.6679,lwr_k=900:11.7402,lwr_k=1000:11.831'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:17.6388,lwr_k=10:9.59,lwr_k=20:10.4628,lwr_k=30:10.5045,lwr_k=40:11.0023,lwr_k=50:11.366,lwr_k=100:12.3607,lwr_k=200:13.3049,lwr_k=300:14.242,lwr_k=400:14.8351,lwr_k=500:15.1269,lwr_k=600:15.249,lwr_k=700:15.348,lwr_k=800:15.4518,lwr_k=900:15.5342,lwr_k=1000:15.6166'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:11.9788,lwr_k=10:11.8748,lwr_k=20:11.1901,lwr_k=30:10.998,lwr_k=40:10.4615,lwr_k=50:10.085,lwr_k=100:9.6972,lwr_k=200:9.231,lwr_k=300:9.1583,lwr_k=400:9.191,lwr_k=500:9.2331,lwr_k=600:9.3044,lwr_k=700:9.319,lwr_k=800:9.4063,lwr_k=900:9.4592,lwr_k=1000:9.5404'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.6006,lwr_k=10:8.9944,lwr_k=20:9.0154,lwr_k=30:9.161,lwr_k=40:11.6943,lwr_k=50:9.7513,lwr_k=100:10.1696,lwr_k=200:10.7597,lwr_k=300:11.3038,lwr_k=400:11.5391,lwr_k=500:11.6128,lwr_k=600:11.6764,lwr_k=700:11.7041,lwr_k=800:11.7346,lwr_k=900:11.7635,lwr_k=1000:11.7887'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:12.0691,lwr_k=10:12.9446,lwr_k=20:11.8321,lwr_k=30:11.3575,lwr_k=40:11.2702,lwr_k=50:11.1131,lwr_k=100:11.0843,lwr_k=200:11.1574,lwr_k=300:11.1802,lwr_k=400:11.2206,lwr_k=500:11.2275,lwr_k=600:11.2363,lwr_k=700:11.2436,lwr_k=800:11.2126,lwr_k=900:11.1898,lwr_k=1000:11.2208'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.3447,lwr_k=10:6.5778,lwr_k=20:6.9483,lwr_k=30:7.1165,lwr_k=40:7.1985,lwr_k=50:7.2428,lwr_k=100:7.2985,lwr_k=200:7.3516,lwr_k=300:7.3782,lwr_k=400:7.3976,lwr_k=500:7.5524,lwr_k=600:7.6326,lwr_k=700:7.7,lwr_k=800:7.7346,lwr_k=900:7.7669,lwr_k=1000:7.8043'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:20.5945,lwr_k=10:16.6442,lwr_k=20:21.3101,lwr_k=30:17.8388,lwr_k=40:16.4494,lwr_k=50:16.6277,lwr_k=100:16.9673,lwr_k=200:17.7691,lwr_k=300:18.1737,lwr_k=400:18.448,lwr_k=500:18.9196,lwr_k=600:19.3512,lwr_k=700:19.7507,lwr_k=800:19.831,lwr_k=900:19.9426,lwr_k=1000:20.0667'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:12.216,lwr_k=10:7.932,lwr_k=20:8.2831,lwr_k=30:8.586,lwr_k=40:8.6922,lwr_k=50:8.693,lwr_k=100:9.0792,lwr_k=200:9.3994,lwr_k=300:9.7021,lwr_k=400:9.9126,lwr_k=500:10.132,lwr_k=600:10.3238,lwr_k=700:10.538,lwr_k=800:10.677,lwr_k=900:10.7486,lwr_k=1000:10.8209'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:14.3141,lwr_k=10:12.3086,lwr_k=20:12.2861,lwr_k=30:12.0496,lwr_k=40:11.5158,lwr_k=50:11.2992,lwr_k=100:11.4658,lwr_k=200:11.6856,lwr_k=300:11.9627,lwr_k=400:12.085,lwr_k=500:12.1152,lwr_k=600:12.207,lwr_k=700:12.3269,lwr_k=800:12.4484,lwr_k=900:12.5201,lwr_k=1000:12.5932'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_73'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.2878,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.2104,lwr_k=40:1.3138,lwr_k=50:2.075,lwr_k=100:4.0294,lwr_k=200:5.1298,lwr_k=300:5.5572,lwr_k=400:5.8251,lwr_k=500:6.0531,lwr_k=600:6.1852,lwr_k=700:6.3478,lwr_k=800:6.4722,lwr_k=900:6.5635,lwr_k=1000:6.6716'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.1618,lwr_k=10:28.4847,lwr_k=20:37.0691,lwr_k=30:1051.5475,lwr_k=40:71.7634,lwr_k=50:19.9055,lwr_k=100:15.5669,lwr_k=200:8.1196,lwr_k=300:8.3953,lwr_k=400:8.2893,lwr_k=500:8.4787,lwr_k=600:8.6356,lwr_k=700:8.8093,lwr_k=800:8.8846,lwr_k=900:9.0322,lwr_k=1000:9.1941'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:17.0455,lwr_k=10:0.0,lwr_k=20:0.082,lwr_k=30:1.0604,lwr_k=40:2.5064,lwr_k=50:3.7078,lwr_k=100:7.1208,lwr_k=200:9.4115,lwr_k=300:10.3767,lwr_k=400:10.9131,lwr_k=500:11.4147,lwr_k=600:11.7993,lwr_k=700:12.0695,lwr_k=800:12.2277,lwr_k=900:12.5078,lwr_k=1000:12.6944'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:14.4905,lwr_k=10:36.2107,lwr_k=20:143.3474,lwr_k=30:17248.3056,lwr_k=40:776.6888,lwr_k=50:529.6638,lwr_k=100:10.327,lwr_k=200:8.5943,lwr_k=300:8.9678,lwr_k=400:9.0487,lwr_k=500:9.2424,lwr_k=600:9.4883,lwr_k=700:9.7587,lwr_k=800:9.9011,lwr_k=900:10.1242,lwr_k=1000:10.2869'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.7343,lwr_k=10:0.0,lwr_k=20:0.0113,lwr_k=30:0.4491,lwr_k=40:1.3335,lwr_k=50:2.088,lwr_k=100:4.4821,lwr_k=200:6.7521,lwr_k=300:7.4474,lwr_k=400:7.8127,lwr_k=500:8.1303,lwr_k=600:8.3307,lwr_k=700:8.4387,lwr_k=800:8.5625,lwr_k=900:8.627,lwr_k=1000:8.7175'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.108,lwr_k=10:23.3846,lwr_k=20:234.9861,lwr_k=30:236.5376,lwr_k=40:36.279,lwr_k=50:64.0054,lwr_k=100:8.6488,lwr_k=200:6.259,lwr_k=300:5.9862,lwr_k=400:5.9723,lwr_k=500:6.0695,lwr_k=600:6.152,lwr_k=700:6.2911,lwr_k=800:6.2695,lwr_k=900:6.2872,lwr_k=1000:6.3924'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:28.7312,lwr_k=10:0.0023,lwr_k=20:0.1436,lwr_k=30:1.3919,lwr_k=40:2.382,lwr_k=50:3.9084,lwr_k=100:8.0977,lwr_k=200:11.654,lwr_k=300:13.2181,lwr_k=400:14.0996,lwr_k=500:15.1219,lwr_k=600:15.8234,lwr_k=700:16.4912,lwr_k=800:17.1781,lwr_k=900:17.6379,lwr_k=1000:18.119'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:22.5717,lwr_k=10:42.4531,lwr_k=20:156.8401,lwr_k=30:1077.1594,lwr_k=40:128547.828,lwr_k=50:3614477.8576,lwr_k=100:15.0575,lwr_k=200:13.3496,lwr_k=300:13.2663,lwr_k=400:13.106,lwr_k=500:13.2986,lwr_k=600:13.5844,lwr_k=700:13.9371,lwr_k=800:14.2991,lwr_k=900:14.498,lwr_k=1000:14.739'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:63.2255,lwr_k=10:0.0,lwr_k=20:0.0519,lwr_k=30:2.3281,lwr_k=40:2.5941,lwr_k=50:3.5769,lwr_k=100:9.0278,lwr_k=200:17.8737,lwr_k=300:21.7278,lwr_k=400:25.0968,lwr_k=500:27.3369,lwr_k=600:29.4925,lwr_k=700:31.4102,lwr_k=800:33.2931,lwr_k=900:35.0152,lwr_k=1000:36.3539'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:129.0229,lwr_k=10:71.4504,lwr_k=20:512.3208,lwr_k=30:447.4747,lwr_k=40:127.6613,lwr_k=50:23095369.9273,lwr_k=100:73.3004,lwr_k=200:70.5127,lwr_k=300:67.2538,lwr_k=400:73.447,lwr_k=500:74.5708,lwr_k=600:77.9536,lwr_k=700:81.265,lwr_k=800:84.6019,lwr_k=900:87.3684,lwr_k=1000:90.2031'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:12.7799,lwr_k=10:3.5859,lwr_k=20:5.9255,lwr_k=30:6.9706,lwr_k=40:7.7993,lwr_k=50:8.2908,lwr_k=100:8.883,lwr_k=200:10.0379,lwr_k=300:10.4663,lwr_k=400:11.0126,lwr_k=500:10.6911,lwr_k=600:10.8191,lwr_k=700:10.7905,lwr_k=800:10.868,lwr_k=900:10.8962,lwr_k=1000:10.9526'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:14.4883,lwr_k=10:216.7596,lwr_k=20:2484.8186,lwr_k=30:6966.4312,lwr_k=40:13246061.229,lwr_k=50:8537240.4198,lwr_k=100:14.236,lwr_k=200:13.164,lwr_k=300:13.0179,lwr_k=400:13.1037,lwr_k=500:13.3571,lwr_k=600:13.3242,lwr_k=700:13.3807,lwr_k=800:13.4768,lwr_k=900:13.4598,lwr_k=1000:13.5674'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_74'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.5272,lwr_k=10:0.0001,lwr_k=20:0.0122,lwr_k=30:0.2618,lwr_k=40:0.8862,lwr_k=50:1.3947,lwr_k=100:2.6218,lwr_k=200:3.3253,lwr_k=300:3.5963,lwr_k=400:3.7233,lwr_k=500:3.8062,lwr_k=600:3.9004,lwr_k=700:3.9448,lwr_k=800:3.9994,lwr_k=900:4.0442,lwr_k=1000:4.1007'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.9931,lwr_k=10:24.1717,lwr_k=20:3591.4,lwr_k=30:1043.0663,lwr_k=40:883.9464,lwr_k=50:37274.5219,lwr_k=100:8.7897,lwr_k=200:7.3315,lwr_k=300:6.9188,lwr_k=400:7.0241,lwr_k=500:7.0862,lwr_k=600:7.1495,lwr_k=700:7.1955,lwr_k=800:7.2634,lwr_k=900:7.3396,lwr_k=1000:7.3752'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:64.0668,lwr_k=10:0.0,lwr_k=20:0.0031,lwr_k=30:1.115,lwr_k=40:4.0257,lwr_k=50:7.0477,lwr_k=100:14.7088,lwr_k=200:20.6152,lwr_k=300:23.9742,lwr_k=400:26.9707,lwr_k=500:28.7348,lwr_k=600:30.6027,lwr_k=700:32.5202,lwr_k=800:33.4841,lwr_k=900:34.8166,lwr_k=1000:35.9254'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:51.4615,lwr_k=10:55.4126,lwr_k=20:174.1111,lwr_k=30:612.1585,lwr_k=40:4901.0715,lwr_k=50:146.8141,lwr_k=100:26.0101,lwr_k=200:24.5957,lwr_k=300:26.2579,lwr_k=400:27.0663,lwr_k=500:27.8271,lwr_k=600:27.8297,lwr_k=700:28.5652,lwr_k=800:29.487,lwr_k=900:30.2131,lwr_k=1000:30.5454'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.0999,lwr_k=10:0.0049,lwr_k=20:0.0339,lwr_k=30:0.2126,lwr_k=40:0.7066,lwr_k=50:1.2952,lwr_k=100:2.8914,lwr_k=200:4.085,lwr_k=300:4.5165,lwr_k=400:4.7817,lwr_k=500:4.9514,lwr_k=600:5.0265,lwr_k=700:5.1039,lwr_k=800:5.1994,lwr_k=900:5.2687,lwr_k=1000:5.3548'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.2246,lwr_k=10:16.092,lwr_k=20:142.8928,lwr_k=30:9602.0361,lwr_k=40:588524.0053,lwr_k=50:96755.9216,lwr_k=100:6.1543,lwr_k=200:5.2608,lwr_k=300:5.1238,lwr_k=400:5.304,lwr_k=500:5.3779,lwr_k=600:5.4611,lwr_k=700:5.4992,lwr_k=800:5.4796,lwr_k=900:5.5236,lwr_k=1000:5.5425'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.7976,lwr_k=10:0.0029,lwr_k=20:0.0071,lwr_k=30:0.0772,lwr_k=40:0.6374,lwr_k=50:1.2892,lwr_k=100:3.2446,lwr_k=200:4.5127,lwr_k=300:4.9308,lwr_k=400:5.2867,lwr_k=500:5.4192,lwr_k=600:5.5607,lwr_k=700:5.6922,lwr_k=800:5.7877,lwr_k=900:5.8694,lwr_k=1000:5.9283'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.407,lwr_k=10:20.9387,lwr_k=20:91.53,lwr_k=30:533.545,lwr_k=40:61451859.3729,lwr_k=50:15587207.707,lwr_k=100:8.5593,lwr_k=200:6.651,lwr_k=300:6.8858,lwr_k=400:7.4221,lwr_k=500:7.4932,lwr_k=600:7.5733,lwr_k=700:7.6348,lwr_k=800:7.7495,lwr_k=900:7.821,lwr_k=1000:7.8791'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.3872,lwr_k=10:0.0135,lwr_k=20:0.1055,lwr_k=30:0.4981,lwr_k=40:0.9237,lwr_k=50:1.2918,lwr_k=100:1.8755,lwr_k=200:2.9372,lwr_k=300:3.3434,lwr_k=400:3.5569,lwr_k=500:3.6954,lwr_k=600:3.804,lwr_k=700:3.8956,lwr_k=800:3.9545,lwr_k=900:4.0209,lwr_k=1000:4.0644'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:16.0309,lwr_k=10:33.8558,lwr_k=20:131.9237,lwr_k=30:1566.0514,lwr_k=40:123699408.5229,lwr_k=50:20051977.1829,lwr_k=100:42798823.4036,lwr_k=200:44.9318,lwr_k=300:15.0744,lwr_k=400:14.0506,lwr_k=500:14.4514,lwr_k=600:15.6191,lwr_k=700:15.7099,lwr_k=800:16.1609,lwr_k=900:16.0788,lwr_k=1000:16.3815'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:4.9511,lwr_k=10:0.0626,lwr_k=20:0.4217,lwr_k=30:0.9505,lwr_k=40:1.5628,lwr_k=50:2.0289,lwr_k=100:2.995,lwr_k=200:3.6723,lwr_k=300:3.9243,lwr_k=400:4.0361,lwr_k=500:4.1294,lwr_k=600:4.2242,lwr_k=700:4.2952,lwr_k=800:4.3655,lwr_k=900:4.4097,lwr_k=1000:4.4401'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.145,lwr_k=10:18562.7478,lwr_k=20:4724.4363,lwr_k=30:103.8584,lwr_k=40:145362760.3046,lwr_k=50:57129301.5748,lwr_k=100:1303862.4889,lwr_k=200:271565.8789,lwr_k=300:2051.7177,lwr_k=400:10.8685,lwr_k=500:8.105,lwr_k=600:8.0037,lwr_k=700:7.355,lwr_k=800:7.6535,lwr_k=900:8.6314,lwr_k=1000:8.8588'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_75'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:3.931,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.5408,lwr_k=40:1.2749,lwr_k=50:1.6932,lwr_k=100:2.5231,lwr_k=200:3.0314,lwr_k=300:3.2261,lwr_k=400:3.3471,lwr_k=500:3.4355,lwr_k=600:3.4699,lwr_k=700:3.5306,lwr_k=800:3.5591,lwr_k=900:3.5866,lwr_k=1000:3.6096'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.8685,lwr_k=10:21.1034,lwr_k=20:192.3509,lwr_k=30:127.4567,lwr_k=40:19.1883,lwr_k=50:13.4832,lwr_k=100:9.6648,lwr_k=200:8.0416,lwr_k=300:8.2594,lwr_k=400:8.309,lwr_k=500:8.4477,lwr_k=600:8.4485,lwr_k=700:8.5378,lwr_k=800:8.5807,lwr_k=900:8.5829,lwr_k=1000:8.6053'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:13.4967,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.9515,lwr_k=40:2.5162,lwr_k=50:3.8157,lwr_k=100:6.5229,lwr_k=200:8.4839,lwr_k=300:9.4901,lwr_k=400:9.9773,lwr_k=500:10.3433,lwr_k=600:10.5889,lwr_k=700:10.7496,lwr_k=800:10.9769,lwr_k=900:11.1695,lwr_k=1000:11.3191'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.8255,lwr_k=10:30.4584,lwr_k=20:536.1761,lwr_k=30:361.7727,lwr_k=40:24.3663,lwr_k=50:16.4409,lwr_k=100:10.6897,lwr_k=200:8.9423,lwr_k=300:8.8301,lwr_k=400:8.9057,lwr_k=500:9.0815,lwr_k=600:9.1891,lwr_k=700:9.3133,lwr_k=800:9.3308,lwr_k=900:9.3991,lwr_k=1000:9.6255'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.0378,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.1141,lwr_k=40:2.7745,lwr_k=50:3.9727,lwr_k=100:6.5472,lwr_k=200:8.1601,lwr_k=300:8.9138,lwr_k=400:9.3456,lwr_k=500:9.6136,lwr_k=600:9.9036,lwr_k=700:10.0892,lwr_k=800:10.2196,lwr_k=900:10.3055,lwr_k=1000:10.3876'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.7283,lwr_k=10:40.6535,lwr_k=20:467.0411,lwr_k=30:2763.3577,lwr_k=40:30.1375,lwr_k=50:14.953,lwr_k=100:10.4122,lwr_k=200:10.4332,lwr_k=300:9.6364,lwr_k=400:7.6769,lwr_k=500:7.6762,lwr_k=600:7.5915,lwr_k=700:7.5323,lwr_k=800:7.4757,lwr_k=900:7.4673,lwr_k=1000:7.4705'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:14.3779,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.0457,lwr_k=40:2.8548,lwr_k=50:4.0679,lwr_k=100:7.0618,lwr_k=200:8.7139,lwr_k=300:9.5342,lwr_k=400:9.9961,lwr_k=500:10.8965,lwr_k=600:11.1707,lwr_k=700:11.2759,lwr_k=800:11.4427,lwr_k=900:11.6021,lwr_k=1000:11.7136'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:14.0762,lwr_k=10:53.2976,lwr_k=20:278.317,lwr_k=30:171.1959,lwr_k=40:51.3755,lwr_k=50:27.5242,lwr_k=100:43.7729,lwr_k=200:13.7969,lwr_k=300:13.3069,lwr_k=400:11.8218,lwr_k=500:11.7138,lwr_k=600:11.4492,lwr_k=700:11.5297,lwr_k=800:11.5345,lwr_k=900:11.4847,lwr_k=1000:11.5708'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.7944,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.646,lwr_k=40:1.8022,lwr_k=50:2.4447,lwr_k=100:3.8286,lwr_k=200:4.489,lwr_k=300:4.7351,lwr_k=400:4.8577,lwr_k=500:4.9356,lwr_k=600:5.0055,lwr_k=700:5.0812,lwr_k=800:5.1243,lwr_k=900:5.1731,lwr_k=1000:5.2058'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:23.1408,lwr_k=10:48.9219,lwr_k=20:310.9607,lwr_k=30:64.7841,lwr_k=40:26.6053,lwr_k=50:18.2296,lwr_k=100:19.6103,lwr_k=200:20.6313,lwr_k=300:20.8328,lwr_k=400:20.6938,lwr_k=500:20.7451,lwr_k=600:21.0585,lwr_k=700:21.0764,lwr_k=800:21.1477,lwr_k=900:21.2175,lwr_k=1000:21.306'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:10.017,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.8595,lwr_k=40:2.3358,lwr_k=50:3.307,lwr_k=100:5.3316,lwr_k=200:6.9399,lwr_k=300:7.394,lwr_k=400:7.8041,lwr_k=500:8.0203,lwr_k=600:8.1748,lwr_k=700:8.3187,lwr_k=800:8.4304,lwr_k=900:8.5234,lwr_k=1000:8.6068'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:13.3543,lwr_k=10:84.192,lwr_k=20:193.9536,lwr_k=30:251.25,lwr_k=40:59.6602,lwr_k=50:46.7998,lwr_k=100:11.1965,lwr_k=200:10.6785,lwr_k=300:10.7037,lwr_k=400:11.0428,lwr_k=500:11.0379,lwr_k=600:11.1714,lwr_k=700:11.3568,lwr_k=800:11.5578,lwr_k=900:11.6579,lwr_k=1000:11.7964'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_76'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.8638,lwr_k=10:0.0,lwr_k=20:1.5402,lwr_k=30:2.906,lwr_k=40:3.6767,lwr_k=50:4.1264,lwr_k=100:4.9203,lwr_k=200:5.5278,lwr_k=300:5.7555,lwr_k=400:5.8557,lwr_k=500:5.969,lwr_k=600:6.0659,lwr_k=700:6.1332,lwr_k=800:6.1725,lwr_k=900:6.1998,lwr_k=1000:6.2334'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.515,lwr_k=10:97.2792,lwr_k=20:87.16,lwr_k=30:20.6092,lwr_k=40:15.3723,lwr_k=50:12.1125,lwr_k=100:10.4304,lwr_k=200:9.6166,lwr_k=300:9.6443,lwr_k=400:8.9107,lwr_k=500:9.0215,lwr_k=600:9.07,lwr_k=700:9.2123,lwr_k=800:9.3083,lwr_k=900:9.3395,lwr_k=1000:9.4008'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.6141,lwr_k=10:0.0,lwr_k=20:2.2386,lwr_k=30:4.0198,lwr_k=40:5.5398,lwr_k=50:6.1166,lwr_k=100:7.7299,lwr_k=200:8.8497,lwr_k=300:9.3959,lwr_k=400:9.6239,lwr_k=500:9.792,lwr_k=600:9.929,lwr_k=700:9.9998,lwr_k=800:10.0605,lwr_k=900:10.1181,lwr_k=1000:10.1938'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.6169,lwr_k=10:169.1508,lwr_k=20:149.9551,lwr_k=30:17.2004,lwr_k=40:12.9294,lwr_k=50:11.5312,lwr_k=100:8.6514,lwr_k=200:8.3276,lwr_k=300:19.3395,lwr_k=400:19.1176,lwr_k=500:26.021,lwr_k=600:22.4969,lwr_k=700:8.5431,lwr_k=800:8.6145,lwr_k=900:8.613,lwr_k=1000:8.6402'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.619,lwr_k=10:0.0,lwr_k=20:2.0328,lwr_k=30:3.6238,lwr_k=40:4.2105,lwr_k=50:4.7356,lwr_k=100:5.7919,lwr_k=200:6.4987,lwr_k=300:6.7149,lwr_k=400:6.877,lwr_k=500:6.9193,lwr_k=600:6.9503,lwr_k=700:7.0275,lwr_k=800:7.0493,lwr_k=900:7.094,lwr_k=1000:7.1045'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.8408,lwr_k=10:135.3307,lwr_k=20:219317.0982,lwr_k=30:5306.8569,lwr_k=40:13.8789,lwr_k=50:18.4602,lwr_k=100:8.0057,lwr_k=200:6.9969,lwr_k=300:6.9092,lwr_k=400:6.973,lwr_k=500:6.8607,lwr_k=600:6.6946,lwr_k=700:6.6613,lwr_k=800:6.6361,lwr_k=900:6.6563,lwr_k=1000:6.6457'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:26.4296,lwr_k=10:13.653,lwr_k=20:16.1118,lwr_k=30:17.015,lwr_k=40:18.0907,lwr_k=50:18.4258,lwr_k=100:19.7651,lwr_k=200:20.7241,lwr_k=300:21.4443,lwr_k=400:21.8285,lwr_k=500:22.1585,lwr_k=600:22.4162,lwr_k=700:22.6542,lwr_k=800:22.8319,lwr_k=900:22.9829,lwr_k=1000:23.2102'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:21.0667,lwr_k=10:315176.5328,lwr_k=20:387.9029,lwr_k=30:232870.5689,lwr_k=40:28.9578,lwr_k=50:32.062,lwr_k=100:23.7464,lwr_k=200:18.5142,lwr_k=300:17.618,lwr_k=400:17.8897,lwr_k=500:18.1949,lwr_k=600:18.1524,lwr_k=700:18.2583,lwr_k=800:18.4127,lwr_k=900:18.4144,lwr_k=1000:18.5155'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.5901,lwr_k=10:0.0,lwr_k=20:1.1709,lwr_k=30:2.4758,lwr_k=40:3.1779,lwr_k=50:3.4853,lwr_k=100:4.2499,lwr_k=200:4.6667,lwr_k=300:4.8091,lwr_k=400:4.9066,lwr_k=500:4.967,lwr_k=600:5.0238,lwr_k=700:5.0695,lwr_k=800:5.094,lwr_k=900:5.1215,lwr_k=1000:5.1647'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:23.2036,lwr_k=10:157.075,lwr_k=20:59.2609,lwr_k=30:24.5294,lwr_k=40:25.5687,lwr_k=50:23.5259,lwr_k=100:21.1836,lwr_k=200:21.3072,lwr_k=300:22.6826,lwr_k=400:21.4427,lwr_k=500:21.6019,lwr_k=600:21.8237,lwr_k=700:21.862,lwr_k=800:21.8683,lwr_k=900:21.9496,lwr_k=1000:21.9605'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:6.3741,lwr_k=10:0.0,lwr_k=20:1.4093,lwr_k=30:2.5001,lwr_k=40:2.9802,lwr_k=50:3.358,lwr_k=100:4.5103,lwr_k=200:5.0877,lwr_k=300:5.3416,lwr_k=400:5.4943,lwr_k=500:5.5708,lwr_k=600:5.6391,lwr_k=700:5.7221,lwr_k=800:5.7814,lwr_k=900:5.82,lwr_k=1000:5.8474'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.2903,lwr_k=10:113.7846,lwr_k=20:2668.6776,lwr_k=30:17.5675,lwr_k=40:534.8708,lwr_k=50:570.5221,lwr_k=100:10.0593,lwr_k=200:9.3548,lwr_k=300:9.4015,lwr_k=400:9.4801,lwr_k=500:9.6026,lwr_k=600:9.7302,lwr_k=700:9.894,lwr_k=800:10.0219,lwr_k=900:10.0704,lwr_k=1000:10.1124'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_77'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.6036,lwr_k=10:0.0,lwr_k=20:0.4548,lwr_k=30:1.7776,lwr_k=40:2.5494,lwr_k=50:2.9718,lwr_k=100:3.9386,lwr_k=200:4.52,lwr_k=300:4.6373,lwr_k=400:4.741,lwr_k=500:4.7986,lwr_k=600:4.859,lwr_k=700:4.9152,lwr_k=800:4.9578,lwr_k=900:4.9895,lwr_k=1000:5.0373'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.5109,lwr_k=10:768.1111,lwr_k=20:103.8653,lwr_k=30:22.7404,lwr_k=40:12.1846,lwr_k=50:9.6831,lwr_k=100:7.5072,lwr_k=200:7.1788,lwr_k=300:7.2303,lwr_k=400:7.3242,lwr_k=500:7.3276,lwr_k=600:7.3811,lwr_k=700:7.3727,lwr_k=800:7.4579,lwr_k=900:7.4993,lwr_k=1000:7.5772'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.285,lwr_k=10:0.0,lwr_k=20:0.2542,lwr_k=30:1.7224,lwr_k=40:2.6717,lwr_k=50:3.3377,lwr_k=100:4.4627,lwr_k=200:5.1823,lwr_k=300:5.3989,lwr_k=400:5.6048,lwr_k=500:5.6943,lwr_k=600:5.8088,lwr_k=700:5.9459,lwr_k=800:6.0018,lwr_k=900:6.0483,lwr_k=1000:6.161'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.7206,lwr_k=10:215.8651,lwr_k=20:4152.9891,lwr_k=30:55.8427,lwr_k=40:36.0531,lwr_k=50:24.7133,lwr_k=100:10.8969,lwr_k=200:7.6895,lwr_k=300:7.5025,lwr_k=400:7.496,lwr_k=500:7.3835,lwr_k=600:7.2989,lwr_k=700:7.3147,lwr_k=800:7.2196,lwr_k=900:7.0941,lwr_k=1000:7.0865'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.1445,lwr_k=10:0.0,lwr_k=20:0.7168,lwr_k=30:2.0875,lwr_k=40:2.8686,lwr_k=50:3.3596,lwr_k=100:4.6427,lwr_k=200:5.4331,lwr_k=300:5.7077,lwr_k=400:5.8519,lwr_k=500:6.0005,lwr_k=600:6.0574,lwr_k=700:6.0938,lwr_k=800:6.1633,lwr_k=900:6.2654,lwr_k=1000:6.3127'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.2607,lwr_k=10:226.7339,lwr_k=20:935.8034,lwr_k=30:31.3536,lwr_k=40:20.9948,lwr_k=50:13.5624,lwr_k=100:8.9235,lwr_k=200:7.0125,lwr_k=300:6.5163,lwr_k=400:5.8522,lwr_k=500:5.7902,lwr_k=600:5.8632,lwr_k=700:5.91,lwr_k=800:5.9749,lwr_k=900:5.9785,lwr_k=1000:6.0251'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.443,lwr_k=10:0.0,lwr_k=20:0.1621,lwr_k=30:1.8615,lwr_k=40:2.9926,lwr_k=50:3.6029,lwr_k=100:5.0855,lwr_k=200:5.9129,lwr_k=300:6.2099,lwr_k=400:6.3574,lwr_k=500:6.5264,lwr_k=600:6.624,lwr_k=700:6.688,lwr_k=800:6.7173,lwr_k=900:6.7558,lwr_k=1000:6.7928'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.9433,lwr_k=10:432.7714,lwr_k=20:679.8977,lwr_k=30:97.8714,lwr_k=40:26.377,lwr_k=50:31.1153,lwr_k=100:10.9058,lwr_k=200:9.0633,lwr_k=300:8.5378,lwr_k=400:8.4588,lwr_k=500:8.5493,lwr_k=600:8.5665,lwr_k=700:8.574,lwr_k=800:8.4803,lwr_k=900:8.5139,lwr_k=1000:8.484'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.8426,lwr_k=10:0.0,lwr_k=20:0.0377,lwr_k=30:1.6301,lwr_k=40:2.5775,lwr_k=50:3.1265,lwr_k=100:4.225,lwr_k=200:4.8001,lwr_k=300:4.987,lwr_k=400:5.1074,lwr_k=500:5.227,lwr_k=600:5.2953,lwr_k=700:5.343,lwr_k=800:5.3726,lwr_k=900:5.4225,lwr_k=1000:5.4262'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:15.7624,lwr_k=10:147.9695,lwr_k=20:644.4468,lwr_k=30:51.1888,lwr_k=40:25.1086,lwr_k=50:19.4718,lwr_k=100:14.8785,lwr_k=200:14.1862,lwr_k=300:14.0045,lwr_k=400:14.0796,lwr_k=500:14.0437,lwr_k=600:14.2149,lwr_k=700:14.3349,lwr_k=800:14.2963,lwr_k=900:14.4662,lwr_k=1000:14.4568'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:5.7294,lwr_k=10:0.0,lwr_k=20:0.4312,lwr_k=30:1.864,lwr_k=40:2.6015,lwr_k=50:3.0769,lwr_k=100:3.8634,lwr_k=200:4.4057,lwr_k=300:4.6306,lwr_k=400:4.7962,lwr_k=500:4.8502,lwr_k=600:4.8923,lwr_k=700:4.9674,lwr_k=800:4.9874,lwr_k=900:5.0223,lwr_k=1000:5.0434'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.6053,lwr_k=10:180.8268,lwr_k=20:320.0909,lwr_k=30:27.4799,lwr_k=40:12.6781,lwr_k=50:10.1728,lwr_k=100:7.317,lwr_k=200:6.9383,lwr_k=300:6.9738,lwr_k=400:7.0286,lwr_k=500:7.0535,lwr_k=600:7.0643,lwr_k=700:7.1575,lwr_k=800:7.3466,lwr_k=900:7.4173,lwr_k=1000:7.5008'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_78'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.0622,lwr_k=10:0.0,lwr_k=20:0.6982,lwr_k=30:1.7069,lwr_k=40:2.186,lwr_k=50:2.4606,lwr_k=100:3.0747,lwr_k=200:3.4452,lwr_k=300:3.6017,lwr_k=400:3.6709,lwr_k=500:3.738,lwr_k=600:3.7762,lwr_k=700:3.8308,lwr_k=800:3.8833,lwr_k=900:3.9357,lwr_k=1000:3.9738'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.5046,lwr_k=10:28.3909,lwr_k=20:33.1089,lwr_k=30:10.67,lwr_k=40:8.2728,lwr_k=50:7.4332,lwr_k=100:6.1611,lwr_k=200:6.4632,lwr_k=300:6.5795,lwr_k=400:6.6842,lwr_k=500:6.6851,lwr_k=600:6.7739,lwr_k=700:6.8579,lwr_k=800:6.9162,lwr_k=900:7.0096,lwr_k=1000:7.0899'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.7614,lwr_k=10:0.0,lwr_k=20:0.8759,lwr_k=30:2.3217,lwr_k=40:3.0374,lwr_k=50:3.5572,lwr_k=100:4.6651,lwr_k=200:5.5286,lwr_k=300:5.8921,lwr_k=400:6.1155,lwr_k=500:6.3455,lwr_k=600:6.5417,lwr_k=700:6.6946,lwr_k=800:6.8535,lwr_k=900:6.982,lwr_k=1000:7.1475'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.6778,lwr_k=10:47.7096,lwr_k=20:29.7949,lwr_k=30:9.5226,lwr_k=40:7.5511,lwr_k=50:7.1826,lwr_k=100:6.1873,lwr_k=200:6.2986,lwr_k=300:6.4499,lwr_k=400:6.4407,lwr_k=500:6.5133,lwr_k=600:6.5231,lwr_k=700:6.5383,lwr_k=800:6.6592,lwr_k=900:6.7,lwr_k=1000:6.7429'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.0835,lwr_k=10:0.0,lwr_k=20:1.0337,lwr_k=30:2.3491,lwr_k=40:3.1156,lwr_k=50:3.415,lwr_k=100:4.3783,lwr_k=200:5.5789,lwr_k=300:6.2246,lwr_k=400:6.3891,lwr_k=500:6.4621,lwr_k=600:6.5575,lwr_k=700:6.6148,lwr_k=800:6.6602,lwr_k=900:6.6987,lwr_k=1000:6.7515'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.2582,lwr_k=10:67.88,lwr_k=20:68.3279,lwr_k=30:9.6024,lwr_k=40:7.0414,lwr_k=50:6.1254,lwr_k=100:5.1763,lwr_k=200:5.0437,lwr_k=300:5.0249,lwr_k=400:5.0199,lwr_k=500:5.04,lwr_k=600:5.0115,lwr_k=700:5.0386,lwr_k=800:5.0822,lwr_k=900:5.0608,lwr_k=1000:5.0675'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.6875,lwr_k=10:0.0,lwr_k=20:0.9535,lwr_k=30:2.061,lwr_k=40:2.8816,lwr_k=50:3.2928,lwr_k=100:4.3169,lwr_k=200:5.0146,lwr_k=300:5.231,lwr_k=400:5.3503,lwr_k=500:5.4323,lwr_k=600:5.5014,lwr_k=700:5.571,lwr_k=800:5.6351,lwr_k=900:5.6621,lwr_k=1000:5.6867'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.1373,lwr_k=10:74.3212,lwr_k=20:46.6278,lwr_k=30:13.9266,lwr_k=40:8.2823,lwr_k=50:7.4675,lwr_k=100:6.2866,lwr_k=200:6.6082,lwr_k=300:6.6587,lwr_k=400:6.7012,lwr_k=500:6.6815,lwr_k=600:6.6626,lwr_k=700:6.6641,lwr_k=800:6.6672,lwr_k=900:6.6647,lwr_k=1000:6.6809'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.9643,lwr_k=10:0.0,lwr_k=20:1.1471,lwr_k=30:2.3688,lwr_k=40:2.9088,lwr_k=50:3.2386,lwr_k=100:3.997,lwr_k=200:4.4401,lwr_k=300:4.5624,lwr_k=400:4.6596,lwr_k=500:4.6761,lwr_k=600:4.7151,lwr_k=700:4.7505,lwr_k=800:4.7655,lwr_k=900:4.7822,lwr_k=1000:4.7948'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:17.4445,lwr_k=10:107.3375,lwr_k=20:120.4182,lwr_k=30:18.4092,lwr_k=40:14.8644,lwr_k=50:14.2534,lwr_k=100:15.626,lwr_k=200:17.2534,lwr_k=300:16.7658,lwr_k=400:16.7474,lwr_k=500:16.6537,lwr_k=600:16.7001,lwr_k=700:16.7775,lwr_k=800:16.9025,lwr_k=900:16.9418,lwr_k=1000:16.9929'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:6.7765,lwr_k=10:0.0,lwr_k=20:0.7414,lwr_k=30:1.8028,lwr_k=40:2.2793,lwr_k=50:2.5563,lwr_k=100:3.6908,lwr_k=200:4.3046,lwr_k=300:4.538,lwr_k=400:4.6495,lwr_k=500:4.7515,lwr_k=600:4.805,lwr_k=700:4.8477,lwr_k=800:4.897,lwr_k=900:4.9314,lwr_k=1000:4.9723'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.2039,lwr_k=10:22.6719,lwr_k=20:34.181,lwr_k=30:9.6104,lwr_k=40:9.6617,lwr_k=50:7.2831,lwr_k=100:6.6376,lwr_k=200:6.6867,lwr_k=300:7.1205,lwr_k=400:7.2691,lwr_k=500:7.4413,lwr_k=600:7.6171,lwr_k=700:7.7383,lwr_k=800:7.883,lwr_k=900:7.9561,lwr_k=1000:8.0286'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_79'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.9881,lwr_k=10:0.237,lwr_k=20:1.2128,lwr_k=30:2.2851,lwr_k=40:3.1214,lwr_k=50:3.7151,lwr_k=100:4.7382,lwr_k=200:5.2968,lwr_k=300:5.5035,lwr_k=400:5.6232,lwr_k=500:5.678,lwr_k=600:5.7242,lwr_k=700:5.7256,lwr_k=800:5.7591,lwr_k=900:5.778,lwr_k=1000:5.805'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.7189,lwr_k=10:2345.3802,lwr_k=20:1745.6262,lwr_k=30:31.0917,lwr_k=40:24.654,lwr_k=50:13.6726,lwr_k=100:19.9441,lwr_k=200:8.6277,lwr_k=300:9.1638,lwr_k=400:8.0743,lwr_k=500:8.1644,lwr_k=600:8.1675,lwr_k=700:7.9524,lwr_k=800:7.6379,lwr_k=900:7.7287,lwr_k=1000:7.7666'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.9716,lwr_k=10:0.0023,lwr_k=20:0.1008,lwr_k=30:1.1345,lwr_k=40:2.023,lwr_k=50:2.6405,lwr_k=100:4.6315,lwr_k=200:5.9866,lwr_k=300:6.3866,lwr_k=400:6.5597,lwr_k=500:6.7228,lwr_k=600:6.8586,lwr_k=700:6.9578,lwr_k=800:7.0283,lwr_k=900:7.0819,lwr_k=1000:7.1306'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.283,lwr_k=10:44.0516,lwr_k=20:179.0107,lwr_k=30:33.5097,lwr_k=40:15.7928,lwr_k=50:9.5897,lwr_k=100:6.8229,lwr_k=200:6.3799,lwr_k=300:6.3641,lwr_k=400:6.341,lwr_k=500:6.3811,lwr_k=600:6.4428,lwr_k=700:6.4672,lwr_k=800:6.5248,lwr_k=900:6.5778,lwr_k=1000:6.5874'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.1983,lwr_k=10:0.3442,lwr_k=20:1.2995,lwr_k=30:2.7052,lwr_k=40:3.5143,lwr_k=50:3.8891,lwr_k=100:5.0078,lwr_k=200:5.5923,lwr_k=300:5.6945,lwr_k=400:5.7722,lwr_k=500:5.8323,lwr_k=600:5.862,lwr_k=700:5.9196,lwr_k=800:5.9252,lwr_k=900:5.9539,lwr_k=1000:5.9552'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:5.9037,lwr_k=10:595883.6411,lwr_k=20:47.4581,lwr_k=30:164.0701,lwr_k=40:88.3649,lwr_k=50:12.0591,lwr_k=100:6.6801,lwr_k=200:5.8654,lwr_k=300:5.7348,lwr_k=400:5.7865,lwr_k=500:5.8391,lwr_k=600:6.1512,lwr_k=700:7.0197,lwr_k=800:5.8359,lwr_k=900:5.8758,lwr_k=1000:5.8728'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.3274,lwr_k=10:0.0142,lwr_k=20:0.1631,lwr_k=30:1.491,lwr_k=40:2.5597,lwr_k=50:3.266,lwr_k=100:4.387,lwr_k=200:5.1279,lwr_k=300:5.4757,lwr_k=400:5.7019,lwr_k=500:5.9183,lwr_k=600:6.0774,lwr_k=700:6.1757,lwr_k=800:6.2347,lwr_k=900:6.2667,lwr_k=1000:6.2895'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.3213,lwr_k=10:33.4146,lwr_k=20:6203.0582,lwr_k=30:75.9461,lwr_k=40:19.7376,lwr_k=50:12.7249,lwr_k=100:8.3911,lwr_k=200:8.0613,lwr_k=300:8.0901,lwr_k=400:8.0825,lwr_k=500:8.2106,lwr_k=600:8.1655,lwr_k=700:8.1996,lwr_k=800:8.1915,lwr_k=900:8.2267,lwr_k=1000:8.241'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.0472,lwr_k=10:0.0018,lwr_k=20:0.0573,lwr_k=30:1.1683,lwr_k=40:2.0904,lwr_k=50:2.4178,lwr_k=100:3.6492,lwr_k=200:4.1616,lwr_k=300:4.4188,lwr_k=400:4.5552,lwr_k=500:4.6267,lwr_k=600:4.69,lwr_k=700:4.73,lwr_k=800:4.7504,lwr_k=900:4.7843,lwr_k=1000:4.8074'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:19.9289,lwr_k=10:47.1802,lwr_k=20:1316.029,lwr_k=30:33.4637,lwr_k=40:22.7047,lwr_k=50:17.6139,lwr_k=100:18.3879,lwr_k=200:18.3509,lwr_k=300:18.6112,lwr_k=400:18.7105,lwr_k=500:18.8571,lwr_k=600:19.0388,lwr_k=700:19.1641,lwr_k=800:19.2352,lwr_k=900:19.3053,lwr_k=1000:19.3842'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:11.2021,lwr_k=10:0.0748,lwr_k=20:0.5232,lwr_k=30:2.3508,lwr_k=40:3.6228,lwr_k=50:4.5095,lwr_k=100:6.9002,lwr_k=200:8.2341,lwr_k=300:8.8476,lwr_k=400:9.1431,lwr_k=500:9.3194,lwr_k=600:9.4801,lwr_k=700:9.562,lwr_k=800:9.6952,lwr_k=900:9.7664,lwr_k=1000:9.8731'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.2454,lwr_k=10:89.3801,lwr_k=20:969.005,lwr_k=30:53.5519,lwr_k=40:23.6585,lwr_k=50:26.3305,lwr_k=100:11.9385,lwr_k=200:10.4695,lwr_k=300:10.5705,lwr_k=400:10.6956,lwr_k=500:10.4245,lwr_k=600:10.4325,lwr_k=700:10.4296,lwr_k=800:10.5452,lwr_k=900:10.619,lwr_k=1000:10.6715'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_80'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.4522,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:0.0227,lwr_k=100:1.5853,lwr_k=200:2.7643,lwr_k=300:3.2835,lwr_k=400:3.4776,lwr_k=500:3.6244,lwr_k=600:3.7729,lwr_k=700:3.8349,lwr_k=800:3.9024,lwr_k=900:3.9818,lwr_k=1000:4.0218'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.139,lwr_k=10:29.5769,lwr_k=20:72.1615,lwr_k=30:139.7145,lwr_k=40:6986.3375,lwr_k=50:47979.4366,lwr_k=100:119.9876,lwr_k=200:12.1909,lwr_k=300:11.6255,lwr_k=400:13.5893,lwr_k=500:7.5482,lwr_k=600:7.1673,lwr_k=700:7.2048,lwr_k=800:6.9861,lwr_k=900:6.7814,lwr_k=1000:6.8062'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.3607,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:0.0359,lwr_k=100:2.0397,lwr_k=200:3.6287,lwr_k=300:4.2145,lwr_k=400:4.5871,lwr_k=500:4.8047,lwr_k=600:5.0157,lwr_k=700:5.1709,lwr_k=800:5.2802,lwr_k=900:5.3831,lwr_k=1000:5.4734'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.3121,lwr_k=10:14.4766,lwr_k=20:25.407,lwr_k=30:32.1212,lwr_k=40:67.7198,lwr_k=50:1484.0297,lwr_k=100:9.5569,lwr_k=200:6.5042,lwr_k=300:6.3309,lwr_k=400:6.4455,lwr_k=500:6.3679,lwr_k=600:6.3456,lwr_k=700:6.328,lwr_k=800:6.3707,lwr_k=900:6.3496,lwr_k=1000:6.3509'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.1123,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0002,lwr_k=50:0.059,lwr_k=100:1.9957,lwr_k=200:3.1995,lwr_k=300:3.6884,lwr_k=400:3.9926,lwr_k=500:4.1692,lwr_k=600:4.3427,lwr_k=700:4.4475,lwr_k=800:4.6135,lwr_k=900:4.7453,lwr_k=1000:4.7932'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.3419,lwr_k=10:17.5124,lwr_k=20:58.8075,lwr_k=30:251.1558,lwr_k=40:791.2959,lwr_k=50:18925.0635,lwr_k=100:77.5068,lwr_k=200:10.8595,lwr_k=300:7.5787,lwr_k=400:7.5546,lwr_k=500:7.2945,lwr_k=600:7.2919,lwr_k=700:7.1338,lwr_k=800:7.0149,lwr_k=900:6.7223,lwr_k=1000:6.6558'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.7703,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:0.0123,lwr_k=100:2.2004,lwr_k=200:3.4375,lwr_k=300:3.9213,lwr_k=400:4.212,lwr_k=500:4.3942,lwr_k=600:4.5475,lwr_k=700:4.6522,lwr_k=800:4.7167,lwr_k=900:4.803,lwr_k=1000:4.8779'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.9878,lwr_k=10:16.6447,lwr_k=20:32.6984,lwr_k=30:68.4918,lwr_k=40:513.886,lwr_k=50:16679.3179,lwr_k=100:213.5631,lwr_k=200:12.0149,lwr_k=300:7.3959,lwr_k=400:7.1598,lwr_k=500:7.2192,lwr_k=600:7.0556,lwr_k=700:7.078,lwr_k=800:7.0979,lwr_k=900:7.0715,lwr_k=1000:7.1153'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.2169,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0001,lwr_k=50:0.0303,lwr_k=100:1.9605,lwr_k=200:2.8522,lwr_k=300:3.1674,lwr_k=400:3.3615,lwr_k=500:3.5187,lwr_k=600:3.621,lwr_k=700:3.7282,lwr_k=800:3.7782,lwr_k=900:3.8338,lwr_k=1000:3.8789'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:16.708,lwr_k=10:21.9232,lwr_k=20:36.7504,lwr_k=30:92.9396,lwr_k=40:77.8712,lwr_k=50:497.2479,lwr_k=100:17.3803,lwr_k=200:15.491,lwr_k=300:15.7611,lwr_k=400:16.0062,lwr_k=500:16.1115,lwr_k=600:16.1679,lwr_k=700:16.2222,lwr_k=800:16.369,lwr_k=900:16.4488,lwr_k=1000:16.5484'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:4.9236,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:0.0197,lwr_k=100:1.9684,lwr_k=200:3.0428,lwr_k=300:3.4303,lwr_k=400:3.6277,lwr_k=500:3.7221,lwr_k=600:3.8049,lwr_k=700:3.901,lwr_k=800:3.9569,lwr_k=900:3.9799,lwr_k=1000:4.018'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.9064,lwr_k=10:41.0835,lwr_k=20:49.8959,lwr_k=30:70.9982,lwr_k=40:1022.1007,lwr_k=50:49999.4893,lwr_k=100:322.2781,lwr_k=200:12.4844,lwr_k=300:7.1304,lwr_k=400:6.7704,lwr_k=500:6.4653,lwr_k=600:6.5547,lwr_k=700:7.3916,lwr_k=800:6.8539,lwr_k=900:7.154,lwr_k=1000:7.3296'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_81'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.7588,lwr_k=10:7.8068,lwr_k=20:8.5502,lwr_k=30:8.8583,lwr_k=40:8.8942,lwr_k=50:9.1134,lwr_k=100:9.3239,lwr_k=200:9.8781,lwr_k=300:10.1262,lwr_k=400:10.3731,lwr_k=500:10.3836,lwr_k=600:10.5514,lwr_k=700:10.8672,lwr_k=800:10.8357,lwr_k=900:11.1057,lwr_k=1000:10.9605'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:15.5728,lwr_k=10:15.4444,lwr_k=20:14.2486,lwr_k=30:14.1047,lwr_k=40:801049119.3053,lwr_k=50:235388.5849,lwr_k=100:14.1932,lwr_k=200:13.1557,lwr_k=300:13.9101,lwr_k=400:14.4222,lwr_k=500:14.0209,lwr_k=600:14.747,lwr_k=700:14.5555,lwr_k=800:14.639,lwr_k=900:14.7482,lwr_k=1000:14.9009'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:33.5813,lwr_k=10:23.4145,lwr_k=20:27.073,lwr_k=30:28.0481,lwr_k=40:28.9254,lwr_k=50:28.8919,lwr_k=100:29.5384,lwr_k=200:29.8811,lwr_k=300:30.2168,lwr_k=400:30.5959,lwr_k=500:30.7247,lwr_k=600:31.1235,lwr_k=700:31.2842,lwr_k=800:31.5794,lwr_k=900:31.6481,lwr_k=1000:31.7893'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:35.6229,lwr_k=10:37.4026,lwr_k=20:6658.1774,lwr_k=30:30.362,lwr_k=40:30.1689,lwr_k=50:29.2619,lwr_k=100:30.1979,lwr_k=200:30.4301,lwr_k=300:30.5901,lwr_k=400:30.4262,lwr_k=500:30.3953,lwr_k=600:30.4225,lwr_k=700:30.7264,lwr_k=800:31.0554,lwr_k=900:30.9746,lwr_k=1000:31.0548'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:18.3279,lwr_k=10:11.2496,lwr_k=20:12.5138,lwr_k=30:12.7685,lwr_k=40:13.3216,lwr_k=50:13.4166,lwr_k=100:14.259,lwr_k=200:15.2811,lwr_k=300:16.0138,lwr_k=400:15.8894,lwr_k=500:15.7617,lwr_k=600:15.7962,lwr_k=700:15.7905,lwr_k=800:16.0927,lwr_k=900:16.4196,lwr_k=1000:16.6691'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:12.6746,lwr_k=10:15.5805,lwr_k=20:13.5509,lwr_k=30:12.6379,lwr_k=40:11.8172,lwr_k=50:10.9475,lwr_k=100:10.6335,lwr_k=200:10.4427,lwr_k=300:10.4714,lwr_k=400:10.5247,lwr_k=500:10.5225,lwr_k=600:10.4947,lwr_k=700:10.5367,lwr_k=800:10.6118,lwr_k=900:10.6295,lwr_k=1000:10.7089'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:17.8228,lwr_k=10:10.0998,lwr_k=20:10.7032,lwr_k=30:11.1227,lwr_k=40:11.7723,lwr_k=50:12.0757,lwr_k=100:13.1387,lwr_k=200:14.4765,lwr_k=300:15.2934,lwr_k=400:15.6661,lwr_k=500:15.7416,lwr_k=600:16.3395,lwr_k=700:16.1246,lwr_k=800:17.3134,lwr_k=900:16.645,lwr_k=1000:16.4151'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:13.9851,lwr_k=10:12.6029,lwr_k=20:12.4184,lwr_k=30:12.6324,lwr_k=40:12.6413,lwr_k=50:12.5745,lwr_k=100:12.5629,lwr_k=200:12.3908,lwr_k=300:12.5024,lwr_k=400:12.5448,lwr_k=500:12.5864,lwr_k=600:12.6177,lwr_k=700:12.2533,lwr_k=800:12.7662,lwr_k=900:12.813,lwr_k=1000:13.1338'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.8541,lwr_k=10:8.4468,lwr_k=20:8.8868,lwr_k=30:8.9561,lwr_k=40:9.0717,lwr_k=50:9.1049,lwr_k=100:9.3012,lwr_k=200:9.5472,lwr_k=300:9.7988,lwr_k=400:9.9832,lwr_k=500:10.5,lwr_k=600:10.8703,lwr_k=700:10.9011,lwr_k=800:11.1326,lwr_k=900:11.1043,lwr_k=1000:11.2192'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:39.1286,lwr_k=10:69.4596,lwr_k=20:23.4073,lwr_k=30:21.7278,lwr_k=40:23.0773,lwr_k=50:22.8244,lwr_k=100:25.6442,lwr_k=200:28.1547,lwr_k=300:29.7506,lwr_k=400:29.1863,lwr_k=500:33.445,lwr_k=600:43.9174,lwr_k=700:35.4777,lwr_k=800:34.0922,lwr_k=900:36.5587,lwr_k=1000:33.6269'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:17.3385,lwr_k=10:12.9873,lwr_k=20:13.6187,lwr_k=30:13.691,lwr_k=40:13.8433,lwr_k=50:13.993,lwr_k=100:14.3177,lwr_k=200:14.5096,lwr_k=300:14.7913,lwr_k=400:15.0192,lwr_k=500:15.3322,lwr_k=600:15.4158,lwr_k=700:15.3848,lwr_k=800:15.3838,lwr_k=900:15.8882,lwr_k=1000:15.742'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:31.5523,lwr_k=10:30.6998,lwr_k=20:30.3466,lwr_k=30:30.0126,lwr_k=40:29.7174,lwr_k=50:28.917,lwr_k=100:32.0596,lwr_k=200:27.0486,lwr_k=300:26.8748,lwr_k=400:29.0254,lwr_k=500:31.1335,lwr_k=600:29.0819,lwr_k=700:33.3031,lwr_k=800:29.226,lwr_k=900:29.2264,lwr_k=1000:29.2841'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_82'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.0886,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.2924,lwr_k=40:1.0396,lwr_k=50:6.1218,lwr_k=100:4.9432,lwr_k=200:6.1098,lwr_k=300:5.3112,lwr_k=400:5.0126,lwr_k=500:4.9127,lwr_k=600:4.8443,lwr_k=700:5.0748,lwr_k=800:4.8656,lwr_k=900:4.7223,lwr_k=1000:4.7421'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.2926,lwr_k=10:82.2212,lwr_k=20:227.4327,lwr_k=30:2434.438,lwr_k=40:583.3172,lwr_k=50:72.0812,lwr_k=100:17.9103,lwr_k=200:14.8733,lwr_k=300:9.7752,lwr_k=400:10.0727,lwr_k=500:9.3799,lwr_k=600:9.2142,lwr_k=700:8.6787,lwr_k=800:8.8468,lwr_k=900:8.3322,lwr_k=1000:8.4497'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.9049,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.0122,lwr_k=40:2.0715,lwr_k=50:6.6198,lwr_k=100:5.6349,lwr_k=200:5.6867,lwr_k=300:6.0801,lwr_k=400:6.056,lwr_k=500:6.2813,lwr_k=600:6.2615,lwr_k=700:6.3316,lwr_k=800:6.3467,lwr_k=900:6.5385,lwr_k=1000:6.4993'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:6.9321,lwr_k=10:594.774,lwr_k=20:1346.0659,lwr_k=30:438.5445,lwr_k=40:147.3724,lwr_k=50:57.8409,lwr_k=100:24.3706,lwr_k=200:13.4152,lwr_k=300:18.4628,lwr_k=400:8.2623,lwr_k=500:8.5513,lwr_k=600:8.0217,lwr_k=700:7.9816,lwr_k=800:7.6971,lwr_k=900:7.2222,lwr_k=1000:6.8953'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.5924,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.1276,lwr_k=40:0.8946,lwr_k=50:6.3198,lwr_k=100:4.7688,lwr_k=200:4.5689,lwr_k=300:4.8685,lwr_k=400:5.0737,lwr_k=500:5.1837,lwr_k=600:5.276,lwr_k=700:5.3987,lwr_k=800:5.4809,lwr_k=900:5.5415,lwr_k=1000:5.5996'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.1826,lwr_k=10:44.8891,lwr_k=20:391.0891,lwr_k=30:2482.046,lwr_k=40:489.324,lwr_k=50:73.553,lwr_k=100:12.7388,lwr_k=200:8.2409,lwr_k=300:7.853,lwr_k=400:8.0865,lwr_k=500:6.6758,lwr_k=600:6.4264,lwr_k=700:6.4145,lwr_k=800:6.3184,lwr_k=900:6.1633,lwr_k=1000:5.9468'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.1504,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.2895,lwr_k=40:1.5701,lwr_k=50:26.3305,lwr_k=100:14.0586,lwr_k=200:7.7871,lwr_k=300:7.1681,lwr_k=400:9.6504,lwr_k=500:8.8644,lwr_k=600:8.8805,lwr_k=700:9.0862,lwr_k=800:8.2813,lwr_k=900:8.6936,lwr_k=1000:9.5722'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:12.3129,lwr_k=10:61.253,lwr_k=20:356.3827,lwr_k=30:2329.4822,lwr_k=40:1103.379,lwr_k=50:474.1874,lwr_k=100:29.367,lwr_k=200:22.0111,lwr_k=300:13.1058,lwr_k=400:14.9957,lwr_k=500:13.2055,lwr_k=600:14.096,lwr_k=700:12.4081,lwr_k=800:10.3436,lwr_k=900:10.9523,lwr_k=1000:14.8317'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.1304,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.1838,lwr_k=40:1.1946,lwr_k=50:4.1203,lwr_k=100:3.7115,lwr_k=200:4.1688,lwr_k=300:4.4261,lwr_k=400:4.5713,lwr_k=500:4.5894,lwr_k=600:4.6829,lwr_k=700:4.7584,lwr_k=800:4.7731,lwr_k=900:4.793,lwr_k=1000:4.8397'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:14.3792,lwr_k=10:59.8756,lwr_k=20:333.7585,lwr_k=30:1047.734,lwr_k=40:1132.6263,lwr_k=50:898.1035,lwr_k=100:20.2692,lwr_k=200:13.8679,lwr_k=300:13.0784,lwr_k=400:13.5027,lwr_k=500:13.4787,lwr_k=600:13.6636,lwr_k=700:13.4022,lwr_k=800:13.4285,lwr_k=900:13.5512,lwr_k=1000:13.7276'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:7.761,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.4973,lwr_k=40:1.5123,lwr_k=50:11.2438,lwr_k=100:7.7512,lwr_k=200:7.7622,lwr_k=300:10.2904,lwr_k=400:9.4776,lwr_k=500:11.0695,lwr_k=600:10.4747,lwr_k=700:11.9531,lwr_k=800:9.334,lwr_k=900:10.6962,lwr_k=1000:11.0412'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.7322,lwr_k=10:182.0244,lwr_k=20:663.6861,lwr_k=30:1690.8987,lwr_k=40:541.8667,lwr_k=50:696.5141,lwr_k=100:136.2785,lwr_k=200:51.5925,lwr_k=300:19.6451,lwr_k=400:14.646,lwr_k=500:14.2815,lwr_k=600:21.0895,lwr_k=700:14.7168,lwr_k=800:21.0974,lwr_k=900:15.3386,lwr_k=1000:17.4834'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_83'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:42.285,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:0.7399,lwr_k=100:5.2102,lwr_k=200:9.274,lwr_k=300:11.6953,lwr_k=400:13.5291,lwr_k=500:15.089,lwr_k=600:16.493,lwr_k=700:17.8171,lwr_k=800:18.8795,lwr_k=900:19.9606,lwr_k=1000:20.5967'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:55.8134,lwr_k=10:39.0301,lwr_k=20:45.7011,lwr_k=30:57.6858,lwr_k=40:145.5735,lwr_k=50:69.99,lwr_k=100:21.191,lwr_k=200:19.7461,lwr_k=300:21.2908,lwr_k=400:21.8832,lwr_k=500:22.9214,lwr_k=600:23.9345,lwr_k=700:25.5921,lwr_k=800:26.9639,lwr_k=900:27.8189,lwr_k=1000:29.2066'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:53.8876,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:0.833,lwr_k=100:5.7938,lwr_k=200:11.0584,lwr_k=300:15.1863,lwr_k=400:17.5935,lwr_k=500:20.3311,lwr_k=600:22.6991,lwr_k=700:24.3913,lwr_k=800:25.8568,lwr_k=900:27.3443,lwr_k=1000:28.6299'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:41.4162,lwr_k=10:30.8268,lwr_k=20:51.1727,lwr_k=30:81.2231,lwr_k=40:201.3918,lwr_k=50:96.6498,lwr_k=100:20.1592,lwr_k=200:15.1558,lwr_k=300:14.7816,lwr_k=400:15.8906,lwr_k=500:16.21,lwr_k=600:17.8617,lwr_k=700:19.466,lwr_k=800:20.5094,lwr_k=900:21.1627,lwr_k=1000:21.7877'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:58.9426,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:0.8912,lwr_k=100:6.0523,lwr_k=200:12.0259,lwr_k=300:16.0257,lwr_k=400:19.1655,lwr_k=500:22.2804,lwr_k=600:24.7719,lwr_k=700:26.6348,lwr_k=800:28.0474,lwr_k=900:29.4989,lwr_k=1000:30.8285'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:41.5059,lwr_k=10:40.6435,lwr_k=20:54.7856,lwr_k=30:83.7136,lwr_k=40:159.2583,lwr_k=50:78.4233,lwr_k=100:19.4745,lwr_k=200:15.4225,lwr_k=300:15.7421,lwr_k=400:17.0456,lwr_k=500:17.9838,lwr_k=600:19.6662,lwr_k=700:20.6199,lwr_k=800:21.5837,lwr_k=900:22.1082,lwr_k=1000:22.4182'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:54.4991,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:0.942,lwr_k=100:6.2972,lwr_k=200:11.6785,lwr_k=300:15.5709,lwr_k=400:18.7866,lwr_k=500:21.8187,lwr_k=600:23.9877,lwr_k=700:25.4572,lwr_k=800:27.39,lwr_k=900:28.5688,lwr_k=1000:29.8681'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:57.8256,lwr_k=10:36.1602,lwr_k=20:50.6748,lwr_k=30:73.6572,lwr_k=40:171.2483,lwr_k=50:72.1275,lwr_k=100:18.4717,lwr_k=200:18.8811,lwr_k=300:21.5993,lwr_k=400:22.9807,lwr_k=500:24.0809,lwr_k=600:25.5287,lwr_k=700:26.3994,lwr_k=800:27.216,lwr_k=900:28.0702,lwr_k=1000:29.1344'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:42.9996,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:0.8548,lwr_k=100:5.5794,lwr_k=200:10.0797,lwr_k=300:12.4826,lwr_k=400:14.3828,lwr_k=500:15.8308,lwr_k=600:17.2242,lwr_k=700:18.3219,lwr_k=800:19.5658,lwr_k=900:20.5982,lwr_k=1000:21.5499'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:78.3128,lwr_k=10:42.0927,lwr_k=20:82.3037,lwr_k=30:112.3047,lwr_k=40:267.4462,lwr_k=50:133.3164,lwr_k=100:39.3229,lwr_k=200:44.5026,lwr_k=300:52.5526,lwr_k=400:54.3316,lwr_k=500:54.1026,lwr_k=600:55.4001,lwr_k=700:56.1315,lwr_k=800:56.8012,lwr_k=900:57.1809,lwr_k=1000:58.0886'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:50.7877,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:0.8522,lwr_k=100:5.1918,lwr_k=200:9.8807,lwr_k=300:13.1928,lwr_k=400:15.7395,lwr_k=500:17.7676,lwr_k=600:19.7013,lwr_k=700:21.1973,lwr_k=800:22.6662,lwr_k=900:23.8752,lwr_k=1000:24.89'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:53.7221,lwr_k=10:40.5614,lwr_k=20:57.656,lwr_k=30:68.8548,lwr_k=40:243.8326,lwr_k=50:109.106,lwr_k=100:25.118,lwr_k=200:22.105,lwr_k=300:23.2359,lwr_k=400:22.628,lwr_k=500:22.4843,lwr_k=600:23.1388,lwr_k=700:23.5835,lwr_k=800:25.3768,lwr_k=900:25.7583,lwr_k=1000:26.5403'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_84'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.8382,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.01,lwr_k=40:0.1866,lwr_k=50:1.9912,lwr_k=100:2.4251,lwr_k=200:3.2799,lwr_k=300:3.4878,lwr_k=400:3.717,lwr_k=500:3.8661,lwr_k=600:4.0314,lwr_k=700:4.1213,lwr_k=800:4.1646,lwr_k=900:4.2104,lwr_k=1000:4.2735'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.5994,lwr_k=10:108.3513,lwr_k=20:95.4212,lwr_k=30:260.2159,lwr_k=40:167.5806,lwr_k=50:426.0672,lwr_k=100:11.849,lwr_k=200:7.9732,lwr_k=300:7.5468,lwr_k=400:7.7726,lwr_k=500:7.521,lwr_k=600:8.4107,lwr_k=700:8.4377,lwr_k=800:8.3794,lwr_k=900:8.3192,lwr_k=1000:8.1365'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.7105,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:0.1552,lwr_k=100:2.27,lwr_k=200:3.5764,lwr_k=300:4.3761,lwr_k=400:4.8932,lwr_k=500:5.2106,lwr_k=600:5.4331,lwr_k=700:5.5909,lwr_k=800:5.7057,lwr_k=900:5.7709,lwr_k=1000:5.8283'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.7389,lwr_k=10:18.0805,lwr_k=20:28.2989,lwr_k=30:51.9304,lwr_k=40:199.4799,lwr_k=50:7517.0828,lwr_k=100:10.0554,lwr_k=200:6.9693,lwr_k=300:6.9031,lwr_k=400:6.9949,lwr_k=500:6.9246,lwr_k=600:6.9957,lwr_k=700:6.9658,lwr_k=800:7.0177,lwr_k=900:7.0393,lwr_k=1000:6.9394'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.4919,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0029,lwr_k=50:0.3214,lwr_k=100:2.9186,lwr_k=200:4.1523,lwr_k=300:4.7479,lwr_k=400:5.1251,lwr_k=500:5.47,lwr_k=600:5.7232,lwr_k=700:5.9332,lwr_k=800:6.1187,lwr_k=900:6.2476,lwr_k=1000:6.3755'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.8801,lwr_k=10:40.715,lwr_k=20:66.1719,lwr_k=30:169.6757,lwr_k=40:952.1241,lwr_k=50:406.4306,lwr_k=100:10.854,lwr_k=200:7.6919,lwr_k=300:6.7763,lwr_k=400:6.6368,lwr_k=500:6.5236,lwr_k=600:6.4905,lwr_k=700:6.4379,lwr_k=800:6.4248,lwr_k=900:6.4061,lwr_k=1000:6.3898'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.8369,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:0.1468,lwr_k=100:2.208,lwr_k=200:3.3282,lwr_k=300:3.7301,lwr_k=400:3.9488,lwr_k=500:4.1357,lwr_k=600:4.2266,lwr_k=700:4.3655,lwr_k=800:4.4301,lwr_k=900:4.5212,lwr_k=1000:4.605'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.2477,lwr_k=10:26.2095,lwr_k=20:28.6534,lwr_k=30:66.3293,lwr_k=40:110.6706,lwr_k=50:1090.5403,lwr_k=100:9.2603,lwr_k=200:8.3315,lwr_k=300:8.0363,lwr_k=400:8.0043,lwr_k=500:8.0255,lwr_k=600:8.1976,lwr_k=700:8.23,lwr_k=800:8.293,lwr_k=900:8.3735,lwr_k=1000:8.315'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.4468,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0,lwr_k=50:0.1832,lwr_k=100:1.9761,lwr_k=200:2.7765,lwr_k=300:3.1842,lwr_k=400:3.3897,lwr_k=500:3.5225,lwr_k=600:3.6261,lwr_k=700:3.6866,lwr_k=800:3.7246,lwr_k=900:3.7759,lwr_k=1000:3.844'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:20.858,lwr_k=10:37.6556,lwr_k=20:67.9609,lwr_k=30:194.4,lwr_k=40:377.8149,lwr_k=50:803.3507,lwr_k=100:16.7259,lwr_k=200:15.716,lwr_k=300:16.9017,lwr_k=400:17.8685,lwr_k=500:18.3563,lwr_k=600:18.409,lwr_k=700:18.4987,lwr_k=800:18.6278,lwr_k=900:18.812,lwr_k=1000:18.9073'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:5.2154,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0006,lwr_k=50:0.2241,lwr_k=100:2.1852,lwr_k=200:3.1291,lwr_k=300:3.4151,lwr_k=400:3.5782,lwr_k=500:3.7506,lwr_k=600:3.8627,lwr_k=700:4.0003,lwr_k=800:4.1246,lwr_k=900:4.2316,lwr_k=1000:4.3183'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.949,lwr_k=10:151.4717,lwr_k=20:75.9854,lwr_k=30:126.3175,lwr_k=40:375.9338,lwr_k=50:414.6464,lwr_k=100:10.8179,lwr_k=200:7.8667,lwr_k=300:7.6042,lwr_k=400:7.8206,lwr_k=500:7.2504,lwr_k=600:7.5463,lwr_k=700:7.648,lwr_k=800:7.5634,lwr_k=900:7.5328,lwr_k=1000:7.6095'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_85'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:55.2101,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:2.0536,lwr_k=40:4.5475,lwr_k=50:6.1161,lwr_k=100:10.7732,lwr_k=200:14.2261,lwr_k=300:16.4454,lwr_k=400:18.2837,lwr_k=500:19.8485,lwr_k=600:21.2229,lwr_k=700:22.5064,lwr_k=800:23.8824,lwr_k=900:25.143,lwr_k=1000:26.3154'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:67.9609,lwr_k=10:76.1821,lwr_k=20:137.7844,lwr_k=30:59.9741,lwr_k=40:32.1707,lwr_k=50:28.8983,lwr_k=100:22.6347,lwr_k=200:23.2803,lwr_k=300:26.1046,lwr_k=400:28.6607,lwr_k=500:30.3377,lwr_k=600:31.988,lwr_k=700:33.1984,lwr_k=800:34.5659,lwr_k=900:35.9027,lwr_k=1000:37.0256'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:69.4892,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:2.3464,lwr_k=40:5.0268,lwr_k=50:7.2941,lwr_k=100:13.5391,lwr_k=200:19.3478,lwr_k=300:22.3193,lwr_k=400:24.7641,lwr_k=500:26.3162,lwr_k=600:28.3845,lwr_k=700:30.0506,lwr_k=800:31.9455,lwr_k=900:33.6179,lwr_k=1000:34.8731'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:53.6902,lwr_k=10:61.6878,lwr_k=20:137.8677,lwr_k=30:59.886,lwr_k=40:31.1114,lwr_k=50:23.0993,lwr_k=100:17.3616,lwr_k=200:17.6322,lwr_k=300:18.588,lwr_k=400:19.6023,lwr_k=500:20.6247,lwr_k=600:21.1455,lwr_k=700:21.8222,lwr_k=800:22.7325,lwr_k=900:23.8216,lwr_k=1000:24.699'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:75.4766,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:2.5034,lwr_k=40:5.3069,lwr_k=50:7.5734,lwr_k=100:13.8604,lwr_k=200:20.3246,lwr_k=300:24.0337,lwr_k=400:26.9771,lwr_k=500:29.0068,lwr_k=600:30.9059,lwr_k=700:32.8615,lwr_k=800:34.6342,lwr_k=900:36.2768,lwr_k=1000:37.8139'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:53.3951,lwr_k=10:68.4164,lwr_k=20:199.9304,lwr_k=30:70.3803,lwr_k=40:35.8477,lwr_k=50:27.4297,lwr_k=100:22.5469,lwr_k=200:20.0628,lwr_k=300:20.3894,lwr_k=400:21.6687,lwr_k=500:22.2788,lwr_k=600:22.6627,lwr_k=700:23.9559,lwr_k=800:25.361,lwr_k=900:25.6943,lwr_k=1000:26.3659'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:72.3269,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:2.3344,lwr_k=40:5.3481,lwr_k=50:7.3303,lwr_k=100:13.1756,lwr_k=200:19.9166,lwr_k=300:23.2687,lwr_k=400:26.3469,lwr_k=500:28.4047,lwr_k=600:30.1566,lwr_k=700:31.9673,lwr_k=800:33.5856,lwr_k=900:35.3182,lwr_k=1000:36.8955'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:67.5258,lwr_k=10:73.7119,lwr_k=20:150.3278,lwr_k=30:56.4936,lwr_k=40:28.3902,lwr_k=50:26.0634,lwr_k=100:19.3755,lwr_k=200:20.1925,lwr_k=300:21.7113,lwr_k=400:22.9988,lwr_k=500:24.9538,lwr_k=600:25.9343,lwr_k=700:27.6266,lwr_k=800:29.1913,lwr_k=900:30.2251,lwr_k=1000:31.5445'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:56.6596,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:2.1905,lwr_k=40:4.6401,lwr_k=50:6.4543,lwr_k=100:10.8933,lwr_k=200:15.0471,lwr_k=300:17.6531,lwr_k=400:19.7312,lwr_k=500:21.5527,lwr_k=600:23.0102,lwr_k=700:24.4531,lwr_k=800:25.8473,lwr_k=900:27.264,lwr_k=1000:28.496'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:99.2036,lwr_k=10:90.2366,lwr_k=20:199.4569,lwr_k=30:77.247,lwr_k=40:42.8997,lwr_k=50:37.2871,lwr_k=100:38.7406,lwr_k=200:41.8741,lwr_k=300:44.3597,lwr_k=400:45.7902,lwr_k=500:46.9083,lwr_k=600:49.4044,lwr_k=700:50.9518,lwr_k=800:53.1752,lwr_k=900:55.3769,lwr_k=1000:56.8948'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:66.7904,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:2.2581,lwr_k=40:4.6728,lwr_k=50:6.5548,lwr_k=100:11.9156,lwr_k=200:16.885,lwr_k=300:20.0589,lwr_k=400:21.9876,lwr_k=500:23.9457,lwr_k=600:25.4344,lwr_k=700:26.7819,lwr_k=800:28.0213,lwr_k=900:29.2398,lwr_k=1000:30.4758'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:62.9883,lwr_k=10:100.4005,lwr_k=20:194.1281,lwr_k=30:72.2946,lwr_k=40:50.8894,lwr_k=50:34.5431,lwr_k=100:22.7162,lwr_k=200:23.7332,lwr_k=300:25.3977,lwr_k=400:25.3686,lwr_k=500:26.4606,lwr_k=600:26.9894,lwr_k=700:27.3105,lwr_k=800:27.9015,lwr_k=900:28.7679,lwr_k=1000:29.2558'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_86'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.6538,lwr_k=10:0.0,lwr_k=20:0.8739,lwr_k=30:3.4557,lwr_k=40:5.0148,lwr_k=50:6.036,lwr_k=100:7.9525,lwr_k=200:9.4211,lwr_k=300:9.9728,lwr_k=400:10.2119,lwr_k=500:10.2566,lwr_k=600:10.56,lwr_k=700:10.6533,lwr_k=800:10.9147,lwr_k=900:10.9231,lwr_k=1000:11.0602'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:15.304,lwr_k=10:72.3171,lwr_k=20:233.0563,lwr_k=30:30.4706,lwr_k=40:20.7032,lwr_k=50:17.4583,lwr_k=100:14.2935,lwr_k=200:13.8743,lwr_k=300:13.9621,lwr_k=400:13.8393,lwr_k=500:13.8991,lwr_k=600:13.9392,lwr_k=700:13.9211,lwr_k=800:13.8438,lwr_k=900:13.9492,lwr_k=1000:13.9913'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.5548,lwr_k=10:0.0013,lwr_k=20:2.5958,lwr_k=30:3.8477,lwr_k=40:4.4823,lwr_k=50:4.8708,lwr_k=100:5.719,lwr_k=200:6.1726,lwr_k=300:6.2027,lwr_k=400:6.3863,lwr_k=500:6.3392,lwr_k=600:6.372,lwr_k=700:6.4029,lwr_k=800:6.4149,lwr_k=900:6.4384,lwr_k=1000:6.4394'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.458,lwr_k=10:446.1525,lwr_k=20:498606.8382,lwr_k=30:505.2859,lwr_k=40:22246676.4198,lwr_k=50:14645165.7988,lwr_k=100:11.3975,lwr_k=200:10.9086,lwr_k=300:10.7103,lwr_k=400:10.5997,lwr_k=500:10.6456,lwr_k=600:10.5684,lwr_k=700:10.6305,lwr_k=800:10.5947,lwr_k=900:10.512,lwr_k=1000:10.4534'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.5216,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.7419,lwr_k=40:1.7114,lwr_k=50:0.0702,lwr_k=100:0.5454,lwr_k=200:1.6904,lwr_k=300:2.7849,lwr_k=400:3.3489,lwr_k=500:3.8759,lwr_k=600:4.2052,lwr_k=700:4.3661,lwr_k=800:4.4262,lwr_k=900:4.7049,lwr_k=1000:4.7697'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.8835,lwr_k=10:26.0185,lwr_k=20:80.7176,lwr_k=30:55.9218,lwr_k=40:19.1102,lwr_k=50:24.6298,lwr_k=100:19.2027,lwr_k=200:15.3986,lwr_k=300:12.9537,lwr_k=400:9.5052,lwr_k=500:8.8528,lwr_k=600:8.739,lwr_k=700:7.4978,lwr_k=800:7.3033,lwr_k=900:6.8981,lwr_k=1000:6.9376'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:16.8721,lwr_k=10:0.0,lwr_k=20:0.0119,lwr_k=30:2.9547,lwr_k=40:4.8194,lwr_k=50:5.9136,lwr_k=100:9.4124,lwr_k=200:10.5726,lwr_k=300:11.9751,lwr_k=400:12.4907,lwr_k=500:12.7044,lwr_k=600:12.9484,lwr_k=700:13.1032,lwr_k=800:13.4648,lwr_k=900:13.5529,lwr_k=1000:13.6489'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:13.7483,lwr_k=10:52.6544,lwr_k=20:6019.3787,lwr_k=30:36.9624,lwr_k=40:24.5728,lwr_k=50:19.8831,lwr_k=100:13.3373,lwr_k=200:12.5763,lwr_k=300:11.902,lwr_k=400:11.8461,lwr_k=500:11.8414,lwr_k=600:11.9715,lwr_k=700:12.0418,lwr_k=800:12.022,lwr_k=900:12.1162,lwr_k=1000:12.0538'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:3.7238,lwr_k=10:0.0,lwr_k=20:0.4945,lwr_k=30:1.637,lwr_k=40:2.1973,lwr_k=50:2.5193,lwr_k=100:3.0417,lwr_k=200:3.3686,lwr_k=300:3.4525,lwr_k=400:3.5178,lwr_k=500:3.5538,lwr_k=600:3.5843,lwr_k=700:3.6226,lwr_k=800:3.6235,lwr_k=900:3.6162,lwr_k=1000:3.6207'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:20.7195,lwr_k=10:30.2702,lwr_k=20:77.1374,lwr_k=30:23.6604,lwr_k=40:22.3252,lwr_k=50:21.724,lwr_k=100:20.5676,lwr_k=200:20.4099,lwr_k=300:20.1203,lwr_k=400:20.2075,lwr_k=500:20.2107,lwr_k=600:20.2169,lwr_k=700:20.7343,lwr_k=800:20.2692,lwr_k=900:20.2811,lwr_k=1000:20.333'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:7.7547,lwr_k=10:0.0,lwr_k=20:0.0237,lwr_k=30:1.9172,lwr_k=40:2.8753,lwr_k=50:3.542,lwr_k=100:4.8237,lwr_k=200:5.4646,lwr_k=300:5.9703,lwr_k=400:6.8362,lwr_k=500:6.4215,lwr_k=600:6.5245,lwr_k=700:6.6101,lwr_k=800:6.7164,lwr_k=900:7.0331,lwr_k=1000:6.7532'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:13.5937,lwr_k=10:169.1967,lwr_k=20:2075.0295,lwr_k=30:25.7223,lwr_k=40:17.0677,lwr_k=50:12.5886,lwr_k=100:11.3092,lwr_k=200:11.8966,lwr_k=300:12.7374,lwr_k=400:12.53,lwr_k=500:12.8257,lwr_k=600:12.7383,lwr_k=700:12.7817,lwr_k=800:12.7705,lwr_k=900:12.8576,lwr_k=1000:12.8448'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_87'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.0497,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.9976,lwr_k=40:1.6219,lwr_k=50:2.0156,lwr_k=100:2.6956,lwr_k=200:3.1427,lwr_k=300:3.3658,lwr_k=400:3.4734,lwr_k=500:3.5387,lwr_k=600:3.564,lwr_k=700:3.5844,lwr_k=800:3.6022,lwr_k=900:3.6261,lwr_k=1000:3.6494'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.8307,lwr_k=10:50.3851,lwr_k=20:315.9738,lwr_k=30:27.6823,lwr_k=40:15.0772,lwr_k=50:11.4132,lwr_k=100:9.3783,lwr_k=200:8.9487,lwr_k=300:8.8414,lwr_k=400:8.8319,lwr_k=500:8.7909,lwr_k=600:8.7936,lwr_k=700:8.8097,lwr_k=800:8.8057,lwr_k=900:8.746,lwr_k=1000:8.7582'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.6111,lwr_k=10:0.0,lwr_k=20:0.0005,lwr_k=30:1.3424,lwr_k=40:1.9665,lwr_k=50:2.4089,lwr_k=100:3.1555,lwr_k=200:3.5367,lwr_k=300:3.665,lwr_k=400:3.76,lwr_k=500:3.8208,lwr_k=600:3.8783,lwr_k=700:3.8982,lwr_k=800:3.9233,lwr_k=900:3.9533,lwr_k=1000:3.9814'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.0164,lwr_k=10:42.1895,lwr_k=20:1449.9994,lwr_k=30:22.4115,lwr_k=40:13.3338,lwr_k=50:11.185,lwr_k=100:10.1385,lwr_k=200:9.4978,lwr_k=300:9.06,lwr_k=400:9.0471,lwr_k=500:9.0182,lwr_k=600:9.0331,lwr_k=700:8.9917,lwr_k=800:8.983,lwr_k=900:8.894,lwr_k=1000:8.8581'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.6065,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.0712,lwr_k=40:2.4907,lwr_k=50:0.0779,lwr_k=100:0.5465,lwr_k=200:1.6785,lwr_k=300:2.8599,lwr_k=400:3.7863,lwr_k=500:4.5849,lwr_k=600:4.8302,lwr_k=700:5.206,lwr_k=800:5.7292,lwr_k=900:6.1781,lwr_k=1000:6.5423'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.8242,lwr_k=10:33.5169,lwr_k=20:95.3143,lwr_k=30:54.9202,lwr_k=40:21.0993,lwr_k=50:21.3279,lwr_k=100:23.4067,lwr_k=200:18.7346,lwr_k=300:13.2416,lwr_k=400:12.3283,lwr_k=500:11.4728,lwr_k=600:11.5832,lwr_k=700:9.6181,lwr_k=800:8.0173,lwr_k=900:7.4224,lwr_k=1000:7.4955'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.2812,lwr_k=10:0.0,lwr_k=20:0.0002,lwr_k=30:1.1359,lwr_k=40:1.9693,lwr_k=50:2.2655,lwr_k=100:2.856,lwr_k=200:3.4352,lwr_k=300:3.6738,lwr_k=400:3.7873,lwr_k=500:3.8514,lwr_k=600:3.8874,lwr_k=700:3.9236,lwr_k=800:3.9541,lwr_k=900:3.9758,lwr_k=1000:4.0059'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.4835,lwr_k=10:28.0362,lwr_k=20:172.0855,lwr_k=30:24.7794,lwr_k=40:12.8446,lwr_k=50:11.8974,lwr_k=100:11.0432,lwr_k=200:10.2287,lwr_k=300:9.9406,lwr_k=400:9.6639,lwr_k=500:9.4508,lwr_k=600:9.6409,lwr_k=700:9.6268,lwr_k=800:9.484,lwr_k=900:9.3824,lwr_k=1000:9.3256'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.3855,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.8562,lwr_k=40:2.1128,lwr_k=50:0.1162,lwr_k=100:0.9536,lwr_k=200:2.7947,lwr_k=300:3.7839,lwr_k=400:4.6163,lwr_k=500:5.0496,lwr_k=600:5.1794,lwr_k=700:5.4446,lwr_k=800:5.5732,lwr_k=900:5.7272,lwr_k=1000:5.8122'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:25.0743,lwr_k=10:33.7476,lwr_k=20:56.4724,lwr_k=30:55.4437,lwr_k=40:32.0529,lwr_k=50:39.1052,lwr_k=100:37.1396,lwr_k=200:34.287,lwr_k=300:28.4147,lwr_k=400:21.9786,lwr_k=500:23.2819,lwr_k=600:23.2062,lwr_k=700:23.029,lwr_k=800:23.2836,lwr_k=900:23.5839,lwr_k=1000:23.7234'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:4.5297,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.2914,lwr_k=40:2.0797,lwr_k=50:2.3618,lwr_k=100:3.0342,lwr_k=200:3.5897,lwr_k=300:3.8629,lwr_k=400:4.036,lwr_k=500:4.0977,lwr_k=600:4.1273,lwr_k=700:4.1759,lwr_k=800:4.198,lwr_k=900:4.2134,lwr_k=1000:4.2376'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.4906,lwr_k=10:62.8654,lwr_k=20:2015.2815,lwr_k=30:37.3883,lwr_k=40:15.7305,lwr_k=50:14.1036,lwr_k=100:12.9506,lwr_k=200:12.581,lwr_k=300:12.14,lwr_k=400:11.9297,lwr_k=500:11.785,lwr_k=600:11.714,lwr_k=700:11.658,lwr_k=800:11.696,lwr_k=900:11.5506,lwr_k=1000:11.5173'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_88'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:173.7868,lwr_k=10:162.9546,lwr_k=20:165.6432,lwr_k=30:168.7808,lwr_k=40:168.0459,lwr_k=50:169.2204,lwr_k=100:170.2859,lwr_k=200:169.8184,lwr_k=300:169.4012,lwr_k=400:169.6806,lwr_k=500:169.7344,lwr_k=600:169.9172,lwr_k=700:170.1313,lwr_k=800:170.3131,lwr_k=900:170.3979,lwr_k=1000:170.4458'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:229.121,lwr_k=10:233.3241,lwr_k=20:227.3377,lwr_k=30:231.946,lwr_k=40:229.2257,lwr_k=50:229.9247,lwr_k=100:229.434,lwr_k=200:227.7357,lwr_k=300:224.4824,lwr_k=400:224.4504,lwr_k=500:225.0383,lwr_k=600:225.2999,lwr_k=700:225.7739,lwr_k=800:225.9552,lwr_k=900:225.8557,lwr_k=1000:225.5152'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:211.0735,lwr_k=10:201.7586,lwr_k=20:206.4394,lwr_k=30:204.6804,lwr_k=40:205.9411,lwr_k=50:205.4612,lwr_k=100:206.4403,lwr_k=200:206.9776,lwr_k=300:207.4357,lwr_k=400:208.7045,lwr_k=500:207.7332,lwr_k=600:207.6437,lwr_k=700:207.7325,lwr_k=800:207.8498,lwr_k=900:207.9091,lwr_k=1000:208.0388'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:177.3878,lwr_k=10:180.0278,lwr_k=20:177.0514,lwr_k=30:172.4207,lwr_k=40:172.8168,lwr_k=50:171.8596,lwr_k=100:171.6145,lwr_k=200:171.9086,lwr_k=300:172.2277,lwr_k=400:173.478,lwr_k=500:172.8571,lwr_k=600:173.1046,lwr_k=700:173.3961,lwr_k=800:173.6981,lwr_k=900:173.8086,lwr_k=1000:173.8618'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:53.7857,lwr_k=10:28.2099,lwr_k=20:29.6076,lwr_k=30:30.6003,lwr_k=40:30.7905,lwr_k=50:31.0973,lwr_k=100:32.6257,lwr_k=200:34.6726,lwr_k=300:36.1839,lwr_k=400:37.0717,lwr_k=500:37.766,lwr_k=600:38.3428,lwr_k=700:38.9033,lwr_k=800:39.4407,lwr_k=900:39.9715,lwr_k=1000:40.5369'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:37.0923,lwr_k=10:31.2681,lwr_k=20:30.0919,lwr_k=30:29.4003,lwr_k=40:28.858,lwr_k=50:28.4911,lwr_k=100:28.3631,lwr_k=200:28.9413,lwr_k=300:29.2238,lwr_k=400:29.5034,lwr_k=500:29.5557,lwr_k=600:29.6116,lwr_k=700:29.7399,lwr_k=800:29.8907,lwr_k=900:29.8256,lwr_k=1000:29.8307'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:230.5372,lwr_k=10:218.1012,lwr_k=20:222.3986,lwr_k=30:222.9549,lwr_k=40:224.5655,lwr_k=50:224.7551,lwr_k=100:226.1479,lwr_k=200:224.8556,lwr_k=300:224.9381,lwr_k=400:225.4837,lwr_k=500:225.3487,lwr_k=600:225.549,lwr_k=700:225.6742,lwr_k=800:225.7313,lwr_k=900:225.9125,lwr_k=1000:226.0546'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:213.5071,lwr_k=10:215.9537,lwr_k=20:211.6304,lwr_k=30:210.3814,lwr_k=40:209.9395,lwr_k=50:210.0171,lwr_k=100:210.1717,lwr_k=200:209.6335,lwr_k=300:209.8779,lwr_k=400:211.5408,lwr_k=500:211.275,lwr_k=600:211.3983,lwr_k=700:211.4138,lwr_k=800:210.8047,lwr_k=900:210.5435,lwr_k=1000:210.7643'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:179.0243,lwr_k=10:171.4459,lwr_k=20:172.1263,lwr_k=30:185.4853,lwr_k=40:175.4927,lwr_k=50:178.3789,lwr_k=100:174.073,lwr_k=200:175.0631,lwr_k=300:176.1363,lwr_k=400:174.9768,lwr_k=500:174.6202,lwr_k=600:174.8207,lwr_k=700:175.2164,lwr_k=800:175.2105,lwr_k=900:175.2417,lwr_k=1000:175.2965'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:287.7229,lwr_k=10:305.252,lwr_k=20:284.6371,lwr_k=30:288.1,lwr_k=40:283.3676,lwr_k=50:283.7962,lwr_k=100:282.9039,lwr_k=200:282.379,lwr_k=300:282.2562,lwr_k=400:282.7777,lwr_k=500:283.8614,lwr_k=600:283.9009,lwr_k=700:283.1681,lwr_k=800:283.7585,lwr_k=900:284.213,lwr_k=1000:284.8071'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:40.6827,lwr_k=10:22.2354,lwr_k=20:23.515,lwr_k=30:24.016,lwr_k=40:24.4457,lwr_k=50:24.8339,lwr_k=100:25.8234,lwr_k=200:27.1075,lwr_k=300:27.9678,lwr_k=400:28.595,lwr_k=500:29.1693,lwr_k=600:29.5528,lwr_k=700:29.8827,lwr_k=800:30.2277,lwr_k=900:30.5615,lwr_k=1000:30.9231'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:36.62,lwr_k=10:31.3209,lwr_k=20:30.4541,lwr_k=30:29.6005,lwr_k=40:29.4845,lwr_k=50:28.899,lwr_k=100:27.5158,lwr_k=200:26.1148,lwr_k=300:25.8774,lwr_k=400:26.0599,lwr_k=500:26.1163,lwr_k=600:26.2312,lwr_k=700:26.3916,lwr_k=800:26.5448,lwr_k=900:26.8027,lwr_k=1000:27.0895'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_89'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.5566,lwr_k=10:3.5441,lwr_k=20:5.6771,lwr_k=30:6.3319,lwr_k=40:6.9028,lwr_k=50:7.154,lwr_k=100:7.7036,lwr_k=200:7.96,lwr_k=300:8.0903,lwr_k=400:8.2166,lwr_k=500:8.2901,lwr_k=600:8.3379,lwr_k=700:8.3833,lwr_k=800:8.4377,lwr_k=900:8.4713,lwr_k=1000:8.5057'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.3624,lwr_k=10:2070322.6554,lwr_k=20:4244.6116,lwr_k=30:22.5506,lwr_k=40:10.4999,lwr_k=50:10.1285,lwr_k=100:9.9557,lwr_k=200:10.1885,lwr_k=300:10.2292,lwr_k=400:10.4222,lwr_k=500:10.536,lwr_k=600:10.6169,lwr_k=700:10.7064,lwr_k=800:10.7592,lwr_k=900:10.8464,lwr_k=1000:10.9415'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:19.9113,lwr_k=10:4.3882,lwr_k=20:7.8966,lwr_k=30:9.3894,lwr_k=40:10.2521,lwr_k=50:10.9935,lwr_k=100:12.4443,lwr_k=200:13.6408,lwr_k=300:14.2461,lwr_k=400:14.7522,lwr_k=500:15.1169,lwr_k=600:15.4631,lwr_k=700:15.6898,lwr_k=800:15.9412,lwr_k=900:16.1913,lwr_k=1000:16.4622'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:14.7097,lwr_k=10:29.1724,lwr_k=20:14.1287,lwr_k=30:13.3204,lwr_k=40:12.8275,lwr_k=50:12.3939,lwr_k=100:11.5322,lwr_k=200:11.1117,lwr_k=300:11.3432,lwr_k=400:11.5797,lwr_k=500:11.5986,lwr_k=600:11.7057,lwr_k=700:11.7851,lwr_k=800:11.9101,lwr_k=900:12.0989,lwr_k=1000:12.1403'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:18.2399,lwr_k=10:5.4518,lwr_k=20:8.5342,lwr_k=30:9.8663,lwr_k=40:10.7207,lwr_k=50:11.4042,lwr_k=100:12.6618,lwr_k=200:13.7153,lwr_k=300:14.4007,lwr_k=400:14.7565,lwr_k=500:15.0171,lwr_k=600:15.2322,lwr_k=700:15.3873,lwr_k=800:15.5462,lwr_k=900:15.648,lwr_k=1000:15.7386'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:11.5258,lwr_k=10:28.8179,lwr_k=20:16.4519,lwr_k=30:11.8336,lwr_k=40:11.437,lwr_k=50:10.977,lwr_k=100:9.9925,lwr_k=200:9.5214,lwr_k=300:9.5482,lwr_k=400:9.517,lwr_k=500:9.5453,lwr_k=600:9.5754,lwr_k=700:9.6275,lwr_k=800:9.6037,lwr_k=900:9.6432,lwr_k=1000:9.741'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:25.6385,lwr_k=10:5.1087,lwr_k=20:9.0801,lwr_k=30:10.4825,lwr_k=40:11.654,lwr_k=50:12.7905,lwr_k=100:14.5742,lwr_k=200:16.0479,lwr_k=300:16.9797,lwr_k=400:17.4571,lwr_k=500:18.0008,lwr_k=600:18.4659,lwr_k=700:18.8647,lwr_k=800:19.2109,lwr_k=900:19.4654,lwr_k=1000:19.7386'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:20.1477,lwr_k=10:53.5253,lwr_k=20:17.0384,lwr_k=30:14.6812,lwr_k=40:14.3892,lwr_k=50:14.9353,lwr_k=100:14.1945,lwr_k=200:14.0585,lwr_k=300:14.2235,lwr_k=400:14.4057,lwr_k=500:14.5536,lwr_k=600:14.6316,lwr_k=700:14.8577,lwr_k=800:15.0087,lwr_k=900:15.2028,lwr_k=1000:15.3184'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:83.6555,lwr_k=10:14.3278,lwr_k=20:24.4338,lwr_k=30:28.9473,lwr_k=40:30.8699,lwr_k=50:32.1591,lwr_k=100:38.4653,lwr_k=200:43.2653,lwr_k=300:47.5185,lwr_k=400:50.7925,lwr_k=500:53.369,lwr_k=600:55.6565,lwr_k=700:57.1213,lwr_k=800:58.3061,lwr_k=900:59.7621,lwr_k=1000:60.9993'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:147.6893,lwr_k=10:101.8019,lwr_k=20:72.5277,lwr_k=30:63.7267,lwr_k=40:71.147,lwr_k=50:68.6078,lwr_k=100:67.3255,lwr_k=200:73.6988,lwr_k=300:76.9321,lwr_k=400:82.6343,lwr_k=500:89.1347,lwr_k=600:94.8738,lwr_k=700:98.714,lwr_k=800:101.1671,lwr_k=900:104.1134,lwr_k=1000:106.8573'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:14.2881,lwr_k=10:4.5326,lwr_k=20:7.3991,lwr_k=30:8.2534,lwr_k=40:8.7474,lwr_k=50:9.3544,lwr_k=100:10.1442,lwr_k=200:10.8883,lwr_k=300:11.3453,lwr_k=400:11.633,lwr_k=500:11.7586,lwr_k=600:11.8594,lwr_k=700:11.9798,lwr_k=800:12.0609,lwr_k=900:12.1372,lwr_k=1000:12.2168'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:15.0461,lwr_k=10:181.643,lwr_k=20:60.8843,lwr_k=30:15.1744,lwr_k=40:14.1755,lwr_k=50:14.0932,lwr_k=100:13.6096,lwr_k=200:13.541,lwr_k=300:13.6423,lwr_k=400:13.749,lwr_k=500:13.7651,lwr_k=600:13.7912,lwr_k=700:13.8396,lwr_k=800:13.8944,lwr_k=900:13.9211,lwr_k=1000:13.8933'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_90'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:35.3256,lwr_k=10:16.255,lwr_k=20:16.5895,lwr_k=30:16.9607,lwr_k=40:16.8514,lwr_k=50:17.1471,lwr_k=100:21.0907,lwr_k=200:24.0982,lwr_k=300:25.5128,lwr_k=400:26.6237,lwr_k=500:27.4444,lwr_k=600:28.0976,lwr_k=700:28.7416,lwr_k=800:29.2432,lwr_k=900:29.8677,lwr_k=1000:30.3364'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:54.9276,lwr_k=10:23.2188,lwr_k=20:24.6403,lwr_k=30:26.0984,lwr_k=40:25.014,lwr_k=50:24.0851,lwr_k=100:27.3885,lwr_k=200:33.0211,lwr_k=300:35.7122,lwr_k=400:38.1316,lwr_k=500:40.284,lwr_k=600:41.9088,lwr_k=700:43.2522,lwr_k=800:44.4418,lwr_k=900:45.6483,lwr_k=1000:46.6261'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.0657,lwr_k=10:0.1478,lwr_k=20:2.0982,lwr_k=30:2.9453,lwr_k=40:3.6363,lwr_k=50:4.125,lwr_k=100:5.3558,lwr_k=200:6.2198,lwr_k=300:6.4985,lwr_k=400:6.677,lwr_k=500:6.8285,lwr_k=600:6.8905,lwr_k=700:6.947,lwr_k=800:7.0174,lwr_k=900:7.0579,lwr_k=1000:7.0724'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.0267,lwr_k=10:6561.0529,lwr_k=20:19.9451,lwr_k=30:2917.4728,lwr_k=40:7.7794,lwr_k=50:6.9767,lwr_k=100:6.6021,lwr_k=200:6.7428,lwr_k=300:6.8666,lwr_k=400:6.9181,lwr_k=500:6.9464,lwr_k=600:7.0092,lwr_k=700:7.0643,lwr_k=800:7.0884,lwr_k=900:7.1038,lwr_k=1000:7.1342'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:13.8122,lwr_k=10:0.5101,lwr_k=20:4.2367,lwr_k=30:5.9611,lwr_k=40:7.6317,lwr_k=50:8.6808,lwr_k=100:10.6768,lwr_k=200:11.7926,lwr_k=300:12.0679,lwr_k=400:12.2967,lwr_k=500:12.404,lwr_k=600:12.4904,lwr_k=700:12.6044,lwr_k=800:12.6839,lwr_k=900:12.7159,lwr_k=1000:12.7961'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.5489,lwr_k=10:793.1217,lwr_k=20:14.1725,lwr_k=30:9.385,lwr_k=40:8.7972,lwr_k=50:8.2036,lwr_k=100:7.8976,lwr_k=200:7.7417,lwr_k=300:7.7855,lwr_k=400:7.8882,lwr_k=500:7.8947,lwr_k=600:7.9678,lwr_k=700:8.0362,lwr_k=800:8.0742,lwr_k=900:8.1073,lwr_k=1000:8.1162'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.2709,lwr_k=10:0.1567,lwr_k=20:2.5738,lwr_k=30:3.5185,lwr_k=40:4.1099,lwr_k=50:4.3948,lwr_k=100:5.4556,lwr_k=200:5.9465,lwr_k=300:6.1486,lwr_k=400:6.3456,lwr_k=500:6.466,lwr_k=600:6.5452,lwr_k=700:6.6178,lwr_k=800:6.6639,lwr_k=900:6.6995,lwr_k=1000:6.7328'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.7901,lwr_k=10:2259.157,lwr_k=20:46.4466,lwr_k=30:13.7353,lwr_k=40:9.9513,lwr_k=50:9.4971,lwr_k=100:8.7148,lwr_k=200:8.4146,lwr_k=300:8.4137,lwr_k=400:8.4038,lwr_k=500:8.43,lwr_k=600:8.6186,lwr_k=700:8.6226,lwr_k=800:8.5893,lwr_k=900:8.6375,lwr_k=1000:8.6263'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.5614,lwr_k=10:1.4111,lwr_k=20:2.9133,lwr_k=30:3.5034,lwr_k=40:3.7213,lwr_k=50:3.891,lwr_k=100:4.279,lwr_k=200:4.5438,lwr_k=300:4.6339,lwr_k=400:4.6991,lwr_k=500:4.7871,lwr_k=600:4.8369,lwr_k=700:4.8667,lwr_k=800:4.8995,lwr_k=900:4.9266,lwr_k=1000:4.9847'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:20.4289,lwr_k=10:112.7984,lwr_k=20:2067298843.8056,lwr_k=30:198.7898,lwr_k=40:181.5377,lwr_k=50:182.2778,lwr_k=100:20.5329,lwr_k=200:20.8661,lwr_k=300:19.4773,lwr_k=400:19.3527,lwr_k=500:19.4146,lwr_k=600:19.4934,lwr_k=700:19.612,lwr_k=800:19.6727,lwr_k=900:19.6566,lwr_k=1000:19.7673'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:5.9358,lwr_k=10:0.636,lwr_k=20:2.3537,lwr_k=30:2.8814,lwr_k=40:3.198,lwr_k=50:3.4814,lwr_k=100:4.0964,lwr_k=200:4.5206,lwr_k=300:4.747,lwr_k=400:4.869,lwr_k=500:4.9612,lwr_k=600:5.0313,lwr_k=700:5.1089,lwr_k=800:5.1617,lwr_k=900:5.2127,lwr_k=1000:5.2575'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.7378,lwr_k=10:228.5918,lwr_k=20:17.7315,lwr_k=30:9.4559,lwr_k=40:7.917,lwr_k=50:8.2391,lwr_k=100:7.7422,lwr_k=200:7.782,lwr_k=300:9.102,lwr_k=400:7.9279,lwr_k=500:7.939,lwr_k=600:7.9984,lwr_k=700:8.04,lwr_k=800:8.09,lwr_k=900:8.0868,lwr_k=1000:8.1414'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_91'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:22.0397,lwr_k=10:0.0,lwr_k=20:1.0567,lwr_k=30:3.4925,lwr_k=40:4.8104,lwr_k=50:5.9479,lwr_k=100:7.8287,lwr_k=200:9.2996,lwr_k=300:10.14,lwr_k=400:10.7779,lwr_k=500:11.2229,lwr_k=600:11.6267,lwr_k=700:11.9736,lwr_k=800:12.2587,lwr_k=900:12.5655,lwr_k=1000:12.9292'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:28.6224,lwr_k=10:176.0262,lwr_k=20:79.7655,lwr_k=30:34.6497,lwr_k=40:20.2151,lwr_k=50:15.3836,lwr_k=100:13.3112,lwr_k=200:13.3403,lwr_k=300:14.0443,lwr_k=400:14.4493,lwr_k=500:15.1923,lwr_k=600:15.6934,lwr_k=700:16.083,lwr_k=800:16.4682,lwr_k=900:16.8559,lwr_k=1000:17.4103'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.4036,lwr_k=10:0.0,lwr_k=20:1.1117,lwr_k=30:3.5685,lwr_k=40:4.9859,lwr_k=50:5.7849,lwr_k=100:7.7081,lwr_k=200:9.1012,lwr_k=300:9.6176,lwr_k=400:10.0067,lwr_k=500:10.1837,lwr_k=600:10.3528,lwr_k=700:10.4622,lwr_k=800:10.503,lwr_k=900:10.5721,lwr_k=1000:10.6423'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.5993,lwr_k=10:201.1247,lwr_k=20:6541.2827,lwr_k=30:116.1819,lwr_k=40:82.6699,lwr_k=50:38.7177,lwr_k=100:11.1267,lwr_k=200:9.8573,lwr_k=300:9.7811,lwr_k=400:9.7656,lwr_k=500:9.9292,lwr_k=600:9.9802,lwr_k=700:10.066,lwr_k=800:10.0891,lwr_k=900:10.0779,lwr_k=1000:10.1739'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:20.0638,lwr_k=10:0.0,lwr_k=20:1.5489,lwr_k=30:4.5799,lwr_k=40:6.7189,lwr_k=50:8.1716,lwr_k=100:11.8848,lwr_k=200:13.9828,lwr_k=300:14.737,lwr_k=400:15.2753,lwr_k=500:15.5728,lwr_k=600:15.9022,lwr_k=700:16.1005,lwr_k=800:16.3132,lwr_k=900:16.5012,lwr_k=1000:16.6939'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:13.7483,lwr_k=10:151.6054,lwr_k=20:138.5826,lwr_k=30:304558.8812,lwr_k=40:20.7149,lwr_k=50:26.2782,lwr_k=100:11.9788,lwr_k=200:11.2124,lwr_k=300:11.3504,lwr_k=400:11.3588,lwr_k=500:10.789,lwr_k=600:10.8503,lwr_k=700:10.9313,lwr_k=800:11.0011,lwr_k=900:11.1059,lwr_k=1000:11.27'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:27.3129,lwr_k=10:0.0,lwr_k=20:1.7582,lwr_k=30:5.2445,lwr_k=40:7.1801,lwr_k=50:9.2174,lwr_k=100:13.3972,lwr_k=200:15.5766,lwr_k=300:16.3911,lwr_k=400:17.3563,lwr_k=500:18.1277,lwr_k=600:18.5611,lwr_k=700:19.1811,lwr_k=800:19.9359,lwr_k=900:20.4664,lwr_k=1000:21.3783'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:21.1304,lwr_k=10:185.1812,lwr_k=20:201.2352,lwr_k=30:30.8648,lwr_k=40:20.5111,lwr_k=50:20.6986,lwr_k=100:15.7222,lwr_k=200:15.2286,lwr_k=300:15.6736,lwr_k=400:15.506,lwr_k=500:16.2089,lwr_k=600:16.3859,lwr_k=700:16.6544,lwr_k=800:16.8515,lwr_k=900:16.8787,lwr_k=1000:17.1658'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.3014,lwr_k=10:0.0,lwr_k=20:0.9667,lwr_k=30:2.8797,lwr_k=40:3.927,lwr_k=50:4.5879,lwr_k=100:6.0759,lwr_k=200:7.0656,lwr_k=300:7.5264,lwr_k=400:7.7697,lwr_k=500:8.1125,lwr_k=600:8.3098,lwr_k=700:8.4841,lwr_k=800:8.6472,lwr_k=900:8.8551,lwr_k=1000:8.9589'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:30.9428,lwr_k=10:104.5592,lwr_k=20:166.8551,lwr_k=30:47.7142,lwr_k=40:33.4838,lwr_k=50:31.3278,lwr_k=100:23.6235,lwr_k=200:24.645,lwr_k=300:24.8678,lwr_k=400:25.2915,lwr_k=500:25.7605,lwr_k=600:26.4704,lwr_k=700:26.8417,lwr_k=800:26.7459,lwr_k=900:26.6742,lwr_k=1000:26.7842'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:9.4795,lwr_k=10:0.0,lwr_k=20:1.0666,lwr_k=30:3.0208,lwr_k=40:4.4101,lwr_k=50:5.137,lwr_k=100:6.6434,lwr_k=200:7.6617,lwr_k=300:7.9861,lwr_k=400:8.2348,lwr_k=500:8.3973,lwr_k=600:8.5224,lwr_k=700:8.6298,lwr_k=800:8.6831,lwr_k=900:8.7514,lwr_k=1000:8.7964'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:14.004,lwr_k=10:1175.1798,lwr_k=20:621.7956,lwr_k=30:735.7986,lwr_k=40:31.1239,lwr_k=50:112.7281,lwr_k=100:17.3578,lwr_k=200:20.7727,lwr_k=300:15.4749,lwr_k=400:14.9505,lwr_k=500:12.8704,lwr_k=600:12.8608,lwr_k=700:13.9263,lwr_k=800:14.0541,lwr_k=900:13.7246,lwr_k=1000:13.6851'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_92'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.7263,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.7719,lwr_k=40:1.5589,lwr_k=50:2.3386,lwr_k=100:3.9608,lwr_k=200:4.7136,lwr_k=300:4.9749,lwr_k=400:5.0899,lwr_k=500:5.1676,lwr_k=600:5.2209,lwr_k=700:5.2608,lwr_k=800:5.3158,lwr_k=900:5.3557,lwr_k=1000:5.3923'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.6632,lwr_k=10:61.3157,lwr_k=20:129.0956,lwr_k=30:377.5267,lwr_k=40:32.5824,lwr_k=50:19.4098,lwr_k=100:9.6516,lwr_k=200:8.3155,lwr_k=300:8.2636,lwr_k=400:8.0778,lwr_k=500:8.0767,lwr_k=600:8.1881,lwr_k=700:8.2034,lwr_k=800:8.233,lwr_k=900:8.2777,lwr_k=1000:8.3114'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.8374,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.7469,lwr_k=40:1.9069,lwr_k=50:2.9725,lwr_k=100:5.3496,lwr_k=200:7.12,lwr_k=300:7.8285,lwr_k=400:8.2454,lwr_k=500:8.52,lwr_k=600:8.7262,lwr_k=700:8.9061,lwr_k=800:9.0899,lwr_k=900:9.2035,lwr_k=1000:9.2722'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.955,lwr_k=10:32.9758,lwr_k=20:115.5498,lwr_k=30:153.8637,lwr_k=40:24.1088,lwr_k=50:13.9572,lwr_k=100:8.4491,lwr_k=200:7.3674,lwr_k=300:7.1681,lwr_k=400:7.0252,lwr_k=500:7.1674,lwr_k=600:7.2049,lwr_k=700:7.1742,lwr_k=800:7.2762,lwr_k=900:7.2662,lwr_k=1000:7.2819'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.4554,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:3.7939,lwr_k=40:2.5539,lwr_k=50:2.992,lwr_k=100:4.5509,lwr_k=200:5.6511,lwr_k=300:5.9737,lwr_k=400:6.1429,lwr_k=500:6.3459,lwr_k=600:6.6471,lwr_k=700:6.9352,lwr_k=800:7.1065,lwr_k=900:7.3964,lwr_k=1000:7.646'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.9974,lwr_k=10:179.6403,lwr_k=20:169.1328,lwr_k=30:1481.82,lwr_k=40:22.0178,lwr_k=50:32.1017,lwr_k=100:8.3355,lwr_k=200:6.54,lwr_k=300:6.4692,lwr_k=400:6.0541,lwr_k=500:6.052,lwr_k=600:6.1266,lwr_k=700:6.1176,lwr_k=800:6.1809,lwr_k=900:6.2061,lwr_k=1000:6.2104'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.0118,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.4942,lwr_k=40:1.8113,lwr_k=50:2.5732,lwr_k=100:3.8033,lwr_k=200:4.7229,lwr_k=300:5.2173,lwr_k=400:5.5261,lwr_k=500:5.8104,lwr_k=600:6.0746,lwr_k=700:6.2258,lwr_k=800:6.3677,lwr_k=900:6.4811,lwr_k=1000:6.5638'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.424,lwr_k=10:24.1208,lwr_k=20:109.0407,lwr_k=30:294.9236,lwr_k=40:19.5766,lwr_k=50:14.8372,lwr_k=100:8.2367,lwr_k=200:7.628,lwr_k=300:7.7172,lwr_k=400:7.9248,lwr_k=500:7.9421,lwr_k=600:7.9563,lwr_k=700:7.9644,lwr_k=800:8.0719,lwr_k=900:8.0617,lwr_k=1000:8.0682'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.7435,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.4981,lwr_k=40:1.6638,lwr_k=50:2.1371,lwr_k=100:3.3301,lwr_k=200:3.9356,lwr_k=300:4.172,lwr_k=400:4.2771,lwr_k=500:4.3657,lwr_k=600:4.4278,lwr_k=700:4.4796,lwr_k=800:4.5225,lwr_k=900:4.5375,lwr_k=1000:4.5505'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:19.7151,lwr_k=10:61.1625,lwr_k=20:5203.4968,lwr_k=30:2471.9216,lwr_k=40:35.0247,lwr_k=50:30.3298,lwr_k=100:21.5764,lwr_k=200:17.0471,lwr_k=300:17.4701,lwr_k=400:17.3586,lwr_k=500:17.7905,lwr_k=600:18.0233,lwr_k=700:18.1785,lwr_k=800:18.2498,lwr_k=900:18.2404,lwr_k=1000:18.442'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:6.7749,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.3851,lwr_k=40:1.4905,lwr_k=50:2.1085,lwr_k=100:3.3925,lwr_k=200:4.3817,lwr_k=300:4.7946,lwr_k=400:5.0852,lwr_k=500:5.2645,lwr_k=600:5.3945,lwr_k=700:5.5018,lwr_k=800:5.5963,lwr_k=900:5.6751,lwr_k=1000:5.7362'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.4161,lwr_k=10:21.5036,lwr_k=20:107.0414,lwr_k=30:359.6478,lwr_k=40:58.5395,lwr_k=50:28.5279,lwr_k=100:8.6556,lwr_k=200:7.4467,lwr_k=300:7.106,lwr_k=400:7.496,lwr_k=500:7.5469,lwr_k=600:7.6232,lwr_k=700:7.6759,lwr_k=800:7.6409,lwr_k=900:7.64,lwr_k=1000:7.7229'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_93'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:3.5459,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0001,lwr_k=50:0.1173,lwr_k=100:1.5116,lwr_k=200:2.2178,lwr_k=300:2.5327,lwr_k=400:2.7275,lwr_k=500:2.8639,lwr_k=600:2.9438,lwr_k=700:3.0322,lwr_k=800:3.1083,lwr_k=900:3.1362,lwr_k=1000:3.17'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.0892,lwr_k=10:19.2732,lwr_k=20:21.6993,lwr_k=30:54.2975,lwr_k=40:243.9236,lwr_k=50:242.1889,lwr_k=100:47.9538,lwr_k=200:15.7152,lwr_k=300:8.9061,lwr_k=400:8.0635,lwr_k=500:7.7335,lwr_k=600:7.8034,lwr_k=700:7.7646,lwr_k=800:7.8431,lwr_k=900:7.7941,lwr_k=1000:7.7761'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:26.0694,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.001,lwr_k=40:0.072,lwr_k=50:0.7032,lwr_k=100:4.6303,lwr_k=200:8.2202,lwr_k=300:10.5224,lwr_k=400:11.4815,lwr_k=500:12.5888,lwr_k=600:13.52,lwr_k=700:14.3686,lwr_k=800:14.8936,lwr_k=900:15.5037,lwr_k=1000:15.9253'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:19.6332,lwr_k=10:31.8003,lwr_k=20:65.113,lwr_k=30:591.6257,lwr_k=40:226.7791,lwr_k=50:465.4692,lwr_k=100:27.5967,lwr_k=200:15.0216,lwr_k=300:11.9212,lwr_k=400:11.9761,lwr_k=500:11.2103,lwr_k=600:11.1767,lwr_k=700:11.3713,lwr_k=800:11.4688,lwr_k=900:11.5171,lwr_k=1000:11.5908'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:15.2437,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.1158,lwr_k=50:1.3303,lwr_k=100:3.8719,lwr_k=200:6.6371,lwr_k=300:8.3475,lwr_k=400:9.124,lwr_k=500:9.7801,lwr_k=600:10.3504,lwr_k=700:10.6477,lwr_k=800:11.0088,lwr_k=900:11.1769,lwr_k=1000:11.3594'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.4776,lwr_k=10:18.1906,lwr_k=20:35.4038,lwr_k=30:465.3219,lwr_k=40:370.883,lwr_k=50:4068.0721,lwr_k=100:56.0565,lwr_k=200:9.5825,lwr_k=300:8.4823,lwr_k=400:8.1023,lwr_k=500:8.0752,lwr_k=600:7.9746,lwr_k=700:8.1199,lwr_k=800:8.1427,lwr_k=900:8.1617,lwr_k=1000:8.0586'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.7023,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0229,lwr_k=50:0.1708,lwr_k=100:2.3477,lwr_k=200:3.749,lwr_k=300:4.1898,lwr_k=400:4.5356,lwr_k=500:4.6846,lwr_k=600:4.815,lwr_k=700:4.8657,lwr_k=800:4.9366,lwr_k=900:4.9675,lwr_k=1000:5.0299'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.9873,lwr_k=10:22.1676,lwr_k=20:42.7559,lwr_k=30:213.5359,lwr_k=40:167.0837,lwr_k=50:570.3583,lwr_k=100:38.7422,lwr_k=200:10.1153,lwr_k=300:8.6527,lwr_k=400:8.6444,lwr_k=500:8.5765,lwr_k=600:8.1999,lwr_k=700:8.2278,lwr_k=800:8.2102,lwr_k=900:8.2417,lwr_k=1000:8.2259'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:3.7095,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0543,lwr_k=50:0.1841,lwr_k=100:1.7635,lwr_k=200:2.6885,lwr_k=300:2.9817,lwr_k=400:3.0984,lwr_k=500:3.2234,lwr_k=600:3.2798,lwr_k=700:3.3455,lwr_k=800:3.4035,lwr_k=900:3.4259,lwr_k=1000:3.4586'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:18.3624,lwr_k=10:18.7601,lwr_k=20:23.8814,lwr_k=30:147.7947,lwr_k=40:167.9037,lwr_k=50:661.9331,lwr_k=100:18.5663,lwr_k=200:16.9604,lwr_k=300:17.4224,lwr_k=400:17.7639,lwr_k=500:18.1035,lwr_k=600:18.2534,lwr_k=700:18.2204,lwr_k=800:18.2872,lwr_k=900:18.2778,lwr_k=1000:18.3002'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:6.7448,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0578,lwr_k=50:0.1854,lwr_k=100:2.5189,lwr_k=200:3.9682,lwr_k=300:4.4329,lwr_k=400:4.6548,lwr_k=500:4.8739,lwr_k=600:5.014,lwr_k=700:5.1478,lwr_k=800:5.2907,lwr_k=900:5.381,lwr_k=1000:5.4711'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.3259,lwr_k=10:26.8326,lwr_k=20:40.9579,lwr_k=30:116.336,lwr_k=40:345.1041,lwr_k=50:334.3757,lwr_k=100:25.4135,lwr_k=200:9.2294,lwr_k=300:9.032,lwr_k=400:8.9003,lwr_k=500:9.0252,lwr_k=600:9.0555,lwr_k=700:9.3048,lwr_k=800:9.492,lwr_k=900:9.6356,lwr_k=1000:9.7669'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_94'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.1289,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.5174,lwr_k=40:1.5367,lwr_k=50:2.0123,lwr_k=100:2.97,lwr_k=200:3.4968,lwr_k=300:3.6453,lwr_k=400:3.7245,lwr_k=500:3.8103,lwr_k=600:3.867,lwr_k=700:3.9029,lwr_k=800:3.9291,lwr_k=900:3.9439,lwr_k=1000:3.9501'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.1717,lwr_k=10:44.9212,lwr_k=20:66.1372,lwr_k=30:63.5012,lwr_k=40:14.2486,lwr_k=50:9.6957,lwr_k=100:6.8477,lwr_k=200:6.9767,lwr_k=300:6.7422,lwr_k=400:6.8519,lwr_k=500:6.909,lwr_k=600:6.9249,lwr_k=700:6.9817,lwr_k=800:6.9927,lwr_k=900:7.0005,lwr_k=1000:7.0254'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.4683,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.6102,lwr_k=40:1.5668,lwr_k=50:2.0792,lwr_k=100:3.1143,lwr_k=200:3.9522,lwr_k=300:4.3747,lwr_k=400:4.6025,lwr_k=500:4.7507,lwr_k=600:4.8854,lwr_k=700:4.963,lwr_k=800:5.0232,lwr_k=900:5.0831,lwr_k=1000:5.1738'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.216,lwr_k=10:42.2799,lwr_k=20:101.8817,lwr_k=30:72.016,lwr_k=40:17.3926,lwr_k=50:11.909,lwr_k=100:7.1765,lwr_k=200:5.8234,lwr_k=300:5.7358,lwr_k=400:5.7443,lwr_k=500:5.8613,lwr_k=600:6.0088,lwr_k=700:6.0502,lwr_k=800:6.0688,lwr_k=900:6.087,lwr_k=1000:6.1302'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.5685,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.6492,lwr_k=40:1.5605,lwr_k=50:2.1444,lwr_k=100:3.7508,lwr_k=200:4.8156,lwr_k=300:5.1828,lwr_k=400:5.508,lwr_k=500:5.72,lwr_k=600:5.8903,lwr_k=700:6.0064,lwr_k=800:6.1224,lwr_k=900:6.1874,lwr_k=1000:6.2685'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.9061,lwr_k=10:24.073,lwr_k=20:40.8117,lwr_k=30:34.7788,lwr_k=40:11.2316,lwr_k=50:9.182,lwr_k=100:5.884,lwr_k=200:5.2935,lwr_k=300:5.3146,lwr_k=400:5.3602,lwr_k=500:5.4591,lwr_k=600:5.5141,lwr_k=700:5.6067,lwr_k=800:5.6909,lwr_k=900:5.7173,lwr_k=1000:5.7344'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.2506,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.6085,lwr_k=40:1.5583,lwr_k=50:2.1459,lwr_k=100:3.4053,lwr_k=200:4.4396,lwr_k=300:4.9076,lwr_k=400:5.2427,lwr_k=500:5.5206,lwr_k=600:5.6599,lwr_k=700:5.8013,lwr_k=800:5.9055,lwr_k=900:5.9784,lwr_k=1000:6.0708'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.7546,lwr_k=10:24.5683,lwr_k=20:49.6497,lwr_k=30:64.0482,lwr_k=40:17.137,lwr_k=50:10.9572,lwr_k=100:7.1098,lwr_k=200:6.8728,lwr_k=300:6.7423,lwr_k=400:6.7275,lwr_k=500:6.584,lwr_k=600:6.5781,lwr_k=700:6.5356,lwr_k=800:6.5138,lwr_k=900:6.5262,lwr_k=1000:6.5204'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:3.6524,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.6421,lwr_k=40:1.4808,lwr_k=50:1.7495,lwr_k=100:2.5881,lwr_k=200:2.9772,lwr_k=300:3.1452,lwr_k=400:3.2271,lwr_k=500:3.3015,lwr_k=600:3.3484,lwr_k=700:3.3739,lwr_k=800:3.4025,lwr_k=900:3.4339,lwr_k=1000:3.458'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:15.2996,lwr_k=10:19.0535,lwr_k=20:96.041,lwr_k=30:133.7038,lwr_k=40:24.7213,lwr_k=50:20.4131,lwr_k=100:15.9774,lwr_k=200:14.3709,lwr_k=300:14.5749,lwr_k=400:14.642,lwr_k=500:14.6938,lwr_k=600:14.5428,lwr_k=700:14.586,lwr_k=800:14.6221,lwr_k=900:14.6023,lwr_k=1000:14.6591'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:5.1032,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.5314,lwr_k=40:1.2984,lwr_k=50:1.8876,lwr_k=100:2.9494,lwr_k=200:3.5235,lwr_k=300:3.7642,lwr_k=400:3.8692,lwr_k=500:3.9787,lwr_k=600:4.0425,lwr_k=700:4.1354,lwr_k=800:4.1734,lwr_k=900:4.2097,lwr_k=1000:4.2435'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.1898,lwr_k=10:27.0122,lwr_k=20:48.5068,lwr_k=30:34.4425,lwr_k=40:13.7628,lwr_k=50:11.7324,lwr_k=100:9.132,lwr_k=200:8.266,lwr_k=300:7.8563,lwr_k=400:7.9067,lwr_k=500:7.8565,lwr_k=600:8.0677,lwr_k=700:8.1725,lwr_k=800:8.2482,lwr_k=900:8.3466,lwr_k=1000:8.3544'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_95'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.4425,lwr_k=10:0.0283,lwr_k=20:0.8129,lwr_k=30:2.5174,lwr_k=40:3.7572,lwr_k=50:4.4742,lwr_k=100:6.3498,lwr_k=200:7.4961,lwr_k=300:7.9459,lwr_k=400:8.3021,lwr_k=500:8.5774,lwr_k=600:8.8275,lwr_k=700:8.9501,lwr_k=800:9.0731,lwr_k=900:9.1876,lwr_k=1000:9.3033'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:15.2196,lwr_k=10:61.499,lwr_k=20:5308.2007,lwr_k=30:30.9327,lwr_k=40:92.0024,lwr_k=50:14.0576,lwr_k=100:20.0446,lwr_k=200:10.9178,lwr_k=300:11.0805,lwr_k=400:11.2472,lwr_k=500:11.4798,lwr_k=600:11.6629,lwr_k=700:11.9225,lwr_k=800:12.0109,lwr_k=900:12.1723,lwr_k=1000:12.3656'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:102.5667,lwr_k=10:0.0,lwr_k=20:4.8389,lwr_k=30:12.9949,lwr_k=40:18.7054,lwr_k=50:21.2191,lwr_k=100:30.6562,lwr_k=200:37.5264,lwr_k=300:42.7745,lwr_k=400:46.3871,lwr_k=500:49.7705,lwr_k=600:53.1773,lwr_k=700:56.2839,lwr_k=800:59.1526,lwr_k=900:61.1142,lwr_k=1000:63.4733'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:86.3172,lwr_k=10:80.7301,lwr_k=20:1794.9978,lwr_k=30:271.774,lwr_k=40:39.5969,lwr_k=50:35.0397,lwr_k=100:29.0634,lwr_k=200:31.0171,lwr_k=300:34.5132,lwr_k=400:36.7793,lwr_k=500:39.2694,lwr_k=600:41.7572,lwr_k=700:43.6878,lwr_k=800:45.3713,lwr_k=900:45.9222,lwr_k=1000:47.1438'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:25.752,lwr_k=10:0.0,lwr_k=20:0.427,lwr_k=30:2.5933,lwr_k=40:4.3017,lwr_k=50:5.7598,lwr_k=100:9.0583,lwr_k=200:12.3277,lwr_k=300:13.5372,lwr_k=400:14.5299,lwr_k=500:15.2773,lwr_k=600:15.7499,lwr_k=700:16.2148,lwr_k=800:16.7266,lwr_k=900:17.071,lwr_k=1000:17.5143'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:16.8973,lwr_k=10:52.9367,lwr_k=20:24214.8292,lwr_k=30:51.2199,lwr_k=40:20.892,lwr_k=50:16.0195,lwr_k=100:11.7281,lwr_k=200:11.7002,lwr_k=300:11.547,lwr_k=400:11.2869,lwr_k=500:11.1755,lwr_k=600:11.4105,lwr_k=700:11.502,lwr_k=800:11.6655,lwr_k=900:11.7222,lwr_k=1000:11.8366'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:20.7525,lwr_k=10:0.0,lwr_k=20:0.6476,lwr_k=30:3.4099,lwr_k=40:4.5876,lwr_k=50:5.6228,lwr_k=100:8.1088,lwr_k=200:10.6048,lwr_k=300:11.7339,lwr_k=400:12.5414,lwr_k=500:13.0513,lwr_k=600:13.5382,lwr_k=700:13.9609,lwr_k=800:14.3876,lwr_k=900:14.7075,lwr_k=1000:14.9631'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:17.4787,lwr_k=10:41.5163,lwr_k=20:12626.1496,lwr_k=30:34.0216,lwr_k=40:19.7835,lwr_k=50:17.6943,lwr_k=100:13.5867,lwr_k=200:12.7555,lwr_k=300:12.2517,lwr_k=400:12.2589,lwr_k=500:12.5902,lwr_k=600:12.8137,lwr_k=700:12.9374,lwr_k=800:13.1425,lwr_k=900:13.277,lwr_k=1000:13.3766'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:83.1052,lwr_k=10:0.0,lwr_k=20:3.4936,lwr_k=30:8.8352,lwr_k=40:12.3995,lwr_k=50:14.8137,lwr_k=100:20.2965,lwr_k=200:26.2768,lwr_k=300:30.7024,lwr_k=400:33.8625,lwr_k=500:36.7627,lwr_k=600:39.0107,lwr_k=700:41.291,lwr_k=800:43.7614,lwr_k=900:45.8475,lwr_k=1000:47.8585'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:148.1485,lwr_k=10:170.9389,lwr_k=20:770.5345,lwr_k=30:87.2957,lwr_k=40:67.8245,lwr_k=50:68.5969,lwr_k=100:65.807,lwr_k=200:70.6582,lwr_k=300:74.7188,lwr_k=400:77.8213,lwr_k=500:82.6217,lwr_k=600:86.569,lwr_k=700:90.9272,lwr_k=800:95.5152,lwr_k=900:98.9405,lwr_k=1000:101.8781'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:25.2376,lwr_k=10:0.0001,lwr_k=20:1.3295,lwr_k=30:4.0533,lwr_k=40:5.6181,lwr_k=50:6.8958,lwr_k=100:9.9346,lwr_k=200:11.93,lwr_k=300:12.9908,lwr_k=400:13.8553,lwr_k=500:14.4239,lwr_k=600:14.8418,lwr_k=700:15.2574,lwr_k=800:15.598,lwr_k=900:15.9409,lwr_k=1000:16.2926'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:22.9282,lwr_k=10:50.3255,lwr_k=20:1831.541,lwr_k=30:25.7835,lwr_k=40:20.6417,lwr_k=50:22.8456,lwr_k=100:14.4063,lwr_k=200:15.087,lwr_k=300:15.2578,lwr_k=400:15.7448,lwr_k=500:15.7824,lwr_k=600:15.9107,lwr_k=700:15.9453,lwr_k=800:16.0223,lwr_k=900:16.0592,lwr_k=1000:16.2829'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_96'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.8422,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.8948,lwr_k=40:1.8716,lwr_k=50:2.4043,lwr_k=100:3.6881,lwr_k=200:4.3552,lwr_k=300:4.613,lwr_k=400:4.7667,lwr_k=500:4.9198,lwr_k=600:4.9675,lwr_k=700:5.0448,lwr_k=800:5.1056,lwr_k=900:5.1363,lwr_k=1000:5.1785'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.044,lwr_k=10:71.6556,lwr_k=20:519.5456,lwr_k=30:1049.1861,lwr_k=40:162.1556,lwr_k=50:30.6992,lwr_k=100:10.9739,lwr_k=200:9.6947,lwr_k=300:9.3343,lwr_k=400:9.5233,lwr_k=500:9.286,lwr_k=600:9.2873,lwr_k=700:9.5587,lwr_k=800:9.3284,lwr_k=900:9.3716,lwr_k=1000:9.3275'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.751,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.6182,lwr_k=40:1.7501,lwr_k=50:2.2881,lwr_k=100:3.7932,lwr_k=200:4.8491,lwr_k=300:5.021,lwr_k=400:5.1475,lwr_k=500:5.2402,lwr_k=600:5.3421,lwr_k=700:5.4112,lwr_k=800:5.5061,lwr_k=900:5.5745,lwr_k=1000:5.6212'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.3123,lwr_k=10:59.7684,lwr_k=20:12987.436,lwr_k=30:267.3415,lwr_k=40:139.8604,lwr_k=50:16.8916,lwr_k=100:39.2882,lwr_k=200:6.6129,lwr_k=300:7.0471,lwr_k=400:6.5129,lwr_k=500:6.4701,lwr_k=600:6.4294,lwr_k=700:6.4588,lwr_k=800:6.4838,lwr_k=900:6.4782,lwr_k=1000:6.4938'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.1526,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.7864,lwr_k=40:1.9887,lwr_k=50:2.7324,lwr_k=100:4.2305,lwr_k=200:5.4363,lwr_k=300:5.9256,lwr_k=400:6.2788,lwr_k=500:6.4804,lwr_k=600:6.6025,lwr_k=700:6.7402,lwr_k=800:6.8377,lwr_k=900:6.9403,lwr_k=1000:7.0153'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.7768,lwr_k=10:26.1558,lwr_k=20:213.3423,lwr_k=30:305.7118,lwr_k=40:53.7971,lwr_k=50:17.9848,lwr_k=100:7.553,lwr_k=200:7.1882,lwr_k=300:6.8152,lwr_k=400:6.8175,lwr_k=500:6.8021,lwr_k=600:6.8523,lwr_k=700:6.8276,lwr_k=800:6.8008,lwr_k=900:6.806,lwr_k=1000:6.8057'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.7222,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.7638,lwr_k=40:1.7614,lwr_k=50:2.4537,lwr_k=100:3.9327,lwr_k=200:4.734,lwr_k=300:5.262,lwr_k=400:5.5687,lwr_k=500:5.75,lwr_k=600:5.909,lwr_k=700:6.0402,lwr_k=800:6.1165,lwr_k=900:6.2013,lwr_k=1000:6.2778'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.5549,lwr_k=10:25.7434,lwr_k=20:183.9123,lwr_k=30:548.5086,lwr_k=40:15.5429,lwr_k=50:12.7506,lwr_k=100:7.8982,lwr_k=200:7.9086,lwr_k=300:8.1494,lwr_k=400:8.1702,lwr_k=500:8.2447,lwr_k=600:8.309,lwr_k=700:8.3439,lwr_k=800:8.3687,lwr_k=900:8.4227,lwr_k=1000:8.3956'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.7285,lwr_k=10:0.0,lwr_k=20:0.0004,lwr_k=30:0.9451,lwr_k=40:1.8687,lwr_k=50:2.6595,lwr_k=100:4.3505,lwr_k=200:5.0715,lwr_k=300:5.3967,lwr_k=400:5.5803,lwr_k=500:5.7436,lwr_k=600:5.8838,lwr_k=700:5.9573,lwr_k=800:6.0125,lwr_k=900:6.096,lwr_k=1000:6.1697'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:23.6718,lwr_k=10:138.1667,lwr_k=20:1951.67,lwr_k=30:254.8066,lwr_k=40:198.7224,lwr_k=50:47.9696,lwr_k=100:86.818,lwr_k=200:21.809,lwr_k=300:21.0907,lwr_k=400:20.983,lwr_k=500:21.248,lwr_k=600:21.0305,lwr_k=700:21.2483,lwr_k=800:21.3231,lwr_k=900:21.5054,lwr_k=1000:21.5785'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:7.5361,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.6526,lwr_k=40:1.6183,lwr_k=50:2.2567,lwr_k=100:3.9308,lwr_k=200:5.2412,lwr_k=300:5.7324,lwr_k=400:5.9864,lwr_k=500:6.1801,lwr_k=600:6.2974,lwr_k=700:6.3931,lwr_k=800:6.5016,lwr_k=900:6.5424,lwr_k=1000:6.6279'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.9354,lwr_k=10:25.6649,lwr_k=20:160.819,lwr_k=30:82.0663,lwr_k=40:304.9778,lwr_k=50:25.3769,lwr_k=100:39.6563,lwr_k=200:10.1144,lwr_k=300:11.0747,lwr_k=400:11.2163,lwr_k=500:11.0985,lwr_k=600:11.1027,lwr_k=700:11.0749,lwr_k=800:11.0412,lwr_k=900:11.0668,lwr_k=1000:11.085'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_97'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:18.1333,lwr_k=10:14.2575,lwr_k=20:15.102,lwr_k=30:15.6242,lwr_k=40:15.4894,lwr_k=50:15.5372,lwr_k=100:15.8154,lwr_k=200:15.9394,lwr_k=300:16.1073,lwr_k=400:16.2361,lwr_k=500:16.3945,lwr_k=600:16.5348,lwr_k=700:16.6714,lwr_k=800:16.7498,lwr_k=900:16.8086,lwr_k=1000:16.863'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:23.1997,lwr_k=10:22.0422,lwr_k=20:21.3996,lwr_k=30:21.175,lwr_k=40:20.6271,lwr_k=50:20.3333,lwr_k=100:19.9141,lwr_k=200:19.8945,lwr_k=300:20.2684,lwr_k=400:20.5315,lwr_k=500:21.0179,lwr_k=600:21.35,lwr_k=700:21.556,lwr_k=800:21.7584,lwr_k=900:21.8703,lwr_k=1000:21.8977'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.6268,lwr_k=10:9.4527,lwr_k=20:10.352,lwr_k=30:10.4818,lwr_k=40:10.8594,lwr_k=50:11.0225,lwr_k=100:11.4883,lwr_k=200:11.6434,lwr_k=300:11.6613,lwr_k=400:11.7212,lwr_k=500:11.7384,lwr_k=600:11.7207,lwr_k=700:11.7279,lwr_k=800:11.7347,lwr_k=900:11.764,lwr_k=1000:11.8016'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.4945,lwr_k=10:10.1156,lwr_k=20:9.5937,lwr_k=30:9.4161,lwr_k=40:9.0854,lwr_k=50:9.1454,lwr_k=100:9.0579,lwr_k=200:9.0957,lwr_k=300:9.0644,lwr_k=400:9.0686,lwr_k=500:9.0755,lwr_k=600:9.0885,lwr_k=700:9.0914,lwr_k=800:9.0889,lwr_k=900:9.1519,lwr_k=1000:9.1945'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.892,lwr_k=10:9.4651,lwr_k=20:10.3015,lwr_k=30:10.5134,lwr_k=40:10.7979,lwr_k=50:10.8276,lwr_k=100:11.0976,lwr_k=200:11.1514,lwr_k=300:11.1858,lwr_k=400:11.2067,lwr_k=500:11.2296,lwr_k=600:11.2466,lwr_k=700:11.246,lwr_k=800:11.2823,lwr_k=900:11.3175,lwr_k=1000:11.3225'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.344,lwr_k=10:10.1597,lwr_k=20:9.4687,lwr_k=30:9.3156,lwr_k=40:9.2823,lwr_k=50:9.2379,lwr_k=100:9.2016,lwr_k=200:9.1934,lwr_k=300:9.196,lwr_k=400:9.1466,lwr_k=500:9.1478,lwr_k=600:9.1584,lwr_k=700:9.1322,lwr_k=800:9.0798,lwr_k=900:9.1217,lwr_k=1000:9.1453'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.1665,lwr_k=10:9.1378,lwr_k=20:9.6669,lwr_k=30:9.8412,lwr_k=40:10.0664,lwr_k=50:10.3547,lwr_k=100:10.8534,lwr_k=200:11.0322,lwr_k=300:11.1434,lwr_k=400:11.2239,lwr_k=500:11.3208,lwr_k=600:11.3992,lwr_k=700:11.5085,lwr_k=800:11.5437,lwr_k=900:11.5525,lwr_k=1000:11.565'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:11.2824,lwr_k=10:11.2408,lwr_k=20:11.0197,lwr_k=30:11.0178,lwr_k=40:11.0003,lwr_k=50:11.0252,lwr_k=100:11.0341,lwr_k=200:10.9479,lwr_k=300:10.9634,lwr_k=400:10.9284,lwr_k=500:10.9684,lwr_k=600:10.9404,lwr_k=700:10.9541,lwr_k=800:10.9834,lwr_k=900:10.9951,lwr_k=1000:11.0084'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.5304,lwr_k=10:6.8156,lwr_k=20:7.1915,lwr_k=30:7.2917,lwr_k=40:7.362,lwr_k=50:7.4452,lwr_k=100:7.5791,lwr_k=200:7.685,lwr_k=300:7.729,lwr_k=400:7.7724,lwr_k=500:7.8139,lwr_k=600:7.8452,lwr_k=700:7.8872,lwr_k=800:7.933,lwr_k=900:7.9554,lwr_k=1000:7.9751'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:28.124,lwr_k=10:19.036,lwr_k=20:18.253,lwr_k=30:19.1563,lwr_k=40:20.4442,lwr_k=50:21.2828,lwr_k=100:22.507,lwr_k=200:23.2705,lwr_k=300:23.8497,lwr_k=400:24.4316,lwr_k=500:24.9513,lwr_k=600:25.2419,lwr_k=700:25.4556,lwr_k=800:25.6654,lwr_k=900:25.8405,lwr_k=1000:25.9583'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:13.9206,lwr_k=10:10.9035,lwr_k=20:11.1478,lwr_k=30:11.4121,lwr_k=40:11.7858,lwr_k=50:12.1382,lwr_k=100:12.5607,lwr_k=200:13.0981,lwr_k=300:13.1912,lwr_k=400:13.2954,lwr_k=500:13.369,lwr_k=600:13.3714,lwr_k=700:13.373,lwr_k=800:13.3955,lwr_k=900:13.4019,lwr_k=1000:13.428'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:13.2771,lwr_k=10:14.36,lwr_k=20:13.3143,lwr_k=30:13.3595,lwr_k=40:13.1925,lwr_k=50:13.0214,lwr_k=100:12.7584,lwr_k=200:12.5716,lwr_k=300:12.5546,lwr_k=400:12.4943,lwr_k=500:12.4881,lwr_k=600:12.4899,lwr_k=700:12.4873,lwr_k=800:12.4558,lwr_k=900:12.4637,lwr_k=1000:12.4678'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_98'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.7512,lwr_k=10:0.0669,lwr_k=20:0.2055,lwr_k=30:0.5497,lwr_k=40:1.0572,lwr_k=50:1.6012,lwr_k=100:2.6064,lwr_k=200:3.3729,lwr_k=300:3.6276,lwr_k=400:3.7968,lwr_k=500:3.9373,lwr_k=600:4.0555,lwr_k=700:4.1474,lwr_k=800:4.2053,lwr_k=900:4.2702,lwr_k=1000:4.3404'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.2159,lwr_k=10:25.4232,lwr_k=20:68.5977,lwr_k=30:565794.5775,lwr_k=40:10080057.4215,lwr_k=50:393105.2138,lwr_k=100:16.0803,lwr_k=200:7.6138,lwr_k=300:20.3029,lwr_k=400:34.2869,lwr_k=500:22.8817,lwr_k=600:19.8261,lwr_k=700:10.9203,lwr_k=800:7.0939,lwr_k=900:7.0999,lwr_k=1000:7.0518'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.7436,lwr_k=10:0.0109,lwr_k=20:0.0663,lwr_k=30:0.2433,lwr_k=40:0.8871,lwr_k=50:1.3478,lwr_k=100:2.6181,lwr_k=200:4.0051,lwr_k=300:4.6698,lwr_k=400:5.1065,lwr_k=500:5.3158,lwr_k=600:5.6038,lwr_k=700:5.7838,lwr_k=800:5.9885,lwr_k=900:6.1089,lwr_k=1000:6.218'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.167,lwr_k=10:60.4778,lwr_k=20:199.6853,lwr_k=30:194.5272,lwr_k=40:288576.9384,lwr_k=50:21.5697,lwr_k=100:11.5745,lwr_k=200:6.6025,lwr_k=300:6.3284,lwr_k=400:6.0242,lwr_k=500:5.9695,lwr_k=600:5.8788,lwr_k=700:5.9229,lwr_k=800:5.9703,lwr_k=900:6.014,lwr_k=1000:6.116'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:25.3973,lwr_k=10:13.2117,lwr_k=20:15.3197,lwr_k=30:16.1431,lwr_k=40:16.8384,lwr_k=50:16.9411,lwr_k=100:17.808,lwr_k=200:18.4414,lwr_k=300:19.1353,lwr_k=400:19.5781,lwr_k=500:19.9616,lwr_k=600:20.5694,lwr_k=700:20.8345,lwr_k=800:21.0165,lwr_k=900:21.4866,lwr_k=1000:21.2976'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:18.2116,lwr_k=10:20.419,lwr_k=20:16.6637,lwr_k=30:16.4684,lwr_k=40:43618.8815,lwr_k=50:478828.1061,lwr_k=100:16.5602,lwr_k=200:16.262,lwr_k=300:16.5659,lwr_k=400:16.4947,lwr_k=500:16.2457,lwr_k=600:16.2201,lwr_k=700:16.2369,lwr_k=800:16.019,lwr_k=900:16.1909,lwr_k=1000:15.7981'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.5745,lwr_k=10:0.0092,lwr_k=20:0.0783,lwr_k=30:0.2514,lwr_k=40:0.9472,lwr_k=50:1.5149,lwr_k=100:3.0463,lwr_k=200:4.0278,lwr_k=300:4.4891,lwr_k=400:4.756,lwr_k=500:4.9467,lwr_k=600:5.0994,lwr_k=700:5.2323,lwr_k=800:5.3262,lwr_k=900:5.4258,lwr_k=1000:5.5196'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.2481,lwr_k=10:34.6572,lwr_k=20:90.9182,lwr_k=30:840.8149,lwr_k=40:88.7354,lwr_k=50:15573625.5013,lwr_k=100:1072.9307,lwr_k=200:6.7887,lwr_k=300:7.1365,lwr_k=400:7.2749,lwr_k=500:7.4129,lwr_k=600:7.4803,lwr_k=700:7.531,lwr_k=800:7.5454,lwr_k=900:7.6051,lwr_k=1000:7.6827'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.6347,lwr_k=10:0.0824,lwr_k=20:0.2292,lwr_k=30:0.6511,lwr_k=40:1.2796,lwr_k=50:1.8807,lwr_k=100:2.6047,lwr_k=200:3.3215,lwr_k=300:3.7345,lwr_k=400:3.9772,lwr_k=500:4.1109,lwr_k=600:4.2344,lwr_k=700:4.34,lwr_k=800:4.389,lwr_k=900:4.4275,lwr_k=1000:4.4605'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:16.6749,lwr_k=10:9544.3761,lwr_k=20:181.6958,lwr_k=30:1020.6035,lwr_k=40:677066560.2483,lwr_k=50:297.6031,lwr_k=100:185112088.6843,lwr_k=200:4001029.3611,lwr_k=300:1437972.8587,lwr_k=400:77259.6147,lwr_k=500:425004.7027,lwr_k=600:15.9596,lwr_k=700:15.9948,lwr_k=800:16.2816,lwr_k=900:16.3129,lwr_k=1000:16.2932'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:6.5113,lwr_k=10:0.0035,lwr_k=20:0.0278,lwr_k=30:0.1971,lwr_k=40:1.107,lwr_k=50:1.649,lwr_k=100:3.0125,lwr_k=200:3.9526,lwr_k=300:4.3399,lwr_k=400:4.5624,lwr_k=500:4.8176,lwr_k=600:4.9466,lwr_k=700:5.0031,lwr_k=800:5.0826,lwr_k=900:5.1427,lwr_k=1000:5.2031'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.3812,lwr_k=10:24.9164,lwr_k=20:526.6648,lwr_k=30:5915.2003,lwr_k=40:67.9446,lwr_k=50:25.1264,lwr_k=100:64493.2967,lwr_k=200:6.9641,lwr_k=300:7.4637,lwr_k=400:7.5734,lwr_k=500:7.8122,lwr_k=600:7.866,lwr_k=700:7.818,lwr_k=800:7.8221,lwr_k=900:7.9208,lwr_k=1000:7.9735'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_99'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.1558,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0076,lwr_k=50:0.4162,lwr_k=100:2.242,lwr_k=200:3.0922,lwr_k=300:3.491,lwr_k=400:3.7323,lwr_k=500:3.8875,lwr_k=600:4.0247,lwr_k=700:4.0701,lwr_k=800:4.1221,lwr_k=900:4.1511,lwr_k=1000:4.2235'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.6864,lwr_k=10:30.8475,lwr_k=20:103.65,lwr_k=30:110.1406,lwr_k=40:3665.9903,lwr_k=50:54.6461,lwr_k=100:9.1748,lwr_k=200:7.2602,lwr_k=300:7.0039,lwr_k=400:7.007,lwr_k=500:7.0584,lwr_k=600:7.2284,lwr_k=700:7.2512,lwr_k=800:7.3077,lwr_k=900:7.3214,lwr_k=1000:7.371'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:14.3741,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0002,lwr_k=40:0.0925,lwr_k=50:0.8439,lwr_k=100:3.8341,lwr_k=200:6.6568,lwr_k=300:8.0076,lwr_k=400:8.6302,lwr_k=500:9.2612,lwr_k=600:9.8682,lwr_k=700:10.2507,lwr_k=800:10.5989,lwr_k=900:10.782,lwr_k=1000:10.9108'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.4179,lwr_k=10:32.5958,lwr_k=20:60.4473,lwr_k=30:288.0159,lwr_k=40:170.241,lwr_k=50:59.2349,lwr_k=100:11.1572,lwr_k=200:8.2301,lwr_k=300:7.6485,lwr_k=400:7.8884,lwr_k=500:7.9962,lwr_k=600:8.2135,lwr_k=700:8.362,lwr_k=800:8.5042,lwr_k=900:8.6514,lwr_k=1000:8.83'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:18.058,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.001,lwr_k=50:0.787,lwr_k=100:4.4938,lwr_k=200:7.5825,lwr_k=300:9.366,lwr_k=400:10.0585,lwr_k=500:10.5983,lwr_k=600:11.0531,lwr_k=700:11.4961,lwr_k=800:11.7987,lwr_k=900:12.0814,lwr_k=1000:12.2584'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:12.3162,lwr_k=10:34.1373,lwr_k=20:27.8919,lwr_k=30:72.4207,lwr_k=40:484.5438,lwr_k=50:184.4889,lwr_k=100:9.5213,lwr_k=200:8.158,lwr_k=300:8.1876,lwr_k=400:8.1697,lwr_k=500:8.2776,lwr_k=600:8.2914,lwr_k=700:8.3264,lwr_k=800:8.496,lwr_k=900:8.5902,lwr_k=1000:8.6588'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:18.1132,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0064,lwr_k=40:0.2578,lwr_k=50:0.8427,lwr_k=100:4.2999,lwr_k=200:7.0837,lwr_k=300:8.4442,lwr_k=400:9.2243,lwr_k=500:9.7568,lwr_k=600:10.2848,lwr_k=700:10.7285,lwr_k=800:11.0419,lwr_k=900:11.3126,lwr_k=1000:11.5517'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:16.489,lwr_k=10:42.4735,lwr_k=20:83.2302,lwr_k=30:210.547,lwr_k=40:207.3273,lwr_k=50:205.1979,lwr_k=100:13.1424,lwr_k=200:10.5891,lwr_k=300:10.4415,lwr_k=400:10.3334,lwr_k=500:10.257,lwr_k=600:10.5435,lwr_k=700:10.6714,lwr_k=800:10.8547,lwr_k=900:11.1356,lwr_k=1000:11.323'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.2355,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0011,lwr_k=40:0.0744,lwr_k=50:0.7254,lwr_k=100:2.774,lwr_k=200:4.1458,lwr_k=300:4.6295,lwr_k=400:4.932,lwr_k=500:5.1197,lwr_k=600:5.2687,lwr_k=700:5.3952,lwr_k=800:5.5026,lwr_k=900:5.6234,lwr_k=1000:5.7185'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:26.9041,lwr_k=10:35.365,lwr_k=20:107.5608,lwr_k=30:283.2746,lwr_k=40:268.0905,lwr_k=50:2864.4827,lwr_k=100:19.4412,lwr_k=200:19.3763,lwr_k=300:20.2772,lwr_k=400:20.4028,lwr_k=500:20.8358,lwr_k=600:21.3135,lwr_k=700:21.7634,lwr_k=800:22.1689,lwr_k=900:22.3472,lwr_k=1000:22.7526'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:7.1305,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0004,lwr_k=40:0.0586,lwr_k=50:0.5527,lwr_k=100:2.6403,lwr_k=200:4.1833,lwr_k=300:4.7539,lwr_k=400:5.0482,lwr_k=500:5.263,lwr_k=600:5.4249,lwr_k=700:5.5413,lwr_k=800:5.6224,lwr_k=900:5.7132,lwr_k=1000:5.797'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.7064,lwr_k=10:35.5516,lwr_k=20:183.1035,lwr_k=30:131.298,lwr_k=40:41884.759,lwr_k=50:107.7751,lwr_k=100:9.0332,lwr_k=200:7.7542,lwr_k=300:8.174,lwr_k=400:8.394,lwr_k=500:8.7084,lwr_k=600:8.9558,lwr_k=700:9.097,lwr_k=800:9.1516,lwr_k=900:9.3252,lwr_k=1000:9.3634'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    }
   ],
   "source": [
    "for deep_name,deep_model in tqdm(deep_models.items()):\n",
    "    logging.getLogger().info(f\"Running model {deep_name}\")\n",
    "    temp_dict = {deep_name:deep_model}\n",
    "\n",
    "    lwr_scheme = DeepLWRScheme_1_to_n(lwr_models = setup_pls_models_exh(nrow),n_neighbours=500,loss_fun_sk = mean_squared_error)\n",
    "    lwr_scores, lwr_preds, _ , _, _,_= eval.evaluate(temp_dict,dataset,lwr_scheme,logger_name=\"log\",load_fun=load_fun_cv,load_fun_pp = load_fun_pp_cv)\n",
    "    lwr_scores_final, lwr_preds_final, _ , _, _,_= eval.build(temp_dict,dataset,lwr_scheme,logger_name=\"test_log\",load_fun=load_fun_build,load_fun_pp = load_fun_pp_build)\n",
    "\n",
    "    #scores\n",
    "    for k,v in ut.flip_dicts(lwr_scores).items():\n",
    "        dict1 = {'model_num':deep_name,\"predictor\":k}\n",
    "        all_scores.append({**dict1,**v})\n",
    "\n",
    "    for k,v in ut.flip_dicts(lwr_scores_final).items():\n",
    "        dict1 = {'model_num':deep_name,\"predictor\":k}\n",
    "        all_scores_final.append({**dict1,**v})\n",
    "\n",
    "    lwr_preds['deep'] = deep_preds[deep_name]\n",
    "    lwr_preds_final['deep'] = deep_preds_final[deep_name]\n",
    "\n",
    "    lwr_preds.to_csv(log_dir/deep_name/ f\"predictions.csv\",index=False)\n",
    "    lwr_preds_final.to_csv(log_dir/deep_name/ f\"predictions_test.csv\",index=False)\n",
    "\n",
    "    #preds\n",
    "    # todo save predictions - appending solns\n",
    "    plot_preds_and_res(lwr_preds,name_lambda=lambda x:f\"{deep_name} with {x} predictor\",save_lambda= lambda x:f\"deep_lwr{x}\",save_loc=log_dir/deep_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% save scores\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank - model_num - predictor - fold_0 - fold_1 - fold_2 - fold_3 - fold_4 - MSE - R2'\n",
      "0 - random_66 - lwr_k=50 - 8.642433101216346 - 5.855391528123524 - 5.7642409888359945 - 6.684839933006882 - 11.451889583613811 - 7.679617927659398 - 0.9674357500701529'\n",
      "1 - random_94 - lwr_k=300 - 6.742202791702786 - 5.735760494779793 - 5.314625739213844 - 6.742313883905043 - 14.574896777368712 - 7.821441523019006 - 0.9668343687190566'\n",
      "2 - random_94 - lwr_k=400 - 6.851944911657329 - 5.7442680328405435 - 5.360233576572587 - 6.727465381532864 - 14.641980222693133 - 7.864665219900357 - 0.9666510852170133'\n",
      "3 - random_94 - lwr_k=200 - 6.97673510982059 - 5.823393356392581 - 5.293482195972126 - 6.872772439528131 - 14.370910704332593 - 7.866978199713183 - 0.96664127737847'\n",
      "4 - random_78 - lwr_k=100 - 6.1610708854564535 - 6.187254334577997 - 5.176259002498922 - 6.286635514169437 - 15.626018598331362 - 7.886886578073809 - 0.9665568589328207'\n",
      "5 - random_94 - lwr_k=500 - 6.909021953535989 - 5.861342134329628 - 5.459099381965878 - 6.584011565347016 - 14.693820891074173 - 7.900962615038872 - 0.9664971716423761'\n",
      "6 - random_66 - lwr_k=100 - 8.503438391898992 - 5.629946589053209 - 5.4751261485623255 - 6.857439605521683 - 13.052668856007156 - 7.903449796241182 - 0.9664866251293782'\n",
      "7 - random_94 - lwr_k=600 - 6.9248930885028095 - 6.008825626350402 - 5.514096142979135 - 6.57809644873572 - 14.54275107015235 - 7.913258634467084 - 0.9664450322451335'\n",
      "8 - random_48 - lwr_k=1000 - 6.922873935519379 - 6.912016996142394 - 5.042579300516703 - 6.851203609007415 - 13.938361583723353 - 7.933074364621885 - 0.9663610066600872'\n",
      "9 - random_94 - lwr_k=700 - 6.981738959221663 - 6.050212227237255 - 5.60674197910361 - 6.535577547048991 - 14.586002850393976 - 7.951584406683506 - 0.966282517646495'\n",
      "10 - random_48 - lwr_k=900 - 7.067880422348136 - 6.961228429916483 - 5.044741167505976 - 6.8715107610853385 - 13.93146418131586 - 7.975050333660394 - 0.9661830139578859'\n",
      "11 - random_94 - lwr_k=800 - 6.992730558948969 - 6.068830191229867 - 5.690861196387068 - 6.51379191461907 - 14.622104399790457 - 7.977189807991023 - 0.9661739418429086'\n",
      "12 - random_94 - lwr_k=900 - 7.000548110639549 - 6.08703771240011 - 5.717273240798557 - 6.526214500087172 - 14.602281909827148 - 7.986198562186817 - 0.9661357415931108'\n",
      "13 - random_48 - lwr_k=800 - 7.168115662837547 - 6.9174531832696555 - 5.005363111181006 - 6.834979274127738 - 14.021910045533602 - 7.989254191583674 - 0.9661227846621399'\n",
      "14 - random_94 - lwr_k=1000 - 7.0254255400224865 - 6.130234225911673 - 5.734428679988963 - 6.520426982166096 - 14.659077364728875 - 8.013448249516427 - 0.9660201934451339'\n",
      "15 - random_48 - lwr_k=700 - 7.273176351896107 - 6.957243901522137 - 4.999395692453756 - 6.759742092687349 - 14.106673838854324 - 8.018950309902655 - 0.9659968627962352'\n",
      "16 - random_48 - lwr_k=600 - 7.458788825600231 - 6.915252437933056 - 5.049875063202691 - 6.7432452999606936 - 14.195281939542365 - 8.072198728613296 - 0.965771070988414'\n",
      "17 - random_48 - lwr_k=500 - 7.6167851348703985 - 6.938667900267861 - 5.027410163687546 - 6.737788644645756 - 14.36493465388562 - 8.136835854971059 - 0.9654969871007403'\n",
      "18 - random_48 - lwr_k=400 - 7.5952275382744086 - 7.089773168178309 - 5.047589424258394 - 6.693187057364047 - 14.567966076150967 - 8.198468237485926 - 0.9652356443715978'\n",
      "19 - random_78 - lwr_k=300 - 6.579451060869097 - 6.449862663624442 - 5.024863912384991 - 6.658687574462496 - 16.765843130167923 - 8.29515837542278 - 0.9648256445711928'\n",
      "20 - random_17 - lwr_k=700 - 6.117059774564313 - 6.761826236752507 - 5.621188014754654 - 7.068043484936154 - 15.992800227641467 - 8.311570238235117 - 0.9647560525670762'\n",
      "21 - random_78 - lwr_k=500 - 6.68510727118925 - 6.513279273587287 - 5.040019984685134 - 6.681525648054928 - 16.65372299841853 - 8.314169208508526 - 0.964745032029552'\n",
      "22 - random_78 - lwr_k=400 - 6.684247083621182 - 6.440674404226596 - 5.0198817319540066 - 6.701190308152117 - 16.747409759124807 - 8.318105507616007 - 0.964728340752832'\n",
      "23 - random_17 - lwr_k=600 - 6.123064795037136 - 6.798206624240768 - 5.611600492107336 - 7.07772239615556 - 16.00058041333025 - 8.321625283380191 - 0.9647134156798971'\n",
      "24 - random_17 - lwr_k=800 - 6.100467466772263 - 6.835164929238145 - 5.643671097020993 - 7.04382300817911 - 16.03556580917337 - 8.33112804050675 - 0.9646731206859265'\n",
      "25 - random_78 - lwr_k=200 - 6.463182327835814 - 6.298581308975671 - 5.043659900320081 - 6.6082170427845615 - 17.2534216499242 - 8.332773005775643 - 0.9646661454612934'\n",
      "26 - random_78 - lwr_k=600 - 6.773888059576831 - 6.5230788346474515 - 5.011535671311032 - 6.662627070378404 - 16.70014966620962 - 8.333703781690978 - 0.9646621986478163'\n",
      "27 - random_17 - lwr_k=900 - 6.101210011354554 - 6.889342218226313 - 5.632720111390842 - 7.0336308138552015 - 16.054452519628455 - 8.341666256875648 - 0.9646284349847778'\n",
      "28 - random_17 - lwr_k=1000 - 6.097520791181289 - 6.9404099421934 - 5.635943540988423 - 7.053694398748336 - 16.028245007541 - 8.350562704242675 - 0.9645907109549788'\n",
      "29 - random_17 - lwr_k=500 - 6.15489919668452 - 6.72824796033923 - 5.631758339560794 - 7.295721922100014 - 15.959721291399909 - 8.35344341283284 - 0.9645784957490416'\n",
      "30 - random_78 - lwr_k=700 - 6.857916924283227 - 6.5382954059447185 - 5.038616206813161 - 6.664099825493474 - 16.777480333187523 - 8.374732475815772 - 0.9644882227205813'\n",
      "31 - random_17 - lwr_k=400 - 6.275673725053434 - 6.656770372821064 - 5.641203103959867 - 7.4380371717475855 - 15.94211883796609 - 8.39013036931358 - 0.9644229302988776'\n",
      "32 - random_66 - lwr_k=200 - 8.744293109580747 - 5.691555144110645 - 5.575880202688791 - 7.019878451283626 - 14.929924206469032 - 8.391921620862927 - 0.9644153347695571'\n",
      "33 - random_78 - lwr_k=800 - 6.916204033993903 - 6.659186843752149 - 5.082168641474227 - 6.667171315163921 - 16.902521653848687 - 8.444907594401524 - 0.9641906558205077'\n",
      "34 - random_17 - lwr_k=300 - 6.446240297442605 - 6.687731637225128 - 5.733286142788046 - 7.4238933872316455 - 15.993213018927536 - 8.45625397190577 - 0.9641425432351777'\n",
      "35 - random_48 - lwr_k=300 - 7.754067267560452 - 7.198376787959304 - 5.123954977196553 - 6.843450254460884 - 15.372850118699699 - 8.458218178914558 - 0.9641342143145789'\n",
      "36 - random_48 - lr - 7.594038743144271 - 7.877483842641018 - 5.443348682911758 - 7.230497328484553 - 14.19274406954139 - 8.467382853881539 - 0.9640953529065034'\n",
      "37 - random_78 - lwr_k=900 - 7.009587710207338 - 6.699953269840776 - 5.060846331849925 - 6.664676080214349 - 16.941751250310862 - 8.474832195739125 - 0.9640637651071691'\n",
      "38 - random_78 - lwr_k=50 - 7.433231790958456 - 7.182601093529691 - 6.125390288747088 - 7.4674660953255865 - 14.253353029630091 - 8.492020546745636 - 0.9639908805231533'\n",
      "39 - random_66 - lwr_k=300 - 9.045674664425304 - 5.722684344568127 - 5.643719701317271 - 6.9569041844640855 - 15.19290073758904 - 8.51200724973395 - 0.963906129953852'\n",
      "40 - random_78 - lwr_k=1000 - 7.089909020157366 - 6.742851381023961 - 5.067480138356161 - 6.680858971248783 - 16.992926345499406 - 8.514281698203046 - 0.9638964855016022'\n",
      "41 - random_66 - lwr_k=40 - 9.353551396769928 - 6.1893656578211145 - 6.17591315295975 - 6.812929358639437 - 14.155616426542485 - 8.53722433341759 - 0.9637992007519944'\n",
      "42 - random_23 - lwr_k=400 - 8.452789073559101 - 5.9632712088610305 - 5.595897592394011 - 6.950830292637068 - 15.994305161630573 - 8.590965615701863 - 0.9635713190312707'\n",
      "43 - random_23 - lwr_k=500 - 8.43372211719771 - 5.966131420831616 - 5.642904472486317 - 6.950119070227137 - 15.967048446286237 - 8.591529215988286 - 0.9635689291701135'\n",
      "44 - random_94 - lwr_k=100 - 6.8477040799394695 - 7.176521636110678 - 5.884045062542358 - 7.109812740508135 - 15.977401021035867 - 8.59857718179511 - 0.9635390433447779'\n",
      "45 - random_17 - lwr_k=200 - 7.002047093764258 - 6.843445608357675 - 5.783860735930037 - 7.734373857141999 - 15.633780704511972 - 8.5989524742661 - 0.9635374519742257'\n",
      "46 - random_23 - lwr_k=600 - 8.42503158488775 - 5.988432544168713 - 5.710514545336609 - 6.990048205817889 - 15.952513976626541 - 8.612847527481192 - 0.9634785321178022'\n",
      "47 - random_77 - lwr_k=500 - 7.327590042126139 - 7.383539180093674 - 5.790223120947097 - 8.549253652889664 - 14.043686752836617 - 8.618444829805274 - 0.963454797609975'\n",
      "48 - random_66 - lwr_k=400 - 9.212444639622312 - 5.744122672880908 - 5.628951212606458 - 7.024828393396593 - 15.528515847902051 - 8.627396103667442 - 0.9634168410967755'\n",
      "49 - random_77 - lwr_k=400 - 7.324224876379322 - 7.496045172720019 - 5.852175005797072 - 8.458803541113992 - 14.079625846222665 - 8.641771403965231 - 0.9633558848257574'\n",
      "50 - random_26 - lwr_k=500 - 8.180697300176245 - 8.378874021807988 - 6.570710333785092 - 10.548190138855594 - 9.606480766399516 - 8.656866980241169 - 0.9632918743341788'\n",
      "51 - random_77 - lwr_k=600 - 7.381050791045831 - 7.298941609897793 - 5.8632200211502665 - 8.566532532549715 - 14.214910428440065 - 8.664497169665319 - 0.9632595196782884'\n",
      "52 - random_80 - lwr_k=900 - 6.781356416610182 - 6.349568362000592 - 6.722285152327078 - 7.07146423404478 - 16.44883492431051 - 8.674011056624455 - 0.9632191774899582'\n",
      "53 - random_77 - lwr_k=800 - 7.457869335159044 - 7.219635125498156 - 5.974933820839779 - 8.48025828475468 - 14.296289072903823 - 8.685355980001555 - 0.9631710710706327'\n",
      "54 - random_23 - lwr_k=300 - 8.446765466739969 - 6.014372021146802 - 5.711575610801663 - 6.99331137408484 - 16.26806280964261 - 8.686340545164185 - 0.9631668961720466'\n",
      "55 - random_80 - lwr_k=1000 - 6.8061943396476146 - 6.3508814554857365 - 6.655769617392667 - 7.1153107028675695 - 16.54842680250169 - 8.69462335329053 - 0.9631317741859676'\n",
      "56 - random_77 - lwr_k=700 - 7.372709334460273 - 7.31470419273957 - 5.910041638569377 - 8.573973259783122 - 14.334922234340887 - 8.700825539423095 - 0.9631054747604969'\n",
      "57 - random_77 - lwr_k=900 - 7.499327316514378 - 7.094089602209479 - 5.978476994109926 - 8.513919066485897 - 14.46620660633734 - 8.709940941707213 - 0.963066822285727'\n",
      "58 - random_23 - lwr_k=700 - 8.46613880109086 - 6.1539421666453045 - 5.963437918813867 - 6.93078790586083 - 16.0599056848786 - 8.714382432575432 - 0.9630479888203052'\n",
      "59 - random_77 - lwr_k=1000 - 7.577236340893169 - 7.086470411086523 - 6.025107628873053 - 8.484002760040033 - 14.456757219935048 - 8.725458326727157 - 0.9630010231784267'\n",
      "60 - random_66 - lwr_k=500 - 9.37245704274124 - 5.8223367482673005 - 5.64496473422958 - 7.110668974458869 - 15.784692023084512 - 8.746647409711352 - 0.9629111740999203'\n",
      "61 - random_77 - lwr_k=300 - 7.230336062426741 - 7.502533599817841 - 6.5162988076458435 - 8.537757114054195 - 14.004455451568514 - 8.757820388738134 - 0.9628637967844189'\n",
      "62 - random_23 - lwr_k=800 - 8.535926032647096 - 6.212721859775845 - 5.968656426044658 - 6.966409798331453 - 16.129297616227902 - 8.762147695090606 - 0.9628454475010415'\n",
      "63 - random_80 - lwr_k=800 - 6.98605580846287 - 6.3707054211174325 - 7.014858157123516 - 7.097937628099028 - 16.369003666537427 - 8.767027894734904 - 0.9628247537578863'\n",
      "64 - random_80 - lwr_k=700 - 7.204823539859618 - 6.327989743085349 - 7.133825371275884 - 7.07803080494863 - 16.22216846361123 - 8.792703768953363 - 0.9627158791246568'\n",
      "65 - random_80 - lwr_k=600 - 7.1672623120479635 - 6.345635132944023 - 7.291939696160951 - 7.055604565404695 - 16.167878723612457 - 8.804992982263414 - 0.9626637685877213'\n",
      "66 - random_66 - lwr_k=600 - 9.505043900134613 - 5.8750861886432695 - 5.653249070176689 - 7.201618947432402 - 15.850525999400054 - 8.81673572347384 - 0.9626139752824762'\n",
      "67 - random_23 - lwr_k=900 - 8.584842962072106 - 6.335867880076712 - 6.049566792760972 - 7.008313061201269 - 16.17806274612792 - 8.830881703588659 - 0.96255399140876'\n",
      "68 - random_94 - lr - 7.171682458146248 - 7.215955961525537 - 6.906132146199509 - 7.75460263001377 - 15.299553204440524 - 8.869036478339154 - 0.9623922018988266'\n",
      "69 - random_66 - lwr_k=700 - 9.619712033067607 - 5.903833603493192 - 5.66879877553227 - 7.198287511880809 - 16.01140738780566 - 8.880041517163807 - 0.9623455367081687'\n",
      "70 - random_31 - lwr_k=300 - 7.811077994538343 - 6.452052224033232 - 6.3059730302458545 - 8.247074835875836 - 15.623769542384636 - 8.887414308642652 - 0.962314273509054'\n",
      "71 - random_23 - lwr_k=1000 - 8.654837930585094 - 6.367077848128358 - 6.096363471210573 - 7.050208324957966 - 16.289362314961807 - 8.891117837117733 - 0.962298569260741'\n",
      "72 - random_80 - lwr_k=500 - 7.548176454598752 - 6.3678647499879855 - 7.29445170769675 - 7.219170161375791 - 16.11145624579026 - 8.907585185893643 - 0.9622287419768484'\n",
      "73 - random_66 - lwr_k=800 - 9.685484132133999 - 5.936629156032546 - 5.67487427724141 - 7.170870203851315 - 16.19247049743804 - 8.931698530690287 - 0.9621264930116005'\n",
      "74 - random_31 - lwr_k=400 - 7.78538516441199 - 6.350721239841336 - 6.240402430834855 - 8.254104582918224 - 16.1544959716018 - 8.95640325382608 - 0.9620217363965944'\n",
      "75 - random_66 - lwr_k=900 - 9.753468948688054 - 5.968846639175349 - 5.682451501125402 - 7.238325680260857 - 16.292717594652213 - 8.986793314215172 - 0.9618928719750548'\n",
      "76 - random_31 - lwr_k=600 - 7.9200945017242965 - 6.287742621661088 - 6.093686861306161 - 8.248722225180956 - 16.444100328753787 - 8.998248724543323 - 0.9618442970526447'\n",
      "77 - random_48 - lwr_k=200 - 8.081465352083658 - 7.586040141417079 - 5.677178289760878 - 7.065698607045658 - 16.6306177390155 - 9.00781540255389 - 0.961803731011903'\n",
      "78 - random_77 - lwr_k=200 - 7.1788466500175065 - 7.689476620920237 - 7.012531609527297 - 9.063307475160233 - 14.186216026274586 - 9.025554335610066 - 0.9617285117685797'\n",
      "79 - random_31 - lwr_k=700 - 7.997374291702288 - 6.274408459161137 - 6.07149555338558 - 8.266970543139772 - 16.574484417539896 - 9.036324070612212 - 0.9616828443479356'\n",
      "80 - random_66 - lwr_k=1000 - 9.835166796055123 - 6.026670442954433 - 5.693455254111387 - 7.285039065465133 - 16.377965462510296 - 9.043294989356097 - 0.9616532852289332'\n",
      "81 - random_23 - lwr_k=200 - 9.779572702800317 - 6.279765143929002 - 5.674001849251182 - 7.034389568588493 - 16.628421033426477 - 9.078886335631113 - 0.9615023655469442'\n",
      "82 - random_10 - lwr_k=300 - 8.467112234894238 - 7.320090611319763 - 6.01060037185792 - 8.0641153723616 - 15.549652944902022 - 9.081925011870133 - 0.9614894805032564'\n",
      "83 - random_31 - lwr_k=500 - 7.894018385137244 - 6.270836590450548 - 6.266474071941332 - 8.266735236343006 - 16.735111588657716 - 9.085978810436309 - 0.9614722909879814'\n",
      "84 - random_13 - lwr_k=100 - 8.088733276435566 - 6.300223227985362 - 5.494204514858864 - 6.899462128746294 - 18.65647021455447 - 9.087198616704647 - 0.9614671185853225'\n",
      "85 - random_13 - lwr_k=50 - 8.35085676695207 - 6.287477608166044 - 6.126788402510535 - 7.3035437598228725 - 17.41228052288002 - 9.095607449654482 - 0.9614314622101778'\n",
      "86 - random_31 - lwr_k=800 - 8.049991432545811 - 6.289780144071998 - 6.065041637904845 - 8.30035571217317 - 16.81521294247251 - 9.103442939877379 - 0.9613982370075251'\n",
      "87 - random_10 - lwr_k=400 - 8.595433295847606 - 7.568920020934632 - 6.061922550603229 - 8.08994049698623 - 15.512001076335885 - 9.165288660294864 - 0.9611359896515059'\n",
      "88 - random_78 - lwr_k=40 - 8.272794315179517 - 7.551086365415387 - 7.041398280164308 - 8.282319652894447 - 14.86441562096149 - 9.201980229432731 - 0.960980404642072'\n",
      "89 - random_31 - lwr_k=900 - 8.104413378775613 - 6.300535323522593 - 6.029679892064846 - 8.310375359030566 - 17.365104512108278 - 9.221360305400378 - 0.96089822638224'\n",
      "90 - random_84 - lwr_k=300 - 7.546829475051317 - 6.903128508422671 - 6.776338647409351 - 8.0363279121889 - 16.901658430998676 - 9.23219902887245 - 0.9608522664264988'\n",
      "91 - random_10 - lwr_k=500 - 8.666181464434025 - 7.650730708719105 - 6.166686397922758 - 8.216956617938882 - 15.47520667092603 - 9.234799761447663 - 0.9608412384162022'\n",
      "92 - random_10 - lwr_k=200 - 8.204310604540927 - 8.536052633050383 - 5.977277501872572 - 7.85315252267433 - 15.628865389469958 - 9.239646893212898 - 0.9608206848923507'\n",
      "93 - random_31 - lwr_k=1000 - 8.07913577569876 - 6.337641131253758 - 6.066846327845752 - 8.367764745533595 - 17.359680134811583 - 9.241547559439512 - 0.9608126254067592'\n",
      "94 - random_13 - lwr_k=200 - 8.138699634341807 - 6.287578583515566 - 5.2647729052514345 - 7.289177549037447 - 19.2362273853636 - 9.242626350816668 - 0.9608080509562645'\n",
      "95 - random_13 - lwr_k=400 - 8.236391786869417 - 6.359354486562869 - 5.306735809915192 - 7.133774834608115 - 19.18265390198487 - 9.243144892323683 - 0.9608058521599973'\n",
      "96 - random_82 - lwr_k=900 - 8.33224039374175 - 7.222211282045994 - 6.163291329239813 - 10.95229483307986 - 13.551174601818998 - 9.24376205024168 - 0.9608032351958622'\n",
      "97 - random_66 - lwr_k=30 - 9.934951775072344 - 7.16220061047109 - 6.8118403700046555 - 7.524329500346906 - 14.788737244174973 - 9.24418401866235 - 0.9608014459030558'\n",
      "98 - random_13 - lwr_k=300 - 8.176274557049505 - 6.334303833626689 - 5.298633306562393 - 7.205189031957574 - 19.22886088011618 - 9.247999509310796 - 0.9607852668962028'\n",
      "99 - random_11 - lwr_k=900 - 7.310762271702158 - 6.594053361164603 - 5.656527284611131 - 8.13178046674748 - 18.63974911333051 - 9.265816626027279 - 0.9607097161269783'\n",
      "100 - random_13 - lwr_k=500 - 8.26841014504014 - 6.408069104995937 - 5.299540693311376 - 7.129948474130167 - 19.235081997296763 - 9.267578031061003 - 0.9607022471572605'\n",
      "101 - random_10 - lwr_k=600 - 8.730618127750704 - 7.683491876797901 - 6.235415241381221 - 8.302580317246749 - 15.425970539040796 - 9.275265274408378 - 0.9606696505729'\n",
      "102 - random_13 - lwr_k=700 - 8.389721784789296 - 6.47457338067353 - 5.291987143233189 - 7.143406363028727 - 19.117248735992565 - 9.282781213359799 - 0.9606377803787345'\n",
      "103 - random_13 - lwr_k=600 - 8.338816983646499 - 6.4202563856027375 - 5.306827890773132 - 7.161371702803487 - 19.210426167811146 - 9.286914968376784 - 0.9606202518203101'\n",
      "104 - random_92 - lwr_k=400 - 8.077845418488643 - 7.025177289929981 - 6.054064988865403 - 7.924792363085649 - 17.358629507200796 - 9.28753319219125 - 0.9606176303364033'\n",
      "105 - random_80 - lr - 7.13899649144626 - 7.3120515009156275 - 7.341948753842058 - 7.987779654158753 - 16.708032255210092 - 9.29708308767754 - 0.9605771354593993'\n",
      "106 - random_13 - lwr_k=800 - 8.421081236587057 - 6.4815322145349406 - 5.323368212284589 - 7.171934317696287 - 19.12347730063999 - 9.303671820936493 - 0.9605491969397246'\n",
      "107 - random_10 - lwr_k=700 - 8.756552398565965 - 7.774400836740056 - 6.252799552444483 - 8.370706917292356 - 15.373188106695096 - 9.30518895224617 - 0.9605427637755216'\n",
      "108 - random_13 - lwr_k=900 - 8.444432775075084 - 6.4963790421825385 - 5.360730107418012 - 7.170392797207829 - 19.141400897482573 - 9.322060521215523 - 0.9604712224574766'\n",
      "109 - random_82 - lwr_k=800 - 8.846823321710696 - 7.697104117419511 - 6.3184217578827075 - 10.343570885294138 - 13.428528000039755 - 9.326544135579596 - 0.9604522103737876'\n",
      "110 - random_84 - lwr_k=200 - 7.973248298196333 - 6.9693077961094545 - 7.691930588113177 - 8.331542424832383 - 15.715987011583378 - 9.335792408251434 - 0.9604129944823797'\n",
      "111 - random_10 - lwr_k=800 - 8.749422029709914 - 7.853517184945792 - 6.2609279191049225 - 8.482620767099316 - 15.353421691612132 - 9.339641812939751 - 0.9603966716682069'\n",
      "112 - random_11 - lwr_k=1000 - 7.335599473205865 - 6.693638948814004 - 5.7351899906573935 - 8.25459199928583 - 18.68959787749349 - 9.34096154737981 - 0.9603910755353586'\n",
      "113 - random_10 - lwr_k=900 - 8.776863115072075 - 7.935808346626437 - 6.259379588376777 - 8.43670121273056 - 15.349240569376969 - 9.351272624784169 - 0.9603473529823873'\n",
      "114 - random_11 - lwr_k=500 - 7.322250089279037 - 7.093027685169466 - 5.803032532040987 - 8.784447826746968 - 17.76597419299347 - 9.353043629949433 - 0.9603398433047742'\n",
      "115 - random_13 - lwr_k=1000 - 8.508852760975897 - 6.513971403693501 - 5.368516202812189 - 7.194513331772481 - 19.186503687827205 - 9.353867888290633 - 0.9603363481628436'\n",
      "116 - random_11 - lwr_k=600 - 7.354472126240109 - 6.763060763988939 - 5.81422360376277 - 8.987318936758895 - 17.92420856453256 - 9.367900326040791 - 0.960276845748228'\n",
      "117 - random_10 - lwr_k=1000 - 8.805697769276106 - 7.976918392653828 - 6.321626454749212 - 8.440995106595308 - 15.324918801480027 - 9.373709469934035 - 0.960252212958498'\n",
      "118 - random_11 - lwr_k=800 - 7.294034453307534 - 6.9130306000039665 - 5.717981136964339 - 8.422539780710325 - 18.548797674719854 - 9.37853143875231 - 0.96023176613429'\n",
      "119 - random_92 - lwr_k=200 - 8.31548219574193 - 7.367423997912524 - 6.539977086652896 - 7.627995016565245 - 17.04710944832087 - 9.37909381743489 - 0.9602293814531577'\n",
      "120 - random_13 - lwr_k=40 - 9.107186479150473 - 6.429384014852561 - 6.180944337293279 - 7.74527000410271 - 17.49269231818312 - 9.390563971674766 - 0.960180744011436'\n",
      "121 - random_79 - lwr_k=400 - 8.074280101967398 - 6.341021772666728 - 5.786482142284239 - 8.082483750787029 - 18.710486455723768 - 9.398233209215924 - 0.9601482237779538'\n",
      "122 - random_92 - lwr_k=500 - 8.076710149862825 - 7.1673764301177645 - 6.052037666861552 - 7.942141061947153 - 17.79049756086575 - 9.405168421553757 - 0.9601188160665269'\n",
      "123 - random_92 - lwr_k=300 - 8.263597084398558 - 7.1680534374549465 - 6.469228300634153 - 7.717161702551363 - 17.470104271937526 - 9.417071630527298 - 0.9600683423221775'\n",
      "124 - random_77 - lr - 8.51088145780535 - 7.72064503104128 - 6.260693489740669 - 8.943267116802925 - 15.762355815286876 - 9.439135045099981 - 0.9599747857737635'\n",
      "125 - random_84 - lwr_k=400 - 7.772620395723364 - 6.9949154706383005 - 6.636793513463921 - 8.004258843892817 - 17.8684799557129 - 9.454735186335677 - 0.9599086357513371'\n",
      "126 - random_79 - lwr_k=200 - 8.627738271239572 - 6.3798532379717 - 5.865433489029287 - 8.06130032147894 - 18.350911587290835 - 9.456407705087411 - 0.9599015436903574'\n",
      "127 - random_11 - lwr_k=700 - 7.315337732833259 - 6.934901552491003 - 5.802296692661595 - 8.901042752715803 - 18.347350033940586 - 9.459421035027592 - 0.9598887661237887'\n",
      "128 - random_84 - lwr_k=500 - 7.520994911962364 - 6.924598056250272 - 6.523612878971962 - 8.025526857579038 - 18.356260904597182 - 9.469462713279478 - 0.9598461859168839'\n",
      "129 - random_79 - lwr_k=800 - 7.637897643960814 - 6.524799144911515 - 5.8359346536846095 - 8.191497365803338 - 19.23518106248384 - 9.48427477469945 - 0.9597833776268623'\n",
      "130 - random_79 - lwr_k=500 - 8.164364898112149 - 6.381110044126936 - 5.839080976401876 - 8.210626115204608 - 18.85710238167638 - 9.48973059543058 - 0.9597602430501769'\n",
      "131 - random_34 - lwr_k=500 - 11.996801055833565 - 6.108402107572197 - 6.05406418826542 - 8.0320276585256 - 15.264228906734859 - 9.49096117663942 - 0.9597550249580243'\n",
      "132 - random_92 - lwr_k=600 - 8.188122993926603 - 7.204867958698749 - 6.126553581682677 - 7.956257675307158 - 18.023296194150387 - 9.499229104625904 - 0.9597199660689116'\n",
      "133 - random_92 - lwr_k=700 - 8.203426237495695 - 7.174153230071698 - 6.117587314402622 - 7.964410297562574 - 18.178458082939645 - 9.527004832625867 - 0.9596021873256076'\n",
      "134 - random_34 - lwr_k=600 - 12.06963941034775 - 6.118083487658584 - 5.9806984497300295 - 7.935232302518848 - 15.567616898080637 - 9.534109884135594 - 0.9595720594370452'\n",
      "135 - random_65 - lwr_k=900 - 8.034264883034668 - 7.211013859962729 - 5.788246386792686 - 8.234503349353044 - 18.416052614889256 - 9.536189339210175 - 0.95956324182458'\n",
      "136 - random_11 - lwr_k=400 - 7.291560603603454 - 7.0744447344076 - 5.9389018893861145 - 9.796079366458402 - 17.590374981779494 - 9.537500980708764 - 0.9595576800086172'\n",
      "137 - random_79 - lwr_k=900 - 7.728661427381845 - 6.577770464515234 - 5.875810630651181 - 8.226710629151302 - 19.305280530535786 - 9.542064148993166 - 0.959538330588648'\n",
      "138 - random_65 - lwr_k=1000 - 7.972132898178722 - 7.185907738516046 - 5.776073851727314 - 8.233260492868292 - 18.566815315146847 - 9.546193612671777 - 0.9595208202269903'\n",
      "139 - random_34 - lwr_k=700 - 12.14638848802193 - 6.1097832376811185 - 5.841155928126068 - 7.813296298776314 - 15.835015380057666 - 9.54898997820654 - 0.9595089626649309'\n",
      "140 - random_79 - lwr_k=1000 - 7.766611161998614 - 6.58735073949518 - 5.872784627823677 - 8.241049252584437 - 19.384241103987797 - 9.56962354667968 - 0.9594214691610831'\n",
      "141 - random_31 - lwr_k=200 - 7.976932784123222 - 6.961995474981831 - 6.7478096761710775 - 9.699452628408983 - 16.475335837243957 - 9.571616614343132 - 0.9594130178612748'\n",
      "142 - random_65 - lwr_k=700 - 8.097913827540559 - 7.523578292708955 - 5.759797322287119 - 8.17908310235557 - 18.30791804856268 - 9.573080777260301 - 0.9594068093014712'\n",
      "143 - random_5 - lwr_k=1000 - 7.897501546036446 - 6.644129295012638 - 6.667467609713993 - 9.213994458478739 - 17.497880693288277 - 9.583437104884606 - 0.9593628948718355'\n",
      "144 - random_34 - lwr_k=400 - 12.128605833293426 - 6.148550232212471 - 6.077580939210002 - 7.967105746696055 - 15.626253663936303 - 9.58947157019907 - 0.9593373066409437'\n",
      "145 - random_65 - lwr_k=800 - 8.06290966786311 - 7.487994879241057 - 5.779546468611105 - 8.224535386731496 - 18.402248558493262 - 9.590852266555572 - 0.9593314520083756'\n",
      "146 - random_79 - lwr_k=300 - 9.163814695822662 - 6.364102222783902 - 5.734788697231127 - 8.090108830507159 - 18.61123806035545 - 9.592211565023689 - 0.9593256881102934'\n",
      "147 - random_79 - lwr_k=600 - 8.16753281809387 - 6.442846557639624 - 6.15124927774019 - 8.165479032829214 - 19.0388460349491 - 9.592441439827152 - 0.9593247133612078'\n",
      "148 - random_92 - lwr_k=800 - 8.233031856058487 - 7.276188288790867 - 6.180932381035702 - 8.07185824338847 - 18.249752261732535 - 9.60174748338026 - 0.9592852525011786'\n",
      "149 - random_78 - lr - 8.504631797078648 - 8.677793463159988 - 6.2582243854470505 - 7.137252542304446 - 17.444474920150686 - 9.60414358511547 - 0.9592750921967861'\n",
      "150 - random_92 - lwr_k=900 - 8.277685071175942 - 7.266224944597738 - 6.206128844359257 - 8.061732876196656 - 18.240363305258175 - 9.609824921531755 - 0.9592510013551924'\n",
      "151 - random_65 - lwr_k=600 - 8.112859140636356 - 7.78946061781097 - 5.811924466863403 - 8.235878796695433 - 18.103152282151687 - 9.610111587708397 - 0.9592497857909403'\n",
      "152 - random_82 - lr - 8.292623032570694 - 6.932111894794594 - 6.182607153480338 - 12.312945134061922 - 14.379225829428096 - 9.619245155077973 - 0.9592110562898928'\n",
      "153 - random_65 - lwr_k=500 - 8.098753001368795 - 8.038420026622113 - 5.892610214671745 - 8.282189873618414 - 17.814822881121014 - 9.624849367259854 - 0.9591872924818662'\n",
      "154 - random_34 - lwr_k=800 - 12.182144366613448 - 6.139585972345523 - 5.7617722931351905 - 7.897123774624842 - 16.16829690306656 - 9.629631094086644 - 0.9591670162976926'\n",
      "155 - random_17 - lr - 6.490814280595283 - 7.8566268071683 - 7.212019085126911 - 7.983250493040973 - 18.687275719454732 - 9.645187623652253 - 0.9591010512039095'\n",
      "156 - random_92 - lwr_k=1000 - 8.311386112671466 - 7.281936813113793 - 6.210354314045324 - 8.068207415231914 - 18.44197712430379 - 9.662161217542495 - 0.9590290772647297'\n",
      "157 - random_34 - lwr_k=900 - 12.265317652543375 - 6.152291869773232 - 5.708342776920207 - 7.917421617931159 - 16.333834406826405 - 9.675288844460367 - 0.9589734115626121'\n",
      "158 - random_84 - lwr_k=600 - 8.410653232049425 - 6.995669345968771 - 6.490500521469706 - 8.197611329253698 - 18.40898742015738 - 9.700030194872063 - 0.9588684996352221'\n",
      "159 - random_5 - lwr_k=900 - 8.189640612052289 - 6.628902327710784 - 6.831251520237651 - 9.555762830352274 - 17.34797525171131 - 9.709952804453952 - 0.9588264243208738'\n",
      "160 - random_84 - lwr_k=700 - 8.437698382296535 - 6.96580430670945 - 6.4379364637901295 - 8.229975282600474 - 18.49873020229324 - 9.713369920633419 - 0.9588119345602691'\n",
      "161 - random_65 - lwr_k=400 - 8.218628403700048 - 8.332282471591776 - 5.891199670085257 - 8.37790515352287 - 17.77497951833137 - 9.718526292961313 - 0.9587900697489207'\n",
      "162 - random_84 - lwr_k=1000 - 8.136480318961247 - 6.939449659624571 - 6.389811469391886 - 8.314962171465556 - 18.907339868608403 - 9.736888329603678 - 0.9587122083297621'\n",
      "163 - random_80 - lwr_k=300 - 11.625456009377148 - 6.330893004982995 - 7.578688304095527 - 7.3959058158072635 - 15.761104671971822 - 9.738160589557392 - 0.958706813505241'\n",
      "164 - random_65 - lwr_k=300 - 8.310669977539078 - 8.339814400532244 - 5.958882280799721 - 8.297183586989654 - 17.79822116167151 - 9.74049064564842 - 0.9586969332573415'\n",
      "165 - random_84 - lwr_k=800 - 8.379421272962636 - 7.0176783447060815 - 6.424825158897891 - 8.293042357216713 - 18.627799413257122 - 9.747881947528091 - 0.9586655915676919'\n",
      "166 - random_23 - lr - 9.17216903560341 - 7.398799302712966 - 6.99921882223602 - 7.851399171020762 - 17.35691766499514 - 9.755219313598856 - 0.9586344785846235'\n",
      "167 - random_79 - lwr_k=700 - 7.952449555569704 - 6.467174009350217 - 7.019660559713853 - 8.19959908169591 - 19.1641227261352 - 9.759765820714678 - 0.9586151998138015'\n",
      "168 - random_11 - lwr_k=300 - 7.3943253705075325 - 8.166858186388636 - 6.082651108717292 - 9.511161696298766 - 17.656587127993387 - 9.761667696861249 - 0.9586071351977287'\n",
      "169 - random_80 - deep - 9.35667500285197 - 8.285823323722752 - 6.716800460456142 - 8.608006351995819 - 15.8545239938272 - 9.764056871816873 - 0.9585970043533256'\n",
      "170 - random_82 - lwr_k=700 - 8.678665882241411 - 7.981631457032763 - 6.414504738070306 - 12.408146213809246 - 13.402194129281689 - 9.776554641017015 - 0.9585440093788733'\n",
      "171 - random_84 - lwr_k=900 - 8.319230455010995 - 7.039305359614953 - 6.406112558003487 - 8.373508242955925 - 18.812037512543213 - 9.7893475628393 - 0.9584897629427254'\n",
      "172 - random_79 - lr - 7.718863049284788 - 7.282965134673252 - 5.903714898782132 - 8.321341148498057 - 19.928947619423273 - 9.83040322872796 - 0.9583156726458344'\n",
      "173 - random_13 - lwr_k=30 - 9.93132879150251 - 6.72836703026367 - 6.738157104759678 - 8.067692323848515 - 17.721055884093534 - 9.83682654020093 - 0.9582884355720434'\n",
      "174 - random_5 - lwr_k=600 - 7.72662247540361 - 6.721371722055468 - 8.491389053795169 - 9.290180614006717 - 17.071010412680824 - 9.859251545368329 - 0.9581933457537961'\n",
      "175 - random_5 - lwr_k=700 - 8.648954902868113 - 6.601080198170142 - 7.351696825013172 - 9.38169170079598 - 17.368273628304017 - 9.869604123733573 - 0.9581494472223223'\n",
      "176 - random_52 - lwr_k=400 - 10.148961417244248 - 7.041224029073289 - 8.104641080145347 - 8.464193997617901 - 15.66316987403296 - 9.884015827991355 - 0.9580883365858576'\n",
      "177 - random_5 - lwr_k=800 - 8.524731505702997 - 6.538680290835149 - 7.365718352598583 - 9.725116749123329 - 17.271127588414387 - 9.884304185365325 - 0.9580871138503408'\n",
      "178 - random_16 - lwr_k=100 - 9.390220608356143 - 7.055367988716435 - 6.135268073661872 - 9.106309899430718 - 17.805818111920395 - 9.898048122715497 - 0.9580288347777219'\n",
      "179 - random_17 - lwr_k=100 - 8.823594873822158 - 8.906229825677828 - 6.924239269275111 - 8.768793651336633 - 16.16635841156641 - 9.91749837880768 - 0.9579463589297629'\n",
      "180 - random_44 - lwr_k=400 - 7.611816464792565 - 6.527543164015549 - 5.526457212476539 - 8.2822152121555 - 21.735957953541128 - 9.935859040142942 - 0.9578685033423846'\n",
      "181 - random_44 - lwr_k=300 - 7.674736176219115 - 6.449146807800746 - 5.7664582014188674 - 8.26083544465012 - 21.681433004215755 - 9.965570696982141 - 0.9577425155877521'\n",
      "182 - random_82 - lwr_k=1000 - 8.449676714509957 - 6.895286425552062 - 5.94681543850286 - 14.831735738084397 - 13.727624538441235 - 9.969475275127815 - 0.9577259588189383'\n",
      "183 - random_52 - lwr_k=500 - 9.943777154167588 - 7.061527877642526 - 7.900503787798137 - 8.51561181467477 - 16.48933959194633 - 9.981667519446184 - 0.95767425946424'\n",
      "184 - random_52 - lwr_k=600 - 9.935177773185954 - 7.051149639423358 - 7.3854696270944 - 8.588884588410643 - 17.112523501001256 - 10.014142752589287 - 0.9575365531852866'\n",
      "185 - random_44 - lwr_k=500 - 7.709918887753867 - 6.7071068325464696 - 5.536260010650261 - 8.44974472725055 - 21.756613871060384 - 10.031004216759232 - 0.9574650547151042'\n",
      "186 - random_44 - lwr_k=200 - 7.709201824167678 - 6.654022887620621 - 5.804235549737201 - 8.376263832901573 - 21.64237138405835 - 10.036283904362616 - 0.9574426670041155'\n",
      "187 - random_16 - lwr_k=200 - 9.524300784276265 - 7.156082858706487 - 6.074995480213535 - 9.603170437553183 - 18.11347048932776 - 10.093829517731665 - 0.9571986535969684'\n",
      "188 - random_10 - lr - 9.277043675982156 - 9.205959423871198 - 7.320615516094034 - 8.962923713418503 - 15.75123071120341 - 10.103272291736525 - 0.9571586129522877'\n",
      "189 - random_44 - lwr_k=600 - 7.82790715664645 - 6.725941300290136 - 5.586724947132695 - 8.583439576132902 - 21.80932336803965 - 10.105740548495206 - 0.9571481466855101'\n",
      "190 - random_16 - lwr_k=700 - 9.836660338885965 - 7.299083810959994 - 6.069109589128392 - 9.837343027440687 - 17.514707322611038 - 10.110875329433549 - 0.9571263734291597'\n",
      "191 - random_5 - lwr_k=500 - 8.408698733340183 - 7.240937675481559 - 8.434453279424266 - 9.437717166619388 - 17.078571768236415 - 10.119324043755713 - 0.9570905479431311'\n",
      "192 - random_52 - lwr_k=700 - 9.930690876850548 - 7.065670403095377 - 7.433877059532724 - 8.67195419503701 - 17.51436501066365 - 10.122779289985898 - 0.957075896497855'\n",
      "193 - random_16 - lwr_k=300 - 9.600663664399805 - 7.283935399760101 - 5.935810741544857 - 9.710839294060271 - 18.168126460919442 - 10.13931916794824 - 0.9570057616649963'\n",
      "194 - random_16 - lwr_k=600 - 9.781295305281784 - 7.32444013952372 - 6.0361119862472465 - 9.806889900762865 - 17.794966307222264 - 10.14821809050435 - 0.9569680271395351'\n",
      "195 - random_80 - lwr_k=400 - 13.589323913108968 - 6.445481657477128 - 7.55463652117439 - 7.159764862530669 - 16.00618931373977 - 10.151035475493098 - 0.9569560804476832'\n",
      "196 - random_55 - lwr_k=400 - 8.321038461544587 - 8.23343809598017 - 6.3806659056485975 - 9.461020353038327 - 18.39385430082764 - 10.157387485911663 - 0.9569291457151514'\n",
      "197 - random_16 - lwr_k=50 - 9.33020803927363 - 6.93792504057234 - 6.629679909831573 - 9.247815453239491 - 18.652335266173647 - 10.158929396083112 - 0.9569226074799599'\n",
      "198 - random_16 - lwr_k=400 - 9.675534317264278 - 7.2389901350668495 - 5.999579636741723 - 9.75480372424312 - 18.130806347907196 - 10.159385216195483 - 0.9569206746442118'\n",
      "199 - random_16 - lwr_k=500 - 9.744538846228668 - 7.312224221858425 - 6.007491964944207 - 9.749573549275722 - 18.059963945575824 - 10.174219328575903 - 0.9568577728504515'\n",
      "200 - random_52 - lwr_k=300 - 10.777256660145733 - 7.021484896322696 - 9.677600309773704 - 8.448257922348972 - 15.022601499489735 - 10.189017768839557 - 0.9567950223188681'\n",
      "201 - random_55 - lwr_k=500 - 8.266235562410595 - 8.330271363808727 - 6.329908792188404 - 9.659864354619721 - 18.36966090209743 - 10.190568272073634 - 0.9567884476461036'\n",
      "202 - random_44 - lwr_k=700 - 7.924468942796907 - 6.709493524836853 - 5.709077288373761 - 8.805136043793786 - 21.842273095531183 - 10.197146236068217 - 0.956760554791854'\n",
      "203 - random_52 - lwr_k=800 - 10.02753659188268 - 7.0660343412456745 - 7.40879799344999 - 8.60642648034076 - 17.93124835415364 - 10.20746471314933 - 0.9567168008616806'\n",
      "204 - random_55 - lwr_k=600 - 8.288402665646634 - 8.373770332482813 - 6.427398943595964 - 9.655382360210044 - 18.38332136727902 - 10.225034675799739 - 0.9566422981116225'\n",
      "205 - random_44 - lwr_k=800 - 8.006622182506694 - 6.669338256069942 - 5.749002157661001 - 8.955158092665695 - 21.876345446296373 - 10.250339137304309 - 0.9565349983974262'\n",
      "206 - random_82 - lwr_k=500 - 9.379949517312086 - 8.551270875875828 - 6.675753296235087 - 13.20549693634048 - 13.478748260318843 - 10.257820448714543 - 0.956503275036074'\n",
      "207 - random_16 - lwr_k=900 - 9.919277801494534 - 7.273854563913705 - 6.484356755449061 - 9.864491820244355 - 17.751345872564237 - 10.258121036818645 - 0.9565020004380094'\n",
      "208 - random_66 - lr - 11.05047242150177 - 8.007655545555185 - 6.933675539629285 - 8.696198818660221 - 16.62703546201128 - 10.262767196058762 - 0.956482299107535'\n",
      "209 - random_55 - lwr_k=700 - 8.351575835389406 - 8.58639152687024 - 6.456667079940551 - 9.618503329882214 - 18.392353951489163 - 10.280504889772908 - 0.9564070851194528'\n",
      "210 - random_82 - lwr_k=600 - 9.214238806626458 - 8.021676676851312 - 6.426409239319567 - 14.096027667797182 - 13.663552712536106 - 10.283835278599309 - 0.9563929631129771'\n",
      "211 - random_52 - lwr_k=900 - 10.06796165800975 - 7.031614312217188 - 7.458954133290648 - 8.625944715316331 - 18.326369051385992 - 10.301594881530823 - 0.9563176562221962'\n",
      "212 - random_34 - lwr_k=200 - 12.60320704000355 - 6.685835176330457 - 6.803015061316833 - 9.202815466823065 - 16.27053204222928 - 10.312862008642693 - 0.9562698796860822'\n",
      "213 - random_44 - lwr_k=900 - 8.06013397588789 - 6.651728735229327 - 5.8072800265725055 - 9.13607940871088 - 21.949643717728463 - 10.320002142265057 - 0.9562396030371654'\n",
      "214 - random_35 - lwr_k=30 - 9.13466347206024 - 8.963811434394433 - 8.00068359301299 - 9.1208029186746 - 16.38793922335104 - 10.321163444927096 - 0.9562346787101368'\n",
      "215 - random_57 - lwr_k=200 - 8.479673740650234 - 6.961473516605476 - 6.886932119726613 - 9.642619611634565 - 19.711740839952327 - 10.335631271973192 - 0.9561733300935378'\n",
      "216 - random_55 - lwr_k=800 - 8.310369469008721 - 8.693999555542348 - 6.4825773276165854 - 9.623562401160148 - 18.615819430420284 - 10.344662040507773 - 0.9561350364952899'\n",
      "217 - random_11 - lr - 8.038039268975234 - 7.428061796840069 - 7.3842269779743965 - 9.286959868061134 - 19.604099788547995 - 10.347421071434875 - 0.9561233372449469'\n",
      "218 - random_16 - lwr_k=40 - 9.856661715831578 - 7.048299670529555 - 6.64264465854436 - 9.368490342915251 - 18.830121842075094 - 10.348622469312131 - 0.9561182429002661'\n",
      "219 - random_16 - lwr_k=800 - 9.884727320175541 - 7.3497311880623615 - 6.894735634598516 - 9.858453907094562 - 17.75991789237017 - 10.34894587752709 - 0.9561168715369978'\n",
      "220 - random_44 - lwr_k=1000 - 8.100948259741777 - 6.700739371330414 - 5.826240596434961 - 9.320651510917005 - 21.827140641495784 - 10.35417656320834 - 0.956094691611195'\n",
      "221 - random_35 - lwr_k=40 - 8.884077199563098 - 9.297094641261097 - 7.534465873895495 - 8.58227884778886 - 17.55253610519862 - 10.369671504684996 - 0.956028987672321'\n",
      "222 - random_55 - lwr_k=900 - 8.323574744542734 - 8.707099551492297 - 6.468247544712325 - 9.707349146768594 - 18.649791960526777 - 10.370604803316882 - 0.9560250301616487'\n",
      "223 - random_52 - lwr_k=1000 - 10.178994600356999 - 7.014113178369723 - 7.4781873705978485 - 8.627067037468976 - 18.594403892861536 - 10.377969623552904 - 0.9559938007633749'\n",
      "224 - random_96 - lwr_k=600 - 9.287253402988814 - 6.429354021658319 - 6.852293012954081 - 8.308979240808222 - 21.030514117000727 - 10.38085237131496 - 0.9559815769106399'\n",
      "225 - random_37 - lwr_k=300 - 7.711329521964305 - 7.561372172017168 - 6.767452053434881 - 9.3329781021682 - 20.582013630199388 - 10.390126958000682 - 0.9559422494386648'\n",
      "226 - random_55 - lwr_k=1000 - 8.336374523936463 - 8.81796245653175 - 6.382643910660035 - 9.771924570997111 - 18.646312211445583 - 10.39044950326029 - 0.9559408817346271'\n",
      "227 - random_96 - lwr_k=400 - 9.523323950687166 - 6.51285743578332 - 6.8175177070010555 - 8.170201962063873 - 20.982997719639986 - 10.400599244453192 - 0.955897843303283'\n",
      "228 - random_96 - lwr_k=500 - 9.285979842662394 - 6.4701186105406885 - 6.802144197422777 - 8.244741745710947 - 21.247994138663483 - 10.409366446649509 - 0.9558606673179415'\n",
      "229 - random_13 - lr - 10.093677897255386 - 7.409672582704506 - 6.9666168210270705 - 7.698868403265601 - 19.893168293208117 - 10.411856923428763 - 0.9558501068305516'\n",
      "230 - random_18 - lwr_k=300 - 9.210479512515608 - 7.841912394051119 - 8.275510894665228 - 9.659981926237531 - 17.100414633347203 - 10.417040430893032 - 0.9558281269567911'\n",
      "231 - random_16 - lwr_k=1000 - 9.942143246945582 - 7.2927803927960335 - 7.192617939442924 - 9.889445076998987 - 17.815524493596495 - 10.425909781811225 - 0.9557905178253556'\n",
      "232 - random_34 - lwr_k=300 - 12.344776264292959 - 6.285868870779406 - 6.184181932565282 - 11.534376798686017 - 15.792540660095735 - 10.42798439690407 - 0.9557817207360957'\n",
      "233 - random_57 - lwr_k=300 - 8.496146255712246 - 7.024874555464341 - 6.74215950666634 - 9.622086212218116 - 20.335628737416513 - 10.44330017068958 - 0.9557167765305221'\n",
      "234 - random_15 - lwr_k=200 - 7.81642616906054 - 7.5024258702862285 - 7.439072808609786 - 9.394417380157781 - 20.124357056155038 - 10.454424214529597 - 0.9556696067172255'\n",
      "235 - random_96 - lwr_k=800 - 9.328434617525145 - 6.483813317106753 - 6.800795271384974 - 8.368702180867883 - 21.3231119012532 - 10.460134762854278 - 0.9556453920069843'\n",
      "236 - random_84 - lr - 7.599386807332461 - 7.738894279235098 - 6.8800850248094445 - 9.24769036014035 - 20.857994153986603 - 10.463894562768447 - 0.9556294491482061'\n",
      "237 - random_57 - lwr_k=400 - 8.57334208607613 - 7.063673217629945 - 6.701577257945822 - 9.709834431538551 - 20.36206429350139 - 10.481225949951604 - 0.955555958041073'\n",
      "238 - random_18 - lwr_k=400 - 9.249951088992152 - 7.698841966241699 - 8.084781734052873 - 9.660120927367272 - 17.72956407887306 - 10.483993614441422 - 0.9555442221814224'\n",
      "239 - random_18 - lwr_k=200 - 8.748066722674777 - 8.293201954207214 - 8.603680954720694 - 9.540657717094833 - 17.23899725626229 - 10.484277630806092 - 0.9555430178532943'\n",
      "240 - random_37 - lwr_k=200 - 7.586411559743607 - 7.642892784641342 - 6.934347276473193 - 9.706064123819727 - 20.556466279466886 - 10.484296308469808 - 0.9555429386535067'\n",
      "241 - random_35 - lwr_k=50 - 8.828879164246597 - 9.366217302227394 - 7.200239663309835 - 8.53508604725714 - 18.49475400745219 - 10.48458084456077 - 0.9555417321215585'\n",
      "242 - random_96 - lwr_k=300 - 9.33432186804578 - 7.047064994119966 - 6.81522274341878 - 8.149357286433194 - 21.09071957653829 - 10.486585158829058 - 0.9555332331322356'\n",
      "243 - random_96 - lwr_k=700 - 9.558672074981349 - 6.4587506388612255 - 6.827582121541635 - 8.343922794726348 - 21.248292652192486 - 10.486632288751574 - 0.9555330332849788'\n",
      "244 - random_57 - lwr_k=500 - 8.623055990985046 - 7.1076891542160485 - 6.670075708412615 - 9.706719614837846 - 20.470311335245757 - 10.514702439407177 - 0.9554140060872551'\n",
      "245 - random_96 - lwr_k=900 - 9.371605412783266 - 6.4782137968905635 - 6.806041663147352 - 8.422735663689478 - 21.505394916137195 - 10.515949465022546 - 0.9554087182651018'\n",
      "246 - random_65 - lwr_k=200 - 8.671293785287125 - 9.15174362242417 - 6.318998962378658 - 10.24227819525669 - 18.212860323047714 - 10.51890839628905 - 0.9553961713678218'\n",
      "247 - random_96 - lwr_k=1000 - 9.327531372523787 - 6.493836907598935 - 6.805684362368762 - 8.395630542437049 - 21.578532985021297 - 10.519388621547218 - 0.9553941350457708'\n",
      "248 - random_55 - lwr_k=300 - 8.234834992920678 - 8.209444690552488 - 7.007292037158694 - 10.325948659854037 - 18.97804080895011 - 10.550349515635308 - 0.9552628500908897'\n",
      "249 - random_92 - lr - 8.663191444985053 - 8.954969442490608 - 6.997428822529172 - 8.423975580926278 - 19.7150562633275 - 10.55035386910713 - 0.9552628316306557'\n",
      "250 - random_57 - lwr_k=600 - 8.641939017096172 - 7.167437259450928 - 6.679059565549382 - 9.703576338744346 - 20.582686891240805 - 10.554071875436092 - 0.9552470659912088'\n",
      "251 - random_98 - lwr_k=800 - 7.093933194977211 - 5.9703084814771685 - 16.019037481575957 - 7.545401605717772 - 16.28157753320262 - 10.580725335230603 - 0.9551340460552667'\n",
      "252 - random_98 - lwr_k=1000 - 7.051750882672253 - 6.116004653554745 - 15.798130643433975 - 7.682735265399439 - 16.293172058182783 - 10.587047261011326 - 0.9551072388920588'\n",
      "253 - random_31 - lr - 9.270479803445642 - 7.488097719515706 - 6.746092281606551 - 8.965978068515247 - 20.50519054231265 - 10.59444199815314 - 0.9550758826357034'\n",
      "254 - random_77 - lwr_k=100 - 7.507185086100277 - 10.896904808364383 - 8.923488921638947 - 10.905818795783484 - 14.878514054823702 - 10.621917182732659 - 0.9549593783009899'\n",
      "255 - random_57 - lwr_k=700 - 8.728920344810973 - 7.230942882954784 - 6.698780249577566 - 9.789500837823294 - 20.678424533657882 - 10.624447425518666 - 0.9549486491919061'\n",
      "256 - random_65 - lr - 8.71162047897096 - 7.789189270105046 - 6.439046172155861 - 8.740605669263054 - 21.454178611060595 - 10.626149745513077 - 0.9549414307632967'\n",
      "257 - random_15 - lwr_k=300 - 8.10074420125611 - 7.709316812692528 - 6.988550994782745 - 9.634003780750586 - 20.76794738612396 - 10.639216914758276 - 0.9548860214227299'\n",
      "258 - random_35 - lwr_k=100 - 8.704902890573752 - 9.30411972530823 - 6.845012474800049 - 8.62813724482383 - 19.72233828632091 - 10.64036621776099 - 0.9548811479784287'\n",
      "259 - random_96 - lwr_k=200 - 9.694741354397463 - 6.612876857750683 - 7.188205713521314 - 7.908591681332828 - 21.80904447993642 - 10.641876925554989 - 0.9548747420521654'\n",
      "260 - random_98 - lwr_k=900 - 7.0999266476332785 - 6.014017557553696 - 16.190918067491477 - 7.605101755041569 - 16.312885316007055 - 10.643231208923522 - 0.954868999419849'\n",
      "261 - random_57 - lwr_k=800 - 8.734074996782075 - 7.264814040050351 - 6.699843899697991 - 9.859070124051817 - 20.66903985527513 - 10.644502061453853 - 0.9548636105632925'\n",
      "262 - random_57 - lwr_k=100 - 8.543950582896919 - 6.885341921107742 - 7.391978949473387 - 9.75940397494121 - 20.76985058630316 - 10.66913731070576 - 0.9547591485417075'\n",
      "263 - random_37 - lwr_k=400 - 7.836592412059308 - 7.809233595919277 - 6.859207173544461 - 9.26131279451457 - 21.6067359791542 - 10.673682478024759 - 0.9547398754520899'\n",
      "264 - random_34 - lwr_k=1000 - 12.370685961260248 - 6.182100357114823 - 5.719999662542913 - 12.217660806754282 - 16.88545008812506 - 10.674721282408205 - 0.9547354705603506'\n",
      "265 - random_18 - lwr_k=500 - 9.298174272917963 - 7.860437158304852 - 7.96603515539514 - 9.701846149249707 - 18.572005442141467 - 10.679011771210147 - 0.9547172773961875'\n",
      "266 - random_93 - lwr_k=600 - 7.803412038455586 - 11.176703063842929 - 7.974601110535289 - 8.199888265810028 - 18.25339979269759 - 10.681210632128735 - 0.9547079534614308'\n",
      "267 - random_57 - lwr_k=900 - 8.802382356433958 - 7.3012834554027295 - 6.7083145319634365 - 9.946594838337747 - 20.668959754028148 - 10.68464447729991 - 0.9546933927640852'\n",
      "268 - random_56 - lwr_k=100 - 8.254808159737754 - 7.922930566255003 - 7.744202789315786 - 10.267392112313047 - 19.310280328820845 - 10.699067689461634 - 0.9546322333301068'\n",
      "269 - random_99 - lwr_k=300 - 7.0038721132314885 - 7.648487470530183 - 8.187593391146997 - 10.44154867252471 - 20.277222630346266 - 10.710636106558491 - 0.9545831792197093'\n",
      "270 - random_57 - lwr_k=1000 - 8.84913684891721 - 7.355355636428119 - 6.706071804631056 - 9.967620878891127 - 20.6950134704735 - 10.713784187182041 - 0.9545698302633953'\n",
      "271 - random_34 - lr - 13.153254916530496 - 6.864108354593673 - 6.35017685234203 - 9.0210406021691 - 18.183759216265297 - 10.714236848261299 - 0.9545679108230458'\n",
      "272 - random_99 - lwr_k=200 - 7.260239120989301 - 8.230112394232174 - 8.157973622698702 - 10.58907130291115 - 19.376324262478672 - 10.721769007738127 - 0.9545359718482133'\n",
      "273 - random_93 - lwr_k=500 - 7.73354557125442 - 11.210334203327035 - 8.075232690703409 - 8.576523486400326 - 18.103538339408182 - 10.739419631439265 - 0.9544611270672573'\n",
      "274 - random_93 - lwr_k=700 - 7.764605241938357 - 11.371297818087346 - 8.119942649595133 - 8.227828765043267 - 18.220428587510433 - 10.74043650597268 - 0.9544568151657062'\n",
      "275 - random_99 - lwr_k=400 - 7.006961730346269 - 7.8883880654065415 - 8.169666881009075 - 10.333395469044401 - 20.402782290533736 - 10.759154045614364 - 0.9543774462902355'\n",
      "276 - random_93 - lwr_k=1000 - 7.776050564744397 - 11.590805526632046 - 8.05860209511503 - 8.225932301606658 - 18.30016251316473 - 10.789948103918999 - 0.9542468687770789'\n",
      "277 - random_93 - lwr_k=800 - 7.843081036706837 - 11.46880871589564 - 8.142729022702 - 8.210242117329596 - 18.287228039536142 - 10.790046254451031 - 0.9542464525846999'\n",
      "278 - random_93 - lwr_k=900 - 7.7940779208474735 - 11.517125568663076 - 8.161707969440569 - 8.241683010454285 - 18.277819392951745 - 10.79810848688842 - 0.954212265916228'\n",
      "279 - random_26 - lwr_k=900 - 8.188019374087 - 8.945869043857874 - 6.809921231303866 - 10.706409297291064 - 19.422482526028272 - 10.813804222437131 - 0.9541457105406799'\n",
      "280 - random_35 - lwr_k=200 - 8.807295550452384 - 9.352450877488126 - 6.6765260350478055 - 9.090133263729665 - 20.31706185132159 - 10.848114239268751 - 0.9540002241410013'\n",
      "281 - random_35 - lwr_k=300 - 8.987909968236233 - 9.28137541265926 - 6.699834929905252 - 9.10644378817326 - 20.21481503454619 - 10.857511414345678 - 0.953960376851628'\n",
      "282 - random_18 - lwr_k=600 - 9.498262395280051 - 7.830008404860116 - 8.133402807957863 - 9.865309836825203 - 19.06726018596504 - 10.878123422976048 - 0.953872974769009'\n",
      "283 - random_16 - lr - 10.416281019347764 - 8.734646685815232 - 6.847828237767181 - 10.145522463098038 - 18.256602738176248 - 10.87974894461746 - 0.9538660819921163'\n",
      "284 - random_99 - lwr_k=500 - 7.058370074569551 - 7.996206004135772 - 8.277583908276046 - 10.257043870609403 - 20.83576229287304 - 10.883893605009828 - 0.9538485071911089'\n",
      "285 - random_5 - deep - 8.363282239573676 - 11.932893898990462 - 6.45366010267553 - 12.08134427146771 - 15.690954698879732 - 10.904179259704094 - 0.9537624890609554'\n",
      "286 - random_37 - lwr_k=500 - 7.917722802570685 - 7.798274584592818 - 7.028188258059286 - 9.298782561919362 - 22.509989769742155 - 10.90959189244521 - 0.9537395375180473'\n",
      "287 - random_93 - lwr_k=400 - 8.063501363976735 - 11.976063604275403 - 8.102257642740087 - 8.644403318329003 - 17.763907717387408 - 10.909735180546832 - 0.9537389299266805'\n",
      "288 - random_55 - lr - 9.485226962548674 - 9.497351467937976 - 6.987098586948574 - 9.72916916273703 - 18.859168255486665 - 10.911137744288528 - 0.9537329825687966'\n",
      "289 - random_15 - lwr_k=100 - 7.815302738772716 - 8.760343942388578 - 8.79683228990789 - 10.689088687631207 - 18.552806877657275 - 10.922011946221366 - 0.9536868721720472'\n",
      "290 - random_35 - lwr_k=400 - 9.019041824521471 - 9.31974282651743 - 6.697105507926347 - 9.310182976444048 - 20.35451790444669 - 10.93953830756758 - 0.9536125543066775'\n",
      "291 - random_18 - lwr_k=700 - 9.593501089789982 - 7.959853399243242 - 8.11882129080776 - 9.84722292997833 - 19.183205836336587 - 10.939812265489667 - 0.9536113926298428'\n",
      "292 - random_35 - lwr_k=20 - 9.965228221835012 - 9.262183761358967 - 8.649841257692758 - 9.88138889502264 - 16.967687487829142 - 10.944829848136227 - 0.9535901163349951'\n",
      "293 - random_56 - lwr_k=200 - 8.31238437538106 - 7.4951579610975365 - 7.532311450274688 - 9.777008092424428 - 21.6232091848201 - 10.947017244388046 - 0.9535808410144114'\n",
      "294 - random_77 - deep - 10.129997791908377 - 10.986665495093638 - 6.516540925684374 - 10.833087069006067 - 16.286100628139735 - 10.95034995834599 - 0.9535667092473011'\n",
      "295 - random_82 - lwr_k=400 - 10.072732789037964 - 8.262276820474971 - 8.086544866557844 - 14.995700088487952 - 13.50274658685549 - 10.983405340672387 - 0.9534265428353826'\n",
      "296 - random_15 - lwr_k=400 - 8.317399811920257 - 8.011425671866055 - 7.115107785376923 - 9.660292347872357 - 21.952680928962096 - 11.010448946806445 - 0.9533118685433215'\n",
      "297 - random_35 - lwr_k=500 - 9.024362213349548 - 9.330894018488515 - 6.757001173034486 - 9.412250746757124 - 20.545307186219716 - 11.013361680652684 - 0.9532995175391646'\n",
      "298 - random_5 - lr - 7.219795100497344 - 7.163503431438895 - 11.971928606408577 - 11.242879905105218 - 17.511977469149265 - 11.020762483775288 - 0.9532681355609417'\n",
      "299 - random_56 - lwr_k=50 - 8.743407948601241 - 9.330466365532219 - 8.20387871410658 - 11.231610721168055 - 17.675516248165657 - 11.036321000712574 - 0.953202162040012'\n",
      "300 - random_36 - lwr_k=30 - 10.315783031115673 - 7.335951835631181 - 7.5969739255461395 - 10.391576636894653 - 19.60969447091911 - 11.04926759340054 - 0.9531472639859678'\n",
      "301 - random_52 - lwr_k=200 - 11.387399413036695 - 9.099353936103917 - 11.258357668051627 - 8.429886524445811 - 15.120938191098396 - 11.05891997429513 - 0.9531063345352021'\n",
      "302 - random_18 - lwr_k=800 - 9.70268269509007 - 8.048903038758887 - 8.120347758964476 - 10.103613336603747 - 19.341745348798586 - 11.06274199039386 - 0.953090127858171'\n",
      "303 - random_93 - lwr_k=300 - 8.906104583638795 - 11.921206419078501 - 8.482307417090341 - 8.652740391484263 - 17.422393007849696 - 11.07673313937688 - 0.9530308005222827'\n",
      "304 - random_37 - lwr_k=600 - 8.124119850481028 - 7.984736997205943 - 7.0214319005820816 - 9.479892361566835 - 22.85278319647454 - 11.091597883552106 - 0.9529677688390629'\n",
      "305 - random_35 - lwr_k=600 - 9.030251479946639 - 9.349293608730793 - 6.936469510189678 - 9.48344989753959 - 20.676209557927393 - 11.09451081798438 - 0.9529554169843515'\n",
      "306 - random_10 - deep - 10.33067052610182 - 11.382877679191127 - 7.954366040366483 - 9.834830644577863 - 16.03341594980351 - 11.107150273958244 - 0.9529018213969005'\n",
      "307 - random_99 - lwr_k=600 - 7.228444648064683 - 8.21353869545863 - 8.291408228439916 - 10.543497454351396 - 21.313533072347585 - 11.116971895498622 - 0.9528601742068126'\n",
      "308 - random_56 - lwr_k=400 - 8.238322056755498 - 7.671480091811358 - 7.392944224506372 - 9.65689516265045 - 22.666916038355975 - 11.124273227460263 - 0.9528292140208946'\n",
      "309 - random_37 - lwr_k=100 - 8.531579799328522 - 9.650196326291468 - 8.12325981309015 - 10.943122655033605 - 18.377409562948603 - 11.124447436621415 - 0.9528284753134852'\n",
      "310 - random_15 - lwr_k=500 - 8.43406097649074 - 8.253096165065458 - 7.024087588691384 - 9.77876306038414 - 22.150276079589393 - 11.127144876810469 - 0.9528170372292843'\n",
      "311 - random_18 - lwr_k=900 - 9.816902511718304 - 8.131680477802083 - 8.176109708338538 - 10.239665788619421 - 19.36029889678901 - 11.144220607165378 - 0.9527446301959853'\n",
      "312 - random_56 - lwr_k=300 - 8.266855564166377 - 7.608910657063787 - 7.4086243389028885 - 9.585910907046003 - 22.903246355207777 - 11.153656076200352 - 0.9527046205269166'\n",
      "313 - random_67 - lwr_k=300 - 11.799047989036918 - 8.280857780549416 - 7.385635809763982 - 12.40749718349218 - 15.963035956422566 - 11.166845773373543 - 0.9526486916253367'\n",
      "314 - random_67 - lwr_k=500 - 11.916996259590352 - 8.293658898966733 - 7.2760949721067485 - 12.398755001950201 - 15.969635841095563 - 11.170679185206808 - 0.9526324366264295'\n",
      "315 - random_35 - lwr_k=700 - 9.061084079979855 - 9.385150215196266 - 7.109939689438469 - 9.490660402559133 - 20.833340052074803 - 11.175395320880568 - 0.9526124385715483'\n",
      "316 - random_26 - lr - 8.137037225303063 - 8.960686054463249 - 6.871598047601815 - 11.008763510201085 - 20.92485351141735 - 11.179725797837294 - 0.9525940758437066'\n",
      "317 - random_66 - lwr_k=20 - 12.483841794964018 - 10.600438088925353 - 7.871938183765786 - 9.723570706077469 - 15.237009834768742 - 11.183477219725573 - 0.9525781685106709'\n",
      "318 - random_35 - lwr_k=800 - 9.103791054816046 - 9.396496121786056 - 7.127063988725631 - 9.481441836401366 - 20.841087949004727 - 11.189340908469395 - 0.9525533044318113'\n",
      "319 - random_67 - lwr_k=400 - 11.822271783670386 - 8.363398688410395 - 7.31823362025612 - 12.468026991904102 - 16.00224576515355 - 11.194474472504147 - 0.9525315363355553'\n",
      "320 - random_35 - lwr_k=900 - 9.15021817474082 - 9.420977048173357 - 7.130425748335515 - 9.471416521523917 - 20.906509022942274 - 11.215277139497625 - 0.952443325795192'\n",
      "321 - random_36 - lwr_k=300 - 9.628491569563037 - 6.904183234825688 - 7.485814816745657 - 11.329689345222095 - 20.757839588619973 - 11.220236012842745 - 0.9524222984481935'\n",
      "322 - random_5 - lwr_k=300 - 7.915507200481742 - 7.179369588427334 - 12.111546355992276 - 11.596746072675028 - 17.314760928073767 - 11.22238211636282 - 0.9524131982231483'\n",
      "323 - random_18 - lwr_k=1000 - 9.95277035185747 - 8.15704902852797 - 8.294387044956935 - 10.401959387653292 - 19.317798024639266 - 11.22408214591987 - 0.9524059895067896'\n",
      "324 - random_56 - lwr_k=500 - 8.29946027181252 - 7.76888187964544 - 7.348846598136647 - 9.987249542843019 - 22.765700022802378 - 11.232979732284184 - 0.9523682606472461'\n",
      "325 - random_92 - lwr_k=100 - 9.651629607509047 - 8.449061828740694 - 8.33547321267161 - 8.236748192647351 - 21.57635504118959 - 11.249133253034897 - 0.9522997641032867'\n",
      "326 - random_35 - lwr_k=1000 - 9.181719091447828 - 9.57983153626953 - 7.140650807666496 - 9.445283548010815 - 20.91334038177167 - 11.251552205910567 - 0.9522895069021142'\n",
      "327 - random_5 - lwr_k=400 - 10.212122609393058 - 7.053548737556793 - 11.902070489555259 - 9.973566022415302 - 17.128439638117094 - 11.253091102870433 - 0.9522829814439873'\n",
      "328 - random_67 - lwr_k=200 - 11.656607721462038 - 8.334381714519084 - 7.498860462914711 - 12.708011599481162 - 16.104439077078553 - 11.26004584772674 - 0.9522534909078914'\n",
      "329 - random_67 - lwr_k=600 - 12.04675246672289 - 8.399952949813999 - 7.209390471488393 - 12.583921835295623 - 16.08504190044613 - 11.26467078860915 - 0.9522338795506305'\n",
      "330 - random_37 - lwr_k=700 - 8.25896436523515 - 8.062291712123287 - 7.053992519618595 - 9.610121193717637 - 23.362409796393223 - 11.268537765173027 - 0.9522174822255962'\n",
      "331 - random_67 - lwr_k=700 - 12.133753881569982 - 8.46978637320178 - 7.171068779725732 - 12.520303063945015 - 16.07356906346783 - 11.273377933346536 - 0.9521969582297948'\n",
      "332 - random_99 - lwr_k=700 - 7.251151006220537 - 8.361989374815225 - 8.326435275982632 - 10.671449466520807 - 21.763402473301444 - 11.273749670234725 - 0.9521953819361497'\n",
      "333 - random_29 - lwr_k=400 - 9.245169855281643 - 7.206770506483242 - 7.091543699079437 - 9.48728903314718 - 23.3464332736301 - 11.274442592963442 - 0.9521924437028766'\n",
      "334 - random_36 - lwr_k=20 - 9.90970993288312 - 7.919009959216366 - 7.708816364623311 - 10.643691226465972 - 20.202121710553545 - 11.275896198698454 - 0.9521862799091957'\n",
      "335 - random_29 - lwr_k=600 - 8.399330171191101 - 7.294542438362763 - 6.838822675593324 - 9.809616686562709 - 24.057621739107866 - 11.278862442036685 - 0.9521737020061894'\n",
      "336 - random_29 - lwr_k=500 - 9.24759286527376 - 7.278575368573296 - 6.9192452346187645 - 9.529868351844454 - 23.43798459063824 - 11.281664394301947 - 0.9521618207544505'\n",
      "337 - random_82 - deep - 9.545721391218969 - 10.144600848714576 - 7.3080058531327685 - 11.571789434168986 - 17.847468303521083 - 11.283046063033243 - 0.9521559621004065'\n",
      "338 - random_15 - lwr_k=600 - 8.503105793347638 - 8.50230266418563 - 7.069320265990054 - 10.032655534710434 - 22.448151844521515 - 11.31018748811556 - 0.9520408729202655'\n",
      "339 - random_67 - lwr_k=800 - 12.242687436971373 - 8.486763260803263 - 7.150255338805287 - 12.594505465599571 - 16.07888639189896 - 11.310309805038083 - 0.952040354253989'\n",
      "340 - random_56 - lwr_k=600 - 8.386000983142898 - 8.08951849262546 - 7.375361307871153 - 9.968330551250311 - 22.748364584006275 - 11.31250789527557 - 0.9520310335871881'\n",
      "341 - random_98 - lwr_k=700 - 10.920330970140387 - 5.922866713107123 - 16.2369067367542 - 7.5310429766303315 - 15.994802722793594 - 11.320240427951852 - 0.9519982449603276'\n",
      "342 - random_57 - lr - 9.151332253731086 - 8.981829074230578 - 6.989904579719313 - 10.548722885150031 - 21.006689241065928 - 11.334952487426907 - 0.9519358607133193'\n",
      "343 - random_36 - lwr_k=400 - 9.800234199224377 - 6.83196921704458 - 7.5919338057045636 - 11.392982162914702 - 21.098169558298732 - 11.342066481586302 - 0.9519056949048131'\n",
      "344 - random_67 - lwr_k=1000 - 12.36546980586969 - 8.471675463903532 - 7.1333037346971295 - 12.677130232193129 - 16.070669228219163 - 11.343346736677825 - 0.9519002661781282'\n",
      "345 - random_67 - lwr_k=900 - 12.319565031981325 - 8.45893604645697 - 7.136834282986961 - 12.720255159107877 - 16.082904293263343 - 11.34338638754281 - 0.9519000980446756'\n",
      "346 - random_29 - lwr_k=700 - 8.578738925666283 - 7.3018174753244205 - 6.82878603826196 - 9.810517992010226 - 24.22203525739202 - 11.347263008391785 - 0.9518836598245195'\n",
      "347 - random_56 - lwr_k=700 - 8.447794774203443 - 8.144099866772645 - 7.345231527383105 - 9.985807326156635 - 22.8354271583301 - 11.350671401959366 - 0.9518692070508217'\n",
      "348 - random_44 - lr - 8.952185215458988 - 8.209780187846457 - 6.953025431721238 - 10.880106317510473 - 21.84270408152061 - 11.36664766205794 - 0.9518014621536542'\n",
      "349 - random_16 - lwr_k=20 - 10.128532724084687 - 8.122994608950576 - 7.5165155348182635 - 9.567311677271215 - 21.54740618090863 - 11.375815027643652 - 0.95176258933642'\n",
      "350 - random_52 - lr - 10.863577895527733 - 8.161457346912014 - 7.542821118347371 - 9.064095620791731 - 21.325164417845805 - 11.390807951357449 - 0.9516990141273933'\n",
      "351 - random_36 - lwr_k=900 - 9.92221546763458 - 6.782462966943225 - 7.662314017542429 - 11.501209172698596 - 21.11317272277179 - 11.39527800177546 - 0.9516800595595513'\n",
      "352 - random_56 - lwr_k=800 - 8.544803687410502 - 8.21405856077237 - 7.326212615450453 - 10.042374360565407 - 22.879966952267488 - 11.400493534257974 - 0.9516579438885794'\n",
      "353 - random_29 - lwr_k=800 - 8.605534370904692 - 7.338295014664384 - 6.831371489727647 - 9.93420766204412 - 24.327777661967147 - 11.406312130136012 - 0.9516332710191479'\n",
      "354 - random_36 - lwr_k=1000 - 9.934713947155016 - 6.982119128886427 - 7.46882098214003 - 11.531615163256692 - 21.129686001727457 - 11.408424621018712 - 0.9516243133233709'\n",
      "355 - random_80 - lwr_k=200 - 12.190898033228503 - 6.504245949751342 - 10.859513978496325 - 12.01485668164357 - 15.491042401773305 - 11.41143528668698 - 0.9516115470542414'\n",
      "356 - random_36 - lwr_k=700 - 9.97812416683486 - 6.912679867916083 - 7.552910343141947 - 11.506869400417093 - 21.127955006242125 - 11.414735002451756 - 0.9515975551122108'\n",
      "357 - random_56 - lwr_k=900 - 8.554017601671797 - 8.197988425785121 - 7.297170959864334 - 10.043461522138513 - 23.074554858462978 - 11.432437384683332 - 0.9515224908570912'\n",
      "358 - random_56 - lwr_k=1000 - 8.57404348191952 - 8.233626328447318 - 7.264443713789842 - 10.074314263671644 - 23.12680452597479 - 11.453647343206807 - 0.9514325532590405'\n",
      "359 - random_35 - lr - 9.411844120830105 - 9.9506041666992 - 7.740794684765573 - 9.4553498276479 - 20.74341389556857 - 11.459818670952496 - 0.9514063846838584'\n",
      "360 - random_36 - lwr_k=800 - 10.249060361675092 - 6.821949713838846 - 7.481927334966129 - 11.542448538956867 - 21.227929229074963 - 11.463703756842728 - 0.9513899105689845'\n",
      "361 - random_99 - lwr_k=800 - 7.307650952271011 - 8.5042099739029 - 8.49596788644025 - 10.854658021400772 - 22.168922606300836 - 11.465115897748767 - 0.9513839226005942'\n",
      "362 - random_29 - lwr_k=1000 - 8.609647309281254 - 7.424475952408523 - 6.798797310382008 - 10.007415295217946 - 24.49935592476256 - 11.466808220297617 - 0.9513767465646297'\n",
      "363 - random_75 - lwr_k=400 - 8.309038316924951 - 8.905688355035318 - 7.676924677418208 - 11.82178868172171 - 20.693796383815762 - 11.48050604049195 - 0.9513186630447847'\n",
      "364 - random_98 - lr - 7.2158913445487345 - 7.166968005063253 - 18.211578855457535 - 8.248071265356664 - 16.674862462011525 - 11.502062221852967 - 0.9512272573415342'\n",
      "365 - random_10 - lwr_k=50 - 9.898532675945368 - 14.575475751387666 - 6.549582789941739 - 10.147435456807756 - 16.347001282120033 - 11.503845774185468 - 0.9512196944595699'\n",
      "366 - random_37 - lwr_k=800 - 8.375534404918165 - 8.394849416486137 - 7.088776301412212 - 9.784146974202184 - 23.937763001974908 - 11.515188631847487 - 0.9511715967995945'\n",
      "367 - random_29 - lwr_k=900 - 8.895970869373405 - 7.393884001197405 - 6.824719142627128 - 10.007916033033524 - 24.48045151705792 - 11.519482807212528 - 0.9511533880031222'\n",
      "368 - random_16 - lwr_k=30 - 10.265243602772184 - 7.525402009263952 - 6.9900576388186595 - 9.309956776132719 - 23.567845661174175 - 11.53083774207589 - 0.9511052391316145'\n",
      "369 - random_23 - lwr_k=100 - 22.163102245670192 - 6.5522085565042385 - 6.304969967236026 - 7.035951203354128 - 15.597279869861675 - 11.531628175970008 - 0.951101887417099'\n",
      "370 - random_75 - lwr_k=500 - 8.447733899166044 - 9.081508220668717 - 7.676248756890734 - 11.7138199243678 - 20.74514985638759 - 11.53198554198526 - 0.9511003720609515'\n",
      "371 - random_75 - lwr_k=600 - 8.448536483404679 - 9.189136580413098 - 7.591538223457824 - 11.44920417936553 - 21.058533621376792 - 11.54649623539114 - 0.951038841676084'\n",
      "372 - random_36 - lwr_k=600 - 10.139459455678256 - 6.760846794598349 - 7.477243663953754 - 11.507618048637273 - 21.896616396189632 - 11.555339611701195 - 0.9510013426860174'\n",
      "373 - random_13 - lwr_k=20 - 13.606377285165271 - 10.884791217851646 - 8.175351359010207 - 10.299717500310456 - 14.846793486614633 - 11.562829840359207 - 0.9509695815124345'\n",
      "374 - random_75 - lwr_k=700 - 8.537754605369848 - 9.31327224785774 - 7.532304621447403 - 11.52969317661241 - 21.076394272835554 - 11.59700860212991 - 0.9508246516798468'\n",
      "375 - random_87 - lwr_k=900 - 8.746023864923604 - 8.893980208239057 - 7.422422187869806 - 9.382448380869143 - 23.583920498773924 - 11.60484671210709 - 0.9507914153685251'\n",
      "376 - random_99 - lwr_k=900 - 7.321427304346893 - 8.651354575555676 - 8.590193735626949 - 11.135555786333184 - 22.347178095941157 - 11.607955473833046 - 0.9507782331380066'\n",
      "377 - random_75 - lwr_k=800 - 8.580741551098113 - 9.33078918494676 - 7.475731769867765 - 11.534496481493086 - 21.147729697106563 - 11.613027217050195 - 0.95075672718351'\n",
      "378 - random_75 - lwr_k=900 - 8.582864683202605 - 9.399114250180553 - 7.467269398834792 - 11.484697233292382 - 21.217494722938003 - 11.62942370576859 - 0.9506872003708968'\n",
      "379 - random_87 - lwr_k=1000 - 8.758198671158748 - 8.858126888287785 - 7.495489453660377 - 9.32564928681186 - 23.723378282280574 - 11.63124367421399 - 0.9506794830719532'\n",
      "380 - random_79 - deep - 10.980880799738358 - 8.982656263485673 - 7.754505259008509 - 10.597175655942975 - 19.866092637364343 - 11.635720435904203 - 0.9506605001508949'\n",
      "381 - random_38 - lwr_k=300 - 9.097027133587483 - 8.648884804166059 - 8.280456487574774 - 10.352533961176078 - 21.813614003841767 - 11.637597580571542 - 0.950652540299981'\n",
      "382 - random_64 - lwr_k=50 - 11.657756955658515 - 8.310411512231394 - 9.02096125785876 - 9.916316155380303 - 19.308469362631598 - 11.642239836433525 - 0.9506328555211871'\n",
      "383 - random_15 - lwr_k=700 - 8.570492405973452 - 8.937908331102443 - 7.28579280480172 - 10.29896670142748 - 23.14088924066774 - 11.645862587304388 - 0.9506174937979994'\n",
      "384 - random_31 - lwr_k=100 - 9.784945345737336 - 9.155894268483031 - 8.226607870471863 - 10.610626236533884 - 20.525251373983032 - 11.659947728977336 - 0.9505577679004279'\n",
      "385 - random_96 - lr - 10.043955210340414 - 7.312252333083808 - 7.776819739913268 - 9.554858642624344 - 23.671771477468006 - 11.670951022946005 - 0.950511110108566'\n",
      "386 - random_37 - lwr_k=900 - 8.48557686900577 - 8.508734258834863 - 7.163489826501414 - 9.97196138884924 - 24.287472421928644 - 11.682403464768184 - 0.9504625477736534'\n",
      "387 - random_69 - lwr_k=50 - 9.112216579389845 - 8.124554068795558 - 7.811745014737863 - 10.33665180741349 - 23.18433398649338 - 11.712886531756638 - 0.9503332889717975'\n",
      "388 - random_87 - lwr_k=800 - 8.805659737056398 - 8.982987698816684 - 8.017272157970451 - 9.483983978495393 - 23.28364852067982 - 11.713786761586995 - 0.9503294716843405'\n",
      "389 - random_75 - lwr_k=1000 - 8.605343012737452 - 9.625460590819344 - 7.470470328907423 - 11.57082604966656 - 21.305953905795644 - 11.714759227241153 - 0.9503253480918769'\n",
      "390 - random_38 - lwr_k=400 - 9.044167140351696 - 8.788217858601236 - 8.285679209637442 - 10.385794723088715 - 22.13688658687278 - 11.727228207491535 - 0.9502724752806161'\n",
      "391 - random_38 - lwr_k=200 - 9.178436293848472 - 8.813677273115248 - 8.458596261438108 - 10.436463318910558 - 21.77772806892133 - 11.732083919860541 - 0.9502518853720215'\n",
      "392 - random_56 - lwr_k=40 - 9.869644930868184 - 9.795627326524551 - 9.841657354570573 - 12.120650650870727 - 17.05683040963493 - 11.736258507178976 - 0.9502341836704431'\n",
      "393 - random_72 - lwr_k=50 - 10.01038784530108 - 10.90104993670454 - 10.08503592147155 - 11.113143289601881 - 16.627740443483702 - 11.74704844732686 - 0.9501884305729565'\n",
      "394 - random_84 - lwr_k=100 - 11.849017732626674 - 10.05543753378979 - 10.853972461368237 - 9.260261495171587 - 16.725887086079954 - 11.748654352153213 - 0.9501816209781796'\n",
      "395 - random_26 - lwr_k=700 - 8.23184470957241 - 8.19169436271256 - 6.719566084907099 - 10.538578057973915 - 25.14515844209284 - 11.7642045525766 - 0.9501156827221601'\n",
      "396 - random_72 - lwr_k=100 - 10.205935827130464 - 10.88261362691988 - 9.697227976935908 - 11.0842775164901 - 16.967289995844666 - 11.767068401056003 - 0.9501035390089542'\n",
      "397 - random_44 - deep - 9.52298825384162 - 8.898159126993482 - 7.309500273283537 - 10.91958726848568 - 22.1971662101152 - 11.7686421942461 - 0.9500968656847886'\n",
      "398 - random_36 - lr - 10.29410822263578 - 7.725626703605729 - 7.751538615116039 - 11.752885705807135 - 21.401445720803178 - 11.784212117375963 - 0.9500308436915271'\n",
      "399 - random_99 - lwr_k=1000 - 7.3709545054827545 - 8.830001850362953 - 8.658754340979325 - 11.322974644111113 - 22.75259068746529 - 11.785847877359423 - 0.9500239075005046'\n",
      "400 - random_57 - lwr_k=30 - 11.58951449857032 - 11.361265697934636 - 9.840321881112068 - 11.640743503918857 - 14.52551765453894 - 11.79136913237987 - 0.9500004954596835'\n",
      "401 - random_67 - lwr_k=100 - 11.97687349489222 - 8.834872827336284 - 7.76129023821001 - 14.085397818942802 - 16.31404134788184 - 11.794040381416497 - 0.9499891684350746'\n",
      "402 - random_61 - lwr_k=200 - 9.260711718922025 - 9.288649473264401 - 8.097498304434533 - 10.092770831615375 - 22.24180351712271 - 11.795460960598588 - 0.9499831446854602'\n",
      "403 - random_38 - lwr_k=500 - 9.101717904182696 - 8.88686214165089 - 8.199133910055423 - 10.27869403875739 - 22.53656527585323 - 11.799675608862925 - 0.9499652731115434'\n",
      "404 - random_48 - lwr_k=100 - 10.093909743725552 - 10.881361425105549 - 9.922751165659033 - 10.011451345256138 - 18.132808914142025 - 11.808023959113548 - 0.949929873204071'\n",
      "405 - random_37 - lwr_k=1000 - 8.574492552317597 - 8.63133197858534 - 7.245544226189414 - 10.08305538266727 - 24.518798838148985 - 11.809594085566943 - 0.9499232153220342'\n",
      "406 - random_61 - lwr_k=100 - 9.203290835394583 - 9.485685409931387 - 8.262061550243743 - 10.596051162700638 - 21.642563692448906 - 11.837113945107832 - 0.9498065215499526'\n",
      "407 - random_55 - lwr_k=200 - 8.299086347655328 - 12.254570019055052 - 7.449401352925185 - 11.063334628769281 - 20.158398522129037 - 11.844444622447778 - 0.9497754369294283'\n",
      "408 - random_64 - lwr_k=100 - 11.738978298778193 - 8.1906586393951 - 8.680969149124543 - 9.918332157007793 - 20.76052491758102 - 11.857272663763187 - 0.9497210415744197'\n",
      "409 - random_64 - lwr_k=300 - 11.450419451369813 - 8.22814793195513 - 8.698152777470161 - 9.775803582603691 - 21.157208524989937 - 11.861284045699358 - 0.9497040319204025'\n",
      "410 - random_72 - lwr_k=40 - 10.134048064902188 - 11.010483326068917 - 10.461496652282689 - 11.270249241358501 - 16.44938704640644 - 11.864709461202693 - 0.9496895069677793'\n",
      "411 - random_38 - lwr_k=600 - 9.167461159672172 - 9.014460582741927 - 8.067907294520246 - 10.210455497453928 - 22.949093575349895 - 11.880961616789374 - 0.9496205921778237'\n",
      "412 - random_69 - lwr_k=40 - 9.178830983144197 - 8.535695692894446 - 7.877800702296254 - 10.325099332959827 - 23.512122311268755 - 11.884917943871958 - 0.9496038159755262'\n",
      "413 - random_64 - lwr_k=400 - 11.400923100361904 - 8.372718084329717 - 8.734270582587945 - 9.677994968154618 - 21.24871950847124 - 11.886270228425287 - 0.9495980818188813'\n",
      "414 - random_15 - lwr_k=800 - 8.73379746427244 - 9.20749534003859 - 7.413483178040771 - 10.512936939710137 - 23.602189183370587 - 11.893023049539451 - 0.9495694475096533'\n",
      "415 - random_57 - lwr_k=50 - 9.387350935799295 - 8.377573198934492 - 8.318934183351823 - 10.99645090649681 - 22.438695304410675 - 11.902811438355217 - 0.949527941338017'\n",
      "416 - random_64 - lwr_k=500 - 11.484444743729005 - 8.37986393311061 - 8.791575925235833 - 9.630604164498786 - 21.260705566901343 - 11.908791319772478 - 0.9495025845618261'\n",
      "417 - random_64 - lwr_k=40 - 11.498796483841184 - 8.387516177250943 - 9.501723731023809 - 10.561311809368014 - 19.667052832057898 - 11.922631729919058 - 0.9494438964110106'\n",
      "418 - random_64 - lwr_k=600 - 11.442383407043698 - 8.398851043519697 - 8.897324435433827 - 9.616248383765534 - 21.274775462310792 - 11.925259824842373 - 0.9494327523748451'\n",
      "419 - random_64 - lwr_k=700 - 11.4729285739792 - 8.463175451356172 - 9.083355579642673 - 9.619545277568148 - 21.2076588384506 - 11.968677338687147 - 0.949248647021494'\n",
      "420 - random_72 - lwr_k=200 - 10.439233098206934 - 11.284833208992282 - 9.23095937229399 - 11.157393552830326 - 17.76907428092751 - 11.97593378843732 - 0.9492178771517564'\n",
      "421 - random_69 - lwr_k=200 - 9.13962655067183 - 8.377356351327824 - 8.084282850589718 - 10.624029163505844 - 23.683475261539574 - 11.980698439461943 - 0.9491976733749207'\n",
      "422 - random_51 - lwr_k=400 - 8.327543205195681 - 9.406012172183173 - 8.786745575110537 - 9.962870900401487 - 23.453971670062856 - 11.98640671310036 - 0.94917346831014'\n",
      "423 - random_64 - lwr_k=800 - 11.492642335428327 - 8.489397886873968 - 9.18903439890906 - 9.621352469647526 - 21.191768940163048 - 11.996182314409706 - 0.9491320163619766'\n",
      "424 - random_38 - lwr_k=700 - 9.195202411165459 - 9.353882412395334 - 8.014206077705644 - 10.238898091225547 - 23.205138168852397 - 12.000572383956944 - 0.9491134009408331'\n",
      "425 - random_64 - lwr_k=900 - 11.475904481457283 - 8.491505272468093 - 9.25125643733635 - 9.627200565823472 - 21.171306873428307 - 12.002773278632866 - 0.9491040683822382'\n",
      "426 - random_69 - lwr_k=100 - 8.88442886286715 - 8.638145463981319 - 8.243291849537119 - 10.843382467814743 - 23.423722928868337 - 12.00553149910655 - 0.9490923725684994'\n",
      "427 - random_61 - lwr_k=300 - 9.608710868907933 - 9.196157308345063 - 8.084342218177616 - 10.078161084248002 - 23.07254766669598 - 12.007130529867469 - 0.9490855921221498'\n",
      "428 - random_69 - lwr_k=400 - 9.46465002767868 - 8.500831124733715 - 8.107012517932993 - 10.77959239705882 - 23.201200299023814 - 12.00966565163927 - 0.9490748423244701'\n",
      "429 - random_87 - lwr_k=700 - 8.809744741783776 - 8.991721511052468 - 9.618142112214946 - 9.626768768932868 - 23.028970957013154 - 12.014049694671758 - 0.9490562524578442'\n",
      "430 - random_57 - lwr_k=40 - 9.97818496810505 - 10.50017560932816 - 8.908195414992779 - 11.72178987655111 - 19.001377400575745 - 12.02136081098763 - 0.9490252507828626'\n",
      "431 - random_51 - lwr_k=600 - 8.36447022100968 - 9.649554175430728 - 8.689592570835856 - 9.946854238437107 - 23.461366608205054 - 12.021380054886334 - 0.9490251691820417'\n",
      "432 - random_64 - lwr_k=1000 - 11.49069138244361 - 8.520734271178839 - 9.28500806413837 - 9.63438202003154 - 21.190361789750803 - 12.023574453378718 - 0.9490158641695235'\n",
      "433 - random_64 - lwr_k=30 - 11.944461654341067 - 8.334105292905472 - 9.90977355462137 - 11.877508100333305 - 18.06763704061125 - 12.026079013725438 - 0.9490052439629113'\n",
      "434 - random_69 - lwr_k=500 - 9.521861133057723 - 8.712637421556847 - 8.12298494964507 - 10.66731230733074 - 23.126881814112217 - 12.029381509667184 - 0.9489912402319637'\n",
      "435 - random_51 - lwr_k=300 - 8.362551214533095 - 9.34745879370959 - 8.967888036652846 - 9.954326124064126 - 23.52949645308614 - 12.03130356795909 - 0.9489830900365781'\n",
      "436 - random_51 - lwr_k=500 - 8.276328353283015 - 9.662722523188707 - 8.665382816017047 - 9.88371963295706 - 23.680945831637004 - 12.032816296327919 - 0.948976675542375'\n",
      "437 - random_69 - lwr_k=600 - 9.488215658922867 - 8.859474349846472 - 8.124868198153752 - 10.729684568459417 - 23.009215184047626 - 12.041352195590791 - 0.9489404803618868'\n",
      "438 - random_87 - lr - 8.8306874938954 - 9.016435525666395 - 7.824243178228371 - 9.483513338358659 - 25.074336995785952 - 12.044820780827234 - 0.9489257723545866'\n",
      "439 - random_79 - lwr_k=100 - 19.94411548003361 - 6.822927372730207 - 6.680109763841428 - 8.39113982853362 - 18.38794651547511 - 12.045686067424667 - 0.9489221032385856'\n",
      "440 - random_69 - lwr_k=30 - 9.448746271603461 - 8.626488647956222 - 7.9966920918183195 - 10.494052168067887 - 23.691326667307234 - 12.050474156509761 - 0.9489018000762249'\n",
      "441 - random_74 - lwr_k=300 - 6.918815791341085 - 26.257870168788592 - 5.123817661475237 - 6.885846796705805 - 15.074421427489195 - 12.053639939361688 - 0.9488883760563083'\n",
      "442 - random_15 - lwr_k=900 - 8.85594487855223 - 9.333701185715428 - 7.4553982884996834 - 10.5978206962772 - 24.04175433406307 - 12.055953809208885 - 0.9488785644437099'\n",
      "443 - random_69 - lwr_k=700 - 9.521701583232824 - 8.986639973038127 - 8.11116146000371 - 10.798620411129866 - 22.94815256369858 - 12.072331967691694 - 0.9488091153576683'\n",
      "444 - random_53 - lwr_k=300 - 10.370532955608732 - 10.29230960795435 - 7.360241497874273 - 11.021699029140903 - 21.352139353810053 - 12.078812043091812 - 0.9487816375850914'\n",
      "445 - random_53 - lwr_k=400 - 10.644718134946459 - 10.471805673251641 - 7.302263273918975 - 11.013106798568156 - 21.069982291744505 - 12.099870203102164 - 0.9486923436655111'\n",
      "446 - random_38 - lwr_k=800 - 9.30021936011252 - 9.419541421794092 - 8.0107479507001 - 10.333008520084203 - 23.470377823163 - 12.105879424987643 - 0.9486668624755344'\n",
      "447 - random_72 - lwr_k=300 - 10.54460693564008 - 11.488586374608545 - 9.158256143167225 - 11.180248202759607 - 18.17368124848203 - 12.108718001620607 - 0.9486548259237435'\n",
      "448 - random_12 - lwr_k=50 - 9.734974762272858 - 9.65975871418476 - 8.935877479772383 - 11.761258218114083 - 20.531666041822476 - 12.123912106175833 - 0.9485903976380062'\n",
      "449 - random_61 - lwr_k=400 - 9.942198079767891 - 9.312105281500845 - 8.351217071065722 - 10.079384022364337 - 22.96357477981035 - 12.128876281103151 - 0.948569347810456'\n",
      "450 - random_53 - lwr_k=500 - 10.849395835923184 - 10.578333148833979 - 7.130378533241259 - 11.172754904296971 - 20.92339981089612 - 12.130388392944422 - 0.9485629359305442'\n",
      "451 - random_69 - lwr_k=800 - 9.500510787950544 - 9.21110197256831 - 8.13280462217977 - 10.872882715381985 - 22.96227971608116 - 12.135005496291473 - 0.9485433578071581'\n",
      "452 - random_51 - lwr_k=700 - 8.468111837036776 - 9.79584382811542 - 8.741051364120148 - 9.933797998253615 - 23.77127233576331 - 12.141029706278262 - 0.9485178130624211'\n",
      "453 - random_65 - lwr_k=100 - 10.942650329273775 - 12.675821860183177 - 9.269854295294566 - 11.936129518421872 - 15.97137047025076 - 12.159050695307082 - 0.9484413978037136'\n",
      "454 - random_72 - lwr_k=30 - 9.545052745463654 - 11.068204186539262 - 10.997963693964834 - 11.357489563883858 - 17.83875817449292 - 12.160886217422085 - 0.9484336145518034'\n",
      "455 - random_51 - lwr_k=800 - 8.618945707120115 - 9.765596113187506 - 8.704685438120944 - 10.051285867864598 - 23.6692139181349 - 12.160972861153919 - 0.9484332471522576'\n",
      "456 - random_38 - lwr_k=900 - 9.310372357192058 - 9.4259491313327 - 8.02904467623374 - 10.358535452700828 - 23.698776773657034 - 12.163619884893011 - 0.9484220228513325'\n",
      "457 - random_15 - lwr_k=1000 - 8.994641790565296 - 9.408643761991254 - 7.5242231526105625 - 10.715311528168971 - 24.210380705440777 - 12.16966786176504 - 0.9483963773267392'\n",
      "458 - random_75 - lwr_k=300 - 8.259370240256716 - 8.830147129350255 - 9.636353503254643 - 13.306884723263968 - 20.83282756072612 - 12.171928369393406 - 0.9483867919885008'\n",
      "459 - random_74 - lwr_k=400 - 7.024139698321094 - 27.066298362046243 - 5.304007148508216 - 7.422108228421687 - 14.050615352642048 - 12.175029233760641 - 0.948373643245529'\n",
      "460 - random_72 - lwr_k=400 - 10.589381781332554 - 11.545481179663406 - 9.191034600857526 - 11.220610605024943 - 18.4479843596756 - 12.198527957734317 - 0.9482740004862511'\n",
      "461 - random_61 - lwr_k=500 - 10.11292377313445 - 9.351408932124903 - 8.462148777125487 - 10.01134884431767 - 23.069952120691735 - 12.200747781394076 - 0.9482645876621839'\n",
      "462 - random_69 - lwr_k=900 - 9.534556171091369 - 9.36636173652102 - 8.138107802870525 - 10.963825013452011 - 23.0243111157737 - 12.204530133321084 - 0.9482485491750308'\n",
      "463 - random_53 - lwr_k=600 - 10.890159791065349 - 10.87929499277502 - 7.155946741412293 - 11.29312825632858 - 20.88820546790251 - 12.220909316260602 - 0.9481790957449365'\n",
      "464 - random_47 - lr - 8.908540940684595 - 12.336903968077552 - 9.083840452329326 - 10.45567383788969 - 20.33397558200337 - 12.223262618714735 - 0.9481691169243756'\n",
      "465 - random_38 - lwr_k=1000 - 9.390099609755596 - 9.49587359036397 - 8.01267877190302 - 10.337896805470683 - 23.91499543166883 - 12.22939601319585 - 0.9481431091994232'\n",
      "466 - random_53 - lwr_k=200 - 10.089928423481865 - 10.258960365045372 - 7.861871833547497 - 11.11818763645446 - 21.83810714492118 - 12.232736783429088 - 0.9481289431721732'\n",
      "467 - random_51 - lwr_k=900 - 8.640460942246762 - 9.912147359028669 - 8.626020932543458 - 10.203586223838876 - 23.786643935819523 - 12.232803328596207 - 0.9481286609975293'\n",
      "468 - random_47 - lwr_k=1000 - 8.857674577293203 - 14.244350380083896 - 8.404804032667787 - 10.1902441223072 - 19.64536753798969 - 12.268253161785605 - 0.9479783413802222'\n",
      "469 - random_69 - lwr_k=1000 - 9.560039437542041 - 9.476332386771544 - 8.171581771303577 - 11.03282793991894 - 23.10548298139435 - 12.268351948091762 - 0.9479779224919398'\n",
      "470 - random_53 - lwr_k=700 - 11.054652259432336 - 10.972055925206108 - 7.1315632261007424 - 11.280688361657965 - 20.9330365609344 - 12.273986283147611 - 0.9479540309524549'\n",
      "471 - random_64 - lwr_k=20 - 11.224293213316672 - 8.578474933101711 - 10.647389315420831 - 11.744177442486926 - 19.250690775052973 - 12.288223206593797 - 0.9478936614473961'\n",
      "472 - random_47 - deep - 8.907901188745827 - 13.110862716325176 - 9.507269719504217 - 9.794840686151378 - 20.125946021099544 - 12.28894504136583 - 0.9478906007278085'\n",
      "473 - random_12 - lwr_k=100 - 9.8771131691818 - 9.671479892953997 - 8.183664829973536 - 11.892684693651757 - 21.83108788782329 - 12.290381824350527 - 0.9478845081576367'\n",
      "474 - random_51 - lwr_k=1000 - 8.651268722555333 - 10.033832270647915 - 8.66561541186518 - 10.05418488396595 - 24.103971058526522 - 12.300805344254963 - 0.947840308809373'\n",
      "475 - random_72 - lwr_k=500 - 10.694937599789437 - 11.46712119920031 - 9.233108438336933 - 11.227487442301966 - 18.91956648510764 - 12.308042263131162 - 0.9478096217573313'\n",
      "476 - random_64 - lr - 11.812752821599984 - 8.984809146599668 - 9.722195543465942 - 9.50606135806265 - 21.586928037674994 - 12.321919360718447 - 0.9477507780390122'\n",
      "477 - random_42 - lwr_k=20 - 9.849355980171119 - 11.649057088599625 - 7.80748720768621 - 14.008644121295264 - 18.30233393961155 - 12.322860138036667 - 0.9477467888161097'\n",
      "478 - random_53 - lwr_k=800 - 11.22998493680274 - 11.071260850744308 - 7.1056426665456724 - 11.297750706225301 - 21.071398310684906 - 12.354813001126924 - 0.9476112975677891'\n",
      "479 - random_75 - lwr_k=200 - 8.041557572134355 - 8.942263790341295 - 10.433195109424073 - 13.79686734211684 - 20.631327480455862 - 12.367772525284641 - 0.9475563446798173'\n",
      "480 - random_87 - lwr_k=400 - 8.831900811998514 - 9.047072951623695 - 12.32825033907893 - 9.663899045719685 - 21.978600997843923 - 12.3688213781485 - 0.9475518971790275'\n",
      "481 - random_56 - lr - 8.939780354526738 - 9.999652436511294 - 7.52962326905854 - 10.424905992795479 - 24.976433441362026 - 12.373127940277593 - 0.9475336358583692'\n",
      "482 - random_12 - lwr_k=200 - 10.036006658979467 - 9.699895479625278 - 7.73241732488851 - 11.97978427401432 - 22.467702407579164 - 12.38233751519396 - 0.9474945840588984'\n",
      "483 - random_61 - lwr_k=600 - 10.334649297262803 - 9.288728920351163 - 8.52650104699075 - 10.049369549493262 - 23.755325753881404 - 12.390070235094228 - 0.9474617946381431'\n",
      "484 - random_26 - lwr_k=30 - 11.757679827049255 - 8.634302942074743 - 10.246275024285392 - 12.253460088965914 - 19.0994782923897 - 12.397518014107938 - 0.9474302134658101'\n",
      "485 - random_87 - lwr_k=500 - 8.790912498610178 - 9.018249475378104 - 11.472766769417172 - 9.450813216132367 - 23.281904566631052 - 12.401783620505352 - 0.947412125811693'\n",
      "486 - random_12 - lwr_k=300 - 10.197179238087728 - 9.659239689514232 - 7.649121491271909 - 11.914838601372626 - 22.60499649802025 - 12.404263947393368 - 0.9474016083633706'\n",
      "487 - random_67 - lr - 13.908086449318706 - 9.529770434704055 - 7.549296301166704 - 14.447583499555405 - 16.60842381289258 - 12.408406226379459 - 0.9473840436603538'\n",
      "488 - random_38 - lwr_k=100 - 10.807173628523675 - 9.874556911582985 - 9.57663756715524 - 10.350593968908175 - 21.465102490546926 - 12.414133709923298 - 0.9473597571388956'\n",
      "489 - random_78 - lwr_k=30 - 10.669999116213765 - 9.52263959995051 - 9.602439523169085 - 13.926560918221194 - 18.40924443077067 - 12.425413705593789 - 0.9473119260356192'\n",
      "490 - random_12 - lwr_k=400 - 10.222768912662898 - 9.830587259489924 - 7.643891174795617 - 11.921716970809218 - 22.533987067356797 - 12.429803012509726 - 0.9472933138482974'\n",
      "491 - random_94 - lwr_k=50 - 9.695651166777962 - 11.90896942247273 - 9.18198245438596 - 10.957192108384001 - 20.413126102518518 - 12.430850740524248 - 0.9472888711172599'\n",
      "492 - random_16 - lwr_k=10 - 11.675798157852233 - 9.611192811889522 - 7.765597963048417 - 9.859527010019018 - 23.257738113381027 - 12.433384443028983 - 0.947278127337766'\n",
      "493 - random_64 - lwr_k=200 - 11.62557253477199 - 8.231025645224314 - 9.141085631850641 - 9.871520105971946 - 23.31241135875242 - 12.43550169491953 - 0.9472691494536606'\n",
      "494 - random_72 - lwr_k=600 - 10.7247742138364 - 11.567449124538191 - 9.304363065464138 - 11.236349057218359 - 19.35119115728815 - 12.436402623929572 - 0.9472653292014386'\n",
      "495 - random_74 - lwr_k=500 - 7.086215347413172 - 27.827079096076446 - 5.377942370280788 - 7.493225103942536 - 14.451394387031186 - 12.448811829463905 - 0.9472127098557553'\n",
      "496 - random_87 - lwr_k=600 - 8.793572808321276 - 9.033106184374724 - 11.583222854955164 - 9.640944412433127 - 23.20615194152368 - 12.450240950246606 - 0.9472066498867847'\n",
      "497 - random_82 - lwr_k=300 - 9.775223959386555 - 18.462846614053582 - 7.853033989229206 - 13.105811185028685 - 13.078387645533944 - 12.45560561875056 - 0.947183901827233'\n",
      "498 - random_53 - lwr_k=900 - 11.400928163630994 - 11.221554306990027 - 7.127066483515085 - 11.435960556002705 - 21.095795805746977 - 12.455886077151986 - 0.947182712586106'\n",
      "499 - random_12 - lwr_k=500 - 10.292370773194056 - 9.910575117064292 - 7.64964760497756 - 11.94342471265775 - 22.625130215155494 - 12.483449348359796 - 0.9470658347334641'\n",
      "500 - random_99 - lwr_k=100 - 9.174811721671418 - 11.15721248401538 - 9.521338020326185 - 13.142393808294601 - 19.441188213822617 - 12.486628614896457 - 0.9470523535380363'\n",
      "501 - random_61 - lwr_k=700 - 10.533611146194534 - 9.407195913958493 - 8.653456619326956 - 10.028727912933608 - 23.8767742923309 - 12.499124767043257 - 0.9469993655084881'\n",
      "502 - random_18 - lr - 10.979830720969579 - 9.579449964621775 - 9.527573674758782 - 12.255804662687098 - 20.172513849230967 - 12.502306428315878 - 0.9469858741585473'\n",
      "503 - random_51 - lwr_k=200 - 8.704352311213182 - 9.937646108365197 - 10.181432202459103 - 10.483250151385835 - 23.212023111638636 - 12.502698451298546 - 0.946984211844809'\n",
      "504 - random_53 - lwr_k=1000 - 11.48325218625707 - 11.316512522811797 - 7.118637106148357 - 11.406728435943274 - 21.19158234235013 - 12.502981143177589 - 0.9469830131329604'\n",
      "505 - random_29 - lwr_k=300 - 9.265729915577515 - 7.091272559711739 - 13.814269466180543 - 9.632768531632626 - 22.80092514042062 - 12.51957098683264 - 0.9469126664281935'\n",
      "506 - random_10 - lwr_k=100 - 8.071052330833153 - 23.50098834608309 - 6.059865688709832 - 7.9559656246542545 - 17.033752901620424 - 12.525393160911696 - 0.9468879783859456'\n",
      "507 - random_12 - lwr_k=600 - 10.320518518535765 - 10.03122582605194 - 7.642666649884671 - 11.999187758829578 - 22.66947048585728 - 12.531842031334211 - 0.9468606329333226'\n",
      "508 - random_72 - lwr_k=700 - 10.811022950127766 - 11.604226451806502 - 9.318973837895019 - 11.243636432509017 - 19.75066675189767 - 12.54526707276798 - 0.9468037060902574'\n",
      "509 - random_57 - deep - 11.417966859820627 - 10.93752107799736 - 7.853435932662427 - 11.291140320845845 - 21.273006048678962 - 12.554163113164224 - 0.9467659838993482'\n",
      "510 - random_72 - lwr_k=800 - 10.866448914514363 - 11.66791414787691 - 9.406269290797358 - 11.212622279404656 - 19.8309639036677 - 12.596408252142957 - 0.9465868497098279'\n",
      "511 - random_61 - lwr_k=800 - 10.748254308484698 - 9.534494699527565 - 8.74680719550814 - 10.103236918367196 - 23.936879211482438 - 12.613124720493339 - 0.9465159660723301'\n",
      "512 - random_12 - lwr_k=700 - 10.38119990036028 - 10.412390435909 - 7.664332420923012 - 12.062530887638763 - 22.673481347627913 - 12.638052761769883 - 0.9464102625107678'\n",
      "513 - random_18 - lwr_k=100 - 10.44964826080623 - 11.282282583671961 - 10.563148293939117 - 10.818357155546275 - 20.087949631289696 - 12.639696110296175 - 0.9464032941416853'\n",
      "514 - random_11 - lwr_k=200 - 7.476984221151436 - 21.19388888616284 - 6.86359655591389 - 9.639379309499407 - 18.082075501958407 - 12.651736475634337 - 0.9463522388066659'\n",
      "515 - random_72 - lwr_k=900 - 10.933560511229214 - 11.740233485376638 - 9.45916902100846 - 11.189799186137238 - 19.942591065938213 - 12.652639616225283 - 0.9463484091765719'\n",
      "516 - random_12 - lwr_k=800 - 10.479539011509996 - 10.3886428927337 - 7.6839605685160235 - 12.1903620530026 - 22.573094899485984 - 12.662389893586445 - 0.9463070646107531'\n",
      "517 - random_49 - lwr_k=30 - 9.596036938498253 - 11.971821138901225 - 11.204033034222428 - 12.474629787627423 - 18.100170611376715 - 12.668720843712428 - 0.9462802191811838'\n",
      "518 - random_0 - lwr_k=600 - 10.241078890641582 - 9.745130999527127 - 9.470129109580164 - 11.530262426790523 - 22.372668875390147 - 12.670976788692672 - 0.9462706531901597'\n",
      "519 - random_0 - lwr_k=700 - 10.017025273078332 - 9.792595907695791 - 9.400652963653837 - 11.6771670889343 - 22.540712417085093 - 12.684720030846075 - 0.9462123770654168'\n",
      "520 - random_61 - lwr_k=900 - 10.873169201684465 - 9.544589584344335 - 8.768723878508043 - 10.212377406850356 - 24.154101780870953 - 12.709773076861508 - 0.9461061434403022'\n",
      "521 - random_12 - lwr_k=900 - 10.602844075137671 - 10.429943810056956 - 7.714860442240331 - 12.260983596273821 - 22.633202068328544 - 12.72764239269081 - 0.9460303713286916'\n",
      "522 - random_74 - lwr_k=600 - 7.149452791342 - 27.82965410900097 - 5.461094156936814 - 7.5733246791224165 - 15.619091524085228 - 12.728083311274847 - 0.9460285016806044'\n",
      "523 - random_72 - lwr_k=1000 - 10.992184822687298 - 11.8310006621793 - 9.54041424513453 - 11.22077071767774 - 20.066719780379255 - 12.729786205001448 - 0.946021280819171'\n",
      "524 - random_42 - lwr_k=30 - 9.827610862379704 - 11.585016558803343 - 7.796930026026481 - 13.88003135661458 - 20.62060797974093 - 12.74137267073098 - 0.945972150176296'\n",
      "525 - random_86 - lwr_k=900 - 13.949181349508523 - 10.51198233123375 - 6.898081138891761 - 12.116228146422097 - 20.2811175661459 - 12.751147568937192 - 0.9459307012095344'\n",
      "526 - random_86 - lwr_k=1000 - 13.991342613952378 - 10.453374088384466 - 6.937641184987358 - 12.053807800434422 - 20.332956448828824 - 12.753650375876145 - 0.9459200884379806'\n",
      "527 - random_49 - lwr_k=20 - 9.859466869401984 - 13.19870103783892 - 11.758731967907876 - 12.571764903588965 - 16.411644754076516 - 12.759658769849919 - 0.9458946107586375'\n",
      "528 - random_12 - lwr_k=1000 - 10.741732432516068 - 10.453064068355156 - 7.7537560154750675 - 12.305052427698172 - 22.663228017026974 - 12.782650702795005 - 0.9457971169695132'\n",
      "529 - random_0 - lwr_k=400 - 10.334912121818265 - 10.034209833466289 - 9.785017116755895 - 11.547052400432996 - 22.23204926904189 - 12.78579598286319 - 0.9457837798885276'\n",
      "530 - random_0 - lwr_k=800 - 10.064137009931315 - 10.015694509764815 - 9.326637654170314 - 11.991020217171576 - 22.619479003170404 - 12.802488659054884 - 0.9457129971380542'\n",
      "531 - random_61 - lwr_k=1000 - 11.044966110917496 - 9.59269086235244 - 8.855547826485726 - 10.256402130493719 - 24.268145895439858 - 12.802736835948886 - 0.9457119447817418'\n",
      "532 - random_86 - lwr_k=800 - 13.843772013341077 - 10.594703041243328 - 7.303347677866787 - 12.022000190263595 - 20.269202033836255 - 12.806412632553366 - 0.9456963581262015'\n",
      "533 - random_4 - lwr_k=300 - 9.311319177086093 - 9.762960272707351 - 8.215623771508639 - 13.60744561369175 - 23.427128408960503 - 12.863805631949901 - 0.945452991855356'\n",
      "534 - random_72 - lwr_k=10 - 10.9838070050442 - 11.93399451225927 - 11.874821920951433 - 12.944614672297671 - 16.644163616914067 - 12.875816163542506 - 0.9454020630257898'\n",
      "535 - random_49 - lwr_k=50 - 9.037320897225346 - 11.602226212127762 - 10.529414866230908 - 12.248037243458693 - 21.056590388304723 - 12.893874645108985 - 0.9453254887856878'\n",
      "536 - random_49 - lwr_k=100 - 8.818122890911646 - 11.38596706842328 - 10.191531871162713 - 12.53422428261357 - 21.546444575315704 - 12.894343379814737 - 0.9453235011875738'\n",
      "537 - random_76 - lwr_k=700 - 9.212284001131717 - 8.54312015021463 - 6.661298638363057 - 18.25829185602275 - 21.862049522976772 - 12.906089132822162 - 0.9452736951112624'\n",
      "538 - random_29 - lr - 9.658339208123715 - 9.098382060717194 - 8.216370137663658 - 10.90837411491333 - 26.72464453677057 - 12.92006174919223 - 0.9452144463600959'\n",
      "539 - random_74 - lwr_k=700 - 7.1954720747975305 - 28.565189717257006 - 5.499190911071902 - 7.634833463102135 - 15.709864048363512 - 12.922534218734073 - 0.9452039622296531'\n",
      "540 - random_0 - lwr_k=900 - 10.001763115626606 - 10.084122319409584 - 9.54415702404314 - 12.214773676106892 - 22.858754588672916 - 12.93976514479565 - 0.945130897112602'\n",
      "541 - random_49 - lwr_k=40 - 9.32216211946832 - 11.78373253469713 - 11.089109027540918 - 12.003888517008916 - 20.507476741492628 - 12.940491627746523 - 0.9451278165723194'\n",
      "542 - random_69 - lr - 10.371930778400177 - 10.851076530676425 - 8.926391905063955 - 11.014852129155127 - 23.560124190144567 - 12.94411094377593 - 0.9451124694140536'\n",
      "543 - random_76 - lwr_k=200 - 9.616638590617258 - 8.327630520705709 - 6.996917914500785 - 18.514249199851996 - 21.307174157086727 - 12.951218527339826 - 0.9450823307887011'\n",
      "544 - random_86 - lwr_k=700 - 13.921088015132538 - 10.630468249239156 - 7.497826233908323 - 12.04178115687848 - 20.73433915756992 - 12.964874814246894 - 0.9450244233844337'\n",
      "545 - random_76 - lwr_k=800 - 9.308295326330498 - 8.6144602302858 - 6.6361279248181715 - 18.412747830566214 - 21.868279888482 - 12.96667010501817 - 0.945016810727408'\n",
      "546 - random_76 - lwr_k=900 - 9.339451405265933 - 8.612960754105947 - 6.656259923576456 - 18.414443650333464 - 21.949568637023617 - 12.9932208986904 - 0.9449042261315175'\n",
      "547 - random_46 - lwr_k=600 - 8.880806847661374 - 18.502623519308585 - 6.078965560504545 - 10.828866315078612 - 20.747318168886736 - 13.007940087233507 - 0.9448418116547822'\n",
      "548 - random_46 - lwr_k=700 - 8.916304986768077 - 18.720700539200436 - 6.042081865382447 - 10.675895498338855 - 20.71589037006731 - 13.014438063810285 - 0.9448142579749926'\n",
      "549 - random_42 - lwr_k=40 - 9.774409310805583 - 11.376551756147789 - 7.666159051461079 - 14.033787011677761 - 22.23033303481207 - 13.015448698610623 - 0.9448099725320792'\n",
      "550 - random_26 - lwr_k=400 - 8.176080159578607 - 8.828908788752162 - 6.5330665280087254 - 10.373310903824427 - 31.215502825998787 - 13.023892629715572 - 0.9447741673285545'\n",
      "551 - random_4 - lwr_k=200 - 9.430077730232076 - 9.670590378093632 - 8.34693602728047 - 14.424274050535136 - 23.26838504534346 - 13.026913717660266 - 0.9447613568653495'\n",
      "552 - random_76 - lwr_k=1000 - 9.400782368823657 - 8.640170735579701 - 6.6456610190218655 - 18.515452281048145 - 21.960519319804742 - 13.03120322948223 - 0.9447431678439211'\n",
      "553 - random_72 - lwr_k=20 - 9.657937900880029 - 11.326418536563418 - 11.190063209079776 - 11.832143767945698 - 21.310124537859934 - 13.062495553664803 - 0.9446104775102137'\n",
      "554 - random_46 - lwr_k=1000 - 8.961264968171218 - 19.322390814538995 - 6.166719496532647 - 10.585490022945892 - 20.2953208489482 - 13.066589478683632 - 0.9445931178448289'\n",
      "555 - random_29 - deep - 9.552723580998796 - 8.929788728392456 - 8.325419445490857 - 10.705373998551364 - 27.84022232511803 - 13.069451673932274 - 0.9445809812597342'\n",
      "556 - random_46 - lwr_k=900 - 8.917539353818048 - 19.106011811896668 - 6.154959673395883 - 10.570885471785378 - 20.605481704640628 - 13.07128370855345 - 0.9445732126781723'\n",
      "557 - random_46 - lwr_k=800 - 8.880474974995622 - 19.10864319508142 - 6.077088182094414 - 10.664807910665154 - 20.6254150118317 - 13.071588220491256 - 0.9445719214416886'\n",
      "558 - random_98 - lwr_k=600 - 19.82605708672437 - 5.878842174588728 - 16.2200541143771 - 7.480270470446726 - 15.95963686989697 - 13.072899923618586 - 0.9445663593643985'\n",
      "559 - random_86 - lwr_k=600 - 13.939212664149842 - 10.56835888925244 - 8.738995218484808 - 11.97152628759187 - 20.216893349789807 - 13.086724410884324 - 0.9445077387321333'\n",
      "560 - random_86 - lwr_k=500 - 13.899134175018773 - 10.645630509495126 - 8.852750834863482 - 11.841406422327628 - 20.21074867582016 - 13.089666380986202 - 0.9444952637560882'\n",
      "561 - random_42 - lwr_k=50 - 9.640676387128742 - 11.336780955454078 - 7.51768423383779 - 14.165700486987667 - 22.803015925044303 - 13.091918791754248 - 0.9444857127513525'\n",
      "562 - random_4 - lwr_k=100 - 9.448844445828874 - 11.003665476181691 - 10.262943895370356 - 13.599011163061713 - 21.14996583039246 - 13.091947360398176 - 0.94448559161037'\n",
      "563 - random_0 - lwr_k=1000 - 10.113163006737317 - 10.29265852349167 - 9.55465832986621 - 12.294219475201723 - 23.231002922626228 - 13.096192611503913 - 0.9444675902697663'\n",
      "564 - random_93 - lr - 8.089195181924577 - 19.63319171780152 - 10.477611353561999 - 8.98732448693661 - 18.36238696902181 - 13.110187971490049 - 0.9444082450777607'\n",
      "565 - random_79 - lwr_k=50 - 13.67256759760727 - 9.589681470194332 - 12.05912081947222 - 12.724910910247782 - 17.61393017402766 - 13.131550654221298 - 0.9443176598759726'\n",
      "566 - random_0 - lwr_k=300 - 10.524951019541232 - 10.529327400596694 - 10.285604560701861 - 11.558573195191977 - 22.886100109141395 - 13.156050024990432 - 0.9442137740263945'\n",
      "567 - random_49 - lwr_k=200 - 8.865038404266324 - 11.33912190195198 - 10.195824001856058 - 12.514550622180895 - 22.923208966691174 - 13.166544859747473 - 0.9441692722783622'\n",
      "568 - random_4 - lwr_k=400 - 9.277827330758882 - 10.084072200757593 - 8.067931625887601 - 13.908579047486871 - 24.526470493427055 - 13.171832525327954 - 0.944146850737977'\n",
      "569 - random_86 - lwr_k=400 - 13.839298764924367 - 10.599712853751484 - 9.50519140085792 - 11.84608352301367 - 20.20752598795428 - 13.199241544352592 - 0.9440306269682195'\n",
      "570 - random_74 - lwr_k=800 - 7.263400595343307 - 29.48695826526415 - 5.479597864445277 - 7.749546450257376 - 16.160870211642663 - 13.229760318377261 - 0.943901216756116'\n",
      "571 - random_42 - lwr_k=10 - 10.490597280915079 - 12.179963798288172 - 7.797034457287556 - 14.925597119155306 - 20.77474415903373 - 13.232965680662076 - 0.9438876249056435'\n",
      "572 - random_42 - lwr_k=100 - 9.561708626865869 - 11.24078545889024 - 7.359702849639916 - 13.785633828969702 - 24.268291711384595 - 13.242293767312413 - 0.9438480705752185'\n",
      "573 - random_53 - lwr_k=100 - 10.682157645756398 - 12.143982931966612 - 8.750501249574306 - 13.024642583462347 - 21.933592943146724 - 13.306355230014317 - 0.9435764277015825'\n",
      "574 - random_87 - deep - 9.875811473827705 - 9.782050730553859 - 8.083531816312274 - 12.99321495090519 - 25.88254552586561 - 13.322286563830856 - 0.9435088734275878'\n",
      "575 - random_75 - lr - 8.868549175333543 - 11.8255447567098 - 8.728306290466632 - 14.076215721790737 - 23.140792103738228 - 13.326905407006318 - 0.9434892878066067'\n",
      "576 - random_4 - lwr_k=500 - 9.311144110148618 - 10.247716445893355 - 7.948851602636184 - 14.02623221301659 - 25.15981925843445 - 13.337587072694252 - 0.9434439938304733'\n",
      "577 - random_49 - lwr_k=300 - 9.084635923087795 - 11.492658025791982 - 10.146895864088409 - 12.412994859158262 - 23.588213569380756 - 13.344078687871162 - 0.9434164671252309'\n",
      "578 - random_36 - deep - 11.889654159545898 - 8.147820039778802 - 8.213650376166017 - 13.295923132275481 - 25.363136249326665 - 13.380935344917832 - 0.9432601822063152'\n",
      "579 - random_67 - lwr_k=50 - 14.10809079372039 - 10.678386393835016 - 8.388414495270569 - 16.770722283710857 - 16.96158722327171 - 13.381116608800356 - 0.943259413463788'\n",
      "580 - random_74 - lwr_k=900 - 7.339585347931431 - 30.21305706480244 - 5.523565818037382 - 7.8209962618347495 - 16.078751995780166 - 13.396953580273824 - 0.9431922591988167'\n",
      "581 - random_67 - deep - 14.454988715301363 - 9.244372554379478 - 7.691564077241415 - 17.007515916660317 - 18.7140467438319 - 13.421982786027003 - 0.9430861267857189'\n",
      "582 - random_86 - lr - 15.303994804063459 - 10.458041699066335 - 6.883507847606163 - 13.748255886623962 - 20.719484705541046 - 13.422479605663208 - 0.9430840199767192'\n",
      "583 - random_69 - lwr_k=300 - 16.916371725193546 - 8.40678176249274 - 8.079979076778635 - 10.6023556068766 - 23.120733746330085 - 13.424994287696858 - 0.9430733568506353'\n",
      "584 - random_93 - lwr_k=200 - 15.715235099487892 - 15.021615545662554 - 9.582487360437728 - 10.115259200104788 - 16.960351067408 - 13.479608431652435 - 0.942841773892964'\n",
      "585 - random_70 - lwr_k=30 - 9.710752249234572 - 10.882986221316092 - 18.226885333258785 - 8.555532554853349 - 20.091202219304353 - 13.492424850527009 - 0.9427874278211452'\n",
      "586 - random_49 - lwr_k=400 - 9.209392574607337 - 11.53638148934185 - 10.260321867283162 - 12.521764521519271 - 24.002510405965193 - 13.505048074759648 - 0.9427339009617748'\n",
      "587 - random_74 - lwr_k=1000 - 7.3751921650024705 - 30.54544214671237 - 5.542504287616025 - 7.879119377154222 - 16.381499700392176 - 13.546525095428011 - 0.9425580239741287'\n",
      "588 - random_70 - lwr_k=50 - 9.506059231039673 - 10.951371613536294 - 17.42396060035756 - 8.794135076117108 - 21.231083263644827 - 13.580224001630544 - 0.9424151288959778'\n",
      "589 - random_47 - lwr_k=600 - 9.559417367481249 - 17.405293225620255 - 8.510656939904 - 11.360640600320062 - 21.16083449977577 - 13.599330205601742 - 0.9423341119486257'\n",
      "590 - random_46 - lwr_k=500 - 8.794583691962778 - 21.93586165619525 - 6.048539822717853 - 10.55944074029835 - 20.680624899331683 - 13.604387012440785 - 0.9423126693295651'\n",
      "591 - random_49 - lwr_k=500 - 9.3175711670476 - 11.663137852053302 - 10.263374447023121 - 12.567981678070549 - 24.253649323537882 - 13.612120202082927 - 0.942279877916941'\n",
      "592 - random_26 - deep - 11.434023209367368 - 9.70428475693673 - 7.863040157071301 - 12.488153873556076 - 26.613444107268112 - 13.619589872117398 - 0.9422482040418677'\n",
      "593 - random_4 - lwr_k=600 - 9.44997522042716 - 10.418739825046664 - 7.886931082211182 - 14.55976364152647 - 25.80715538587945 - 13.623304530769854 - 0.9422324524749347'\n",
      "594 - random_70 - lwr_k=40 - 9.522372833716917 - 10.778139039972547 - 17.877125964991407 - 8.799336112110694 - 21.1614832880189 - 13.626552611832532 - 0.9422186794819924'\n",
      "595 - random_12 - lwr_k=10 - 11.795401315520929 - 10.87354619620854 - 11.16311781486558 - 13.47237677443587 - 20.90585096982201 - 13.641302896188497 - 0.9421561331481997'\n",
      "596 - random_60 - lwr_k=100 - 9.25865260947841 - 11.05698469190208 - 12.842750588938365 - 13.070863563157321 - 21.983063049007892 - 13.641321703755432 - 0.9421560533975772'\n",
      "597 - random_42 - lwr_k=200 - 10.979986756196341 - 11.244914070035442 - 7.287706865271129 - 13.626647963363862 - 25.10139954467016 - 13.64730062214849 - 0.9421307007049439'\n",
      "598 - random_12 - lr - 11.382727957721189 - 12.50888108805502 - 8.127271860844962 - 12.917134287865046 - 23.32688301720871 - 13.652020685365201 - 0.9421106859959152'\n",
      "599 - random_60 - lwr_k=200 - 9.313601441410894 - 10.504901270836683 - 12.557676507811241 - 12.96396222885041 - 23.161194987072626 - 13.699025755861488 - 0.941911368155102'\n",
      "600 - random_19 - lwr_k=500 - 9.629047898428707 - 21.68017361632778 - 8.437439698146067 - 9.951853847067943 - 18.91394051500814 - 13.723123870732548 - 0.9418091837700334'\n",
      "601 - random_70 - lwr_k=20 - 9.81318677888097 - 11.342944269212921 - 18.815237049453607 - 8.652239678617752 - 20.015927279216196 - 13.726875459909749 - 0.9417932757276345'\n",
      "602 - random_11 - deep - 11.001792139780697 - 12.530085692429113 - 7.132792166663817 - 10.585779569853925 - 27.395862474683657 - 13.728619616867492 - 0.9417858800195368'\n",
      "603 - random_19 - lwr_k=400 - 9.62288234642114 - 21.713042890994522 - 8.433295275560555 - 9.912553751209513 - 18.963720339589113 - 13.729733885123055 - 0.9417811550109623'\n",
      "604 - random_19 - lwr_k=300 - 9.657622899099565 - 21.709180087611053 - 8.33205389694482 - 9.789972769591028 - 19.21253843405087 - 13.740909978316155 - 0.941733764490313'\n",
      "605 - random_19 - lwr_k=600 - 9.596248435966244 - 21.751961532127858 - 8.480976933614643 - 9.996030874700942 - 18.940668833426145 - 13.753806412408325 - 0.9416790791261533'\n",
      "606 - random_49 - lwr_k=600 - 9.346474050070992 - 11.85703152786966 - 10.367474785401686 - 12.624748427938073 - 24.619119526264747 - 13.761934386982247 - 0.9416446136881664'\n",
      "607 - random_19 - lwr_k=700 - 9.600920039309585 - 21.7283639936031 - 8.525794781300576 - 10.003667643690749 - 18.970010112562445 - 13.7663731875835 - 0.9416257916304118'\n",
      "608 - random_5 - lwr_k=200 - 10.487337461175684 - 8.80053464478451 - 15.084245057301407 - 14.954548850103745 - 19.52446491627892 - 13.768874854626102 - 0.9416151836924149'\n",
      "609 - random_60 - lwr_k=300 - 9.327417795064271 - 10.685184939316771 - 12.470347425891559 - 12.957010925771623 - 23.432753007182917 - 13.773308745794122 - 0.941596382452364'\n",
      "610 - random_1 - lwr_k=500 - 8.041975311114644 - 6.98157882489578 - 17.278993630376082 - 11.053960678759257 - 25.526848258257992 - 13.77461963140527 - 0.9415908238416267'\n",
      "611 - random_19 - lwr_k=800 - 9.588015027093881 - 21.75869711253382 - 8.519565609997564 - 10.00963932639225 - 19.022863013656817 - 13.780376158773864 - 0.9415664141642541'\n",
      "612 - random_19 - lwr_k=900 - 9.588707291388499 - 21.75107361216002 - 8.544614270461366 - 10.011139774163611 - 19.029072670383513 - 13.785538837917413 - 0.9415445226098147'\n",
      "613 - random_19 - lwr_k=1000 - 9.613574833751937 - 21.75632184532702 - 8.560467514905955 - 10.006817644149411 - 19.06672486678709 - 13.801398392563964 - 0.941477272584338'\n",
      "614 - random_42 - lwr_k=300 - 10.950852246334039 - 11.239319745975104 - 7.276649361491952 - 13.66427456458653 - 25.9097064938884 - 13.807271969441842 - 0.9414523665763522'\n",
      "615 - random_0 - lwr_k=500 - 10.152201927772774 - 9.660401925604775 - 9.644582223859251 - 11.391686772546421 - 28.258837751785894 - 13.820259906397455 - 0.9413972932082395'\n",
      "616 - random_13 - deep - 16.136629760167796 - 11.44541146478169 - 7.486267835756094 - 10.295060170663369 - 23.78743164115612 - 13.830147224221394 - 0.9413553676679113'\n",
      "617 - random_72 - lr - 11.418909873921262 - 13.111906748317404 - 11.978797799456132 - 12.06908322692273 - 20.594523561952975 - 13.834130328267369 - 0.941338477797283'\n",
      "618 - random_46 - lwr_k=400 - 8.902863344692676 - 22.499742869963757 - 6.1012927684671645 - 10.566643152523088 - 21.12486959138291 - 13.839692209745829 - 0.9413148934861556'\n",
      "619 - random_49 - lwr_k=700 - 9.384463415286422 - 11.942162583797474 - 10.388193542501606 - 12.691672851346487 - 24.81658826148447 - 13.843574276247187 - 0.9412984321745402'\n",
      "620 - random_42 - lwr_k=400 - 10.902557219818867 - 11.230902028131966 - 7.241652005654494 - 13.625831787843925 - 26.223489922366326 - 13.843976765689664 - 0.9412967254793736'\n",
      "621 - random_75 - deep - 9.853691032397181 - 11.445362464105633 - 8.857515161687678 - 14.231261677082008 - 24.843453543973105 - 13.845209825905277 - 0.9412914970002029'\n",
      "622 - random_4 - lwr_k=700 - 9.629534810648243 - 10.362707646127445 - 7.94002888857444 - 14.94770697722907 - 26.388440702175576 - 13.852420480052784 - 0.9412609211949096'\n",
      "623 - random_16 - deep - 15.536192911151193 - 9.59987766691979 - 7.227671823181352 - 9.994330274970878 - 27.06740295916283 - 13.884663785670856 - 0.9411241984944831'\n",
      "624 - random_87 - lwr_k=300 - 8.841395001724935 - 9.060039369983325 - 13.241550013948244 - 9.940599916316993 - 28.414677107816466 - 13.898031540154783 - 0.9410675144428136'\n",
      "625 - random_60 - lwr_k=400 - 9.375363578286635 - 10.920182365346585 - 12.509836577428953 - 13.01992127104778 - 23.715294580867823 - 13.906888187219502 - 0.9410299591801334'\n",
      "626 - random_42 - lwr_k=500 - 10.896388158864903 - 11.261714741776965 - 7.268364712390124 - 13.496987621410462 - 26.648316055966298 - 13.913425716143726 - 0.9410022377846399'\n",
      "627 - random_19 - lwr_k=200 - 9.64541780755428 - 21.642298952592885 - 8.717101430404565 - 9.85241645038725 - 19.713933597737196 - 13.914800087798225 - 0.9409964099710071'\n",
      "628 - random_1 - lwr_k=400 - 7.944047856796098 - 7.060270064219636 - 17.514827659684585 - 11.513182737432256 - 25.57801891144011 - 13.919966969908181 - 0.9409745005945288'\n",
      "629 - random_1 - lwr_k=900 - 8.180643265855108 - 6.92265304004576 - 18.665210275321797 - 10.60519855777646 - 25.279802588397157 - 13.928612449886485 - 0.94093784075371'\n",
      "630 - random_86 - lwr_k=300 - 13.962137092311368 - 10.710288041972559 - 12.953738804456862 - 11.902015454548136 - 20.120335195243975 - 13.929181060706233 - 0.9409354296461495'\n",
      "631 - random_97 - lwr_k=20 - 21.39963023580903 - 9.593731453460428 - 9.468658955338249 - 11.019650252552124 - 18.25300400302206 - 13.947442511022002 - 0.9408579947479858'\n",
      "632 - random_60 - lwr_k=500 - 9.443314947229881 - 11.03852268968714 - 12.307618283366034 - 13.131207294371718 - 23.915021919519415 - 13.965916716303195 - 0.9407796576948018'\n",
      "633 - random_49 - lwr_k=800 - 9.454379904106734 - 12.1654090262581 - 10.44132749559611 - 12.739298818528317 - 25.065229528203414 - 13.972093017256938 - 0.9407534680314886'\n",
      "634 - random_93 - deep - 9.857819746051794 - 20.939418599180232 - 11.469891884793618 - 9.095197556157467 - 18.539932850049617 - 13.98091660610008 - 0.9407160530732173'\n",
      "635 - random_1 - lwr_k=700 - 8.200931984325841 - 6.903136675803586 - 17.742483997888954 - 11.715985288484102 - 25.36226484702189 - 13.98285382013319 - 0.9407078384861637'\n",
      "636 - random_42 - lwr_k=600 - 10.897225582933206 - 11.285197807672846 - 7.418538944831716 - 13.50231281331608 - 26.834752604017886 - 13.986657001723415 - 0.9406917116739968'\n",
      "637 - random_97 - lwr_k=30 - 21.174996136085547 - 9.416075056856533 - 9.315580869536095 - 11.017757898036642 - 19.156265554392355 - 14.016554097739414 - 0.9405649375927844'\n",
      "638 - random_60 - lwr_k=600 - 9.454398131041005 - 11.175401968177173 - 12.272819408206235 - 13.174165573356383 - 24.117122264496476 - 14.03756192380592 - 0.9404758571066252'\n",
      "639 - random_1 - lwr_k=1000 - 8.180160716222266 - 6.928981491910923 - 19.104397806238275 - 10.745528358234484 - 25.249486624070737 - 14.039586506165117 - 0.9404672721735505'\n",
      "640 - random_49 - lwr_k=900 - 9.499417772378562 - 12.236457631009294 - 10.445484981821057 - 12.82654167652701 - 25.20078332130684 - 14.040697679400457 - 0.9404625604127191'\n",
      "641 - random_1 - lwr_k=600 - 8.085338902336177 - 6.921754385744594 - 17.596615276319064 - 12.151431717560326 - 25.515036756995123 - 14.051890168307267 - 0.9404151003685436'\n",
      "642 - random_42 - lwr_k=700 - 10.917945975850998 - 11.33209031569189 - 7.416014319435752 - 13.489633354085681 - 27.14197141613807 - 14.058570043677689 - 0.9403867753746296'\n",
      "643 - random_1 - lwr_k=800 - 8.178541320316038 - 6.869494969084986 - 17.766585773648853 - 11.918699368392424 - 25.588645875855647 - 14.062251534059307 - 0.9403711645754947'\n",
      "644 - random_70 - lwr_k=10 - 9.99651142005114 - 11.710231639339023 - 19.03079206061192 - 9.079947209969653 - 20.529197030154247 - 14.068282666083709 - 0.9403455904789118'\n",
      "645 - random_97 - lwr_k=40 - 20.62709805109367 - 9.085368350278959 - 9.282266324566866 - 11.000293859918973 - 20.444199724320367 - 14.088096903738288 - 0.9402615712225852'\n",
      "646 - random_49 - lwr_k=1000 - 9.53732978040721 - 12.294521578308215 - 10.475128124390643 - 12.888306482136635 - 25.328418905182573 - 14.103696659267724 - 0.9401954228356876'\n",
      "647 - random_42 - lwr_k=800 - 10.92223918562666 - 11.369586569336507 - 7.429223571677215 - 13.466407991422393 - 27.385378000947135 - 14.1135948502155 - 0.9401534510648374'\n",
      "648 - random_4 - lwr_k=800 - 9.73178122674484 - 10.691119158325902 - 8.069816766922983 - 15.233358137575093 - 26.94595336906389 - 14.133120991088195 - 0.9400706534390266'\n",
      "649 - random_60 - lwr_k=700 - 9.505422405422477 - 11.296451785531753 - 12.305896252130337 - 13.223056617585089 - 24.377515088383284 - 14.140443366331196 - 0.9400396040222797'\n",
      "650 - random_42 - lwr_k=900 - 10.972013797625433 - 11.378151658538693 - 7.452433171130934 - 13.464612897261977 - 27.454560876172714 - 14.143382064310986 - 0.9400271429211698'\n",
      "651 - random_47 - lwr_k=700 - 9.850821760077833 - 21.085582497001273 - 8.444332611550298 - 10.767230064955946 - 20.59528730255906 - 14.149082991090454 - 0.9400029690096253'\n",
      "652 - random_1 - lwr_k=300 - 8.014677440192758 - 7.06954137452949 - 17.791122985521447 - 11.921291907784903 - 26.019513905894623 - 14.161061152024724 - 0.9399521774429045'\n",
      "653 - random_53 - lr - 14.217839215454527 - 14.052632546872191 - 8.10665951776737 - 12.165393771150567 - 22.289390995001177 - 14.166373008715981 - 0.9399296533237992'\n",
      "654 - random_19 - lwr_k=100 - 9.586667707591522 - 21.827470053700445 - 8.536189617428718 - 10.072135411383844 - 20.80930777009802 - 14.166858685405044 - 0.939927593885786'\n",
      "655 - random_10 - lwr_k=40 - 13.466091499389472 - 13.337721813793062 - 12.907402591966736 - 14.62910998535658 - 16.52908917239073 - 14.17363019569509 - 0.9398988803279547'\n",
      "656 - random_42 - lwr_k=1000 - 11.006787908208933 - 11.391397292982392 - 7.4828739245012486 - 13.476159758324197 - 27.62661935305935 - 14.195785929698893 - 0.9398049323130566'\n",
      "657 - random_60 - lwr_k=800 - 9.480239890156449 - 11.40452044682142 - 12.259759995059115 - 13.214058271601134 - 24.66824987476638 - 14.204133344108216 - 0.9397695363738766'\n",
      "658 - random_97 - lwr_k=50 - 20.333268949736052 - 9.145403905706038 - 9.237929327209999 - 11.025197428859379 - 21.282810895084896 - 14.205097118292057 - 0.9397654496362688'\n",
      "659 - random_86 - deep - 16.718913182884503 - 10.715471840529123 - 7.703454439411585 - 14.071203446212506 - 21.91623016891667 - 14.224888249551285 - 0.9396815284873478'\n",
      "660 - random_90 - lwr_k=100 - 27.388462656370937 - 6.602061003267842 - 7.897554139550801 - 8.71478881885359 - 20.532850869682264 - 14.228050037034944 - 0.9396681212809257'\n",
      "661 - random_47 - lwr_k=500 - 9.666728906960536 - 17.824130740057186 - 8.649377087049857 - 14.443075418692136 - 20.857278844996426 - 14.287940472884769 - 0.9394141650112529'\n",
      "662 - random_60 - lwr_k=900 - 9.50821654970679 - 11.522719193631705 - 12.319920269298942 - 13.219434341224638 - 24.893358855777315 - 14.291492814919442 - 0.939399101846009'\n",
      "663 - random_4 - lwr_k=900 - 9.815402541487252 - 10.797651171073493 - 8.093521226954946 - 15.286249739147442 - 27.484416705474814 - 14.29414193282217 - 0.939387868665102'\n",
      "664 - random_97 - lwr_k=100 - 19.914059475030314 - 9.057863828146395 - 9.201620252912067 - 11.034148958342827 - 22.506971476680437 - 14.342979639176516 - 0.9391807798111116'\n",
      "665 - random_60 - lwr_k=1000 - 9.538963442331507 - 11.56542103252796 - 12.344313485485753 - 13.210303573228867 - 25.13449607027119 - 14.35745291612411 - 0.9391194080850355'\n",
      "666 - random_65 - deep - 14.649402707376183 - 10.49988449341732 - 7.15253514173478 - 12.753595994399474 - 26.76430199597333 - 14.363358477775192 - 0.9390943665799959'\n",
      "667 - random_12 - deep - 11.468095334579044 - 15.36842699487939 - 8.252218548730198 - 14.28105270657551 - 22.461967538557122 - 14.366041573888802 - 0.9390829893196859'\n",
      "668 - random_69 - deep - 11.636907154526531 - 14.561334305621209 - 8.718981585865818 - 12.875821527645883 - 24.06340261954543 - 14.370872671754105 - 0.9390625037851882'\n",
      "669 - random_76 - lwr_k=100 - 10.430411622438262 - 8.651404064618681 - 8.00571671366528 - 23.746366043150914 - 21.18359678501082 - 14.401906580997933 - 0.9389309090910901'\n",
      "670 - random_1 - lwr_k=200 - 8.346653136424255 - 7.588201484720246 - 18.60422812508444 - 10.983892702638377 - 26.54046735805333 - 14.410577783178526 - 0.938894140179191'\n",
      "671 - random_60 - lwr_k=50 - 10.10549986142528 - 12.709513885627379 - 13.813222476489278 - 13.957546949679596 - 21.554532909265973 - 14.427074005315916 - 0.9388241904622077'\n",
      "672 - random_19 - lr - 9.710801034255221 - 23.38846384432577 - 8.919424663166676 - 10.274288769433724 - 19.86596060787407 - 14.4324813631972 - 0.9388012613848544'\n",
      "673 - random_76 - lr - 10.51498902058797 - 10.616884688118224 - 6.84083658330832 - 21.06667517288769 - 23.20364915379223 - 14.447335376311546 - 0.9387382752050636'\n",
      "674 - random_31 - deep - 11.679184419628836 - 10.76052265003348 - 7.318017936162925 - 10.718760060834455 - 31.770921804785825 - 14.44842343065235 - 0.938733661606654'\n",
      "675 - random_18 - deep - 14.40680645725731 - 11.591185887784693 - 10.027969419712125 - 13.88743562096948 - 22.338173798319747 - 14.449839021241074 - 0.9387276590104225'\n",
      "676 - random_4 - lwr_k=1000 - 9.868718977331932 - 10.85811261007758 - 8.07258513776957 - 15.695333681054858 - 27.898139255055366 - 14.477230245295834 - 0.9386115105672398'\n",
      "677 - random_69 - lwr_k=20 - 10.072572418863297 - 9.480262856375626 - 8.80287343513236 - 11.52107917555237 - 32.529657780396626 - 14.4797483205915 - 0.9386008330525454'\n",
      "678 - random_97 - lwr_k=200 - 19.894466118011024 - 9.095687135482935 - 9.193414991770538 - 10.9478925187536 - 23.27047956179648 - 14.480392875564615 - 0.9385980999153717'\n",
      "679 - random_82 - lwr_k=200 - 14.873333507131813 - 13.41517605671529 - 8.240902975332261 - 22.011126959546857 - 13.86788599015093 - 14.48157459169524 - 0.9385930890281392'\n",
      "680 - random_97 - lwr_k=10 - 22.042216312298695 - 10.115626955039211 - 10.159722597872863 - 11.240750614647121 - 19.035954039340865 - 14.519365015098902 - 0.9384328444945877'\n",
      "681 - random_19 - lwr_k=50 - 9.724872670125347 - 22.23519157030611 - 8.768088930015448 - 10.128679898019659 - 21.868311557025752 - 14.545498878658352 - 0.9383220278273265'\n",
      "682 - random_47 - lwr_k=400 - 11.098082140906595 - 17.2803822497327 - 8.673233662688052 - 13.210518689985971 - 22.494524359770633 - 14.551229630138552 - 0.938297727448682'\n",
      "683 - random_44 - lwr_k=100 - 13.169266538466083 - 8.626169948252096 - 14.90208800891419 - 11.312626381843891 - 25.06769908322746 - 14.614352421566391 - 0.938030064867589'\n",
      "684 - random_86 - lwr_k=200 - 13.874340525323728 - 10.908585557127324 - 15.398647428406438 - 12.576338831406236 - 20.409931518065832 - 14.632834498042381 - 0.9379516944378025'\n",
      "685 - random_38 - lr - 10.96541697004445 - 12.940505332474585 - 10.210493202765214 - 11.542495135028593 - 27.579161034841885 - 14.646731854865964 - 0.9378927648132781'\n",
      "686 - random_46 - lwr_k=300 - 8.932744294585007 - 26.03322066096987 - 6.161435579435456 - 10.640799500432998 - 21.51347492226213 - 14.657260698917554 - 0.9378481188540132'\n",
      "687 - random_97 - lwr_k=300 - 20.26839019010852 - 9.064360179643716 - 9.196028463608565 - 10.963374118941653 - 23.84971362715472 - 14.66837266151743 - 0.9378010002693751'\n",
      "688 - random_19 - lwr_k=40 - 9.824752420564488 - 22.398070331015685 - 8.587159334736832 - 10.141123792926194 - 22.402472351655362 - 14.67118746400503 - 0.9377890645282259'\n",
      "689 - random_76 - deep - 10.462007857383565 - 10.78806992794605 - 6.840141374492723 - 21.95431542689443 - 23.71539915711249 - 14.75063508791374 - 0.937452179114875'\n",
      "690 - random_47 - lwr_k=800 - 9.353423313457949 - 26.668636912430177 - 8.445124549564069 - 9.694260981562808 - 19.643857283936995 - 14.76212495058593 - 0.9374034579695523'\n",
      "691 - random_9 - lwr_k=200 - 7.909787141636104 - 6.342118148056519 - 5.6956478169316345 - 7.592678723638398 - 46.34811984035444 - 14.775164449830221 - 0.9373481659594092'\n",
      "692 - random_48 - deep - 15.079861428655697 - 13.893923826186434 - 7.113819763174221 - 13.177420286933092 - 24.69054599124618 - 14.791014710955526 - 0.9372809554750539'\n",
      "693 - random_97 - lwr_k=400 - 20.531484581776244 - 9.068558855970329 - 9.14662078502889 - 10.928380285979372 - 24.431626900832843 - 14.821327302201457 - 0.937152419416254'\n",
      "694 - random_91 - lwr_k=200 - 13.340288890929184 - 9.85734538261799 - 11.212395460620705 - 15.228566054726622 - 24.644992429153163 - 14.855650703866011 - 0.9370068762602304'\n",
      "695 - random_76 - lwr_k=400 - 8.910692082211293 - 19.117641808502967 - 6.973025546955456 - 17.889733667908796 - 21.44269895770155 - 14.866479194883848 - 0.9369609596936526'\n",
      "696 - random_67 - lwr_k=40 - 15.472415110693357 - 11.612243198187759 - 9.582648665573856 - 19.897952165597488 - 17.781566501939725 - 14.868930533352112 - 0.9369505651663083'\n",
      "697 - random_60 - lwr_k=40 - 10.400003260675579 - 12.879460666468184 - 14.557213823671367 - 14.48890409725011 - 22.11262725427343 - 14.886578152592543 - 0.9368757331253101'\n",
      "698 - random_60 - lwr_k=30 - 10.953176456815404 - 14.059193673376466 - 16.28437933641837 - 14.690027030867578 - 18.64638263321981 - 14.925839146669725 - 0.9367092528608404'\n",
      "699 - random_61 - lr - 12.664753618070739 - 13.288814104957668 - 11.275159949395515 - 11.423793765512531 - 26.236003174772513 - 14.97704963481917 - 0.936492102587111'\n",
      "700 - random_56 - deep - 10.931306915470504 - 12.732127769879156 - 7.764929458218261 - 13.231821710803683 - 30.238090083210512 - 14.978624154431387 - 0.9364854262116052'\n",
      "701 - random_28 - lwr_k=20 - 11.67170652745386 - 17.064009908404447 - 9.583939426548437 - 11.458624673631356 - 25.296642309338875 - 15.014772639698895 - 0.93633214393154'\n",
      "702 - random_92 - deep - 13.921040107302503 - 10.83285432556452 - 8.446470366844283 - 11.072180727092483 - 30.852914202515947 - 15.02422504349736 - 0.9362920625886493'\n",
      "703 - random_38 - deep - 11.529250602285133 - 12.932572183749485 - 10.342650770457624 - 11.69257495268557 - 28.658987877117035 - 15.03029024646719 - 0.9362663440194672'\n",
      "704 - random_97 - lwr_k=500 - 21.017880011708854 - 9.075489700893057 - 9.14777984609182 - 10.96836096666391 - 24.951346623570604 - 15.032176182831833 - 0.9362583468580937'\n",
      "705 - random_64 - lwr_k=10 - 13.53366330716049 - 9.914270430773835 - 12.849074002239115 - 12.866251055508533 - 26.017140854777058 - 15.034995236114035 - 0.9362463930920976'\n",
      "706 - random_55 - deep - 13.800012263065469 - 12.902726078969952 - 7.951137500547367 - 14.422855849933859 - 26.30258644300831 - 15.075298715301956 - 0.9360754924642485'\n",
      "707 - random_40 - lr - 8.906128862155516 - 9.188167447011189 - 6.31986371289839 - 9.941222111278227 - 41.18631630014187 - 15.106354688651477 - 0.9359438042043201'\n",
      "708 - random_28 - lwr_k=30 - 11.118598269174042 - 16.098383550242904 - 9.241588660906057 - 11.558854211726608 - 27.5257792856074 - 15.108149506883677 - 0.9359361935510243'\n",
      "709 - random_28 - lwr_k=40 - 10.992181067272806 - 15.584963262923159 - 9.201351667922813 - 11.536813108825474 - 28.3091799909962 - 15.124296435715653 - 0.9358677249590921'\n",
      "710 - random_91 - lwr_k=300 - 14.044307745805677 - 9.781065800333963 - 11.350384094616158 - 15.673624911037356 - 24.867844338535512 - 15.142387327106182 - 0.9357910132900684'\n",
      "711 - random_91 - lwr_k=100 - 13.311186880272954 - 11.126737919098467 - 11.97878028609227 - 15.722189504383696 - 23.623479012384685 - 15.151514015576344 - 0.9357523129579459'\n",
      "712 - random_97 - lwr_k=600 - 21.35003773088569 - 9.08851451955245 - 9.158407182621557 - 10.940413483141615 - 25.24192967891984 - 15.155881287193935 - 0.93573379420795'\n",
      "713 - random_49 - lwr_k=10 - 11.640258997654303 - 16.741320906905635 - 13.552536506104095 - 15.206088154183561 - 18.78546093637612 - 15.184807459783642 - 0.9356111371796189'\n",
      "714 - random_42 - lr - 10.169574623480456 - 12.273357187121771 - 8.059097496645148 - 14.5955157772776 - 31.016398852126734 - 15.221478381932588 - 0.9354556397205962'\n",
      "715 - random_97 - lwr_k=700 - 21.556021510560253 - 9.091395201306385 - 9.132186971510972 - 10.954117868532116 - 25.455638995727906 - 15.237900220300869 - 0.9353860054166543'\n",
      "716 - random_76 - lwr_k=300 - 9.644312801209603 - 19.339505376657275 - 6.909171343053808 - 17.6179555184815 - 22.682579914863208 - 15.238460420392324 - 0.9353836299735059'\n",
      "717 - random_20 - lwr_k=50 - 10.658302669314326 - 12.177189022189406 - 12.874295481525815 - 13.429771170498631 - 27.1677235940147 - 15.260197599055404 - 0.9352914567787685'\n",
      "718 - random_91 - lwr_k=400 - 14.449287206471153 - 9.7656286945789 - 11.358778867293275 - 15.506020189825 - 25.291451373127565 - 15.273196169381585 - 0.9352363383214677'\n",
      "719 - random_35 - deep - 12.649850740760517 - 15.203695824805726 - 8.989437774983124 - 15.051950790958264 - 24.541922770397864 - 15.286926026637543 - 0.935178119091175'\n",
      "720 - random_47 - lwr_k=900 - 9.640975456469855 - 29.16893466432687 - 8.375546702960973 - 9.641556748079012 - 19.630987774226938 - 15.292947364281071 - 0.9351525863883404'\n",
      "721 - random_28 - lwr_k=50 - 10.946598407174568 - 15.273663313400837 - 9.018044456792422 - 11.619182996881603 - 29.637807277518537 - 15.298342431525995 - 0.9351297094275585'\n",
      "722 - random_97 - lwr_k=800 - 21.75841516735556 - 9.088923477015708 - 9.079821572247203 - 10.983429562342288 - 25.66544309717295 - 15.315242096021553 - 0.9350580489747146'\n",
      "723 - random_90 - lwr_k=200 - 33.021108645323125 - 6.742846533742832 - 7.74171731208178 - 8.414605714254902 - 20.86613376273958 - 15.358764199816722 - 0.9348735001236117'\n",
      "724 - random_99 - lr - 8.686422486795935 - 12.417897685119806 - 12.316217078090155 - 16.489038725042885 - 26.904059692444818 - 15.36115170626848 - 0.9348633762662093'\n",
      "725 - random_97 - lwr_k=900 - 21.87033170552198 - 9.15188328829274 - 9.121669208704901 - 10.995141183092327 - 25.840484333469178 - 15.395939672833123 - 0.9347158631934974'\n",
      "726 - random_97 - lwr_k=1000 - 21.89767704114368 - 9.194545091198401 - 9.145291408580922 - 11.0084219050538 - 25.958265488982757 - 15.44087466247224 - 0.9345253232152085'\n",
      "727 - random_20 - lwr_k=40 - 10.984499950713099 - 12.749398911882501 - 13.858340684963705 - 13.490187798456967 - 26.210735286513806 - 15.457456275287676 - 0.9344550114120631'\n",
      "728 - random_69 - lwr_k=10 - 13.528093191389724 - 18.608281683014148 - 10.04582336121335 - 14.384477716108377 - 20.751330859267206 - 15.463799359930675 - 0.934428114527923'\n",
      "729 - random_46 - lr - 9.38216974427689 - 30.274822653095658 - 6.910245806720416 - 11.494184497675993 - 19.437935226803152 - 15.50128918015129 - 0.9342691446563761'\n",
      "730 - random_28 - lwr_k=100 - 10.810474049173918 - 15.188774621848857 - 8.673184920145728 - 11.840431960888331 - 31.126045548640658 - 15.526954266026308 - 0.9341603157694734'\n",
      "731 - random_20 - lwr_k=100 - 9.991162421814085 - 11.86596481866345 - 12.275242030255619 - 13.280923960226161 - 30.345732634965007 - 15.550291093750463 - 0.9340613595065789'\n",
      "732 - random_91 - lwr_k=500 - 15.192336839682962 - 9.929205896110982 - 10.78897486563139 - 16.208874931798 - 25.760491359691777 - 15.574989319794524 - 0.9339566304415006'\n",
      "733 - random_94 - deep - 15.338658040947298 - 13.21058543848328 - 6.827452146450483 - 12.447995132349437 - 30.204974585337094 - 15.605497025587841 - 0.9338272673032498'\n",
      "734 - random_64 - deep - 15.741566656459177 - 11.674125251520286 - 10.195453817194158 - 13.034237646060728 - 27.40299413010881 - 15.609052508285721 - 0.9338121908205342'\n",
      "735 - random_76 - lwr_k=600 - 9.069970596747964 - 22.496858621545442 - 6.6945918774794375 - 18.152350961896282 - 21.823748420649107 - 15.647548605410979 - 0.9336489538443309'\n",
      "736 - random_90 - lwr_k=300 - 35.7121538734947 - 6.866621124270118 - 7.78547880064184 - 8.413734202856517 - 19.477326051402336 - 15.652909322630153 - 0.9336262225396003'\n",
      "737 - random_37 - lr - 9.86531604702767 - 12.783546479128189 - 10.96396513784495 - 14.248509515970529 - 30.418058688399878 - 15.654460654627057 - 0.9336196443525931'\n",
      "738 - random_51 - lr - 10.3893175734768 - 13.614814995010835 - 13.211218683787653 - 13.360856975701072 - 27.72316025940926 - 15.658675791004802 - 0.9336017707089058'\n",
      "739 - random_86 - lwr_k=100 - 14.29348485265216 - 11.39746160044478 - 19.202659854808314 - 13.337275698250593 - 20.567576920182177 - 15.758737398970332 - 0.9331774747034449'\n",
      "740 - random_84 - deep - 11.192078306319866 - 14.197927281431207 - 7.420606145304212 - 13.079616214094544 - 32.91040286111792 - 15.759122479227138 - 0.933175841971367'\n",
      "741 - random_96 - deep - 16.968235543433657 - 11.465680585945492 - 7.5256727001493235 - 13.398236512161493 - 29.46022990646175 - 15.763104175069634 - 0.9331589581967437'\n",
      "742 - random_40 - deep - 15.159725609075418 - 11.292915824977543 - 7.299582702424271 - 12.861496915395488 - 32.27218217951269 - 15.77634520698399 - 0.9331028116181336'\n",
      "743 - random_41 - lwr_k=700 - 8.821350287944835 - 18.48930093708168 - 18.368386691678232 - 13.310368662709255 - 20.00231876511174 - 15.79764324494694 - 0.9330125003903933'\n",
      "744 - random_10 - lwr_k=30 - 16.835335211698013 - 13.320963987624195 - 7.872724961607383 - 24.91393522968243 - 16.139707042433837 - 15.816291471090278 - 0.9329334254282533'\n",
      "745 - random_41 - lwr_k=600 - 8.724028060673533 - 18.602946095191943 - 18.428604024745276 - 13.383857145910337 - 20.024831283189414 - 15.8321428695943 - 0.9328662099876565'\n",
      "746 - random_41 - lwr_k=800 - 8.836299017473047 - 18.582344487256517 - 18.47876741701869 - 13.437545523888911 - 20.02463082596617 - 15.87120921927827 - 0.9327005550830845'\n",
      "747 - random_91 - lwr_k=600 - 15.693411154095708 - 9.980179158071078 - 10.850321671567805 - 16.38594198819189 - 26.470403939308216 - 15.875056246918247 - 0.9326842423484231'\n",
      "748 - random_41 - lwr_k=900 - 8.862343781564595 - 18.812262076600902 - 18.670302437874778 - 13.392636848371275 - 19.996917886976533 - 15.946201730339448 - 0.9323825607640875'\n",
      "749 - random_20 - lwr_k=30 - 12.12815001936652 - 12.677652451970124 - 14.760726156170724 - 14.339385658621026 - 25.866301493124396 - 15.953280050587685 - 0.9323525462253672'\n",
      "750 - random_9 - lwr_k=300 - 7.400243214251413 - 6.141330496111416 - 5.299140437032951 - 7.710781999228936 - 53.292362285052455 - 15.965759410314096 - 0.9322996294015196'\n",
      "751 - random_41 - lwr_k=500 - 8.713342580718772 - 18.980900599429937 - 18.731341436666437 - 13.422246471325444 - 20.122537982396217 - 15.993370702195119 - 0.9321825479245281'\n",
      "752 - random_41 - lwr_k=400 - 8.74779514354178 - 19.143972959583074 - 18.63870358056379 - 13.403610968593487 - 20.120995620996876 - 16.010339338427507 - 0.9321105950075412'\n",
      "753 - random_41 - lwr_k=1000 - 8.83364180247857 - 19.104462649673536 - 18.937989935835915 - 13.531510361162788 - 20.108503985091858 - 16.102522821182653 - 0.9317197049919025'\n",
      "754 - random_91 - lwr_k=700 - 16.08301114042871 - 10.066041971963358 - 10.931312866316082 - 16.654370338467082 - 26.841728841952868 - 16.114297201966234 - 0.9316697775238674'\n",
      "755 - random_41 - lwr_k=300 - 8.807194513660251 - 19.728522096651727 - 18.4488367026653 - 13.532985716308023 - 20.14329936939273 - 16.131557131389584 - 0.9315965894224789'\n",
      "756 - random_90 - lwr_k=400 - 38.13162514626996 - 6.918134083226756 - 7.888220208439067 - 8.403836120533603 - 19.352700147715073 - 16.140994504090507 - 0.9315565716812048'\n",
      "757 - random_51 - lwr_k=100 - 11.439030421716481 - 14.735090635000008 - 14.307732274633095 - 15.38846984014897 - 24.901898844788782 - 16.153439856385056 - 0.9315037990606913'\n",
      "758 - random_1 - lwr_k=100 - 9.194288119516811 - 8.752224849576148 - 22.928784791741393 - 12.783396482097837 - 27.352605120112184 - 16.199892424307563 - 0.9313068240228755'\n",
      "759 - random_28 - lwr_k=200 - 11.12738990723226 - 15.406556477222654 - 8.475704894433232 - 11.871143337146004 - 34.14807538900255 - 16.204811564194245 - 0.9312859651595545'\n",
      "760 - random_91 - lwr_k=800 - 16.46819995132915 - 10.089087349042554 - 11.001141463115381 - 16.851479616929616 - 26.74593486532208 - 16.23020171775789 - 0.9311783020812359'\n",
      "761 - random_28 - lwr_k=10 - 12.082162079049338 - 19.229727705125537 - 11.121403235475526 - 13.072499068962722 - 25.687815053431034 - 16.23853057314886 - 0.9311429848387459'\n",
      "762 - random_46 - deep - 10.208048437308953 - 32.61233824715872 - 7.438219751518931 - 11.628706392160113 - 19.329425896122064 - 16.245039982247764 - 0.931115382827558'\n",
      "763 - random_20 - lwr_k=200 - 10.259356908222092 - 11.68377295861544 - 12.6021424826877 - 13.005933824170508 - 33.80047859019298 - 16.268601641816762 - 0.9310154730530056'\n",
      "764 - random_66 - deep - 14.171684880108373 - 13.754087944467797 - 8.103796359069225 - 14.646562896333299 - 30.711338444677754 - 16.27673621484644 - 0.9309809797820282'\n",
      "765 - random_56 - lwr_k=30 - 11.141317441627985 - 11.472313831229949 - 11.201467942242498 - 28.392827451241438 - 19.367050909821295 - 16.313355371175383 - 0.9308257016813234'\n",
      "766 - random_91 - lwr_k=900 - 16.855854584584634 - 10.077919102080184 - 11.105904626813663 - 16.878687008372104 - 26.674218562823672 - 16.317582887976737 - 0.9308077755403358'\n",
      "767 - random_87 - lwr_k=200 - 8.948709069003105 - 9.49783166530697 - 18.73458591610278 - 10.228686774933362 - 34.287031592990616 - 16.337038530886453 - 0.9307252768503976'\n",
      "768 - random_76 - lwr_k=500 - 9.021534635050548 - 26.021041844493883 - 6.860652321481613 - 18.194881880612876 - 21.601942507313247 - 16.340397497984302 - 0.9307110336621153'\n",
      "769 - random_38 - lwr_k=50 - 16.411167426238002 - 15.247129362259967 - 13.674830454982349 - 14.11432646719575 - 22.32687754435098 - 16.35469408206591 - 0.9306504111752204'\n",
      "770 - random_19 - deep - 9.615690685528195 - 29.70321013455305 - 8.08904249642737 - 12.387385853287824 - 22.08273310352797 - 16.376687857173916 - 0.9305571500700047'\n",
      "771 - random_51 - deep - 10.570224875123918 - 13.058366315064376 - 14.61215201837913 - 13.386902080414043 - 30.273465335417928 - 16.378726945899267 - 0.9305485036249126'\n",
      "772 - random_57 - lwr_k=20 - 15.62911577584483 - 11.61507144688263 - 14.27609752181619 - 17.850953791866136 - 22.69064630629386 - 16.411463170159468 - 0.9304096905052103'\n",
      "773 - random_17 - deep - 16.50222815936599 - 11.200119385352501 - 7.5645210951884 - 13.342448192771393 - 33.48573398980618 - 16.418169374779023 - 0.9303812540141574'\n",
      "774 - random_41 - lwr_k=200 - 9.235134859938348 - 19.646279477897068 - 18.982318372844418 - 13.611820433602853 - 20.62203283425115 - 16.418869149193984 - 0.9303782865793214'\n",
      "775 - random_49 - deep - 10.377915966530283 - 14.778031381959806 - 13.071431190434486 - 15.62921530452544 - 28.2502326262378 - 16.420106418826823 - 0.9303730402739743'\n",
      "776 - random_70 - lwr_k=100 - 9.482843279018423 - 24.80340435870429 - 17.05024134338 - 8.65436948287065 - 22.32394687835237 - 16.46318381703249 - 0.9301903769811298'\n",
      "777 - random_12 - lwr_k=40 - 9.59798667962319 - 9.686584673384672 - 31.51602020111484 - 12.00157832392548 - 19.565645998107502 - 16.471325979390716 - 0.930155851382005'\n",
      "778 - random_34 - deep - 19.181682446194163 - 9.163497075316558 - 7.954170517605118 - 14.69710676812618 - 31.423702619585416 - 16.48327483449336 - 0.9301051842308169'\n",
      "779 - random_97 - lr - 23.199659975545174 - 10.494522210376486 - 9.343987624786713 - 11.282416181281775 - 28.123975451145423 - 16.489029589692887 - 0.9300807819194391'\n",
      "780 - random_47 - lwr_k=300 - 14.599756005362705 - 19.56677266111485 - 8.585834458393647 - 15.246298487092828 - 24.54680516682392 - 16.50928139275693 - 0.9299949072336513'\n",
      "781 - random_61 - deep - 14.130083480560252 - 13.644701883562654 - 12.635681103136967 - 14.351879069494673 - 27.809366158244064 - 16.51348205886225 - 0.9299770950978404'\n",
      "782 - random_50 - lwr_k=200 - 11.077375246055475 - 11.261893297687674 - 12.122632491092745 - 12.57164289435253 - 35.54430356696963 - 16.51381874264823 - 0.929975667292645'\n",
      "783 - random_49 - lr - 10.59480031184236 - 14.990195496928509 - 13.252087292341463 - 15.727340018822632 - 28.052415872565113 - 16.52214596457366 - 0.9299403569766169'\n",
      "784 - random_37 - deep - 9.944884663127644 - 12.540853509731262 - 11.144245150829319 - 15.993997383273888 - 33.0358691422496 - 16.530238588463614 - 0.9299060415868617'\n",
      "785 - random_28 - lwr_k=300 - 11.337432030735373 - 15.620218134912239 - 8.460207886001125 - 11.74951971835704 - 35.492811328939126 - 16.531037913865624 - 0.92990265202022'\n",
      "786 - random_91 - lwr_k=1000 - 17.41025435774567 - 10.173949028870666 - 11.269987509673545 - 17.16578846740531 - 26.784199677721116 - 16.55992906712886 - 0.929780143486008'\n",
      "787 - random_60 - deep - 9.733871631653532 - 13.458541759297422 - 16.426127278345906 - 15.189095729887242 - 28.030186258701885 - 16.565936550907917 - 0.9297546698155703'\n",
      "788 - random_15 - lr - 10.337424368303394 - 15.541119852265409 - 10.880633553160807 - 15.525316273922162 - 30.564804107098137 - 16.568670638803045 - 0.9297430761829912'\n",
      "789 - random_60 - lr - 9.938517851815762 - 13.920011338780585 - 16.73264717381463 - 15.116880046069104 - 27.178621963724552 - 16.57581346390936 - 0.9297127881212467'\n",
      "790 - random_78 - deep - 16.90639553132502 - 12.933087179196447 - 7.582394868599207 - 13.68034257056965 - 31.8628598774778 - 16.592468255971145 - 0.9296421661622577'\n",
      "791 - random_90 - lwr_k=500 - 40.28404444681942 - 6.946430583864705 - 7.894725648169679 - 8.429970754035129 - 19.41456333286059 - 16.596246376946265 - 0.9296261454662089'\n",
      "792 - random_99 - deep - 9.349355251231092 - 12.606784400690207 - 13.555476473746586 - 18.89352816699666 - 28.669504833455754 - 16.61308361834116 - 0.9295547498588026'\n",
      "793 - random_98 - deep - 10.626133879741163 - 9.332177869231728 - 22.008431144858072 - 9.59582238837796 - 31.6009893425169 - 16.630531878324437 - 0.929480763170522'\n",
      "794 - random_22 - lwr_k=30 - 41.46076479162546 - 7.292880534610586 - 9.374597847748568 - 8.68681273898518 - 16.50947449746934 - 16.667431681099135 - 0.9293242950281269'\n",
      "795 - random_9 - lwr_k=400 - 7.28429724984804 - 5.956769128811154 - 5.184196124646286 - 7.719814656459934 - 57.90671336844491 - 16.807021008895283 - 0.928732387748271'\n",
      "796 - random_26 - lwr_k=800 - 8.227999243831357 - 8.955340059898166 - 6.740557294405185 - 10.610363754235639 - 49.724605830824565 - 16.849068110198907 - 0.9285540934205356'\n",
      "797 - random_50 - lwr_k=300 - 11.140183492424514 - 11.097885243530234 - 11.955285289337786 - 12.502202602405001 - 37.62827499334605 - 16.862884636978904 - 0.9284955065434977'\n",
      "798 - random_28 - lwr_k=400 - 11.595633649846045 - 15.7495115745208 - 8.632245400620622 - 11.723295919628073 - 36.757612321877474 - 16.890605544655696 - 0.9283779602574229'\n",
      "799 - random_20 - lwr_k=300 - 10.64003194114591 - 11.763286796086584 - 13.062805815293604 - 13.411218921467356 - 35.81714298350333 - 16.93701838600635 - 0.9281811536740833'\n",
      "800 - random_94 - lwr_k=40 - 14.248560552549057 - 17.392620561344877 - 11.231556091670372 - 17.13698239953252 - 24.721296449412336 - 16.945834580591058 - 0.9281437699439822'\n",
      "801 - random_90 - lwr_k=600 - 41.90882126495649 - 7.009204993234048 - 7.967801318921959 - 8.618597596685134 - 19.493386001187215 - 17.002005153252775 - 0.9279055871874845'\n",
      "802 - random_24 - lwr_k=100 - 10.205745771644596 - 14.485554193436446 - 20.27653183369143 - 15.586759855848983 - 24.93191400693415 - 17.09574499962739 - 0.9275080976490098'\n",
      "803 - random_28 - lwr_k=500 - 11.798043990673593 - 15.86049335029532 - 8.636395512013383 - 11.705349797561661 - 37.77114000712782 - 17.1531956119794 - 0.9272644870792032'\n",
      "804 - random_15 - deep - 9.947233520045803 - 15.114234921194722 - 10.735981396824293 - 17.178219596252003 - 32.90775129738448 - 17.17516220526167 - 0.9271713412415781'\n",
      "805 - random_50 - lwr_k=400 - 11.176990388066951 - 11.17097022383762 - 12.005520853895328 - 12.473979576836193 - 39.07514771502941 - 17.17855465101056 - 0.9271569559373195'\n",
      "806 - random_1 - lr - 9.089841573908728 - 7.66761877954789 - 32.511179544149975 - 12.317581439191875 - 24.901083934624918 - 17.29454023198975 - 0.9266651367501095'\n",
      "807 - random_19 - lwr_k=10 - 10.576396531677565 - 23.656441607701918 - 9.41242799103296 - 10.745895788785534 - 32.132208238639706 - 17.304612379377634 - 0.9266224273434751'\n",
      "808 - random_90 - lwr_k=700 - 43.25219957815316 - 7.064284871084317 - 8.036249465953151 - 8.62263538821787 - 19.612037025303177 - 17.32004906039073 - 0.9265569704492216'\n",
      "809 - random_22 - lwr_k=40 - 43.65479786186202 - 6.994576695642368 - 8.624193132771477 - 8.599476772068119 - 18.716638879690063 - 17.3205588235109 - 0.9265548088763664'\n",
      "810 - random_28 - lwr_k=600 - 12.010357084916727 - 15.956932799368758 - 8.65163071701049 - 11.700243448681585 - 38.62305622921761 - 17.38732900782407 - 0.9262716800813773'\n",
      "811 - random_20 - lwr_k=400 - 10.904751033655419 - 11.828985649285903 - 13.258923857341166 - 13.708608191126624 - 37.253997075870366 - 17.38908028285102 - 0.92626425406641'\n",
      "812 - random_22 - lwr_k=100 - 42.37393137525507 - 6.545005440148612 - 7.073620973643432 - 12.212163356422751 - 18.815954330416705 - 17.4064456677375 - 0.926190618797195'\n",
      "813 - random_72 - deep - 11.249637002835687 - 16.437458047695127 - 12.549408613512695 - 16.197317018653408 - 30.65058709753038 - 17.415711110303977 - 0.9261513302564899'\n",
      "814 - random_9 - lwr_k=100 - 9.745785773682263 - 7.749495774090108 - 8.808505195556029 - 12.593008778030788 - 48.31764541589795 - 17.440040554435686 - 0.9260481647979063'\n",
      "815 - random_60 - lwr_k=20 - 13.559506769220024 - 19.910108250715652 - 19.884597296396077 - 16.56903957249822 - 17.320159937793736 - 17.44844857627763 - 0.926012511862141'\n",
      "816 - random_68 - deep - 20.12994358387399 - 12.265366342375986 - 8.601593823632093 - 11.787320688061788 - 34.49983888293367 - 17.456400223776413 - 0.9259787942696472'\n",
      "817 - random_9 - lwr_k=500 - 7.383870540049427 - 5.949996679878982 - 5.175695168128889 - 7.726900344212281 - 61.33488744753457 - 17.510717608770392 - 0.9257484695157528'\n",
      "818 - random_24 - lwr_k=200 - 10.53850688131539 - 14.412864750916603 - 20.767276795895274 - 15.526876026173145 - 26.35617694967037 - 17.51868819156387 - 0.9257146714736343'\n",
      "819 - random_70 - lwr_k=200 - 9.344979881546204 - 30.10833793850529 - 16.88634796269379 - 8.562233049826661 - 22.74704300176404 - 17.53050782668156 - 0.9256645521114892'\n",
      "820 - random_12 - lwr_k=30 - 10.061340393900762 - 9.914615946652711 - 37.55007163511529 - 11.779747405701924 - 18.421919642874435 - 17.543063956270483 - 0.9256113097567344'\n",
      "821 - random_52 - deep - 18.436032955463116 - 15.08344180143016 - 9.407539641046798 - 12.280595329528358 - 32.57277027336327 - 17.55581580292709 - 0.9255572376518577'\n",
      "822 - random_70 - lwr_k=1000 - 9.390894461827076 - 30.095292919874332 - 17.077878955848945 - 8.572312644246429 - 22.6850151367327 - 17.564992370412018 - 0.925518325657083'\n",
      "823 - random_70 - lwr_k=900 - 9.374089469563177 - 30.154258494800082 - 17.04218888799287 - 8.549831273715125 - 22.738299614538136 - 17.57245155710634 - 0.9254866961122183'\n",
      "824 - random_90 - lwr_k=800 - 44.44180592122294 - 7.0883876480063535 - 8.074150860316474 - 8.589340660762327 - 19.672742347805176 - 17.57596824881374 - 0.9254717841167565'\n",
      "825 - random_70 - lwr_k=800 - 9.375716434470874 - 30.231744539790473 - 17.051706763238542 - 8.539301188802797 - 22.756276861413692 - 17.591673828029467 - 0.9254051870006379'\n",
      "826 - random_22 - lwr_k=50 - 45.70056258073234 - 6.849456164140065 - 8.106027782090553 - 8.966761904864514 - 18.37457843787592 - 17.602318550295188 - 0.9253600496774586'\n",
      "827 - random_70 - lwr_k=300 - 9.322017783058628 - 30.381992691055967 - 16.86956785847483 - 8.537187739010577 - 22.915188797208888 - 17.605926789796914 - 0.9253447494875211'\n",
      "828 - random_70 - lwr_k=500 - 9.38189417761261 - 30.374409058283693 - 16.94118124745269 - 8.553951485428573 - 22.864944930210072 - 17.62401463581133 - 0.9252680507319508'\n",
      "829 - random_28 - lwr_k=700 - 12.211986533832444 - 16.0887343510055 - 8.67746921322928 - 11.778737415987928 - 39.369341431067774 - 17.624115785636146 - 0.9252676218215277'\n",
      "830 - random_70 - lwr_k=700 - 9.396414897600701 - 30.351042322121522 - 17.03591880126325 - 8.53739612940167 - 22.82436675429341 - 17.629762904836202 - 0.9252436760728268'\n",
      "831 - random_70 - lwr_k=400 - 9.385083135039256 - 30.401109369951858 - 16.918972773717798 - 8.54259631956021 - 22.910912289387486 - 17.63247535767704 - 0.925232174329763'\n",
      "832 - random_70 - lwr_k=600 - 9.386594275262803 - 30.39220274079536 - 16.986257202807334 - 8.540927644578872 - 22.863879771296084 - 17.63471096332637 - 0.9252226945845732'\n",
      "833 - random_23 - deep - 19.26891012550766 - 10.034756993701142 - 8.42310145133641 - 10.944351558700925 - 39.637948869975446 - 17.660828044028896 - 0.9251119492416955'\n",
      "834 - random_50 - lwr_k=500 - 11.265685372570045 - 11.334998521003312 - 12.057332333211583 - 12.609977781772113 - 41.06719052173117 - 17.664951857765764 - 0.9250944626785197'\n",
      "835 - random_24 - lwr_k=300 - 10.676260133028638 - 14.317061420982137 - 21.171254934412055 - 15.552445601546694 - 26.72006463014734 - 17.68571740737133 - 0.9250064095287853'\n",
      "836 - random_74 - lwr_k=200 - 7.331515366276032 - 24.595735850365795 - 5.2608160288649595 - 6.651014183951088 - 44.93179715944093 - 17.753589325079762 - 0.9247186090011686'\n",
      "837 - random_67 - lwr_k=30 - 18.033784914632733 - 15.731676411258672 - 10.769180183077154 - 25.973613728459167 - 18.611279381223564 - 17.82359869505471 - 0.9244217449328295'\n",
      "838 - random_90 - lwr_k=900 - 45.6483310771899 - 7.103762767793036 - 8.107344574261052 - 8.637468026975078 - 19.656565951888716 - 17.83349302307755 - 0.924379789542124'\n",
      "839 - random_20 - lwr_k=500 - 11.146586514806822 - 11.910809705796945 - 13.677632903138216 - 13.995963672723828 - 38.46203752765751 - 17.836539613881637 - 0.9243668709379287'\n",
      "840 - random_28 - lwr_k=800 - 12.341406695275543 - 16.27204896692682 - 8.789081456933213 - 11.824756945082436 - 40.01032622943826 - 17.84636447260791 - 0.9243252101211926'\n",
      "841 - random_24 - lwr_k=400 - 10.87709906023165 - 14.397193354256395 - 21.422531493685014 - 15.484904495556247 - 27.165812715461815 - 17.867794661690986 - 0.9242343386690033'\n",
      "842 - random_24 - lwr_k=50 - 10.999616912984308 - 15.009307231770453 - 21.232215479780315 - 17.33696831813438 - 25.289164101175515 - 17.971827098359174 - 0.9237932049693341'\n",
      "843 - random_50 - lwr_k=600 - 11.429768274278592 - 11.390765595387208 - 12.044454671355192 - 12.695068678786873 - 42.42114401317168 - 17.994083386169592 - 0.9236988305713373'\n",
      "844 - random_24 - lwr_k=500 - 10.944425397171232 - 14.597587120537025 - 21.66319378645813 - 15.547748688456387 - 27.36649578948305 - 18.022169873500722 - 0.9235797341004297'\n",
      "845 - random_28 - lwr_k=900 - 12.455633198432464 - 16.41521408613068 - 8.775815776704253 - 11.869826333566671 - 40.614167639860334 - 18.024955475134508 - 0.9235679221810528'\n",
      "846 - random_74 - lr - 7.99313261919617 - 51.461536955537525 - 6.224583769017886 - 8.406950134489765 - 16.03093904181375 - 18.027261451868675 - 0.9235581440379979'\n",
      "847 - random_90 - lwr_k=1000 - 46.62614308652002 - 7.1341522893370435 - 8.116165271094273 - 8.626290866648255 - 19.767269587277113 - 18.05689472073789 - 0.9234324886756107'\n",
      "848 - random_42 - deep - 13.315588565582924 - 14.770604459821886 - 8.303626344791697 - 15.779567514649187 - 38.31796463037689 - 18.096141933776416 - 0.9232660670106961'\n",
      "849 - random_24 - lwr_k=600 - 11.014178258097497 - 14.636564070369715 - 21.755403322396557 - 15.661989114229439 - 27.456862939330016 - 18.103270499333423 - 0.923235839251238'\n",
      "850 - random_97 - deep - 24.538915904180897 - 10.571947153188203 - 9.364918678339928 - 13.781495291907508 - 32.27294563134121 - 18.10586462052603 - 0.9232248394387501'\n",
      "851 - random_20 - lwr_k=20 - 13.799027764911761 - 13.95371486790554 - 17.898018159899358 - 16.711433932461823 - 28.264094456602557 - 18.123866355589893 - 0.9231485056602999'\n",
      "852 - random_75 - lwr_k=50 - 13.483216448955167 - 16.44087713758497 - 14.952971434814124 - 27.5241907859575 - 18.229570111938585 - 18.125128957096972 - 0.9231431517909504'\n",
      "853 - random_24 - lwr_k=700 - 11.1113244634205 - 14.658479935240608 - 21.83544264021341 - 15.593424011425506 - 27.48779249257059 - 18.135572587234925 - 0.923098867168305'\n",
      "854 - random_28 - lwr_k=1000 - 12.55994334967425 - 16.533775262601488 - 8.809538467141182 - 11.891553633734793 - 41.07687780492971 - 18.17314972968556 - 0.9229395270201443'\n",
      "855 - random_9 - lwr_k=600 - 7.392256482310665 - 6.0520206065155335 - 5.18990852978 - 7.806311445376836 - 64.54698425235357 - 18.193738163474016 - 0.9228522249030511'\n",
      "856 - random_20 - lwr_k=600 - 11.355426763606369 - 11.974291797861575 - 13.671561821564227 - 14.198007742247079 - 39.86948815202246 - 18.21161053760387 - 0.9227764398231829'\n",
      "857 - random_87 - lwr_k=100 - 9.378322619277245 - 10.138467502198534 - 23.406728646802367 - 11.043188079539126 - 37.139571098411594 - 18.218484061317653 - 0.9227472936929688'\n",
      "858 - random_24 - lwr_k=800 - 11.206697670067712 - 14.732618824869935 - 21.89349393219386 - 15.69356047432163 - 27.61481322510011 - 18.226514677422053 - 0.9227132410887406'\n",
      "859 - random_15 - lwr_k=50 - 10.25433934700044 - 17.431619882523723 - 21.154129138755014 - 18.323370349284623 - 24.184861260904185 - 18.268214287365144 - 0.9225364202451836'\n",
      "860 - random_35 - lwr_k=10 - 13.313950952488696 - 14.811416401175718 - 13.110928190062836 - 14.064451697438042 - 36.08462874783904 - 18.275695013906656 - 0.9225046993638781'\n",
      "861 - random_24 - lwr_k=40 - 10.87331709095158 - 15.115655976515592 - 21.513475199408273 - 18.838640079240783 - 25.07455452926085 - 18.281396581611737 - 0.9224805227345863'\n",
      "862 - random_81 - lwr_k=30 - 14.104747436225287 - 30.361992536638144 - 12.637928267317196 - 12.632370192829022 - 21.727813783646326 - 18.294260896759425 - 0.9224259735659177'\n",
      "863 - random_50 - lwr_k=700 - 11.557406863001336 - 11.374898232564528 - 12.012582157890746 - 12.735312124280743 - 43.82440468885568 - 18.298682474276347 - 0.9224072245400285'\n",
      "864 - random_24 - lwr_k=900 - 11.271500182534664 - 14.776040035430611 - 22.042007242513712 - 15.787199049117813 - 27.758796639596575 - 18.325371823380234 - 0.9222940524209373'\n",
      "865 - random_22 - lwr_k=200 - 47.61266731542917 - 6.560674369552416 - 6.933951781940106 - 11.670575798635188 - 18.872690292922226 - 18.33297962380677 - 0.9222617927021806'\n",
      "866 - random_24 - lwr_k=1000 - 11.338072090799523 - 14.847619278795598 - 22.104098562587172 - 15.925466704077227 - 28.07400847191356 - 18.456096019068106 - 0.9217397363832962'\n",
      "867 - random_22 - lwr_k=20 - 44.316900040093834 - 8.355540171874923 - 13.051871909522566 - 10.141934967520765 - 16.47224070566763 - 18.470274445311265 - 0.9216796149267155'\n",
      "868 - random_50 - lwr_k=800 - 11.642660489973798 - 11.513382956151235 - 12.153012514473973 - 12.758536070529678 - 44.761720770707335 - 18.56357409112296 - 0.9212839919916688'\n",
      "869 - random_20 - lwr_k=700 - 11.563221233017847 - 12.115745900731243 - 13.85150095571044 - 14.362210726670774 - 41.220233714016246 - 18.620361088331865 - 0.9210431953807832'\n",
      "870 - random_81 - lwr_k=100 - 14.19323527547937 - 30.197922931048122 - 10.633479769502445 - 12.562852149185215 - 25.6442489666988 - 18.647510166928683 - 0.9209280738488111'\n",
      "871 - random_9 - lwr_k=700 - 7.356383287105168 - 6.208796408915923 - 5.2363220883820905 - 7.866678386087789 - 66.76382900295312 - 18.682503419104513 - 0.9207796902937377'\n",
      "872 - random_0 - lr - 12.134938212438925 - 18.18020810232312 - 14.894873827283993 - 22.20223963395558 - 26.171061851069645 - 18.715498747825247 - 0.9206397786287173'\n",
      "873 - random_50 - lwr_k=900 - 11.706006039439844 - 11.599712199550911 - 12.048296460805473 - 12.74882589218657 - 45.502639282395535 - 18.718781176494993 - 0.9206258599899796'\n",
      "874 - random_53 - deep - 25.151440440925562 - 20.597928710537925 - 9.062706146740112 - 14.336263321541452 - 24.695426687854514 - 18.770097822241674 - 0.9204082597403678'\n",
      "875 - random_75 - lwr_k=100 - 9.664831087614926 - 10.689686990248138 - 10.412204653405656 - 43.772919379659726 - 19.61033747425181 - 18.827162205924832 - 0.9201662867451577'\n",
      "876 - random_87 - lwr_k=40 - 15.077163312107794 - 13.333805725258982 - 21.099288379508284 - 12.844591497898396 - 32.05290634534296 - 18.880019672775425 - 0.9199421527091424'\n",
      "877 - random_19 - lwr_k=30 - 9.891822017470101 - 43.61214166188466 - 8.473323170495282 - 10.312460534499397 - 22.172339078678633 - 18.894991245298286 - 0.9198786680365885'\n",
      "878 - random_36 - lwr_k=500 - 9.918394986759996 - 7.0207038520610325 - 7.328591986050664 - 11.451980580320537 - 58.860481685954475 - 18.91260946833616 - 0.9198039606457091'\n",
      "879 - random_81 - lwr_k=200 - 13.15573075494685 - 30.430139681459107 - 10.442662123049361 - 12.390849033420029 - 28.15474223581171 - 18.915767326881166 - 0.9197905702276088'\n",
      "880 - random_50 - lwr_k=1000 - 11.765741279264212 - 11.633995494354107 - 12.174960274538984 - 12.75834513948868 - 46.416159181148814 - 18.947465958420185 - 0.919656157009452'\n",
      "881 - random_20 - lwr_k=800 - 11.692695517740795 - 12.185824029034265 - 13.987740074316244 - 14.532289877809726 - 42.38404853258473 - 18.95422150253658 - 0.919627511153748'\n",
      "882 - random_87 - lwr_k=50 - 11.41319219789356 - 11.18495902709883 - 21.327939723906645 - 11.89740499491194 - 39.10518986276139 - 18.983219833150457 - 0.9195045481503077'\n",
      "883 - random_9 - lwr_k=800 - 7.458862686682804 - 6.299530092122057 - 5.302368546539772 - 7.890353606274862 - 68.4127909279071 - 19.068787857746972 - 0.9191417099772095'\n",
      "884 - random_26 - lwr_k=1000 - 8.142754594172747 - 8.298356009035189 - 6.830422227503613 - 10.710128419690559 - 61.847071663346206 - 19.16216210877516 - 0.9187457706901073'\n",
      "885 - random_4 - lr - 12.098846828541104 - 14.659768142950988 - 11.553011156193868 - 23.85271763373277 - 33.770868523421726 - 19.185140464182364 - 0.918648334474464'\n",
      "886 - random_12 - lwr_k=20 - 10.13907239666413 - 10.396765066455947 - 46.525707340324615 - 11.973147214357164 - 17.031549295322478 - 19.210318729456024 - 0.9185415698761605'\n",
      "887 - random_20 - lwr_k=900 - 11.832101874765804 - 12.293385504524792 - 14.117291621911193 - 14.697737718115027 - 43.154959460666454 - 19.2167515806679 - 0.9185142923505598'\n",
      "888 - random_1 - deep - 10.42079413971222 - 9.320572140173905 - 39.85576488110591 - 12.500365526631267 - 24.100519917618534 - 19.236535228776788 - 0.9184304029779844'\n",
      "889 - random_76 - lwr_k=40 - 15.372299563984978 - 12.929418663840794 - 13.878904110122926 - 28.957825949811166 - 25.568661789031918 - 19.339722142130437 - 0.9179928544176834'\n",
      "890 - random_81 - lwr_k=400 - 14.422244571114096 - 30.426174888082816 - 10.524687492081808 - 12.544833161472706 - 29.18632546908037 - 19.421836694756152 - 0.9176446601663847'\n",
      "891 - random_81 - lwr_k=300 - 13.910111154509812 - 30.590135327424935 - 10.471411824792785 - 12.502439846670162 - 29.75058264337097 - 19.44585483860103 - 0.9175428148862701'\n",
      "892 - random_22 - lwr_k=300 - 53.374611722542454 - 6.632898560813146 - 6.865681707865512 - 11.637004422568282 - 18.933181407574033 - 19.492119179657458 - 0.917346637995806'\n",
      "893 - random_39 - lwr_k=100 - 17.96581753891972 - 16.011828268744914 - 15.51162765854796 - 18.180176453621208 - 29.87282710353845 - 19.50763024256512 - 0.9172808656964597'\n",
      "894 - random_9 - lwr_k=900 - 7.441013828859224 - 6.447338659311787 - 5.3167061464929 - 7.907578052533339 - 70.4863745840913 - 19.51568382425602 - 0.9172467157101605'\n",
      "895 - random_20 - lwr_k=1000 - 11.999837147911997 - 12.357495288635624 - 14.316781947359257 - 14.798842702848708 - 44.12709689611985 - 19.51760655731578 - 0.9172385626535245'\n",
      "896 - random_76 - lwr_k=50 - 12.112486453331947 - 11.531234028071548 - 18.460152953060035 - 32.06195740451492 - 23.525905953157785 - 19.535820262596978 - 0.9171613302109066'\n",
      "897 - random_77 - lwr_k=50 - 9.683106973593128 - 24.713279648779398 - 13.562439642804657 - 31.115321409512546 - 19.471755587680107 - 19.708358321594226 - 0.9164297088557138'\n",
      "898 - random_30 - lwr_k=200 - 13.637831876966 - 38.83120184533646 - 12.093398844739742 - 12.145588147693 - 22.681522773708938 - 19.879990442625274 - 0.9157019289924585'\n",
      "899 - random_47 - lwr_k=200 - 23.787287699385015 - 20.110686108760593 - 8.93757767226133 - 18.25387878349625 - 28.322602659223566 - 19.88308337529817 - 0.9156888138826282'\n",
      "900 - random_30 - lwr_k=100 - 13.508992467209227 - 39.4147610337129 - 11.913825308425555 - 11.530852381002576 - 23.09282363211059 - 19.894402471217557 - 0.9156408169807028'\n",
      "901 - random_30 - lwr_k=400 - 14.042492612012952 - 38.537658699293324 - 12.054598720296998 - 12.145663907895573 - 22.761209280533297 - 19.910414623050688 - 0.9155729198901026'\n",
      "902 - random_30 - lwr_k=300 - 13.927093983280459 - 38.63482551500665 - 12.126670266418992 - 12.164975956581383 - 22.732340110217844 - 19.919265259499966 - 0.9155353901145217'\n",
      "903 - random_41 - lwr_k=100 - 9.940371928139983 - 28.341690139248605 - 22.00624795042112 - 16.26220114027515 - 23.097883665531672 - 19.929420688014275 - 0.9154923275669574'\n",
      "904 - random_19 - lwr_k=20 - 10.069909638617796 - 46.956076152569985 - 8.809711901101334 - 10.497609635546043 - 23.355032417462326 - 19.94047630830185 - 0.9154454478933115'\n",
      "905 - random_9 - lwr_k=1000 - 7.46313645434612 - 6.544896230113472 - 5.3311930939300245 - 7.966278025435787 - 72.42484891968672 - 19.941832112017813 - 0.9154396988141935'\n",
      "906 - random_30 - lwr_k=500 - 14.132528136973193 - 38.63890843490245 - 12.047985256484921 - 12.101415823203805 - 22.80867780103724 - 19.948012085180682 - 0.9154134935794369'\n",
      "907 - random_30 - lwr_k=600 - 14.176787685415658 - 38.82470215678051 - 12.032049759598126 - 12.127817016257831 - 22.808311418338157 - 19.996064542700957 - 0.9152097344535046'\n",
      "908 - random_80 - lwr_k=10 - 29.576938738890217 - 14.476555933473623 - 17.51238658138776 - 16.64466854337941 - 21.9231709470435 - 20.02739913596028 - 0.9150768648842164'\n",
      "909 - random_30 - lwr_k=700 - 14.198771889774413 - 39.18348527232163 - 12.005997934326604 - 12.168687953637479 - 22.890586992241097 - 20.09166799391058 - 0.9148043425826234'\n",
      "910 - random_90 - lr - 54.92758985486036 - 8.026680611735463 - 8.548949640007407 - 8.790089344376227 - 20.428916875656512 - 20.148156642424937 - 0.9145648110739275'\n",
      "911 - random_30 - lwr_k=800 - 14.216834283188955 - 39.41987057602384 - 12.026720804441076 - 12.199013760061266 - 22.90513963338782 - 20.155698499030937 - 0.914532830979896'\n",
      "912 - random_81 - lwr_k=500 - 14.020943595668465 - 30.395336311012834 - 10.522544398941475 - 12.58638901289215 - 33.44499197112627 - 20.19470066129992 - 0.9143674482522791'\n",
      "913 - random_30 - lwr_k=50 - 13.681150012559968 - 40.7854938638297 - 12.049461898050591 - 11.118210103809307 - 23.345671496970443 - 20.198302149831342 - 0.9143521766888028'\n",
      "914 - random_30 - lwr_k=40 - 13.895700598162506 - 40.35334074204565 - 11.930968863436119 - 11.442321319689546 - 23.504085517086203 - 20.227542860273957 - 0.9142281859106222'\n",
      "915 - random_30 - lwr_k=900 - 14.198173916789951 - 39.709174930661995 - 12.025372766524605 - 12.263255904381541 - 22.95995431673377 - 20.233387935073623 - 0.9142034007613572'\n",
      "916 - random_22 - lwr_k=400 - 57.472160993034024 - 6.673249376291555 - 6.882269716056515 - 11.449847256251257 - 18.91672513010583 - 20.282712899682807 - 0.9139942457629647'\n",
      "917 - random_38 - lwr_k=40 - 22.0364208125151 - 21.263429822760223 - 16.774259433357027 - 18.267905704999343 - 23.06959917899493 - 20.28277087077 - 0.9139939999454006'\n",
      "918 - random_30 - lwr_k=1000 - 14.16827039577468 - 39.939189692249876 - 12.025968568472912 - 12.288505608999637 - 22.98820465710243 - 20.284245469884073 - 0.913987747132494'\n",
      "919 - random_4 - lwr_k=50 - 15.862777726165856 - 18.190673835439664 - 18.090313286520946 - 25.35786943007606 - 24.67840290590292 - 20.43489092080471 - 0.9133489580467888'\n",
      "920 - random_30 - lwr_k=30 - 14.344652384923464 - 41.60962895126135 - 11.52508458572355 - 11.407578594941162 - 23.459320819728976 - 20.471711848255858 - 0.9131928245131335'\n",
      "921 - random_27 - lwr_k=500 - 9.198248064391894 - 23.762047396269033 - 28.983478596156818 - 21.70979369189215 - 19.39683161497684 - 20.608727348701844 - 0.912611831156066'\n",
      "922 - random_81 - lwr_k=800 - 14.638977482849459 - 31.05537057527956 - 10.611819768441988 - 12.766209469659724 - 34.09223580049473 - 20.633647771272294 - 0.9125061599004665'\n",
      "923 - random_70 - lr - 10.04195874081976 - 32.31849244561419 - 28.032913938259806 - 8.984514545019536 - 23.925659288226864 - 20.66087793005262 - 0.9123906945603277'\n",
      "924 - random_30 - lr - 14.42020765424731 - 41.229730165024655 - 12.150752737540584 - 12.294953905635584 - 23.20383839456414 - 20.66224308272986 - 0.9123849058383636'\n",
      "925 - random_27 - lwr_k=700 - 9.13963843783967 - 24.130441337191865 - 28.677239466983178 - 21.88678015391498 - 19.522640177427288 - 20.67002605188209 - 0.9123519033433103'\n",
      "926 - random_81 - lwr_k=1000 - 14.90088390393745 - 31.05483327188409 - 10.708920839363893 - 13.13384491895538 - 33.62694704191278 - 20.6858368622789 - 0.912284860010399'\n",
      "927 - random_27 - lwr_k=600 - 9.176496485565819 - 24.33107897732525 - 28.631179623188636 - 21.83688618422744 - 19.517806712497233 - 20.69739766865352 - 0.9122358381721096'\n",
      "928 - random_81 - lwr_k=700 - 14.555528195104369 - 30.72642377147277 - 10.536718689112732 - 12.25332571950664 - 35.47769632158593 - 20.710570940680846 - 0.9121799789188587'\n",
      "929 - random_27 - lwr_k=800 - 9.132519916235905 - 23.95941982663419 - 28.836049414388846 - 22.121198047359762 - 19.53341353559773 - 20.715154321851504 - 0.9121605437892245'\n",
      "930 - random_27 - lwr_k=400 - 9.236801744955933 - 24.228837775179606 - 29.350599231851966 - 21.447013558021485 - 19.34271806299405 - 20.719887909768598 - 0.9121404717307642'\n",
      "931 - random_73 - lwr_k=300 - 8.39534078103553 - 8.967793633250503 - 5.986161164335312 - 13.266292038848004 - 67.25382554888962 - 20.769922484122908 - 0.9119283077403475'\n",
      "932 - random_27 - lwr_k=900 - 9.102621446575077 - 24.17715889897326 - 29.046452236855828 - 22.328722719628416 - 19.546688666839913 - 20.838953179357926 - 0.9116355935931544'\n",
      "933 - random_22 - lwr_k=600 - 62.61953286249445 - 6.737305687768509 - 6.9211920777095415 - 9.447731389232667 - 18.742228268119963 - 20.898112490082244 - 0.9113847375481986'\n",
      "934 - random_22 - lwr_k=500 - 60.27264062520982 - 6.698425062652925 - 6.919461119122368 - 11.876935328437126 - 18.826728482060954 - 20.9229536285631 - 0.911279402484703'\n",
      "935 - random_82 - lwr_k=100 - 17.910266050450875 - 24.37064476733103 - 12.738839009660264 - 29.36700790336368 - 20.269159226043588 - 20.931251926499254 - 0.9112442148164451'\n",
      "936 - random_27 - lwr_k=1000 - 9.159135985431323 - 24.191226374302758 - 29.191879474317215 - 22.61452235110775 - 19.524211387665897 - 20.934799662154738 - 0.9112291711838505'\n",
      "937 - random_27 - lwr_k=300 - 9.227554718332225 - 24.605039622144407 - 29.94985836097715 - 22.043234458423502 - 19.153245669246864 - 20.99445056143787 - 0.9109762306324984'\n",
      "938 - random_0 - lwr_k=200 - 11.041846712499376 - 15.578654465169226 - 33.064982920521935 - 12.10941136837138 - 33.37811613293798 - 21.032072648463576 - 0.9108166999037137'\n",
      "939 - random_71 - lwr_k=200 - 17.92854236258769 - 18.252037354561267 - 15.373224618046898 - 17.528665399663332 - 36.15701197836477 - 21.04692774710391 - 0.9107537091209152'\n",
      "940 - random_29 - lwr_k=100 - 14.951469862993381 - 15.482141712097889 - 22.946334242224246 - 15.229291509671562 - 36.67436443757584 - 21.054807821533718 - 0.910720294865725'\n",
      "941 - random_0 - deep - 12.351709666306766 - 21.065932760847375 - 16.050840541830226 - 28.744383124227312 - 27.084733941143014 - 21.05809527544463 - 0.9107063551080838'\n",
      "942 - random_26 - lwr_k=600 - 8.137226482591275 - 8.403278811594397 - 6.6472776554100665 - 10.527357574702787 - 71.74629159112801 - 21.088087296255118 - 0.9105791783228748'\n",
      "943 - random_81 - lwr_k=900 - 14.748229786750517 - 30.974611966991667 - 10.629494527800365 - 12.813006468611224 - 36.55865162510643 - 21.145361056811275 - 0.9103363176661675'\n",
      "944 - random_39 - lwr_k=200 - 17.11324494300816 - 17.260041285629335 - 15.83718798658328 - 18.63063134028475 - 37.040804355811666 - 21.175075370549685 - 0.9102103186453586'\n",
      "945 - random_91 - lr - 28.62236446698099 - 11.599342343896705 - 13.748291721379111 - 21.13042386909488 - 30.94283982254409 - 21.208292923233603 - 0.9100694646734753'\n",
      "946 - random_22 - lwr_k=700 - 64.69295363603457 - 6.696347483437131 - 6.912096556319798 - 9.673908580727513 - 18.621013105372793 - 21.323971711476887 - 0.909578946394126'\n",
      "947 - random_73 - lwr_k=200 - 8.119616972208707 - 8.594328945280726 - 6.259030013902606 - 13.349615159931654 - 70.51266263088333 - 21.36279003450667 - 0.9094143432087973'\n",
      "948 - random_25 - lwr_k=200 - 11.265458068774267 - 22.223973703333268 - 20.607242179138844 - 31.88050916142526 - 21.012066283425018 - 21.396326010218786 - 0.9092721390125706'\n",
      "949 - random_3 - lwr_k=100 - 8.99444416928272 - 6.481806841602773 - 7.764934923160057 - 66.95716348052183 - 17.805761800543515 - 21.596282306314272 - 0.9084242547997835'\n",
      "950 - random_22 - lwr_k=800 - 66.55688726086821 - 6.7182958118571765 - 6.908946089512468 - 9.7399897772708 - 18.55974765449659 - 21.70166633279075 - 0.9079773898894299'\n",
      "951 - random_71 - lwr_k=300 - 18.990330035885027 - 18.368209355735022 - 14.920246907380672 - 17.874581168542946 - 39.15572162353753 - 21.860775555854225 - 0.9073027114673013'\n",
      "952 - random_30 - lwr_k=20 - 14.468168430330241 - 47.273741018361115 - 11.881400452159786 - 11.938561481993846 - 23.73199427651468 - 21.86172455862565 - 0.9072986873656177'\n",
      "953 - random_25 - lwr_k=300 - 11.374093664941217 - 21.248919091077536 - 20.874686651915063 - 35.59188546583088 - 20.65659996170916 - 21.94739064956109 - 0.9069354333571484'\n",
      "954 - random_73 - lwr_k=400 - 8.289274478461058 - 9.048666763780119 - 5.972269954960238 - 13.105982452163827 - 73.4469765989439 - 21.968277192472193 - 0.9068468671537337'\n",
      "955 - random_22 - lwr_k=900 - 67.92420574034408 - 6.737282005398317 - 6.898180103906314 - 9.760926556318596 - 18.524544062773796 - 21.97405854889121 - 0.9068223521925552'\n",
      "956 - random_93 - lwr_k=10 - 19.27315186329064 - 31.800337112555596 - 18.190563319597555 - 22.167638973593526 - 18.760108119657744 - 22.03950557435279 - 0.9065448340511097'\n",
      "957 - random_4 - deep - 12.300096499354087 - 14.373173228262294 - 12.751088063507956 - 35.00793709313645 - 35.93496138457877 - 22.07058987808808 - 0.9064130259954147'\n",
      "958 - random_92 - lwr_k=50 - 19.40983926968332 - 13.957181875689548 - 32.101748944482026 - 14.837222243279212 - 30.329755609933674 - 22.125366834719358 - 0.9061807524563994'\n",
      "959 - random_28 - lr - 16.00424283620173 - 22.695689862089626 - 9.802499961998882 - 14.059065815307724 - 48.13329453245621 - 22.13804522605738 - 0.9061269916693226'\n",
      "960 - random_25 - lwr_k=400 - 11.627081567317685 - 20.698796593554928 - 20.60746432688691 - 37.37358380849502 - 20.403496909930393 - 22.140126512676055 - 0.9061181662904737'\n",
      "961 - random_28 - deep - 16.370855161289146 - 22.897015462335315 - 8.896804178473795 - 13.86403295273277 - 48.84732393394427 - 22.174374068820768 - 0.9059729449457538'\n",
      "962 - random_22 - lwr_k=1000 - 69.08963860855157 - 6.749679360713912 - 6.91784030328581 - 9.724473885625397 - 18.510041771520303 - 22.203483410206207 - 0.905849511018117'\n",
      "963 - random_41 - lr - 9.18762619937621 - 35.111609110719755 - 31.25249943441646 - 15.02354808068048 - 20.868896465423745 - 22.28879026513068 - 0.9054877802952271'\n",
      "964 - random_73 - lwr_k=500 - 8.478679866122672 - 9.242423761154337 - 6.0694682237739555 - 13.29857650744315 - 74.57082919836358 - 22.327583707311373 - 0.9053232826133488'\n",
      "965 - random_24 - lr - 12.70486474907817 - 19.358648441353502 - 31.137599774088624 - 19.711186195381337 - 29.115595888166503 - 22.40349162712979 - 0.9050014066429677'\n",
      "966 - random_81 - lwr_k=600 - 14.747021695739424 - 30.422476180925617 - 10.494670805420956 - 12.617728709272273 - 43.91739973472328 - 22.439906875523533 - 0.9048469933295552'\n",
      "967 - random_59 - lwr_k=100 - 18.565086945569714 - 17.980470765292 - 12.105083646515968 - 13.725498697339823 - 50.26142761368143 - 22.526120137682323 - 0.904481419124868'\n",
      "968 - random_32 - lwr_k=100 - 22.804679741877106 - 18.967196017829536 - 16.150482607982767 - 20.460500797019655 - 34.496489959065535 - 22.575316383803585 - 0.9042728099376149'\n",
      "969 - random_89 - lwr_k=100 - 9.955697886166137 - 11.53218286446425 - 9.992456134291055 - 14.19451284056458 - 67.32545088451079 - 22.59617732529175 - 0.9041843522931342'\n",
      "970 - random_25 - lwr_k=500 - 11.608756353012497 - 20.565607694910035 - 20.603573448054128 - 40.465170342057846 - 20.203909486545506 - 22.687241283340448 - 0.9037982094514677'\n",
      "971 - random_83 - lwr_k=200 - 19.74611176012283 - 15.155821065509949 - 15.4224726717945 - 18.881053542511783 - 44.502618355845335 - 22.73988282840648 - 0.903574990998887'\n",
      "972 - random_50 - lr - 13.14801213451959 - 14.06635560437895 - 17.700192488253776 - 14.541473053960788 - 54.56765710415907 - 22.801725942059313 - 0.9033127546964562'\n",
      "973 - random_47 - lwr_k=40 - 21.804578767280542 - 34.13305050459363 - 17.077528687298503 - 18.297319158069183 - 23.39090837069677 - 22.942323777639587 - 0.9027165709929751'\n",
      "974 - random_59 - lwr_k=200 - 19.85610070122379 - 14.177055347729539 - 11.864462754997179 - 12.775655268669759 - 56.40720226342016 - 23.014130467063094 - 0.9024120856653132'\n",
      "975 - random_73 - lwr_k=600 - 8.635569299959506 - 9.48831394498684 - 6.152010988298996 - 13.584444313242347 - 77.95357479470312 - 23.158164740926626 - 0.9018013302687897'\n",
      "976 - random_50 - deep - 13.424381921022107 - 13.730584115716713 - 16.630698773433302 - 14.308945983086913 - 57.93223357766891 - 23.202214949254945 - 0.9016145422800622'\n",
      "977 - random_25 - lwr_k=600 - 11.532346920743041 - 20.303128531185116 - 21.012369009437148 - 42.935202443298785 - 20.248251910028948 - 23.203872822693977 - 0.9016075121111018'\n",
      "978 - random_30 - lwr_k=10 - 15.092082157994824 - 51.13997070981172 - 12.813761614142594 - 12.580563311162459 - 24.96744601010922 - 23.32197329566079 - 0.9011067250465953'\n",
      "979 - random_81 - lr - 15.572834382497371 - 35.62292921543032 - 12.674566786627032 - 13.98513833775166 - 39.12857038541734 - 23.397528657954574 - 0.9007863440426896'\n",
      "980 - random_89 - lwr_k=50 - 10.128501468018872 - 12.39394032835214 - 10.976953317401742 - 14.93528988190594 - 68.60778710703576 - 23.404516272999953 - 0.9007567141255642'\n",
      "981 - random_39 - lwr_k=300 - 18.747305590785217 - 18.755740939060303 - 16.95459995116263 - 19.42331748102863 - 43.81910967168227 - 23.538446529114264 - 0.9001888032770875'\n",
      "982 - random_24 - lwr_k=20 - 12.316313948081202 - 16.996914311815452 - 23.37416284898694 - 43.77948661141241 - 21.708320355068338 - 23.632099243209392 - 0.8997916832097703'\n",
      "983 - random_25 - lwr_k=700 - 11.513823672422388 - 20.041443009790896 - 21.381004531526308 - 45.283694843278546 - 20.289353869393544 - 23.69926885489289 - 0.8995068607038649'\n",
      "984 - random_89 - lwr_k=200 - 10.188487851777813 - 11.111668792610251 - 9.5214446652395 - 14.058482709524391 - 73.69882181530954 - 23.711502240137854 - 0.8994549868973347'\n",
      "985 - random_83 - lwr_k=100 - 21.191004729413407 - 20.159225248101666 - 19.47451042752542 - 18.471744457308812 - 39.322885494566584 - 23.722875624000626 - 0.8994067597956554'\n",
      "986 - random_3 - lwr_k=50 - 9.43909417588047 - 14.303814205263775 - 9.358024844745751 - 67.34016528917321 - 18.424735325757563 - 23.769269040121713 - 0.8992100355820425'\n",
      "987 - random_17 - lwr_k=50 - 22.318875572916834 - 41.96999912748321 - 16.91458832286847 - 17.709127042669927 - 20.045602945540974 - 23.7943740857568 - 0.8991035814604622'\n",
      "988 - random_71 - lwr_k=100 - 21.924983589801208 - 19.4536112430898 - 19.913074095235594 - 23.251705300930134 - 34.56478162477663 - 23.820605353260753 - 0.8989923517666126'\n",
      "989 - random_71 - lwr_k=400 - 21.59477577615153 - 18.534276718880598 - 16.186000160453315 - 19.422790557942744 - 44.014781199283675 - 23.949252245032948 - 0.8984468442197749'\n",
      "990 - random_54 - lwr_k=100 - 9.419827649014449 - 7.109105887047497 - 7.317576199371628 - 8.39088562541796 - 87.66941024541879 - 23.976213947730823 - 0.8983325172267581'\n",
      "991 - random_73 - lwr_k=700 - 8.809319027877299 - 9.75873164337316 - 6.291137528338681 - 13.937072743651632 - 81.26503778987475 - 24.00743635235063 - 0.8982001233763052'\n",
      "992 - random_54 - lwr_k=200 - 9.505279428594577 - 7.188140930860129 - 6.691739205952867 - 8.232709793609594 - 88.4892439864316 - 24.016289309833372 - 0.8981625837587354'\n",
      "993 - random_32 - lwr_k=200 - 19.61240967217349 - 20.173275472698215 - 16.249547251100566 - 21.03870598176912 - 43.12751340723206 - 24.038932096708276 - 0.8980665704703338'\n",
      "994 - random_89 - lwr_k=40 - 10.499857432266818 - 12.827472480918308 - 11.437032089493666 - 14.389245105044619 - 71.14695638879991 - 24.056052948938618 - 0.8979939721045958'\n",
      "995 - random_9 - lwr_k=10 - 17.325603105978598 - 20.379703524298392 - 14.243568121041088 - 22.819851782999038 - 45.63198126435122 - 24.078429593712844 - 0.8978990873512286'\n",
      "996 - random_3 - lwr_k=40 - 9.406868450292693 - 15.038362968318973 - 10.13737562400812 - 66.63352589238846 - 19.40654673454574 - 24.12063813786541 - 0.8977201084496861'\n",
      "997 - random_85 - lwr_k=100 - 22.634674139137992 - 17.361591507480394 - 22.546937222133273 - 19.375462280141797 - 38.740590246153175 - 24.130497315048068 - 0.8976783021107636'\n",
      "998 - random_25 - lwr_k=800 - 11.299309519895194 - 19.817892403962166 - 21.709873128781854 - 47.57204035519207 - 20.274218954824423 - 24.131858270628207 - 0.8976725311859504'\n",
      "999 - random_77 - lwr_k=40 - 12.184574367345192 - 36.0530873690982 - 20.994811241776965 - 26.37701830082525 - 25.10862608955331 - 24.143615354314015 - 0.8976226770636232'\n",
      "1000 - random_91 - deep - 32.90436477473832 - 12.642543864523331 - 13.755440533112347 - 29.610969808626916 - 32.99302929605645 - 24.38074310216457 - 0.896617173192092'\n",
      "1001 - random_59 - lwr_k=300 - 20.897989712101868 - 14.514526820232637 - 12.421406971372173 - 13.285752180130135 - 60.90101827206069 - 24.40194528191963 - 0.8965272683588997'\n",
      "1002 - random_33 - lwr_k=200 - 30.63602661357168 - 19.35993252949198 - 15.685172920752553 - 20.029674079774253 - 36.53615571909016 - 24.449572031026047 - 0.8963253143846452'\n",
      "1003 - random_89 - lwr_k=300 - 10.22922075323441 - 11.34320718422659 - 9.54817955274286 - 14.22345923829805 - 76.93213700711534 - 24.45076423597939 - 0.8963202590211579'\n",
      "1004 - random_70 - deep - 13.014082232785888 - 38.13488360746793 - 33.271337688799086 - 13.313619193046627 - 25.108273235917775 - 24.56876825570979 - 0.8958198811734263'\n",
      "1005 - random_73 - lwr_k=100 - 15.566865562764573 - 10.327028727834229 - 8.648819942144398 - 15.057478851329613 - 73.3003569410076 - 24.576300225137352 - 0.8957879427829494'\n",
      "1006 - random_68 - lwr_k=800 - 17.299051405508646 - 7.960348542156984 - 6.975328293381605 - 13.672013749370855 - 77.10246766858 - 24.59792113862524 - 0.8956962625929759'\n",
      "1007 - random_85 - lwr_k=200 - 23.280254424545056 - 17.63223245039698 - 20.06277262564964 - 20.192471281425764 - 41.874124040074534 - 24.607011171548915 - 0.8956577176931143'\n",
      "1008 - random_68 - lwr_k=700 - 18.52023526146639 - 7.763175746053367 - 6.891768195202781 - 13.957904871940485 - 76.15594096850894 - 24.654033566189696 - 0.8954583263919055'\n",
      "1009 - random_25 - lwr_k=900 - 11.584680069665508 - 19.74390889892777 - 22.054089324919627 - 49.65870869430029 - 20.27792394006619 - 24.660914889783744 - 0.8954291471874873'\n",
      "1010 - random_68 - lr - 14.575021389881767 - 8.335147867117549 - 6.581605908326292 - 11.46760817200596 - 82.63463233778482 - 24.71445935951762 - 0.8952021000204017'\n",
      "1011 - random_73 - lwr_k=800 - 8.884570929627253 - 9.901076319872343 - 6.269491620766915 - 14.29913145052491 - 84.6018504009341 - 24.786181275335903 - 0.8948979741622984'\n",
      "1012 - random_3 - lwr_k=200 - 8.810691293139806 - 6.586445904759921 - 7.442785606468012 - 83.77923103476381 - 17.627922759022063 - 24.843798536686336 - 0.894653656942801'\n",
      "1013 - random_59 - lwr_k=50 - 22.64932846733959 - 22.17146389179414 - 12.675428686497249 - 17.667702928653654 - 49.19431379398912 - 24.870841510958144 - 0.8945389853300556'\n",
      "1014 - random_36 - lwr_k=10 - 10.34413167748109 - 14.133461218126481 - 14.982450507598838 - 27.807595015816307 - 57.14055781231474 - 24.877498803235405 - 0.8945107560963079'\n",
      "1015 - random_22 - lr - 81.65078631166011 - 7.13068730034621 - 7.313364671100864 - 10.187994204795556 - 18.326936949047262 - 24.928329778308164 - 0.8942952150899662'\n",
      "1016 - random_25 - lwr_k=100 - 11.150750272823602 - 23.593536543935286 - 24.229187884394012 - 44.0439867181902 - 21.956176649038085 - 24.992231270396296 - 0.8940242505473484'\n",
      "1017 - random_24 - lwr_k=10 - 14.561473279302355 - 22.28892437383304 - 30.26986159894335 - 32.11294607817383 - 26.01223880641411 - 25.046919549455247 - 0.8937923532310647'\n",
      "1018 - random_33 - lwr_k=300 - 30.0498128811666 - 21.451518192646372 - 15.852980984244503 - 21.588108360106094 - 36.57441009107541 - 25.103578087908076 - 0.8935521013299651'\n",
      "1019 - random_25 - lwr_k=1000 - 11.589276669353524 - 19.762067649063408 - 22.30223097069618 - 51.60018495717407 - 20.321746102091435 - 25.11200992233318 - 0.8935163474205685'\n",
      "1020 - random_47 - lwr_k=100 - 23.890369527282584 - 36.386632044992346 - 9.629052766760724 - 27.864778040369224 - 27.8060051592133 - 25.11701254891519 - 0.8934951345446343'\n",
      "1021 - random_24 - lwr_k=30 - 11.859907249869604 - 15.685790009070072 - 22.14882542263101 - 52.35624639340907 - 23.63214664479103 - 25.132861599983475 - 0.8934279290579811'\n",
      "1022 - random_83 - lwr_k=300 - 21.290756875987334 - 14.781588648973182 - 15.742117657780176 - 21.599347376019658 - 52.55263710979327 - 25.190945627776145 - 0.8931816325864912'\n",
      "1023 - random_89 - lwr_k=30 - 22.550576759608703 - 13.320376092129683 - 11.833593843586462 - 14.68123238931661 - 63.72669493713316 - 25.220108356547982 - 0.8930579724777061'\n",
      "1024 - random_96 - lwr_k=50 - 30.699199879518343 - 16.89158430502495 - 17.984801428637866 - 12.7506352155661 - 47.969553828789664 - 25.258675559364193 - 0.8928944341293882'\n",
      "1025 - random_71 - lwr_k=500 - 22.91263290435283 - 19.70584403895501 - 17.168671312493085 - 21.1602799582803 - 45.55374711537458 - 25.2989280422808 - 0.8927237496075355'\n",
      "1026 - random_37 - lwr_k=50 - 15.285970956601208 - 24.466509081071305 - 21.208239473583266 - 23.123380934147068 - 42.87085699661727 - 25.389185445676144 - 0.8923410268380083'\n",
      "1027 - random_20 - lr - 15.018322043275198 - 15.322838994039158 - 21.56594798836422 - 19.55270247482907 - 55.55778094098005 - 25.400167272325824 - 0.8922944600750423'\n",
      "1028 - random_3 - lwr_k=300 - 8.798771033094239 - 6.604949890112759 - 7.539417280200934 - 86.49991291861433 - 17.698852236772115 - 25.422575359752933 - 0.8921994419938957'\n",
      "1029 - random_73 - lwr_k=900 - 9.03220392774363 - 10.124158952203933 - 6.287227605635632 - 14.49801479290543 - 87.36842890416341 - 25.456804994252185 - 0.8920542964432522'\n",
      "1030 - random_41 - deep - 8.997948544310274 - 40.255459696493446 - 38.92610016931382 - 18.18127491901782 - 20.93248532035134 - 25.458381395447415 - 0.892047612183669'\n",
      "1031 - random_90 - deep - 72.12042071191067 - 10.898658430127583 - 8.859052091031462 - 9.79632442276757 - 26.230885725154142 - 25.586284466788534 - 0.8915052586952109'\n",
      "1032 - random_81 - deep - 15.7935793856357 - 45.35724001820466 - 12.70990668975555 - 15.490688867397136 - 38.87353833204015 - 25.64660506099066 - 0.8912494784051183'\n",
      "1033 - random_89 - lwr_k=400 - 10.422170040763126 - 11.579712424624743 - 9.517048582135459 - 14.405690157816677 - 82.63432982500873 - 25.7069725054071 - 0.8909934992220473'\n",
      "1034 - random_59 - lwr_k=400 - 22.048806588936113 - 14.870414175056375 - 12.524272667214898 - 13.725178486644024 - 66.11975008872885 - 25.855261583559603 - 0.890364701976102'\n",
      "1035 - random_3 - lwr_k=400 - 8.829427095745478 - 6.752482412899574 - 7.528350578617977 - 88.64589038910928 - 17.670436254216007 - 25.87939156829163 - 0.8902623824517429'\n",
      "1036 - random_39 - lwr_k=400 - 21.39418010745355 - 20.22432646291461 - 18.697894648201547 - 20.812561903000034 - 48.292222268112056 - 25.8825750560622 - 0.8902488833568158'\n",
      "1037 - random_20 - deep - 15.035809351457512 - 15.923429989385527 - 22.335115399934736 - 20.28782128428554 - 56.926887680916 - 26.09833433864849 - 0.8893339892225094'\n",
      "1038 - random_73 - lwr_k=1000 - 9.19408968907824 - 10.286912481139186 - 6.392372177852993 - 14.739047971828505 - 90.20308022691727 - 26.157722222340396 - 0.8890821637133943'\n",
      "1039 - random_85 - lwr_k=300 - 26.104638205416027 - 18.587991359618314 - 20.38944402198499 - 21.711269785754222 - 44.359677986766414 - 26.22933219413187 - 0.8887785125368836'\n",
      "1040 - random_32 - lwr_k=300 - 22.054410482156168 - 22.198759925780184 - 17.49035470212156 - 22.772327029241186 - 47.089651163352414 - 26.31972698590886 - 0.8883952071928533'\n",
      "1041 - random_83 - lwr_k=400 - 21.883207696172306 - 15.890591777170444 - 17.04562472418589 - 22.980665504063367 - 54.33162021745326 - 26.423872868941803 - 0.8879535924411566'\n",
      "1042 - random_91 - lwr_k=50 - 15.383560714207837 - 38.717739216215435 - 26.278214619405663 - 20.6985923631053 - 31.32778102894687 - 26.481364086616548 - 0.8877098096906646'\n",
      "1043 - random_68 - lwr_k=900 - 14.676780803113584 - 7.876364568827369 - 6.575503009016032 - 13.89716946857266 - 89.82861934180497 - 26.565878656622157 - 0.8873514385312787'\n",
      "1044 - random_9 - lr - 8.254861838530562 - 7.943390982112032 - 6.0999979279690875 - 8.324622086281057 - 102.42602091498341 - 26.603716640153824 - 0.8871909923262495'\n",
      "1045 - random_24 - deep - 13.735732617042654 - 24.54719817072592 - 32.539442181099055 - 24.79397809671438 - 37.60670615687515 - 26.64215479493159 - 0.8870280014266936'\n",
      "1046 - random_92 - lwr_k=40 - 32.5823529021066 - 24.10876508604769 - 22.01781322381803 - 19.576564974928626 - 35.024672601473874 - 26.662585100473848 - 0.8869413695881259'\n",
      "1047 - random_33 - lwr_k=400 - 31.17117805257405 - 22.52959746819548 - 16.92771741542293 - 22.584570581076417 - 40.19364613010899 - 26.681397290808764 - 0.8868615993608092'\n",
      "1048 - random_27 - lr - 9.255692981635272 - 35.68496838291723 - 38.65794611337361 - 31.296256423791895 - 18.80089169388548 - 26.737753110135323 - 0.8866226310940493'\n",
      "1049 - random_3 - lwr_k=500 - 8.870928379758096 - 6.827505700404141 - 7.4952588492714485 - 93.07166768777348 - 17.699957102346513 - 26.78685956601527 - 0.8864144026486361'\n",
      "1050 - random_59 - lwr_k=500 - 23.28941433901466 - 15.12572935075825 - 12.86256100440123 - 14.10330772843781 - 68.91109475675651 - 26.855915835876644 - 0.8861215800561358'\n",
      "1051 - random_33 - lwr_k=100 - 33.77816787035257 - 25.037158151593246 - 19.42919197277815 - 22.760814056525412 - 33.89583619998091 - 26.981028616468663 - 0.8855910583693807'\n",
      "1052 - random_83 - lwr_k=500 - 22.92137513126171 - 16.209987092988293 - 17.98384686188547 - 24.080892778999967 - 54.10264039986454 - 27.057294198098774 - 0.8852676657885724'\n",
      "1053 - random_89 - lwr_k=500 - 10.535996226116206 - 11.598630034560525 - 9.545284037793817 - 14.553603193730241 - 89.13468597253105 - 27.068397932323276 - 0.8852205820951065'\n",
      "1054 - random_2 - lwr_k=100 - 23.623487846104105 - 22.24435858308548 - 18.120817559590957 - 23.36278926705767 - 48.334525244744505 - 27.135819157629065 - 0.8849346926599658'\n",
      "1055 - random_71 - lwr_k=600 - 24.71633885320356 - 21.31093617838432 - 18.38978603997171 - 22.489086781464508 - 48.78801747045493 - 27.137482091861713 - 0.8849276412406792'\n",
      "1056 - random_47 - lwr_k=50 - 31.24376280977723 - 38.36341719641023 - 13.040909550642672 - 28.561125296411586 - 25.77345461496722 - 27.39895965297025 - 0.8838188854755886'\n",
      "1057 - random_95 - lwr_k=200 - 10.917751798828789 - 31.01714911594151 - 11.700165857710935 - 12.755525336812484 - 70.65824927362797 - 27.407658462332105 - 0.883781999499643'\n",
      "1058 - random_30 - deep - 24.827889980934255 - 48.93192047768451 - 11.815314663911236 - 15.119366940272625 - 36.58216141209458 - 27.458416999856954 - 0.8835667659012617'\n",
      "1059 - random_3 - lwr_k=600 - 8.919778983657267 - 6.861585260598955 - 7.461467391039905 - 96.49110359965611 - 17.68274157945414 - 27.476918905294657 - 0.8834883111422104'\n",
      "1060 - random_15 - lwr_k=40 - 17.546212549255905 - 28.63218854189977 - 29.236638201621556 - 29.859313645646356 - 33.44424852289239 - 27.742195969438015 - 0.8823634441633067'\n",
      "1061 - random_85 - lwr_k=400 - 28.66072838467773 - 19.60229099739843 - 21.668730592692405 - 22.998819494797967 - 45.7902055342143 - 27.74297188455845 - 0.8823601540134394'\n",
      "1062 - random_59 - lwr_k=600 - 24.37220563524125 - 15.502120378505715 - 13.156286694282013 - 14.47930892742356 - 71.71928115741437 - 27.843250521713482 - 0.881934937728762'\n",
      "1063 - random_3 - lwr_k=700 - 8.92010689877722 - 6.890862497760557 - 7.425781237026714 - 99.07223985706818 - 17.753132395098522 - 28.005839694264676 - 0.8812455031109571'\n",
      "1064 - random_68 - lwr_k=500 - 19.75289375480697 - 7.731299360174637 - 7.247428115110208 - 12.884450220113953 - 92.58245491114933 - 28.035022904917223 - 0.8811217561518769'\n",
      "1065 - random_95 - lwr_k=100 - 20.044626088013402 - 29.06338273648265 - 11.728134328867197 - 13.586747233357741 - 65.80699130893272 - 28.044832742626113 - 0.8810801590294768'\n",
      "1066 - random_33 - lwr_k=500 - 31.03094753485913 - 23.879180498386386 - 17.691463251362514 - 22.869480135206896 - 44.75516666427424 - 28.045054335795992 - 0.8810792193973968'\n",
      "1067 - random_32 - lwr_k=400 - 24.384095388874506 - 23.625274423415938 - 18.077651454891242 - 23.345797185220224 - 51.44870661882462 - 28.174938837691155 - 0.8805284639533669'\n",
      "1068 - random_22 - deep - 94.0411822011545 - 8.122244499318924 - 7.050653488884003 - 13.01507129520788 - 18.86260481685229 - 28.22583953881743 - 0.8803126275388099'\n",
      "1069 - random_89 - lwr_k=600 - 10.616916859922563 - 11.705666505358383 - 9.575393949876199 - 14.63156287329313 - 94.87382889758115 - 28.275067338694882 - 0.8801038842981679'\n",
      "1070 - random_29 - lwr_k=10 - 22.235571700727462 - 19.56505737385175 - 30.916792278825163 - 35.94842961458446 - 33.19158159583301 - 28.369039755935983 - 0.8797054085783629'\n",
      "1071 - random_3 - lwr_k=800 - 8.89973851995835 - 6.961235008186057 - 7.404564194208347 - 101.29885894261145 - 17.828442179560795 - 28.471838415374677 - 0.8792695065230851'\n",
      "1072 - random_83 - lwr_k=600 - 23.934518690036725 - 17.86168168590576 - 19.66624100523287 - 25.528694796101135 - 55.40013411582088 - 28.475771610978853 - 0.879252828406298'\n",
      "1073 - random_85 - lwr_k=50 - 28.898264999699443 - 23.09927049254703 - 27.42968729132104 - 26.063392730834202 - 37.28713055262992 - 28.55471188526901 - 0.8789180941987202'\n",
      "1074 - random_71 - lwr_k=700 - 26.259130021174027 - 22.62242061110405 - 18.98479912299465 - 23.906011607937693 - 51.13773039904321 - 28.580662124176186 - 0.8788080561638242'\n",
      "1075 - random_43 - lwr_k=300 - 22.58383584939098 - 24.808979843656157 - 22.22852363069599 - 25.212151685964855 - 48.089040390551766 - 28.582905461832354 - 0.8787985436322372'\n",
      "1076 - random_39 - lwr_k=500 - 24.212035571929203 - 22.182591663313325 - 21.115331517940927 - 22.986239935045475 - 52.74877994794671 - 28.64721033928372 - 0.8785258685954372'\n",
      "1077 - random_44 - lwr_k=10 - 21.345501896998865 - 17.21713510366584 - 30.041218380431292 - 24.48361485011681 - 50.43386688449575 - 28.701181472945933 - 0.8782970122249603'\n",
      "1078 - random_59 - lwr_k=700 - 25.452693096364396 - 15.771572427626968 - 13.660615118158272 - 14.685618584948674 - 74.25187488469525 - 28.761804987067944 - 0.8780399474485494'\n",
      "1079 - random_95 - lwr_k=300 - 11.080503189595266 - 34.513171155071454 - 11.547008620138415 - 12.251721118645033 - 74.71882402188001 - 28.820272341731265 - 0.8777920255378595'\n",
      "1080 - random_68 - lwr_k=400 - 20.324336965741235 - 7.723433129854554 - 7.588579618705552 - 13.209964248995623 - 96.20048671625715 - 29.004452497106787 - 0.8770110376465005'\n",
      "1081 - random_85 - lwr_k=500 - 30.337742751608637 - 20.624726835501008 - 22.278798392349383 - 24.9538272772173 - 46.90826617569408 - 29.019513145326897 - 0.8769471752620218'\n",
      "1082 - random_3 - lwr_k=900 - 8.944187892949616 - 6.990264474890732 - 7.381036242069402 - 104.06058633369253 - 17.874558099099936 - 29.04322210537207 - 0.8768466410287096'\n",
      "1083 - random_54 - lwr_k=300 - 9.7188064535794 - 7.23292444644081 - 6.4850340733879275 - 8.14317861549617 - 114.0086816135253 - 29.110964975121433 - 0.8765593877093045'\n",
      "1084 - random_33 - lwr_k=600 - 32.06298913183022 - 24.824925862443155 - 17.84796993288066 - 25.115331822422675 - 45.724444761742156 - 29.114912497185955 - 0.8765426488433448'\n",
      "1085 - random_89 - lwr_k=700 - 10.706377161625513 - 11.78507146071922 - 9.627512227689367 - 14.857709050328909 - 98.71395600660148 - 29.132265544625184 - 0.8764690658891954'\n",
      "1086 - random_3 - lwr_k=1000 - 8.942824364897739 - 7.103042396150861 - 7.394616664220614 - 106.01128199671015 - 17.908344615271094 - 29.464997580193852 - 0.8750581663798721'\n",
      "1087 - random_56 - lwr_k=20 - 17.729572678636995 - 23.941310746344502 - 40.3078406658112 - 38.02295476681719 - 27.353992379304508 - 29.46830611817545 - 0.875044137028575'\n",
      "1088 - random_59 - lwr_k=800 - 26.336779784686087 - 16.208800574715784 - 13.887002895579114 - 14.967289222484835 - 76.18500130060276 - 29.51427484253826 - 0.8748492136556667'\n",
      "1089 - random_83 - lwr_k=700 - 25.592079862125416 - 19.466047914232682 - 20.619874182657217 - 26.399373609678005 - 56.131486334502576 - 29.639443018106853 - 0.874318457074952'\n",
      "1090 - random_25 - lwr_k=50 - 11.531366050590304 - 37.634523320715275 - 31.297635489298806 - 43.667923210043305 - 24.184827878878146 - 29.66159142357006 - 0.874224540135614'\n",
      "1091 - random_89 - lwr_k=800 - 10.759216999567426 - 11.910144093276685 - 9.603727009938703 - 15.008653892968445 - 101.16713254442921 - 29.683763742369933 - 0.8741305217954147'\n",
      "1092 - random_43 - lwr_k=400 - 24.56987270936871 - 27.045186087828974 - 23.556660062863376 - 24.066763162682225 - 49.980480382339195 - 29.842470632946277 - 0.8734575494029077'\n",
      "1093 - random_95 - lwr_k=400 - 11.247166406151017 - 36.77928576414591 - 11.28694958142676 - 12.25892150341642 - 77.82126792670225 - 29.87679732850824 - 0.8733119922796139'\n",
      "1094 - random_62 - lwr_k=300 - 24.800354627553432 - 27.422461713260567 - 26.214365186966432 - 24.785141437295124 - 46.367560765574105 - 29.916730121412158 - 0.8731426633545594'\n",
      "1095 - random_32 - lwr_k=500 - 26.165191700028657 - 24.902352436806765 - 18.78423249567517 - 24.43149628733716 - 55.38453068994699 - 29.93211982179404 - 0.8730774056745156'\n",
      "1096 - random_81 - lwr_k=10 - 15.444391783076409 - 37.40263319058618 - 15.580481713536704 - 12.6029118003297 - 69.45957168712108 - 30.09679466648459 - 0.8723791270817294'\n",
      "1097 - random_74 - lwr_k=10 - 24.17170800289338 - 55.41257559193343 - 16.091995690944525 - 20.938746255499716 - 33.85578550249428 - 30.09733822957791 - 0.8723768221852372'\n",
      "1098 - random_43 - lwr_k=500 - 24.997267111267973 - 28.450505420129435 - 24.067893959122387 - 24.160379677118037 - 48.854779381441546 - 30.105057437032475 - 0.8723440900619495'\n",
      "1099 - random_71 - lwr_k=800 - 27.66516113196569 - 23.945159559520874 - 19.839502152282922 - 25.339749359800667 - 53.78021963850309 - 30.11254726704813 - 0.8723123305787487'\n",
      "1100 - random_85 - lwr_k=600 - 31.987951602148325 - 21.145492733067574 - 22.662662897174446 - 25.93427179959765 - 49.40440351499091 - 30.225757808121248 - 0.8718322785255183'\n",
      "1101 - random_59 - lwr_k=900 - 27.256810435495332 - 16.398006864545636 - 14.085589917167944 - 15.371885155582378 - 78.26656994186182 - 30.273005682575977 - 0.8716319311115074'\n",
      "1102 - random_95 - lwr_k=50 - 14.05764416038371 - 35.039682876291984 - 16.01952867932493 - 17.69425978501584 - 68.59692271262972 - 30.279730143425354 - 0.8716034170596619'\n",
      "1103 - random_59 - lwr_k=40 - 27.637311690112632 - 26.180884397892502 - 18.598657035992993 - 25.615028601863546 - 53.502848858683656 - 30.305833345311584 - 0.8714927304085551'\n",
      "1104 - random_75 - lwr_k=40 - 19.188332160226402 - 24.3663379114225 - 30.137546005560065 - 51.375475230355946 - 26.605327117633596 - 30.33180123910906 - 0.8713826175107862'\n",
      "1105 - random_89 - lwr_k=900 - 10.846366099922994 - 12.098899385043742 - 9.643200239079516 - 15.202760648442377 - 104.11343005772967 - 30.374738949857793 - 0.8712005466893719'\n",
      "1106 - random_96 - lwr_k=100 - 10.973913911664999 - 39.28823430431641 - 7.552988395089678 - 7.898165426060841 - 86.81798986857471 - 30.504498045699002 - 0.8706503230106097'\n",
      "1107 - random_2 - lwr_k=50 - 24.60834540979971 - 28.339397795012133 - 22.819780120620393 - 28.576472564031775 - 48.38632001076688 - 30.544729565643806 - 0.8704797273790457'\n",
      "1108 - random_2 - lwr_k=200 - 26.54396478224202 - 23.90673050299963 - 19.6816465695464 - 24.445741008165207 - 58.29096747725387 - 30.572058483818267 - 0.8703638432647409'\n",
      "1109 - random_33 - lwr_k=700 - 33.947422191318545 - 25.314492399037697 - 19.023426938473254 - 26.627578792056063 - 48.148558223130436 - 30.611974327302093 - 0.8701945862111202'\n",
      "1110 - random_83 - lwr_k=800 - 26.963943122064556 - 20.509401533824715 - 21.583656417699533 - 27.21603929601822 - 56.801163742710116 - 30.612588270148215 - 0.8701919828800077'\n",
      "1111 - random_54 - lwr_k=400 - 10.065475579954962 - 7.392850785824145 - 6.530946177766639 - 8.190599868822158 - 121.09620665940626 - 30.64803518470641 - 0.8700416756387115'\n",
      "1112 - random_39 - lwr_k=600 - 26.56176176270513 - 23.390241979446603 - 21.91293985514836 - 25.118503710048017 - 56.69649192182609 - 30.734101490513154 - 0.8696767245800482'\n",
      "1113 - random_43 - lwr_k=600 - 26.07603932277814 - 30.465591402783616 - 24.7669378449964 - 25.159783666880898 - 47.52208805005366 - 30.797260393278883 - 0.8694089088743279'\n",
      "1114 - random_94 - lwr_k=10 - 44.921169979127384 - 42.27987876694829 - 24.072984832988574 - 24.568321427941342 - 19.05353364066426 - 30.9833111327249 - 0.8686199890561775'\n",
      "1115 - random_89 - lwr_k=1000 - 10.941529028730091 - 12.14034481152866 - 9.741040149779794 - 15.318364204397435 - 106.85728030990425 - 30.993339087545383 - 0.868577467041393'\n",
      "1116 - random_59 - lwr_k=1000 - 28.388015723046696 - 16.705348139494788 - 14.245276946134672 - 15.580211103884624 - 80.06514748142754 - 30.994032523976145 - 0.8685745266298447'\n",
      "1117 - random_83 - lwr_k=900 - 27.818877690168737 - 21.162714173797664 - 22.108177733513063 - 28.070235648793176 - 57.18093108381013 - 31.26596771752006 - 0.8674214268674996'\n",
      "1118 - random_71 - lwr_k=900 - 28.94808971724813 - 25.157094682488616 - 20.485802588501414 - 26.704204769105715 - 55.515273599934915 - 31.36068173991111 - 0.8670198064840304'\n",
      "1119 - random_95 - lwr_k=500 - 11.479788515278692 - 39.269395897777166 - 11.175527703278114 - 12.590241332702746 - 82.62166119045449 - 31.425340700709324 - 0.8667456300107381'\n",
      "1120 - random_27 - deep - 11.831145174959874 - 38.454081705471104 - 40.10302887462769 - 47.39033277809962 - 19.359050818684647 - 31.425469576897438 - 0.8667450838142366'\n",
      "1121 - random_85 - lwr_k=700 - 33.19837783093279 - 21.822234215667713 - 23.955877733417804 - 27.626633618381927 - 50.95183521992591 - 31.50968152694004 - 0.866387995585362'\n",
      "1122 - random_54 - lwr_k=500 - 10.194895383138382 - 7.538121830323865 - 6.551908361184547 - 8.236984579142252 - 125.11683521899295 - 31.52032767600791 - 0.8663428522120455'\n",
      "1123 - random_32 - lwr_k=600 - 28.548033892417408 - 26.343718243489384 - 20.159518295670022 - 25.573672734413133 - 57.238026800769646 - 31.57124252195012 - 0.8661269555640586'\n",
      "1124 - random_46 - lwr_k=200 - 9.094163665903181 - 34.9420447229885 - 6.221969811042479 - 10.222050902508665 - 98.27573123221131 - 31.748004548387353 - 0.8653774231184044'\n",
      "1125 - random_43 - lwr_k=700 - 26.23272225829489 - 32.61519910255938 - 25.65752913924272 - 25.22141956748982 - 49.07060099842308 - 31.758729342223504 - 0.8653319462639285'\n",
      "1126 - random_33 - lwr_k=800 - 34.734519256077476 - 26.84458713170317 - 19.67231848537639 - 28.04370583331207 - 50.33207158855445 - 31.925068463830286 - 0.8646266105584105'\n",
      "1127 - random_62 - lwr_k=400 - 27.180336141603842 - 29.176723759039245 - 27.5713438441044 - 26.672224289628844 - 49.4110937552507 - 32.00109208531771 - 0.8643042439727276'\n",
      "1128 - random_71 - lwr_k=10 - 30.953391599634294 - 29.74396376860247 - 27.4283681364844 - 33.34005011984556 - 38.70222567332774 - 32.033048059292184 - 0.8641687395331763'\n",
      "1129 - random_31 - lwr_k=10 - 17.516718222830484 - 26.68280891162286 - 32.17601711180477 - 24.060016544735088 - 59.77464825347639 - 32.03878578196034 - 0.8641444095942712'\n",
      "1130 - random_83 - lwr_k=1000 - 29.206587940986736 - 21.787701245654677 - 22.418198880002905 - 29.134449579458355 - 58.08859469072929 - 32.124935200992056 - 0.8637791061066386'\n",
      "1131 - random_54 - lwr_k=600 - 10.451000192513495 - 7.640990565352118 - 6.595810294428645 - 8.18829144246934 - 128.44581698922153 - 32.256778036794444 - 0.8632200466460025'\n",
      "1132 - random_0 - lwr_k=10 - 28.643553426080388 - 29.617017400016604 - 30.02100895911277 - 43.417446370516814 - 29.893075400132357 - 32.31737621917961 - 0.8629630892849627'\n",
      "1133 - random_71 - lwr_k=1000 - 29.760026979241218 - 26.2003657335884 - 21.027228976723897 - 27.78171005490773 - 57.12177001594096 - 32.37678002982871 - 0.8627111964753224'\n",
      "1134 - random_43 - lwr_k=800 - 26.994448269723033 - 33.92291566897096 - 26.045274381686678 - 26.027138683431676 - 49.08202761996735 - 32.413720453508056 - 0.8625545562361151'\n",
      "1135 - random_62 - lwr_k=200 - 47.56491300644254 - 25.36672825539345 - 22.512993912307685 - 24.21740652618941 - 43.465942534727894 - 32.62685449357643 - 0.861650793807473'\n",
      "1136 - random_95 - lwr_k=600 - 11.66286654189985 - 41.75715976254892 - 11.410530126617276 - 12.813708213995824 - 86.56903500950607 - 32.840651531772046 - 0.8607442200362718'\n",
      "1137 - random_43 - lwr_k=200 - 23.740677495676554 - 24.149678534248565 - 22.927088024985338 - 24.88347967417932 - 68.94511922993503 - 32.92626638431295 - 0.860381183296409'\n",
      "1138 - random_39 - lwr_k=700 - 28.84458327580171 - 24.703530788193312 - 23.598087206573105 - 27.478072238816296 - 60.079744748760774 - 32.9387840850269 - 0.8603281038934445'\n",
      "1139 - random_85 - lwr_k=800 - 34.56588314836984 - 22.732544163489624 - 25.360986253889624 - 29.191305475885848 - 53.17519576978573 - 33.00375641265049 - 0.8600525986358667'\n",
      "1140 - random_9 - lwr_k=20 - 21.17194468635365 - 14.870115586951052 - 13.06563099111517 - 60.6127500907108 - 55.629244046689635 - 33.065008668068764 - 0.8597928677777698'\n",
      "1141 - random_5 - lwr_k=100 - 12.905441667122833 - 12.983629470190198 - 14.684143958156211 - 67.10190694396701 - 58.001859921377694 - 33.12878402575588 - 0.8595224380888644'\n",
      "1142 - random_41 - lwr_k=50 - 13.129055046847514 - 53.80062172356365 - 45.82139471719571 - 23.626885401306037 - 29.65283018504072 - 33.206242131008324 - 0.8591939887933014'\n",
      "1143 - random_33 - lwr_k=900 - 35.6718989728791 - 29.220528007074787 - 20.316890535126117 - 29.745762828853287 - 51.395039893607134 - 33.26975425528357 - 0.8589246753055788'\n",
      "1144 - random_32 - lwr_k=700 - 31.070233859802787 - 28.02821280356695 - 21.166639142627375 - 26.957107969884163 - 59.831719526078835 - 33.40951802658533 - 0.8583320283246118'\n",
      "1145 - random_43 - lwr_k=900 - 27.751626856335875 - 35.04503452497152 - 26.78500716571695 - 27.274008920632166 - 50.304585975776305 - 33.431386658885344 - 0.8582392977207522'\n",
      "1146 - random_32 - lwr_k=50 - 33.837112620563914 - 28.313561799833465 - 27.82859367136283 - 29.68686991338099 - 47.64632534600429 - 33.461710893428695 - 0.8581107124236838'\n",
      "1147 - random_54 - lwr_k=700 - 10.824254788245794 - 7.7211534625778615 - 6.607213830780517 - 8.214326272343625 - 135.98706131839097 - 33.86274622861904 - 0.8564101831774562'\n",
      "1148 - random_2 - lwr_k=300 - 29.666324267080824 - 26.058763618584916 - 22.802596991814767 - 27.861965737321217 - 63.347506635320535 - 33.945438691711956 - 0.8560595383848586'\n",
      "1149 - random_62 - lwr_k=500 - 30.055624955528756 - 32.13770370326169 - 28.03304078193805 - 28.433068443025945 - 51.17021238963875 - 33.96499039012148 - 0.8559766323862064'\n",
      "1150 - random_43 - lwr_k=1000 - 28.729315413645278 - 36.15409776364336 - 27.360840432392045 - 28.101430422478174 - 49.59821112735016 - 33.988272376443064 - 0.8558779086699251'\n",
      "1151 - random_85 - lwr_k=40 - 32.170737052831775 - 31.111448047573276 - 35.847668136402945 - 28.39017121860377 - 42.89970685382896 - 34.083146244257954 - 0.8554756104862865'\n",
      "1152 - random_95 - lwr_k=700 - 11.922544705901151 - 43.68780642011492 - 11.502045237806696 - 12.937447275283562 - 90.92716291694833 - 34.19330855674943 - 0.8550084839819976'\n",
      "1153 - random_85 - lwr_k=900 - 35.902741386234624 - 23.8215553586006 - 25.694310170759483 - 30.225097905459283 - 55.37693305119288 - 34.20270560647753 - 0.8549686372241468'\n",
      "1154 - random_33 - lwr_k=1000 - 37.349410784394905 - 29.465017416457673 - 20.391385003995687 - 31.25552876170415 - 52.57173817524115 - 34.206354229790534 - 0.8549531657928096'\n",
      "1155 - random_79 - lwr_k=40 - 24.6540375511021 - 15.792758157916579 - 88.36485644795827 - 19.737592382005584 - 22.70470663724867 - 34.24619636186135 - 0.8547842213012068'\n",
      "1156 - random_18 - lwr_k=50 - 31.473549692081157 - 27.61400830419827 - 33.76216534943504 - 28.06741958276607 - 50.583168527240254 - 34.298504641465755 - 0.8545624160100473'\n",
      "1157 - random_39 - lwr_k=800 - 30.804600222127476 - 25.470255429019158 - 24.813212304489145 - 28.714152326458326 - 61.7126822422944 - 34.300961329282714 - 0.8545519987996046'\n",
      "1158 - random_32 - lwr_k=800 - 33.16232352842213 - 29.13059833003493 - 21.68525547361714 - 28.13819591017388 - 61.543290946504456 - 34.73075862065258 - 0.8527295088597171'\n",
      "1159 - random_4 - lwr_k=40 - 24.944936227320493 - 27.270926991629427 - 43.387754441233504 - 47.13410735157391 - 31.28001033070611 - 34.80069931494462 - 0.8524329360001495'\n",
      "1160 - random_54 - lwr_k=800 - 11.103853272233998 - 7.798283556060154 - 6.6141694215245685 - 8.267244986456745 - 141.56167051151638 - 35.06065464072084 - 0.8513306350420923'\n",
      "1161 - random_99 - lwr_k=10 - 30.8475488150241 - 32.59582232847183 - 34.13726545537798 - 42.47350020748177 - 35.36496624473247 - 35.08271953439039 - 0.8512370722217932'\n",
      "1162 - random_85 - lwr_k=1000 - 37.025637906961705 - 24.699025370261403 - 26.36592698337536 - 31.544541699028766 - 56.894786914530044 - 35.30452850968123 - 0.8502965250518567'\n",
      "1163 - random_39 - lwr_k=50 - 34.703452013194564 - 27.84366838403614 - 28.187457642237458 - 37.161864277998795 - 48.79175218117122 - 35.33630794223587 - 0.8501617692093023'\n",
      "1164 - random_91 - lwr_k=40 - 20.21505553449207 - 82.6698660000146 - 20.71486296123872 - 20.511071456813266 - 33.48380007187898 - 35.52414605000374 - 0.8493652703280661'\n",
      "1165 - random_95 - lwr_k=800 - 12.01086422654137 - 45.37131046935736 - 11.665506534146987 - 13.14251143840642 - 95.51521226984194 - 35.53883766241247 - 0.8493029727837276'\n",
      "1166 - random_39 - lwr_k=900 - 32.80621946985506 - 26.877068673363482 - 25.77848592727946 - 29.567463463142314 - 63.603872396312006 - 35.724694696678405 - 0.848514874342885'\n",
      "1167 - random_54 - lwr_k=900 - 11.40688689000878 - 7.8564273138486636 - 6.60217597436357 - 8.284098857376959 - 145.26710036332014 - 35.87474063702029 - 0.8478786273904612'\n",
      "1168 - random_32 - lwr_k=900 - 34.8112928039901 - 30.13041623281494 - 22.637494956876328 - 29.17844701822621 - 63.934135837603826 - 36.1371562882414 - 0.8467658937971544'\n",
      "1169 - random_38 - lwr_k=10 - 33.08477706009154 - 33.22679185042098 - 35.669975791959715 - 39.36896145197794 - 39.50415938056444 - 36.16994566680258 - 0.8466268554324117'\n",
      "1170 - random_1 - lwr_k=50 - 14.279414407431632 - 41.59044919920897 - 34.561155402282594 - 56.90962232744977 - 33.67191433306634 - 36.199803559193064 - 0.8465002475882011'\n",
      "1171 - random_2 - lwr_k=400 - 32.00734472059336 - 27.840028768680803 - 24.386304748568133 - 29.76724396972964 - 67.23072662981689 - 36.24425914588849 - 0.846311740445064'\n",
      "1172 - random_15 - lwr_k=10 - 30.76320703985456 - 40.88793241000713 - 37.38452965199019 - 36.98369908236517 - 35.2817147954668 - 36.26007425211132 - 0.8462446788963568'\n",
      "1173 - random_62 - lwr_k=600 - 33.05450531559094 - 32.27992739074774 - 29.381278032501314 - 30.9830778015254 - 55.960053961597296 - 36.330568385066776 - 0.8459457592644464'\n",
      "1174 - random_95 - lwr_k=900 - 12.172335491611317 - 45.92221703905461 - 11.722200933338856 - 13.277023935986367 - 98.94045907126532 - 36.404437085956246 - 0.8456325302912843'\n",
      "1175 - random_47 - lwr_k=10 - 36.020645423046425 - 45.74255518559479 - 39.83998085551196 - 25.57623834535874 - 35.23701776227956 - 36.48472793118815 - 0.8452920691933703'\n",
      "1176 - random_54 - lwr_k=1000 - 11.72553327314988 - 7.911980676017164 - 6.63296839866457 - 8.311875002833618 - 148.61551672849635 - 36.63079118464147 - 0.8446727102179857'\n",
      "1177 - random_3 - lr - 9.11783341545177 - 7.426096857008894 - 8.16151953217716 - 140.339026877301 - 18.283528318168706 - 36.65630228187388 - 0.8445645342964638'\n",
      "1178 - random_2 - lwr_k=40 - 32.486718574232924 - 36.229099495517644 - 28.465488085842495 - 35.58109795560587 - 51.423137700238 - 36.83629644176097 - 0.84380129648129'\n",
      "1179 - random_39 - lwr_k=1000 - 34.392052292556926 - 27.915688864175177 - 26.370236010680724 - 30.531002677685784 - 65.30011919294404 - 36.89993739423724 - 0.8435314367172063'\n",
      "1180 - random_39 - lwr_k=10 - 30.593409140681192 - 39.359766963194964 - 34.16637529647061 - 29.965849194307072 - 51.07355216121245 - 37.03111748665738 - 0.8429751875189221'\n",
      "1181 - random_87 - lwr_k=30 - 27.682318400905764 - 22.411525894004516 - 54.92024896394584 - 24.779444206437777 - 55.44368431108878 - 37.04351426776783 - 0.8429226208571157'\n",
      "1182 - random_73 - lr - 11.161782608283856 - 14.490472823003511 - 8.107971321496048 - 22.571691884503934 - 129.02292002936994 - 37.06302771695101 - 0.8428398770484808'\n",
      "1183 - random_37 - lwr_k=10 - 29.879888045096337 - 28.986038839341088 - 27.677145604251038 - 35.51473284545735 - 63.583095441987545 - 37.125660025305926 - 0.8425742942861396'\n",
      "1184 - random_25 - lr - 11.666442204332132 - 28.944968637061997 - 34.68642215452271 - 90.19443416591412 - 20.5698829351049 - 37.20689318473883 - 0.842229837448406'\n",
      "1185 - random_95 - lwr_k=1000 - 12.365594338819065 - 47.14376006841304 - 11.836562968757697 - 13.376560458481226 - 101.87810186757233 - 37.31763831187199 - 0.8417602396074134'\n",
      "1186 - random_3 - lwr_k=30 - 9.512274091118865 - 26.767445348102374 - 10.972960212421011 - 119.69693991720266 - 19.908728969461688 - 37.36537142372877 - 0.841557834618119'\n",
      "1187 - random_87 - lwr_k=10 - 50.38505465798439 - 42.189526214253995 - 33.51685625293998 - 28.036249571968735 - 33.74760823707626 - 37.57791218201391 - 0.8406565879104997'\n",
      "1188 - random_51 - lwr_k=10 - 30.976057781454315 - 38.52962375492904 - 29.266553996610572 - 45.85882484315574 - 43.42913269151644 - 37.61110224613668 - 0.8405158504996724'\n",
      "1189 - random_32 - lwr_k=1000 - 36.651220998995285 - 31.251283926633054 - 23.42564275344811 - 30.3348186209107 - 66.76961089683346 - 37.685292166733845 - 0.8402012594432736'\n",
      "1190 - random_67 - lwr_k=20 - 31.045567210898827 - 29.029250732288222 - 19.39397195239258 - 75.71865698166448 - 33.53193964575323 - 37.74135348985147 - 0.8399635399428249'\n",
      "1191 - random_83 - lwr_k=10 - 39.03007330141299 - 30.82678376969898 - 40.643487059410546 - 36.16017291476848 - 42.09272980475709 - 37.74972511232681 - 0.8399280413530288'\n",
      "1192 - random_93 - lwr_k=20 - 21.699257719077824 - 65.11296516522512 - 35.40375645309868 - 42.75590574889672 - 23.88135056367824 - 37.772492705339076 - 0.8398314988432143'\n",
      "1193 - random_93 - lwr_k=100 - 47.95381388916299 - 27.596746327432164 - 56.056495425408016 - 38.74220431600218 - 18.566338678166055 - 37.783117159813635 - 0.8397864474625159'\n",
      "1194 - random_62 - lwr_k=700 - 35.276687840199735 - 33.38015938028943 - 30.925644203042925 - 32.64032594106675 - 58.64706114973912 - 38.172716311319334 - 0.8381344116110454'\n",
      "1195 - random_71 - lwr_k=20 - 39.38955291705876 - 36.22838319492009 - 33.271307533327246 - 37.039004076232146 - 45.35259702432465 - 38.256022493992226 - 0.8377811644340614'\n",
      "1196 - random_63 - lwr_k=200 - 21.02700638524131 - 102.17265975513659 - 25.43897351271716 - 13.5477029217404 - 31.16010171847603 - 38.67679845231271 - 0.835996928082684'\n",
      "1197 - random_17 - lwr_k=10 - 51.84313536504078 - 25.988707170470928 - 34.47783667193371 - 17.73814664616637 - 63.63203357370876 - 38.7360308176345 - 0.8357457622608366'\n",
      "1198 - random_22 - lwr_k=10 - 49.06579269269143 - 24.84541941824573 - 24.58008929507145 - 56.2667497861192 - 39.55380425296681 - 38.861746636740044 - 0.8352126834811302'\n",
      "1199 - random_75 - lwr_k=10 - 21.10339443660373 - 30.458366642858106 - 40.65347564873365 - 53.29763182367324 - 48.92187731914564 - 38.8826570279332 - 0.8351240161527085'\n",
      "1200 - random_2 - lwr_k=500 - 34.64922065254401 - 30.46908350038032 - 26.385399765517136 - 31.706313721041397 - 72.35349684990975 - 39.110556655592156 - 0.8341576424992385'\n",
      "1201 - random_32 - lwr_k=10 - 37.40779405443624 - 37.140964243328284 - 32.2035787487604 - 35.57679166617309 - 54.569978636656984 - 39.37913195216312 - 0.8330187898681696'\n",
      "1202 - random_65 - lwr_k=10 - 51.180562774754414 - 46.254571658421426 - 24.249111221035157 - 30.561082796535672 - 44.97875080341137 - 39.44785261237271 - 0.8327273903772718'\n",
      "1203 - random_62 - lwr_k=800 - 36.59979052926989 - 34.58902796121164 - 32.053474013609254 - 33.97412173275965 - 60.267524411244004 - 39.49550972782279 - 0.8325253076391629'\n",
      "1204 - random_63 - lwr_k=300 - 21.79366261951435 - 105.35995716728794 - 25.559589034801498 - 13.618526680519956 - 32.95946159184334 - 39.86600708556093 - 0.8309542700342478'\n",
      "1205 - random_3 - lwr_k=20 - 12.805569586230119 - 39.09820208313502 - 15.365891502546575 - 107.09839288905386 - 25.06384807899525 - 39.88181737785518 - 0.8308872288982716'\n",
      "1206 - random_73 - lwr_k=10 - 28.48469512912888 - 36.21065679479821 - 23.384598604438544 - 42.45310500793331 - 71.45040254587657 - 40.394055619823185 - 0.8287151606662058'\n",
      "1207 - random_62 - lwr_k=900 - 38.64989077944866 - 34.971345929439465 - 32.442074618928174 - 35.38090452635259 - 61.45415811667982 - 40.57844045441545 - 0.8279333048638083'\n",
      "1208 - random_63 - lwr_k=400 - 22.282070194855315 - 108.16232577481375 - 25.84595643749936 - 13.726999676551673 - 33.945943433902954 - 40.80065960749753 - 0.8269910183974334'\n",
      "1209 - random_89 - lr - 12.362379108044742 - 14.709689946071645 - 11.525760455086406 - 20.147698524591224 - 147.68931989869992 - 41.27788134839189 - 0.8249674323038506'\n",
      "1210 - random_62 - lwr_k=1000 - 40.006255810339155 - 35.67435955747783 - 33.28711398593997 - 36.20433758703634 - 62.37790206056474 - 41.50879200360668 - 0.823988290846722'\n",
      "1211 - random_63 - lwr_k=500 - 22.909676046401934 - 110.92942859895112 - 26.21624988275113 - 13.923106621344656 - 34.891308685679085 - 41.78218897549313 - 0.8228289926359988'\n",
      "1212 - random_2 - lwr_k=600 - 36.91565481145637 - 32.28347581180691 - 28.418501771741088 - 34.73097910703933 - 77.29001984640523 - 41.92532634797436 - 0.8222220404131555'\n",
      "1213 - random_58 - lwr_k=100 - 35.00596276937294 - 39.50281430089129 - 34.53745133318344 - 49.65708213305179 - 53.45170162664719 - 42.42930712760314 - 0.8200849866922195'\n",
      "1214 - random_63 - lwr_k=600 - 23.524683895988826 - 112.36182077640575 - 27.567015256355653 - 13.997828019939542 - 35.57195498326339 - 42.612958798851686 - 0.819306239757293'\n",
      "1215 - random_10 - lwr_k=20 - 23.417907068860778 - 61.25084673318225 - 12.120557161718482 - 98.07203474285636 - 20.99625520798129 - 43.17124602432087 - 0.816938907825669'\n",
      "1216 - random_63 - lwr_k=700 - 23.9759604668043 - 113.92928237073816 - 28.272025866209383 - 13.909941792622861 - 35.94016301455114 - 43.213906713554614 - 0.8167580116717883'\n",
      "1217 - random_20 - lwr_k=10 - 32.31526030533363 - 34.06497244320873 - 49.67194956636829 - 45.776161731617755 - 54.28807928236427 - 43.21999887298616 - 0.8167321787977821'\n",
      "1218 - random_63 - lwr_k=800 - 24.27496618863993 - 116.20089627502529 - 26.691073857775958 - 14.059441114044612 - 36.23072841774823 - 43.5001804665568 - 0.815544111432358'\n",
      "1219 - random_60 - lwr_k=10 - 37.60366251802885 - 49.44264586282595 - 56.953792643831704 - 44.721461645961966 - 29.083150454300167 - 43.560930249540384 - 0.8152865112320752'\n",
      "1220 - random_39 - lwr_k=20 - 32.978518089170265 - 45.25425864884661 - 39.276917196427405 - 43.36837885859329 - 57.76064604099605 - 43.72623358000541 - 0.8145855676411041'\n",
      "1221 - random_59 - lr - 46.990668627552886 - 25.959847350261672 - 23.688977956187223 - 26.629242787690774 - 95.86402417436582 - 43.82414468152368 - 0.8141703905306305'\n",
      "1222 - random_63 - lwr_k=900 - 24.564629173178805 - 117.62523649246855 - 26.895697119176717 - 14.066038062128504 - 36.379947013395174 - 43.915213656975645 - 0.8137842264134298'\n",
      "1223 - random_54 - lwr_k=50 - 11.047822789466055 - 11.284905578860588 - 7.777358683797709 - 9.880267542132822 - 179.87546073703942 - 43.962419067935386 - 0.8135840590594007'\n",
      "1224 - random_27 - lwr_k=200 - 9.369931631820055 - 25.1215128392614 - 145.12875603770385 - 21.795714970452167 - 18.883578211455685 - 44.05111728193999 - 0.8132079477949621'\n",
      "1225 - random_2 - lwr_k=700 - 38.950570039209275 - 34.96684249893626 - 29.941337011950974 - 36.79861107613884 - 80.35822399757681 - 44.20074443068819 - 0.8125734766644939'\n",
      "1226 - random_63 - lwr_k=1000 - 25.082163362993338 - 118.54521088064203 - 26.971486268166043 - 14.117029540866428 - 36.54936415331468 - 44.262076757777685 - 0.812313406274673'\n",
      "1227 - random_23 - lwr_k=30 - 155.42956246513097 - 12.361283481710084 - 17.570440370548774 - 13.566553111628599 - 25.005409181260127 - 44.79945757312869 - 0.8100347247903288'\n",
      "1228 - random_58 - lwr_k=200 - 39.9540168405699 - 42.77109669886313 - 36.45507522801544 - 45.3527965767308 - 60.82253905038081 - 45.06989035521488 - 0.8088879957748968'\n",
      "1229 - random_59 - lwr_k=30 - 45.90556891046867 - 47.47489456490176 - 27.561425037161136 - 48.51011829683564 - 56.2790041785816 - 45.14670785651889 - 0.8085622628627892'\n",
      "1230 - random_80 - lwr_k=20 - 72.16145872592544 - 25.4069691913481 - 58.80748816883204 - 32.6984359025165 - 36.750354329637105 - 45.166126550272104 - 0.8084799208501204'\n",
      "1231 - random_38 - lwr_k=30 - 43.253567109002574 - 52.37118538330732 - 42.91667399265138 - 41.09881695464111 - 48.11693994325102 - 45.55217711854533 - 0.8068429322252657'\n",
      "1232 - random_41 - lwr_k=40 - 22.01970988303605 - 66.24529601444378 - 70.0848991337478 - 36.50464025207562 - 33.93540990578808 - 45.757458701781225 - 0.8059724669435996'\n",
      "1233 - random_43 - lwr_k=10 - 55.43553406826284 - 43.86238410739682 - 40.13644868778547 - 42.85378292352666 - 47.60483464019803 - 45.97979890351236 - 0.8050296671888686'\n",
      "1234 - random_90 - lwr_k=50 - 24.0851191360171 - 6.976691491597982 - 8.203585655061808 - 9.497088553970247 - 182.27783957557156 - 46.19801832617432 - 0.8041043409700319'\n",
      "1235 - random_84 - lwr_k=10 - 108.35131025352472 - 18.08049853633173 - 40.7149917214323 - 26.20948402510881 - 37.655627921847504 - 46.207954301671364 - 0.8040622089795566'\n",
      "1236 - random_2 - lwr_k=800 - 40.447972475574474 - 37.32366207262247 - 31.114489861067025 - 39.26675363162388 - 84.1337565709505 - 46.454847305649714 - 0.8030152968072042'\n",
      "1237 - random_90 - lwr_k=40 - 25.013962864834863 - 7.779435570934618 - 8.79718091408071 - 9.951300497627637 - 181.5376908377725 - 46.60601755446642 - 0.8023742823959672'\n",
      "1238 - random_18 - lwr_k=10 - 55.53867308776044 - 62.81916662475067 - 42.167597951085206 - 39.890159459232 - 37.746961907242095 - 47.63629317429646 - 0.7980055555795018'\n",
      "1239 - random_8 - lwr_k=100 - 52.99441682239089 - 39.00322135255807 - 36.873839383829 - 43.49376423105818 - 67.41265158000571 - 47.95493784882133 - 0.796654391378654'\n",
      "1240 - random_95 - lwr_k=40 - 92.00241113539958 - 39.596916707558144 - 20.892049851669775 - 19.783530448613487 - 67.82451059815489 - 48.02570650255236 - 0.7963543076832145'\n",
      "1241 - random_2 - lwr_k=900 - 41.955997726737934 - 38.76701958049322 - 32.25483275976219 - 41.002526335966216 - 86.4721352597476 - 48.087971140899405 - 0.7960902839695317'\n",
      "1242 - random_62 - lwr_k=100 - 129.27664047960286 - 26.473736905271558 - 20.561906471482374 - 23.24985686412421 - 43.656412231258706 - 48.65328369599788 - 0.7936931621978347'\n",
      "1243 - random_32 - lwr_k=40 - 46.05984957566825 - 41.606838170326974 - 59.45869889302102 - 44.18626714845761 - 56.10963278458841 - 49.48240667958638 - 0.7901773924922986'\n",
      "1244 - random_2 - lwr_k=1000 - 43.37453778023104 - 41.33914619803022 - 33.44449049662155 - 42.926462564564545 - 88.44370943400422 - 49.90319710628175 - 0.7883930947899817'\n",
      "1245 - random_1 - lwr_k=40 - 26.209162210947188 - 89.71830919890333 - 53.207378799939754 - 42.90854623151252 - 37.74179372127831 - 49.95966017028351 - 0.7881536717685139'\n",
      "1246 - random_25 - lwr_k=10 - 20.12834771077816 - 74.57453306365704 - 69.51678996229784 - 54.225326461019186 - 31.98539500674896 - 50.08518286596862 - 0.7876214118992519'\n",
      "1247 - random_54 - lwr_k=40 - 14.320958209222104 - 15.179913432618998 - 8.033291503656086 - 17.02592708324957 - 196.70652848106576 - 50.24169679324102 - 0.7869577384335651'\n",
      "1248 - random_4 - lwr_k=10 - 37.3305190788069 - 53.020898411042886 - 38.53896352112355 - 78.83618858457335 - 45.07614035031327 - 50.55877849375877 - 0.7856132017858951'\n",
      "1249 - random_58 - lwr_k=300 - 45.42027049224744 - 47.73458908911398 - 38.18238237560015 - 50.03237315878165 - 73.62278273845395 - 50.997031707803515 - 0.7837548557940573'\n",
      "1250 - random_54 - lr - 15.61783621584717 - 9.411177930072792 - 7.210973382889765 - 9.415080417243196 - 213.61587030855145 - 51.041566173737976 - 0.7835660141755233'\n",
      "1251 - random_32 - lwr_k=20 - 49.0049115237789 - 46.47674838289815 - 50.337841779402595 - 58.52067969955084 - 54.396926782793265 - 51.74610950279222 - 0.7805785055169094'\n",
      "1252 - random_77 - lwr_k=30 - 22.74041689154165 - 55.842682946817725 - 31.353624167600685 - 97.87142321044236 - 51.18883363102462 - 51.79529993688337 - 0.7803699209746847'\n",
      "1253 - random_33 - lwr_k=10 - 95.88498093325906 - 48.37045809631777 - 35.96799285949711 - 32.37158790191264 - 47.515626564743194 - 52.02871371203137 - 0.779380165418798'\n",
      "1254 - random_23 - lwr_k=10 - 114.86743844074337 - 25.167094617056602 - 44.740260358280906 - 21.73100721284679 - 53.699743019053855 - 52.046995796175416 - 0.7793026430260287'\n",
      "1255 - random_7 - lwr_k=100 - 54.455748638788435 - 46.108122908000304 - 40.19420800783691 - 48.953582933902894 - 75.08636943364422 - 52.95872946594396 - 0.7754365752904365'\n",
      "1256 - random_48 - lwr_k=10 - 36.799863203171476 - 43.29648396950644 - 20.217616617895175 - 36.12608798108146 - 130.32213043312083 - 53.34807938745759 - 0.7737855962607805'\n",
      "1257 - random_7 - lwr_k=50 - 54.205651110886436 - 48.62426405545091 - 43.287270943127865 - 46.299054260248774 - 75.79363833871382 - 53.64124640873697 - 0.7725424661673228'\n",
      "1258 - random_11 - lwr_k=10 - 25.022834501492905 - 36.09488926672151 - 33.54194345589417 - 136.22083852056463 - 38.2874115045379 - 53.82596114050352 - 0.7717592114862134'\n",
      "1259 - random_63 - lr - 24.8720466828362 - 156.3493019993768 - 34.67099134265801 - 17.016341245841932 - 38.58971079616619 - 54.31157001184569 - 0.7697000610436396'\n",
      "1260 - random_46 - lwr_k=100 - 9.854926459498579 - 39.67985302325736 - 7.637078089276437 - 20.087434417075368 - 194.65728619326538 - 54.37361662684794 - 0.7694369618247374'\n",
      "1261 - random_37 - lwr_k=20 - 55.5841843450492 - 39.51870648243246 - 27.232792549263884 - 45.01514966176402 - 106.12230698301447 - 54.69228866171743 - 0.7680856816065162'\n",
      "1262 - random_8 - lwr_k=200 - 60.902329061855575 - 44.79737411273888 - 42.03292872889989 - 49.633592423266386 - 76.29897259827702 - 54.73242265410934 - 0.7679154995256832'\n",
      "1263 - random_83 - lr - 55.81336873833061 - 41.416190211530136 - 41.505940333187 - 57.825611976558484 - 78.31276661618227 - 54.972692720695534 - 0.7668966709104217'\n",
      "1264 - random_2 - lwr_k=30 - 51.48775637360409 - 54.86761376220673 - 50.52256193266033 - 58.362319678752115 - 60.53773271696226 - 55.15494914047919 - 0.7661238403267832'\n",
      "1265 - random_58 - lwr_k=400 - 51.63654005685744 - 51.09102796619561 - 40.99540551625365 - 54.02454324262069 - 78.78975707128917 - 55.306163245914604 - 0.7654826399482355'\n",
      "1266 - random_51 - lwr_k=20 - 36.01705400620342 - 57.21958186466091 - 60.386161418940944 - 58.91556292220858 - 66.73007172499804 - 55.850661868505384 - 0.7631737764865987'\n",
      "1267 - random_71 - lr - 51.157920909890876 - 48.79856545857028 - 41.44817513869953 - 49.84187746070606 - 88.49808498968521 - 55.94696943483533 - 0.7627653989228128'\n",
      "1268 - random_7 - lwr_k=40 - 55.895734194525865 - 51.734449330599425 - 48.18787725638897 - 48.75972202399514 - 78.69790405083104 - 56.65420727609576 - 0.7597664645956634'\n",
      "1269 - random_95 - lr - 15.219593779054101 - 86.31721581171989 - 16.897330702974514 - 17.47870340335239 - 148.14846012165683 - 56.810281442982465 - 0.7591046558669001'\n",
      "1270 - random_83 - lwr_k=20 - 45.70108414013611 - 51.17268320282717 - 54.7855777499301 - 50.674819258205524 - 82.30372883100937 - 56.92479799290931 - 0.7586190658820794'\n",
      "1271 - random_84 - lwr_k=20 - 95.42119363589852 - 28.29893316237722 - 66.17191838047036 - 28.653447458737173 - 67.96088524473508 - 57.302768547729954 - 0.7570163393233827'\n",
      "1272 - random_7 - lwr_k=200 - 56.6268301702189 - 50.7017513472391 - 43.30823032788826 - 55.86082704128784 - 83.18688707524242 - 57.93550594088237 - 0.7543333127972369'\n",
      "1273 - random_71 - lwr_k=30 - 45.875988247551724 - 43.488572734441306 - 49.28472582102782 - 71.83971544910844 - 79.79666594134652 - 58.052753467367936 - 0.7538361425223485'\n",
      "1274 - random_58 - lwr_k=500 - 55.27398117990722 - 52.52208721264668 - 42.15260267449869 - 57.17283395576492 - 83.34115503654401 - 58.09115834302816 - 0.7536732924976071'\n",
      "1275 - random_34 - lwr_k=10 - 76.36019723475432 - 18.954219316912603 - 81.43459873176094 - 52.832685035470575 - 61.546596563122044 - 58.222198282216915 - 0.75311763759775'\n",
      "1276 - random_39 - lr - 59.972399749459825 - 47.3623486839769 - 44.753385577755346 - 53.97876370855177 - 90.03001171218814 - 59.21756364251225 - 0.7488969424187062'\n",
      "1277 - random_78 - lwr_k=20 - 33.10889621270574 - 29.794930305029393 - 68.32787842343139 - 46.627822198522864 - 120.41817473940237 - 59.64630388421299 - 0.7470789347369124'\n",
      "1278 - random_2 - lwr_k=10 - 57.865020890776776 - 47.95731368727482 - 53.07275017263699 - 57.99643710768422 - 82.59598051544877 - 59.89521249926546 - 0.7460234756728532'\n",
      "1279 - random_5 - lwr_k=10 - 34.02580031277332 - 48.9946449635521 - 52.40630379640208 - 117.6426779537089 - 47.29335290849151 - 60.06647695199334 - 0.7452972548510279'\n",
      "1280 - random_89 - deep - 12.501188296147923 - 14.533573157658553 - 10.712831489381783 - 21.598839316184552 - 247.51741835135695 - 61.35709731769617 - 0.7398245763531232'\n",
      "1281 - random_33 - lr - 73.9138599325443 - 56.71091195867607 - 41.05411379614201 - 58.03508599226185 - 77.2767352265093 - 61.39942326908142 - 0.7396450990508818'\n",
      "1282 - random_8 - lwr_k=300 - 67.70454368916624 - 50.07947736179211 - 46.96634784650125 - 57.925988903513144 - 84.5284715817407 - 61.44013111140398 - 0.7394724836468365'\n",
      "1283 - random_45 - lwr_k=300 - 55.67399935365445 - 79.2088352623091 - 46.72753669918975 - 50.9815546181897 - 75.09580330484287 - 61.53947932445405 - 0.739051212683838'\n",
      "1284 - random_58 - lwr_k=600 - 59.46519899190431 - 55.441276872443744 - 44.95692470149809 - 59.6117663341144 - 88.30256976853137 - 61.55420385591828 - 0.7389887755512594'\n",
      "1285 - random_15 - lwr_k=20 - 56.52113042750692 - 62.69753851023803 - 47.71706163335179 - 79.9553419175357 - 62.232172827775905 - 61.82392356316888 - 0.73784507022104'\n",
      "1286 - random_7 - lwr_k=300 - 60.638393938512216 - 53.468827927326274 - 45.126885661769784 - 60.05436106807056 - 90.62788776059138 - 61.981656841799854 - 0.7371762262816698'\n",
      "1287 - random_8 - lwr_k=50 - 51.24850906498977 - 42.91591793406864 - 34.66725036959435 - 47.13960707323156 - 134.6640193955795 - 62.12213368471301 - 0.7365805556291619'\n",
      "1288 - random_8 - lwr_k=40 - 53.12822079028913 - 45.2778063104601 - 38.508984002917984 - 48.7520504971822 - 126.33166173090467 - 62.39542282874938 - 0.7354217146460107'\n",
      "1289 - random_62 - lwr_k=10 - 64.25663337760038 - 47.79359749902772 - 55.36247615063394 - 66.94380752934073 - 78.11984749385144 - 62.493153478709885 - 0.7350073027128783'\n",
      "1290 - random_33 - lwr_k=20 - 115.29341496031549 - 58.39229519379319 - 42.00060069869892 - 46.62222533630826 - 55.046590075634896 - 63.47867936456768 - 0.7308283303262677'\n",
      "1291 - random_96 - lwr_k=10 - 71.65556960627083 - 59.76841438214866 - 26.155758397471644 - 25.74340076446883 - 138.16670239121106 - 64.29843219096882 - 0.7273522933448602'\n",
      "1292 - random_58 - lwr_k=700 - 62.865608182096956 - 56.09149187350697 - 47.731898847357805 - 62.23023604986836 - 92.6558671594112 - 64.31343651205209 - 0.7272886698070244'\n",
      "1293 - random_85 - lwr_k=30 - 59.974131998376265 - 59.88601800525701 - 70.3803341738054 - 56.49359599720582 - 77.24696822003821 - 64.79461605369433 - 0.7252483012622912'\n",
      "1294 - random_7 - lwr_k=30 - 60.0870780778953 - 59.951182960662706 - 54.37270832756566 - 64.44365300598605 - 86.62276870642027 - 65.09381574716288 - 0.7239795905414164'\n",
      "1295 - random_78 - lwr_k=10 - 28.390916617656877 - 47.70963710264045 - 67.88004691962318 - 74.32116147138649 - 107.3375317572646 - 65.11899105407666 - 0.7238728384875872'\n",
      "1296 - random_7 - lwr_k=400 - 65.27456583185209 - 55.67070104295529 - 46.945872591086776 - 63.8400740359745 - 97.15285584508199 - 65.77507678721165 - 0.7210908068825688'\n",
      "1297 - random_45 - lwr_k=400 - 61.240435728961934 - 84.22048827330003 - 51.276969403601974 - 57.41877953829493 - 77.15567840353341 - 66.26458849181833 - 0.719015107070084'\n",
      "1298 - random_58 - lwr_k=800 - 66.31155404425289 - 57.46541183041918 - 48.14500223385426 - 64.63803540117463 - 96.24223876950501 - 66.55891841964716 - 0.7177670458485574'\n",
      "1299 - random_32 - lr - 69.54411928935149 - 56.52119616133895 - 47.10999059849 - 63.41280561149985 - 100.45845491264316 - 67.40787999041731 - 0.7141671536362346'\n",
      "1300 - random_79 - lwr_k=30 - 31.091714240877106 - 33.50968718917416 - 164.07010664294677 - 75.9461260686663 - 33.46374063064524 - 67.6047093496676 - 0.7133325287227904'\n",
      "1301 - random_62 - lr - 68.51854489988936 - 56.972156455276675 - 56.85130156768795 - 62.65326971742872 - 94.4486580467803 - 67.88710169809384 - 0.7121350869881817'\n",
      "1302 - random_14 - lwr_k=50 - 69.22939236888809 - 201.01937948010064 - 29.178408446733506 - 14.794641631811995 - 25.81305112037242 - 68.02895509470166 - 0.7115335792694322'\n",
      "1303 - random_14 - lwr_k=100 - 71.49127511266845 - 198.50656635434294 - 29.56530900830168 - 13.96016110131264 - 27.245189219368903 - 68.17559150374592 - 0.7109117899150771'\n",
      "1304 - random_8 - lwr_k=400 - 74.89017592417831 - 55.74017098098361 - 52.69393556272189 - 63.68956255518163 - 94.24131385907093 - 68.25007030325752 - 0.7105959739116487'\n",
      "1305 - random_37 - lwr_k=40 - 37.9537731753553 - 64.06815249734231 - 44.97197413498677 - 73.71148397811906 - 120.68295316067493 - 68.2720126637175 - 0.7105029306747584'\n",
      "1306 - random_85 - lr - 67.96092009746566 - 53.69023227144692 - 53.39514630455335 - 67.5257701069356 - 99.20359691120002 - 68.35266726081782 - 0.7101609271424595'\n",
      "1307 - random_14 - lwr_k=200 - 72.25931934451567 - 197.10928721174332 - 29.495629345679458 - 13.825438588550995 - 29.530208503743747 - 68.46566984485264 - 0.7096817568993703'\n",
      "1308 - random_58 - lwr_k=900 - 69.46998778341114 - 58.852094486194495 - 48.494953532064955 - 66.44874454343322 - 99.19140261004232 - 68.49001841665351 - 0.7095785104898449'\n",
      "1309 - random_51 - lwr_k=30 - 55.679084952861494 - 66.92851029262752 - 76.98853988959146 - 73.2366680963484 - 70.3370050264698 - 68.63156107387256 - 0.7089783204141362'\n",
      "1310 - random_39 - lwr_k=40 - 64.22036051017884 - 64.94958729988709 - 65.34763283374964 - 75.83308307395077 - 72.87395699737856 - 68.64359453758014 - 0.7089272943444297'\n",
      "1311 - random_14 - lwr_k=30 - 71.14960621771235 - 202.68229497008525 - 29.957457255440083 - 15.93390668029127 - 23.415263808789412 - 68.65006971140471 - 0.7088998373562373'\n",
      "1312 - random_59 - lwr_k=10 - 79.42387984033182 - 71.37291389740898 - 52.528083628303364 - 60.95988103879515 - 79.75574025531418 - 68.81025800847964 - 0.7082205832851465'\n",
      "1313 - random_14 - lwr_k=300 - 72.50696623761259 - 197.17451113552457 - 29.80509255676641 - 13.903322007473548 - 30.63037104356786 - 68.8256791513193 - 0.7081551922781504'\n",
      "1314 - random_14 - lwr_k=40 - 71.29656782434039 - 202.62568721758765 - 29.474458809389855 - 15.269887213031506 - 25.603115519822648 - 68.87624794568791 - 0.7079407630091448'\n",
      "1315 - random_58 - lwr_k=50 - 35.68983592052066 - 103.61779367515099 - 70.0859848446641 - 73.32002953822175 - 64.13262779956854 - 69.3693475471897 - 0.7058498492664469'\n",
      "1316 - random_14 - lwr_k=400 - 74.46655017080151 - 197.0048503673443 - 29.967401350639854 - 13.921793167217155 - 31.423016653278964 - 69.37846099518113 - 0.7058112050785574'\n",
      "1317 - random_7 - lwr_k=500 - 68.83813947161839 - 58.02206832680004 - 48.534835015055855 - 66.52671684972721 - 105.2592292926497 - 69.4342308372184 - 0.7055747215592265'\n",
      "1318 - random_14 - lwr_k=500 - 75.40738315922857 - 196.72400344282556 - 29.946047421050682 - 13.998675015441115 - 32.23153897025897 - 69.68327650325084 - 0.7045186813513646'\n",
      "1319 - random_17 - lwr_k=40 - 79.73610965468416 - 60.77915674670835 - 47.78137366125593 - 94.81296355488925 - 66.64488694361901 - 69.95099856583816 - 0.7033834467290239'\n",
      "1320 - random_14 - lwr_k=600 - 75.88956096862647 - 197.08134258542344 - 30.159444229414575 - 14.07211249376969 - 32.730794746342234 - 70.00842889918795 - 0.7031399221492369'\n",
      "1321 - random_14 - lwr_k=700 - 76.27889397515017 - 196.90871071152887 - 30.253743369105692 - 14.127624956260968 - 33.187094976250535 - 70.1729730830844 - 0.7024421976036437'\n",
      "1322 - random_14 - lwr_k=800 - 76.57828996145976 - 197.04046873076393 - 30.359286734837173 - 14.10751376247872 - 33.54944529454209 - 70.3487734130925 - 0.7016967430851948'\n",
      "1323 - random_14 - lwr_k=20 - 74.79918316572663 - 207.34081131299965 - 30.615637981004973 - 16.082962061835346 - 22.84417609551759 - 70.35971883378721 - 0.701650330696135'\n",
      "1324 - random_14 - lwr_k=900 - 76.90264565765071 - 197.05243813904985 - 30.42061692620539 - 14.126997469546248 - 33.90380750243653 - 70.503078195064 - 0.7010424371636212'\n",
      "1325 - random_65 - lwr_k=20 - 85.72268073030614 - 118.96377425309038 - 42.23552391355265 - 58.12829959996336 - 47.495628664041554 - 70.51960686073494 - 0.7009723498747762'\n",
      "1326 - random_14 - lwr_k=1000 - 77.22722728441134 - 197.00727741839174 - 30.45121625292669 - 14.151020060716139 - 34.18500188719458 - 70.62613109383459 - 0.7005206500919331'\n",
      "1327 - random_94 - lwr_k=20 - 66.1371930546483 - 101.88174486015801 - 40.81170447024282 - 49.64970330546545 - 96.0409975593213 - 70.9085605118658 - 0.6993230511693116'\n",
      "1328 - random_43 - lwr_k=100 - 30.550188227091926 - 28.114173348583265 - 25.562220540451143 - 29.235820774406214 - 241.78423314281375 - 71.03566513153618 - 0.6987840833923762'\n",
      "1329 - random_58 - lwr_k=1000 - 72.14326737373428 - 60.52562974309431 - 49.357649045559846 - 68.968697722749 - 104.2818144840052 - 71.0538655912625 - 0.6987069071155159'\n",
      "1330 - random_45 - lwr_k=500 - 66.72940830697362 - 89.43597973005267 - 54.863157341405824 - 61.31780546638148 - 82.91980071939052 - 71.0555324132848 - 0.6986998392106496'\n",
      "1331 - random_48 - lwr_k=20 - 96.38957926931607 - 71.81311477683899 - 33.355348021804566 - 75.27576180224989 - 81.90278287353598 - 71.75136320749293 - 0.6957492747288063'\n",
      "1332 - random_92 - lwr_k=10 - 61.31573939025666 - 32.97581095195852 - 179.64031399770107 - 24.12078510858679 - 61.16246205104576 - 71.83493412334882 - 0.6953949049910886'\n",
      "1333 - random_7 - lwr_k=600 - 72.42996005907297 - 60.506945307008564 - 49.79807669237777 - 70.24949872406688 - 109.2854912771666 - 72.45203418867709 - 0.6927781861714325'\n",
      "1334 - random_6 - lwr_k=900 - 47.109775014567916 - 43.8790235867517 - 34.54733495058114 - 154.63705539558475 - 83.63883309558992 - 72.75347432670647 - 0.6914999751867008'\n",
      "1335 - random_6 - lwr_k=1000 - 48.12817443688354 - 44.77177588450774 - 35.067804313621735 - 152.2952985145895 - 85.52449879985386 - 73.14876385796752 - 0.6898238101467351'\n",
      "1336 - random_94 - lwr_k=30 - 63.50123006620626 - 72.01597401522598 - 34.77876526180144 - 64.04824393918078 - 133.7037711738675 - 73.60768073116124 - 0.6878778430561224'\n",
      "1337 - random_8 - lwr_k=500 - 79.66661002549243 - 60.396626124889885 - 57.54126220462012 - 69.32581116668436 - 101.57773773438177 - 73.70040755457838 - 0.6874846490871775'\n",
      "1338 - random_85 - lwr_k=10 - 76.18209752452499 - 61.687839566796555 - 68.41643937759878 - 73.71194387894752 - 90.23661625294947 - 74.04531316959051 - 0.6860221293687148'\n",
      "1339 - random_43 - lr - 51.04469059013922 - 54.97507674294833 - 39.756009329146934 - 44.54626014133382 - 184.2980830205662 - 74.9168472363847 - 0.6823265219256356'\n",
      "1340 - random_14 - lr - 85.21984874717798 - 197.78300647189258 - 38.38665648607377 - 15.647182984632702 - 37.76546521325817 - 74.98222369360161 - 0.6820493030180814'\n",
      "1341 - random_6 - lwr_k=800 - 45.86036619790743 - 42.872797979313795 - 33.248211172681025 - 169.78982518048363 - 83.45108597134126 - 75.0344105097079 - 0.6818280127743823'\n",
      "1342 - random_45 - lwr_k=600 - 71.05822108027962 - 99.06957783625316 - 57.24888474235471 - 64.31006722107196 - 83.85133815786385 - 75.11087842051724 - 0.6815037622475116'\n",
      "1343 - random_14 - lwr_k=10 - 71.51433346895095 - 218.33680609164554 - 34.19634710606009 - 19.474091371425867 - 31.922649687937334 - 75.11171658638132 - 0.681500208132849'\n",
      "1344 - random_7 - lwr_k=700 - 75.65023227861685 - 62.90852118650515 - 51.120183600218105 - 73.45886679067175 - 112.79081203403462 - 75.18378889065201 - 0.6811945965058905'\n",
      "1345 - random_99 - lwr_k=20 - 103.6500495797841 - 60.44725813408708 - 27.891948084245982 - 83.23019840214332 - 107.56075760504027 - 76.55784115310071 - 0.6753681372064122'\n",
      "1346 - random_71 - lwr_k=50 - 68.84778465590628 - 72.00557497272916 - 79.67275476738797 - 74.47618996435465 - 92.9251238261236 - 77.58314117821305 - 0.6710205086413785'\n",
      "1347 - random_51 - lwr_k=50 - 48.33720977841984 - 70.64318566869012 - 56.0851995510397 - 60.85790072802784 - 153.2137301648366 - 77.8214398572745 - 0.6700100393946'\n",
      "1348 - random_7 - lwr_k=800 - 78.91734451304323 - 65.58791546177827 - 52.309101297721966 - 76.86390037806557 - 116.09238694084677 - 77.95226251680732 - 0.669455305836024'\n",
      "1349 - random_45 - lwr_k=700 - 75.59587990722407 - 101.90096272198346 - 59.595855054637084 - 68.05445962646561 - 86.56016467179832 - 78.34487260238804 - 0.6677905025772861'\n",
      "1350 - random_45 - lwr_k=200 - 50.37316161556949 - 78.08746083120518 - 42.814142831611804 - 153.64719754367343 - 67.00768864971057 - 78.38129442762052 - 0.667636061375791'\n",
      "1351 - random_31 - lwr_k=20 - 25.577643462608272 - 50.040880432425425 - 36.91839820218915 - 49.1318299128743 - 230.78564331384987 - 78.4775561182961 - 0.6672278783407288'\n",
      "1352 - random_0 - lwr_k=100 - 31.635735654379516 - 44.67778601691344 - 83.40377348970473 - 22.799187505104587 - 212.1617526345641 - 78.92229225868428 - 0.6653420424873233'\n",
      "1353 - random_67 - lwr_k=10 - 145.9789435345078 - 72.24483544992009 - 57.25694909504161 - 73.16070581301891 - 46.59638382007967 - 79.05740940003767 - 0.6647690988327971'\n",
      "1354 - random_8 - lwr_k=600 - 85.24849408636939 - 64.89117584745853 - 62.08244142516534 - 74.28711100972583 - 109.61961982606277 - 79.22440739966636 - 0.6640609692553989'\n",
      "1355 - random_17 - lwr_k=20 - 60.96522530023364 - 53.16714309554451 - 67.11062033357742 - 72.02294898876639 - 145.51078038584328 - 79.74791307898106 - 0.6618411231717156'\n",
      "1356 - random_7 - lwr_k=900 - 82.05212786614963 - 67.9713754801801 - 53.858537769416955 - 78.95036219315594 - 118.95652914242845 - 80.35603570106788 - 0.6592624718325253'\n",
      "1357 - random_62 - lwr_k=50 - 225.7664624845664 - 44.60856812208316 - 34.74094682391875 - 30.7446393573117 - 70.05297981082443 - 81.20040551459387 - 0.6556820502673004'\n",
      "1358 - random_45 - lwr_k=800 - 79.9507984325461 - 104.56723355201127 - 62.316475425695266 - 70.53165414288533 - 89.56344357859805 - 81.3894818892802 - 0.6548803007038307'\n",
      "1359 - random_95 - lwr_k=10 - 61.49903214048613 - 80.73013759541003 - 52.936707665791225 - 41.51632828480941 - 170.93887831621953 - 81.52080771852833 - 0.6543234335307286'\n",
      "1360 - random_83 - lwr_k=30 - 57.68578650022206 - 81.22311956843255 - 83.71356881416517 - 73.65718767725517 - 112.30472156255892 - 81.71286096607247 - 0.6535090609913942'\n",
      "1361 - random_7 - lwr_k=1000 - 84.70154793707829 - 69.52840334735588 - 54.96481917992148 - 81.587041880364 - 121.38356075855549 - 82.43133298024945 - 0.6504624898653759'\n",
      "1362 - random_18 - lwr_k=20 - 55.305922964276384 - 45.97646366899528 - 41.89780246940501 - 74.89748520692 - 195.01250999939535 - 82.60756466807146 - 0.6497152062398291'\n",
      "1363 - random_61 - lwr_k=40 - 30.789495051196816 - 30.536622451065377 - 9.320357628790116 - 326.3803544661625 - 20.869602107276663 - 83.56195664461869 - 0.6456682524527939'\n",
      "1364 - random_45 - lwr_k=900 - 83.38056969486416 - 107.73782225219986 - 64.55777291580492 - 72.83358675724699 - 92.17244583557974 - 84.14018036439356 - 0.6432163828541388'\n",
      "1365 - random_47 - lwr_k=20 - 48.16644130106691 - 71.09752902830022 - 138.93327501931623 - 52.59904196397646 - 110.04600548426217 - 84.16042303510253 - 0.6431305468927117'\n",
      "1366 - random_45 - lwr_k=50 - 48.45278863018804 - 141.33731116896408 - 42.35452113012795 - 130.65713876277806 - 58.45407339824064 - 84.25465241576124 - 0.6427309815586795'\n",
      "1367 - random_8 - lwr_k=700 - 91.63510719541883 - 70.27265891472949 - 65.48820311456382 - 79.35393460792953 - 116.98349174370095 - 84.74543700095374 - 0.6406498842899777'\n",
      "1368 - random_6 - lwr_k=300 - 35.657742442135856 - 32.57288880520501 - 26.693713002361093 - 261.3819239862716 - 69.35669359700572 - 85.11588456350334 - 0.6390790578339739'\n",
      "1369 - random_6 - lwr_k=200 - 32.249545949062565 - 30.10512951573738 - 25.202739907224206 - 274.389042381892 - 64.22906028852536 - 85.21740006636415 - 0.6386485968086921'\n",
      "1370 - random_45 - lwr_k=1000 - 86.00665928442407 - 110.30460984471587 - 67.20053150391836 - 75.36016378284586 - 94.4961229357867 - 86.67737775103055 - 0.632457783845948'\n",
      "1371 - random_6 - lwr_k=700 - 44.46662225311699 - 40.24885926299969 - 32.235131696643535 - 237.79934043407422 - 80.02439750666886 - 86.94026498115993 - 0.6313430505941197'\n",
      "1372 - random_6 - lwr_k=10 - 95.7689538604732 - 117.05188051899951 - 60.35429153902839 - 67.01093207941955 - 96.21317094561627 - 87.28611091770722 - 0.629876543585433'\n",
      "1373 - random_80 - lwr_k=100 - 119.98764284214374 - 9.5569209500362 - 77.50682559233202 - 213.56312520932767 - 17.380260539148527 - 87.59147945000339 - 0.6285816748432541'\n",
      "1374 - random_6 - lwr_k=500 - 41.16858869877541 - 36.08478502076409 - 30.577860512518384 - 257.85201811066196 - 73.75337485623214 - 87.87119292347141 - 0.6273955924698131'\n",
      "1375 - random_6 - lwr_k=400 - 38.96453462193608 - 34.85569838829438 - 28.70638350806315 - 268.6421305760387 - 73.20274205563211 - 88.85727992223951 - 0.623214240769385'\n",
      "1376 - random_6 - lwr_k=600 - 42.77972299854594 - 38.259324478503856 - 31.54811561623393 - 256.95185617451216 - 76.26276020187811 - 89.14442635990649 - 0.6219966400435761'\n",
      "1377 - random_8 - lwr_k=800 - 96.93253017980996 - 74.50656202234227 - 69.15875743744694 - 83.44714610708047 - 123.35314095525919 - 89.47839593994915 - 0.620580492915435'\n",
      "1378 - random_83 - lwr_k=50 - 69.98996494666126 - 96.64984593610967 - 78.42330066383903 - 72.12751180786896 - 133.31642631525506 - 90.09918903841951 - 0.6179481143513268'\n",
      "1379 - random_33 - lwr_k=30 - 152.50290886866912 - 83.00991096124635 - 67.0577651300811 - 70.95468552805866 - 81.44599643215345 - 91.00301780406168 - 0.6141155661574704'\n",
      "1380 - random_8 - lwr_k=30 - 123.84176295018979 - 91.54202133190479 - 49.62131534253848 - 56.29070671702715 - 134.82248021057418 - 91.22905054274773 - 0.6131571087622933'\n",
      "1381 - random_45 - lwr_k=100 - 47.35097973153687 - 79.4765177853697 - 38.26010830662043 - 237.52620315265258 - 58.59644337319425 - 92.23260940168699 - 0.6089016703004257'\n",
      "1382 - random_6 - lr - 69.84838618370881 - 70.26582952274433 - 52.58387654531715 - 77.32980832402374 - 193.45220255203128 - 92.68860653918146 - 0.6069680839042609'\n",
      "1383 - random_8 - lwr_k=900 - 100.91138905787604 - 77.8257751628216 - 72.38980130734183 - 87.1112688035802 - 127.36442021371872 - 93.11930217194674 - 0.605141784684537'\n",
      "1384 - random_61 - lwr_k=50 - 52.91833408371718 - 30.048038090251318 - 8.972530216316 - 357.1472135220248 - 19.73467206770415 - 93.74703594166866 - 0.6024799752613097'\n",
      "1385 - random_34 - lwr_k=100 - 19.025427606122403 - 8.602118316730492 - 15.606898706303914 - 376.0007833612382 - 51.452954960874784 - 94.11133108390752 - 0.6009352372063905'\n",
      "1386 - random_95 - lwr_k=30 - 30.93273558557771 - 271.77395030255127 - 51.219890684741074 - 34.02162270274778 - 87.29573936521078 - 95.0672270781627 - 0.5968819058613826'\n",
      "1387 - random_23 - lwr_k=40 - 428.51762742515706 - 8.222969465441974 - 11.26841370307711 - 9.808030788213843 - 20.708625530964984 - 95.74530537047431 - 0.5940066181594632'\n",
      "1388 - random_2 - lr - 88.22207201300556 - 83.8968702409866 - 65.23320017720567 - 91.28134024876728 - 150.24728485758675 - 95.77297136165136 - 0.5938893047384398'\n",
      "1389 - random_58 - lwr_k=40 - 45.555317597073945 - 97.27380326597455 - 168.8339342105155 - 86.89884257098296 - 81.301077177622 - 95.96455237902923 - 0.5930769346191872'\n",
      "1390 - random_8 - lwr_k=1000 - 104.68407597138076 - 80.35275175950639 - 74.54805412225726 - 89.90581553473817 - 131.73636240042822 - 96.24419139172397 - 0.591891167985312'\n",
      "1391 - random_43 - lwr_k=20 - 92.50613793702017 - 81.24571102567904 - 91.48530282558086 - 89.76207883209621 - 127.59932747460073 - 96.51655334582114 - 0.5907362586098526'\n",
      "1392 - random_54 - lwr_k=30 - 189.3241000131488 - 20.507372355842023 - 8.071956591575189 - 18.23206933871274 - 253.7760962955348 - 97.98458956486594 - 0.5845112746596922'\n",
      "1393 - random_34 - lwr_k=20 - 118.21603473764664 - 28.749935530704562 - 81.23224339512733 - 165.48660357922788 - 98.16243314580402 - 98.36129993362364 - 0.5829138917280221'\n",
      "1394 - random_23 - lwr_k=50 - 467.0687225548381 - 7.303261836763435 - 7.925255772094753 - 7.962581719034944 - 17.99553098658731 - 101.69545731747661 - 0.5687759052583847'\n",
      "1395 - random_48 - lwr_k=30 - 89.95154560543473 - 76.2185972555194 - 111.40288332353522 - 72.93450310249455 - 159.81195515108635 - 102.05768145435479 - 0.5672399489862061'\n",
      "1396 - random_1 - lwr_k=20 - 40.01277933467376 - 110.71017009825216 - 119.45274748271984 - 192.51187333514665 - 70.65193955094475 - 106.6576493232936 - 0.5477344859847357'\n",
      "1397 - random_63 - lwr_k=50 - 23.88224387149343 - 95.74532187458273 - 26.191386727038417 - 360.433869482724 - 27.441749884423956 - 106.72354671405054 - 0.5474550581378625'\n",
      "1398 - random_0 - lwr_k=20 - 82.29746702445394 - 132.6815679637548 - 95.19927318756125 - 130.06609066783645 - 94.46265920351648 - 106.94159111028742 - 0.5465304740919186'\n",
      "1399 - random_11 - lwr_k=100 - 11.468938573216404 - 123.88843362763892 - 350.2276075465755 - 32.991132449311706 - 25.30871304644338 - 108.76350564829391 - 0.5388049230390749'\n",
      "1400 - random_11 - lwr_k=20 - 32.19128324701611 - 57.3386429748808 - 91.42760986978163 - 221.9388215406305 - 141.0365012928522 - 108.76560515373237 - 0.5387960204060962'\n",
      "1401 - random_47 - lwr_k=30 - 86.94633053065665 - 229.72677064726702 - 44.06896447190013 - 117.94846603154393 - 68.18280818055398 - 109.39070264820441 - 0.5361453897064629'\n",
      "1402 - random_74 - deep - 8.774464610360454 - 514.5137824575171 - 6.378507316551865 - 8.652528414269337 - 15.122101706427497 - 110.73771437453959 - 0.5304335926795358'\n",
      "1403 - random_98 - lwr_k=20 - 68.59768421503288 - 199.68531763517984 - 16.663744126122516 - 90.91815185255294 - 181.69579281969484 - 111.51954908854923 - 0.5271183406789363'\n",
      "1404 - random_96 - lwr_k=40 - 162.1556390268115 - 139.86038901796013 - 53.79708290499031 - 15.542926113045972 - 198.7223623684211 - 114.02779461823735 - 0.5164825076096887'\n",
      "1405 - random_45 - lwr_k=40 - 52.20646421655295 - 151.65724545215912 - 46.15315733146322 - 260.1784728770356 - 60.388990521894975 - 114.11287557343654 - 0.516121734780171'\n",
      "1406 - random_39 - lwr_k=30 - 107.49602173529138 - 113.80046924938253 - 108.57709445092627 - 143.45618414273974 - 103.19996095825653 - 115.30442074278189 - 0.5110691690063566'\n",
      "1407 - random_33 - lwr_k=50 - 186.88434378292408 - 125.83209857923364 - 87.4153355453368 - 78.89194016859463 - 101.40299954556501 - 116.09853261149532 - 0.5077018585999489'\n",
      "1408 - random_80 - lwr_k=30 - 139.71453704048886 - 32.121160963467545 - 251.1557756104573 - 68.49183769885256 - 92.93955887235029 - 116.87443265017878 - 0.5044117726844422'\n",
      "1409 - random_3 - deep - 11.160089843987247 - 9.23615572222321 - 8.166752889447286 - 550.8917464147719 - 27.124250406519884 - 121.27940849209266 - 0.4857331447625003'\n",
      "1410 - random_58 - lr - 126.7480077678049 - 99.38739157473299 - 97.76477051037817 - 118.49713289265335 - 177.40400328201594 - 123.9566939674184 - 0.4743805224613765'\n",
      "1411 - random_86 - lwr_k=10 - 72.31712334447649 - 446.15247416004206 - 26.01852076901597 - 52.65438449656693 - 30.27017277270489 - 125.52633804409218 - 0.46772468586960914'\n",
      "1412 - random_4 - lwr_k=20 - 72.65753824353176 - 105.34855652527298 - 163.60432111760937 - 119.55395831693593 - 178.54548611584832 - 127.92921983699198 - 0.4575356316753836'\n",
      "1413 - random_7 - lwr_k=20 - 260.9419724016087 - 95.64348944411128 - 71.50860586575692 - 124.39825859540203 - 87.41604811841101 - 127.9981514057843 - 0.4572433378587699'\n",
      "1414 - random_63 - deep - 24.80448056359923 - 518.5152334160189 - 39.008080066177904 - 19.860228914984127 - 48.115320669638145 - 130.10704091698548 - 0.448300914322176'\n",
      "1415 - random_7 - lr - 141.6289888283302 - 109.53935521141547 - 86.80084185781507 - 130.18338225147602 - 184.3549210950716 - 130.49988745883346 - 0.4466351072335738'\n",
      "1416 - random_86 - lwr_k=30 - 30.470649582915964 - 505.2859414764077 - 55.92183782223605 - 36.96236232320822 - 23.660392044506356 - 130.50524010781447 - 0.44661241014097797'\n",
      "1417 - random_37 - lwr_k=30 - 95.97993310440158 - 108.02408170199006 - 73.82507881075486 - 93.76849423818325 - 284.8212207349503 - 131.27417214729627 - 0.4433518709645994'\n",
      "1418 - random_38 - lwr_k=20 - 88.50756785206711 - 136.27258465502908 - 85.2716423278571 - 254.67118796765857 - 93.37456292603406 - 131.613211640279 - 0.441914225642973'\n",
      "1419 - random_21 - lwr_k=100 - 24.255724449947085 - 160.1366477652399 - 23.06670924145511 - 191.5328199490384 - 266.7261701443741 - 133.13020431223708 - 0.4354816493121286'\n",
      "1420 - random_45 - lr - 144.36611614243057 - 138.60910275908648 - 105.42918979906686 - 124.50984432353553 - 154.51294693164851 - 133.4880606459627 - 0.4339642140438391'\n",
      "1421 - random_21 - lwr_k=200 - 24.419192963993925 - 160.4973866859188 - 23.90783862965854 - 195.97264219908888 - 266.0257280265481 - 134.15089918855563 - 0.4311535481790191'\n",
      "1422 - random_71 - lwr_k=40 - 96.89153193559346 - 132.8131614284703 - 142.71349925398047 - 149.94370913059606 - 149.31881965566453 - 134.32976347273953 - 0.43039510142964243'\n",
      "1423 - random_41 - lwr_k=20 - 66.93125484274762 - 183.64203319136158 - 145.067171145724 - 172.15727050808476 - 106.03136850952917 - 134.76271527544083 - 0.4285592352650037'\n",
      "1424 - random_21 - lwr_k=50 - 24.976059936391493 - 162.40241139911205 - 23.004738787363504 - 192.65315108083942 - 270.8795687381397 - 134.76972800562137 - 0.4285294988509737'\n",
      "1425 - random_21 - lwr_k=300 - 24.876323300946755 - 160.74003922298022 - 24.34746340535102 - 197.57502607582356 - 267.88144131172646 - 135.0702136077241 - 0.4272553354304276'\n",
      "1426 - random_21 - lwr_k=400 - 25.548143978057993 - 161.69153094995576 - 24.52156528655681 - 198.26553480855503 - 269.0571915931829 - 135.802974113876 - 0.4241481761308602'\n",
      "1427 - random_21 - lwr_k=500 - 25.96844411042446 - 162.90665403972037 - 24.591825309057597 - 199.14497139069445 - 270.4318064224434 - 136.59492948283568 - 0.42079001003314276'\n",
      "1428 - random_53 - lwr_k=10 - 153.4388951231539 - 198.52391584003848 - 85.93427161808273 - 135.2373720870773 - 110.22517199990727 - 136.6847989148368 - 0.4204089323972102'\n",
      "1429 - random_21 - lwr_k=40 - 24.961211990644987 - 166.71070483972574 - 23.378430425202282 - 195.57225640691686 - 273.56677816338566 - 136.82444852238814 - 0.4198167695102354'\n",
      "1430 - random_21 - lwr_k=600 - 26.466851880478995 - 163.60313892171263 - 24.707641075715934 - 200.09191724227222 - 271.2020795426151 - 137.20051229541096 - 0.4182221283692784'\n",
      "1431 - random_25 - deep - 12.034310921514484 - 45.659476448783316 - 42.93041640398055 - 565.3695050488722 - 20.51916150632791 - 137.26705495391565 - 0.41793996560046043'\n",
      "1432 - random_21 - lwr_k=700 - 26.903483423561692 - 164.63419359367322 - 24.728519295162393 - 200.99356254344983 - 272.38475831470214 - 137.91509630624822 - 0.4151920437291341'\n",
      "1433 - random_41 - lwr_k=30 - 63.94966803834303 - 219.37832206242297 - 168.9800404842277 - 124.5726056380359 - 115.11834568298609 - 138.40086538355098 - 0.4131322139575898'\n",
      "1434 - random_21 - lwr_k=800 - 27.34620573355662 - 166.02404554101858 - 24.819144611753735 - 201.72587340007834 - 272.75189099855544 - 138.51972702787197 - 0.41262819926191074'\n",
      "1435 - random_21 - lwr_k=900 - 27.814926295491794 - 167.33132987673318 - 24.811902909371256 - 202.55924084470783 - 274.2668227491041 - 139.34316065872696 - 0.40913655438994034'\n",
      "1436 - random_18 - lwr_k=30 - 115.79300768085727 - 95.96223572899906 - 107.76270664651005 - 132.57452458406703 - 246.55114282808594 - 139.71763749344555 - 0.4075486424191125'\n",
      "1437 - random_21 - lwr_k=1000 - 28.28062835401313 - 168.51115633500314 - 24.87149076630614 - 203.53363674628523 - 275.3588160044631 - 140.0974841860654 - 0.4059379603838842'\n",
      "1438 - random_9 - deep - 8.970106681318018 - 8.43681700686191 - 6.677912561352579 - 8.889165362796268 - 669.9140729099682 - 140.53442711668472 - 0.40408517171640757'\n",
      "1439 - random_21 - lwr_k=30 - 39.50017086169132 - 168.00160874769594 - 24.068686258658524 - 201.52084706994276 - 274.1349237628727 - 141.43290270040296 - 0.4002753216079097'\n",
      "1440 - random_62 - lwr_k=40 - 234.7605968170518 - 229.3221490675718 - 100.67416302897087 - 39.36265909417152 - 106.73945242030351 - 142.2012357439917 - 0.3970173223823663'\n",
      "1441 - random_18 - lwr_k=40 - 109.24801513336888 - 87.61861443279116 - 155.9847551646141 - 119.26128520500875 - 241.52220622913543 - 142.71246936743597 - 0.39484951408210944'\n",
      "1442 - random_73 - deep - 11.779905051529505 - 15.137531865833239 - 8.202169985384554 - 28.01074727102931 - 666.9692107004577 - 145.97649932509938 - 0.381008893596373'\n",
      "1443 - random_48 - lwr_k=50 - 170.49002544452833 - 77.29967628357936 - 90.81196982946443 - 273.9176029373766 - 118.03045650568784 - 146.10267091100462 - 0.3804738808908147'\n",
      "1444 - random_54 - deep - 23.63588054363544 - 13.16677041841997 - 8.02760528639435 - 14.597599453070826 - 672.860661899526 - 146.4157667152887 - 0.37914624714905165'\n",
      "1445 - random_14 - deep - 108.90070903047602 - 520.6075016902043 - 47.18695702580133 - 17.34769628323267 - 45.540950147955265 - 147.9714012795998 - 0.3725498157743127'\n",
      "1446 - random_84 - lwr_k=30 - 260.2159054000366 - 51.93035150368773 - 169.67570430198472 - 66.32929904527975 - 194.3999996337765 - 148.51272876636682 - 0.3702543976968995'\n",
      "1447 - random_68 - lwr_k=1000 - 14.709374209445912 - 8.014911026244583 - 621.296429705557 - 13.532572708926779 - 91.26476075751101 - 149.71828416424097 - 0.3651424236832892'\n",
      "1448 - random_44 - lwr_k=20 - 23.372721498081653 - 32.43238144214398 - 36.014042383350926 - 38.36620837749905 - 637.3685840787275 - 153.46965173277871 - 0.34923532098217624'\n",
      "1449 - random_31 - lwr_k=30 - 49.00305590056365 - 189.4771861934697 - 286.94107496481547 - 58.88920485958046 - 197.95594305309046 - 156.44110593946615 - 0.33663532208208047'\n",
      "1450 - random_21 - lr - 36.952148454343906 - 189.34407740856423 - 30.066971110702085 - 227.30541268672786 - 302.0188828044644 - 157.12309227532123 - 0.33374346291685575'\n",
      "1451 - random_58 - lwr_k=10 - 115.66011111745182 - 128.6094041270916 - 134.51290957809218 - 197.69232036275648 - 223.7851857247336 - 160.0395685535464 - 0.32137658954721393'\n",
      "1452 - random_43 - lwr_k=40 - 372.8149421219748 - 130.1476589144491 - 93.34180975173446 - 98.01135288324711 - 122.1884573575144 - 163.3297226915585 - 0.3074251921382285'\n",
      "1453 - random_91 - lwr_k=10 - 176.02618826137387 - 201.1246812868908 - 151.60543873130953 - 185.181247478715 - 104.55918376746683 - 163.70749465048246 - 0.30582330769517807'\n",
      "1454 - random_6 - lwr_k=100 - 30.525545814374546 - 29.937503965369153 - 167.67711081640303 - 519.1433603164971 - 75.10418211761488 - 164.43357597015319 - 0.3027444704683947'\n",
      "1455 - random_85 - lwr_k=20 - 137.78437513412064 - 137.86767383482308 - 199.9304428339402 - 150.3278343350547 - 199.45693938662794 - 165.06452976166116 - 0.3000690070333131'\n",
      "1456 - random_21 - lwr_k=20 - 164.17647196243277 - 174.25453747313324 - 24.578351836943867 - 196.34522252822265 - 275.6974766381408 - 167.01113424026525 - 0.2918147272822964'\n",
      "1457 - random_82 - lwr_k=10 - 82.22121422984243 - 594.7739615564213 - 44.88910813081106 - 61.25298922729403 - 59.875599257363355 - 168.65821391490115 - 0.28483053683370174'\n",
      "1458 - random_8 - lr - 186.25546415996553 - 148.60550311569915 - 127.42830378786168 - 168.2766483455903 - 224.00670924924168 - 170.9133847321322 - 0.27526782852990594'\n",
      "1459 - random_59 - deep - 86.37688812842735 - 41.78747891053045 - 33.256041636923904 - 32.46872290346488 - 668.2979835429023 - 172.40193733197336 - 0.26895584954681684'\n",
      "1460 - random_34 - lwr_k=30 - 224.45416310067276 - 38.22959189432966 - 164.86928131512573 - 312.3324320863334 - 135.02076395157047 - 174.9669548799643 - 0.2580792818280254'\n",
      "1461 - random_45 - lwr_k=30 - 65.1947664978184 - 245.5332883706711 - 53.72410062918368 - 436.91910003180175 - 74.5942371153096 - 175.18660464616835 - 0.2571478904552932'\n",
      "1462 - random_1 - lwr_k=10 - 49.328143850824866 - 93.73495445506961 - 144.98472400396957 - 267.43807713255853 - 333.16835535730473 - 177.69607142766876 - 0.2465068788537781'\n",
      "1463 - random_11 - lwr_k=30 - 58.23225663559436 - 107.2265909969057 - 182.0235411079965 - 391.26814529363617 - 170.12473729241347 - 181.7426175106182 - 0.22934811662900523'\n",
      "1464 - random_88 - lwr_k=300 - 224.482444120695 - 172.22765881803073 - 29.22377589411848 - 209.87793102075196 - 282.256226195584 - 183.618434930242 - 0.22139399861808406'\n",
      "1465 - random_88 - lwr_k=200 - 227.73568542798688 - 171.90864019793085 - 28.941338877109178 - 209.63349811427017 - 282.3789742400138 - 184.12476984254172 - 0.21924696255617904'\n",
      "1466 - random_88 - lwr_k=400 - 224.45035856468348 - 173.4780224467693 - 29.50337430618318 - 211.54081816099708 - 282.7776984582322 - 184.3548404152487 - 0.2182713833409491'\n",
      "1467 - random_88 - lwr_k=100 - 229.4339665346074 - 171.6145213077594 - 28.363123132942295 - 210.1717292895399 - 282.9039329327763 - 184.50270330148072 - 0.21764439329691754'\n",
      "1468 - random_88 - lwr_k=500 - 225.03833289575908 - 172.8571354593336 - 29.555652919328903 - 211.27497624269267 - 283.8614328956211 - 184.52223188182492 - 0.2175615853268994'\n",
      "1469 - random_88 - lwr_k=600 - 225.29992907733543 - 173.10464835631902 - 29.611581895697583 - 211.39827263386096 - 283.9009380702466 - 184.6678354983278 - 0.2169441753719834'\n",
      "1470 - random_88 - lwr_k=700 - 225.77390387046492 - 173.39610226390224 - 29.739900471281427 - 211.41378157809692 - 283.16812046704894 - 184.70323700159724 - 0.21679406069033924'\n",
      "1471 - random_88 - lwr_k=50 - 229.9246959317214 - 171.8596166374355 - 28.49107180935406 - 210.01711492979905 - 283.7961519020501 - 184.8229945047354 - 0.21628624724181877'\n",
      "1472 - random_88 - lwr_k=800 - 225.95521820787062 - 173.69813290887117 - 29.890688596003102 - 210.80472619006957 - 283.75853189716213 - 184.82637366381007 - 0.21627191843253457'\n",
      "1473 - random_88 - lwr_k=40 - 229.2257477612476 - 172.8167632146681 - 28.858030094124413 - 209.93953167934225 - 283.3676497201275 - 184.84684323665422 - 0.21618512032716874'\n",
      "1474 - random_88 - lwr_k=900 - 225.8556607165249 - 173.80856257751122 - 29.82559914127245 - 210.54345537431078 - 284.2129803610498 - 184.85415841651894 - 0.2161541014213273'\n",
      "1475 - random_88 - lwr_k=1000 - 225.51523900380212 - 173.86182799442923 - 29.83070091236074 - 210.7642651664913 - 284.80711552948173 - 184.96065457927205 - 0.21570252066651252'\n",
      "1476 - random_88 - lwr_k=20 - 227.33771604787864 - 177.05141355981178 - 30.091902215261797 - 211.630410479857 - 284.63707617049613 - 186.15495827501448 - 0.21063825778173773'\n",
      "1477 - random_88 - lwr_k=30 - 231.94604739282255 - 172.42073951663534 - 29.400253006285606 - 210.3814225780963 - 288.09995270189376 - 186.45483571981336 - 0.20936667315959345'\n",
      "1478 - random_88 - lr - 229.12097785651443 - 177.3878012840744 - 37.092336360021314 - 213.5071106445249 - 287.7229469338344 - 188.970913887107 - 0.1986976269837828'\n",
      "1479 - random_83 - lwr_k=40 - 145.57352676494213 - 201.39176859068476 - 159.2582853905435 - 171.24825254422726 - 267.44619755006113 - 188.9785297120609 - 0.1986653332382402'\n",
      "1480 - random_44 - lwr_k=30 - 51.920336232045294 - 270.70027331434056 - 132.86869649710005 - 107.83077281785947 - 389.3810312278638 - 190.5306494233618 - 0.19208380604822162'\n",
      "1481 - random_99 - lwr_k=30 - 110.14057521474933 - 288.0159287680844 - 72.42071882059669 - 210.54700886243958 - 283.27461199376825 - 192.8817986920585 - 0.18211411574208525'\n",
      "1482 - random_33 - lwr_k=40 - 263.14394462399184 - 176.47704970741555 - 160.5599768175822 - 183.69827057774077 - 181.89343310095018 - 193.16326460765174 - 0.1809206024047746'\n",
      "1483 - random_88 - lwr_k=10 - 233.3241192445155 - 180.02775281039501 - 31.268135908954438 - 215.95374707089368 - 305.25201824065 - 193.1695793419566 - 0.1808938257359246'\n",
      "1484 - random_2 - lwr_k=20 - 165.24427809722252 - 224.66696183979613 - 202.44470885960428 - 204.02563883671567 - 206.04911136503372 - 200.4843285929179 - 0.14987674584650046'\n",
      "1485 - random_4 - lwr_k=30 - 180.0614830476513 - 129.63076215928007 - 277.10009765719184 - 286.6255390436492 - 167.57465705831936 - 208.18103525836008 - 0.11724003372733793'\n",
      "1486 - random_21 - lwr_k=10 - 307.9613006985452 - 192.5222335289283 - 26.375098080231446 - 219.17334616858994 - 299.06593827218217 - 209.0330833280036 - 0.11362705368673154'\n",
      "1487 - random_61 - lwr_k=30 - 13.568243130011377 - 82.30681647514106 - 11.570856937116803 - 917.3400918752452 - 23.447920197928056 - 209.59382706714592 - 0.11124930528309562'\n",
      "1488 - random_41 - lwr_k=10 - 61.13325873797958 - 239.02232972175824 - 206.60321112683616 - 530.387630385133 - 45.537326093540344 - 216.5149863671405 - 0.08190118362230858'\n",
      "1489 - random_73 - lwr_k=20 - 37.06914131545421 - 143.34739364580318 - 234.98609110428617 - 156.84013128694704 - 512.3207561246163 - 216.87120787465346 - 0.08039068058552856'\n",
      "1490 - random_68 - lwr_k=600 - 17.943837837411525 - 7.633662407397871 - 1026.9848222222943 - 13.897649829562022 - 67.05801596453222 - 226.63354202835558 - 0.03899498977454996'\n",
      "1491 - random_8 - lwr_k=20 - 259.09229811640466 - 85.12453927733924 - 185.59312393996586 - 250.89891367862487 - 357.4683563625048 - 227.61726155986122 - 0.03482367695839461'\n",
      "1492 - random_51 - lwr_k=40 - 182.4125941406585 - 185.14352736370415 - 242.0832104238724 - 214.87846490693877 - 348.3594767430555 - 234.5588189226334 - 0.005389060419785263'\n",
      "1493 - random_93 - lwr_k=40 - 243.923571134818 - 226.7790828556106 - 370.88298046146826 - 167.08372993804107 - 167.90368551050278 - 235.31462200465504 - 0.0021841925874864776'\n",
      "1494 - random_95 - deep - 15.493063097889802 - 509.0416056981063 - 17.9772229206357 - 18.91385168017763 - 667.5593415681307 - 245.80241153980515 - -0.042287679038367276'\n",
      "1495 - random_17 - lwr_k=30 - 192.18180329250183 - 284.79009434707257 - 99.46036178997572 - 384.8796901560005 - 317.49502006692865 - 255.75573634244583 - -0.08449323881739557'\n",
      "1496 - random_46 - lwr_k=10 - 53.494900989586945 - 161.16473072213432 - 79.13992707895122 - 73.63281557957987 - 912.5275306778708 - 255.94329518511057 - -0.08528855351750453'\n",
      "1497 - random_58 - lwr_k=30 - 338.4247138514203 - 91.04781903042237 - 442.52281862175136 - 321.5306983547621 - 122.31643901346332 - 263.1526365556531 - -0.11585866734756656'\n",
      "1498 - random_25 - lwr_k=20 - 13.251075352209746 - 433.44571580981653 - 649.9055182758736 - 168.2751395144181 - 188.37039508359445 - 290.62752814131505 - -0.23236175966529027'\n",
      "1499 - random_43 - lwr_k=30 - 268.1593870251801 - 253.68282469008977 - 529.7495134138455 - 173.9996793956513 - 239.3060800278183 - 292.96899801054883 - -0.24229040595253748'\n",
      "1500 - random_93 - lwr_k=30 - 54.29752343949412 - 591.625659826526 - 465.3218914439837 - 213.5359215682289 - 147.79473115754072 - 294.52446150030215 - -0.24888611192570154'\n",
      "1501 - random_5 - lwr_k=50 - 82.77335536874455 - 124.68203256419527 - 57.65108374443239 - 534.483654209855 - 693.966774888461 - 298.64752435484655 - -0.26636933186406275'\n",
      "1502 - random_62 - lwr_k=20 - 357.3893020283618 - 383.64925104925914 - 254.78328083470535 - 184.4995287336361 - 321.367541127938 - 300.3607647062404 - -0.2736340665837096'\n",
      "1503 - random_65 - lwr_k=30 - 588.7117183165607 - 593.754096357067 - 148.9524771504365 - 110.66357572224194 - 97.62129289059281 - 308.03340833602874 - -0.30616874306588016'\n",
      "1504 - random_5 - lwr_k=20 - 324.64306804939355 - 281.4452287414947 - 411.1325466396418 - 429.20774329615733 - 95.93957800491498 - 308.47185482791514 - -0.30802790862296203'\n",
      "1505 - random_82 - lwr_k=50 - 72.08119909071354 - 57.84089699367729 - 73.55300045435457 - 474.1874268051791 - 898.1035058250256 - 315.0712696432573 - -0.3360117218102723'\n",
      "1506 - random_9 - lwr_k=30 - 603.6876179158241 - 27.22343333368972 - 25.370245017408756 - 867.564673212342 - 147.63722302497098 - 334.2904681674998 - -0.4175078053511796'\n",
      "1507 - random_0 - lwr_k=30 - 821.9475591188799 - 202.58595495506194 - 160.4341907556079 - 214.04634115709447 - 294.87566109113897 - 338.8347577953246 - -0.43677717325307985'\n",
      "1508 - random_6 - lwr_k=20 - 299.85910669090407 - 425.9712640152013 - 279.71021742794807 - 332.0331407329126 - 410.6547885808129 - 349.6500491525207 - -0.4826377686807992'\n",
      "1509 - random_75 - lwr_k=20 - 192.35089308668577 - 536.1760566131079 - 467.0410789590154 - 278.31698882573454 - 310.96073902386473 - 356.9715401418809 - -0.5136834358848859'\n",
      "1510 - random_77 - lwr_k=10 - 768.1111364784579 - 215.86506352751957 - 226.7338799301258 - 432.771373341938 - 147.96949366510728 - 358.33397452392177 - -0.5194606313323311'\n",
      "1511 - random_84 - lwr_k=40 - 167.5806076092581 - 199.47987082232038 - 952.124059853927 - 110.67063841678709 - 377.814881752829 - 361.4757166943073 - -0.5327827104012777'\n",
      "1512 - random_62 - lwr_k=30 - 155.11524439234265 - 466.99323397223935 - 842.2116434326566 - 124.34113159259846 - 252.71197092756876 - 368.2559055788304 - -0.5615330684903124'\n",
      "1513 - random_21 - deep - 38.39181450658071 - 525.223448308517 - 35.49507341064653 - 572.2249045532127 - 684.6707978119721 - 371.1719298180405 - -0.5738980217107754'\n",
      "1514 - random_6 - lwr_k=40 - 94.55694059379813 - 157.5635604286466 - 297.7152492158674 - 1092.3110530010304 - 242.8757356419945 - 376.9223252783912 - -0.598281701007173'\n",
      "1515 - random_6 - lwr_k=50 - 47.064616422445965 - 131.22178836293165 - 146.7918761506026 - 1329.7316961758308 - 235.21100923615285 - 377.9095973040198 - -0.6024680776334792'\n",
      "1516 - random_58 - lwr_k=20 - 410.0897460955338 - 422.5841341951194 - 593.9476172642389 - 243.62983737040025 - 289.908131703423 - 392.0398530594321 - -0.6623852745990513'\n",
      "1517 - random_87 - lwr_k=20 - 315.97375492137036 - 1449.9994318516053 - 95.31427256236435 - 172.08554336427673 - 56.47238396570145 - 418.12136733340867 - -0.7729799626899345'\n",
      "1518 - random_1 - lwr_k=30 - 121.47258031147936 - 1125.6641626412775 - 215.2454707280236 - 440.0909674785392 - 288.37069179661455 - 438.22949166892886 - -0.8582454007169915'\n",
      "1519 - random_54 - lwr_k=10 - 54.04214308429069 - 637.8073665884061 - 91.64535208195143 - 70.53215784841903 - 1369.882069579907 - 444.74944283643106 - -0.8858922604109154'\n",
      "1520 - random_26 - lwr_k=300 - 2138.9248676112475 - 8.344503648123275 - 6.383492537890415 - 11.480055776468545 - 70.6350718030002 - 447.35876647163036 - -0.8969566997886156'\n",
      "1521 - random_88 - deep - 610.0345145817084 - 515.1847636071437 - 41.07610371657613 - 562.1513830086523 - 674.3816928972874 - 480.59256096991146 - -1.0378795382783133'\n",
      "1522 - random_53 - lwr_k=50 - 12.419359259717442 - 17.272138976326172 - 17.38278605712569 - 2335.312464539395 - 22.68451138461048 - 480.86158510289937 - -1.039020298465001'\n",
      "1523 - random_96 - lwr_k=30 - 1049.18611094411 - 267.3414759984528 - 305.7117543319555 - 548.5085856745935 - 254.80658399162533 - 485.1676085550323 - -1.0572793349457101'\n",
      "1524 - random_11 - lwr_k=40 - 73.00325510712528 - 799.452983579567 - 391.2259981162654 - 975.0623899210952 - 237.63429551458353 - 495.25644674833177 - -1.100059516397852'\n",
      "1525 - random_68 - lwr_k=10 - 188.37879196315308 - 641.4993940092529 - 619.6766856685564 - 637.4259383104206 - 398.3175557895977 - 497.0327792296888 - -1.1075917836833216'\n",
      "1526 - random_40 - lwr_k=1000 - 1315.5483124452785 - 7.829525000035901 - 5.708266428634114 - 156.02092468069722 - 1111.9649848759486 - 519.4609966000003 - -1.2026952227876624'\n",
      "1527 - random_82 - lwr_k=20 - 227.43267015035048 - 1346.0659230000351 - 391.0891211593394 - 356.3826904241243 - 333.7585486498161 - 531.0295645447202 - -1.2517499728326422'\n",
      "1528 - random_52 - lwr_k=100 - 15.42272962873452 - 611.395010677961 - 121.69566322187305 - 10.51631608114824 - 1948.471251903376 - 541.425495981299 - -1.2958323364011495'\n",
      "1529 - random_29 - lwr_k=20 - 366.4519919599028 - 90.94894686744743 - 1230.1722534864882 - 90.07806375014357 - 934.130823785798 - 542.2536958302618 - -1.2993441916949324'\n",
      "1530 - random_52 - lwr_k=10 - 92.00671202175884 - 49.825162650973155 - 2339.128610144096 - 38.598087441062106 - 200.36028919433636 - 543.8288458666843 - -1.3060233754702515'\n",
      "1531 - random_33 - deep - 599.2790810542879 - 509.8001558144627 - 446.52143603675586 - 556.8461625382707 - 668.6645477944763 - 556.2217234299386 - -1.358573479864448'\n",
      "1532 - random_2 - deep - 603.885742936704 - 509.31309107204504 - 445.91457600011677 - 556.233758437448 - 668.328187997179 - 556.7350240618762 - -1.3607500529947099'\n",
      "1533 - random_39 - deep - 604.3833718557405 - 509.8178211468136 - 446.40633872842125 - 556.7504983538004 - 668.8236327175231 - 557.2362895805823 - -1.362875593060819'\n",
      "1534 - random_8 - deep - 604.4301979077428 - 509.8460132954749 - 446.4224228417649 - 556.8033151462564 - 668.915894679414 - 557.2835165602726 - -1.3630758518733388'\n",
      "1535 - random_71 - deep - 605.2509016920424 - 510.6479761900956 - 447.29350746898353 - 557.4834060231551 - 669.564556887265 - 558.0480402647704 - -1.3663176981698788'\n",
      "1536 - random_43 - deep - 606.8097021224651 - 512.16791380837 - 448.7172339437049 - 559.1137257044087 - 671.2834083750831 - 559.6183522771216 - -1.372976367026976'\n",
      "1537 - random_6 - deep - 607.3073171562532 - 512.5579690137198 - 449.0653329610239 - 559.5201507243438 - 671.7107109425988 - 560.0322601582429 - -1.374731480339523'\n",
      "1538 - random_7 - deep - 607.790277659015 - 513.0072701761649 - 449.5335260214716 - 559.9678828233973 - 672.1811779375256 - 560.4959919579811 - -1.3766978643884733'\n",
      "1539 - random_85 - deep - 608.5262012138304 - 513.7074003578598 - 450.2103412450092 - 560.6917255473469 - 672.935266768122 - 561.2141561590064 - -1.379743130273238'\n",
      "1540 - random_83 - deep - 609.4607305698426 - 514.5929121900892 - 451.0706498679521 - 561.5904272330969 - 673.8619531729884 - 562.115301770133 - -1.3835643009509906'\n",
      "1541 - random_32 - deep - 611.1023203448656 - 516.1392868216807 - 452.5736128223616 - 563.1901695757592 - 675.540324280635 - 563.7091190346403 - -1.3903226402489168'\n",
      "1542 - random_62 - deep - 611.8407029871464 - 516.8064551220783 - 453.2051820497255 - 563.7953229774519 - 676.1829888221184 - 564.3661128033094 - -1.393108522234367'\n",
      "1543 - random_59 - lwr_k=20 - 490.39658266096967 - 758.8275982523663 - 380.4219099726755 - 704.4629426482046 - 497.6746494150709 - 566.3758148128364 - -1.4016303625416486'\n",
      "1544 - random_58 - deep - 616.0417689746413 - 520.7895382945159 - 457.1386498803491 - 567.8816313247618 - 680.3670535200839 - 568.4437106396988 - -1.4103989546524955'\n",
      "1545 - random_45 - deep - 617.8651608531096 - 516.3615196215541 - 458.78701054903445 - 569.6291013275571 - 682.2373902666872 - 568.9754325448326 - -1.412653640385333'\n",
      "1546 - random_45 - lwr_k=20 - 91.18952004871251 - 1399.5835575090236 - 105.0002523930792 - 1185.4373275669207 - 180.7813651039762 - 592.4485070736316 - -1.512187642229558'\n",
      "1547 - random_8 - lwr_k=10 - 645.7633913776463 - 458.27024545986967 - 709.3044645183126 - 507.82436621016774 - 738.7712526478588 - 611.9671043094734 - -1.5949532803974056'\n",
      "1548 - random_90 - lwr_k=30 - 26.098381415935116 - 2917.47282214382 - 9.38501470741166 - 13.735332342206561 - 198.7898113129411 - 633.3709373032101 - -1.6857129736048488'\n",
      "1549 - random_25 - lwr_k=40 - 11.849924515581638 - 41.92233764057122 - 38.41561723797133 - 3132.5743170279056 - 24.998178953831307 - 649.7480253145706 - -1.7551575204753709'\n",
      "1550 - random_99 - lwr_k=50 - 54.64607247426513 - 59.23485476912841 - 184.48892373241404 - 205.19794875081064 - 2864.482689881659 - 673.4081429079665 - -1.8554846448113262'\n",
      "1551 - random_82 - lwr_k=40 - 583.3171688902655 - 147.37236410121565 - 489.3239921582181 - 1103.3790322691111 - 1132.626267219496 - 691.0970483734015 - -1.9304917537570998'\n",
      "1552 - random_75 - lwr_k=30 - 127.45667704056822 - 361.77265804898235 - 2763.3577344856653 - 171.19587907757216 - 64.7841072318273 - 697.5650244916006 - -1.9579182211721933'\n",
      "1553 - random_0 - lwr_k=50 - 214.90938112656693 - 693.7789370823974 - 215.616863833554 - 253.71791311394963 - 2209.929097277775 - 717.5042271623415 - -2.042467372612827'\n",
      "1554 - random_48 - lwr_k=40 - 178.64194382317206 - 179.66944567250727 - 2657.1688434153534 - 212.7168270597735 - 390.18723841954755 - 723.4985327860024 - -2.067885312453524'\n",
      "1555 - random_32 - lwr_k=30 - 1250.976725903673 - 534.3190992294448 - 576.0802046549945 - 984.1996913433808 - 569.4848504214496 - 783.048019271537 - -2.320395838286418'\n",
      "1556 - random_0 - lwr_k=40 - 430.31857938948104 - 2275.390075942815 - 447.47091829964194 - 764.740302821464 - 208.10598275703222 - 825.377973268589 - -2.499889559779286'\n",
      "1557 - random_74 - lwr_k=20 - 3591.3999732087523 - 174.11109207175394 - 142.89279252577694 - 91.52996285425138 - 131.92368176621684 - 826.7174589042645 - -2.5055694445637195'\n",
      "1558 - random_61 - lwr_k=20 - 2736.360200592558 - 186.5875437465599 - 16.55612596660045 - 664.4667893408672 - 621.9592869746062 - 845.3878193169829 - -2.584738263701235'\n",
      "1559 - random_45 - lwr_k=10 - 789.9004290480228 - 1108.959912773254 - 550.2084537603514 - 1065.3717050373186 - 760.9934471303055 - 855.1176863506892 - -2.6259962826358247'\n",
      "1560 - random_89 - lwr_k=20 - 4244.611609261933 - 14.128669869037612 - 16.45191776898786 - 17.038375758282097 - 72.52765107148886 - 873.3631130592979 - -2.7033632351343235'\n",
      "1561 - random_44 - lwr_k=40 - 183.0562399384626 - 147.47962603411614 - 2714.8616717591817 - 1292.4212407294604 - 129.12165741662596 - 893.1496328922258 - -2.787264958260505'\n",
      "1562 - random_5 - lwr_k=40 - 117.71152015996576 - 103.70357275009285 - 64.83043473391942 - 181.3396092560666 - 4253.9189193742795 - 944.0278152618079 - -3.003006139952909'\n",
      "1563 - random_92 - lwr_k=30 - 377.5267454614365 - 153.86365537023678 - 1481.820017725549 - 294.92356531960115 - 2471.9215753964413 - 955.7850381977219 - -3.052860852748852'\n",
      "1564 - random_99 - lwr_k=40 - 3665.9903244079355 - 170.2410298597827 - 484.5437943324052 - 207.3273109135639 - 268.09048682977556 - 959.552614841925 - -3.068836687566076'\n",
      "1565 - random_46 - lwr_k=50 - 14.703435981107456 - 50.18754560265516 - 11.415982092068203 - 36.798107419491885 - 4747.601327196073 - 971.8335358269817 - -3.120912062160653'\n",
      "1566 - random_40 - lwr_k=900 - 2136.761860233232 - 7.9658360452873005 - 5.751267087766326 - 863.1595037066531 - 1984.189490557904 - 999.5894324485896 - -3.238606713525752'\n",
      "1567 - random_5 - lwr_k=30 - 735.085002224197 - 928.9922251010829 - 1199.8260708970988 - 1823.1311144822894 - 406.50615881487687 - 1018.6469813415243 - -3.3194173464303702'\n",
      "1568 - random_43 - lwr_k=50 - 2452.2987345299866 - 712.656004478389 - 890.4139679180083 - 507.6333445085041 - 874.9132699426525 - 1087.7451387163887 - -3.612417556550562'\n",
      "1569 - random_65 - lwr_k=50 - 165.29592326686668 - 4608.656253242782 - 193.32370596010304 - 428.44324920760334 - 80.17028884770356 - 1095.6009390269537 - -3.645728881035968'\n",
      "1570 - random_92 - lwr_k=20 - 129.09563490680742 - 115.54975070652094 - 169.13276704607924 - 109.0407156109445 - 5203.496810233148 - 1144.9281298245276 - -3.8548933192411656'\n",
      "1571 - random_93 - lwr_k=50 - 242.18892233776566 - 465.469185011654 - 4068.0721120408475 - 570.3583134456647 - 661.9330646616838 - 1201.3266789998263 - -4.094042775415342'\n",
      "1572 - random_29 - lwr_k=200 - 12.755769572536908 - 8.91961164879741 - 6108.901642997257 - 11.657997903982835 - 26.962021619392754 - 1233.43888419685 - -4.230209689666131'\n",
      "1573 - random_77 - lwr_k=20 - 103.86529415988956 - 4152.989149376138 - 935.8034186215748 - 679.8977468554008 - 644.4467926585795 - 1303.6706708505874 - -4.528016882049339'\n",
      "1574 - random_81 - lwr_k=20 - 14.248586907448633 - 6658.177428691498 - 13.550882115952023 - 12.418443424296688 - 23.407269337716347 - 1345.0128399226965 - -4.703322052044492'\n",
      "1575 - random_25 - lwr_k=30 - 12.294879206710585 - 50.36623692198999 - 60.000445444994455 - 6566.048869532561 - 51.42996275358413 - 1347.5968694971332 - -4.714279235810758'\n",
      "1576 - random_56 - lwr_k=10 - 164.56058569613495 - 2931.8809027674374 - 994.9143206535615 - 1939.932178913867 - 753.7602390831512 - 1357.072265727009 - -4.754458210067105'\n",
      "1577 - random_91 - lwr_k=20 - 79.76545599922787 - 6541.282667115901 - 138.58255854388986 - 201.23522426966062 - 166.85507352368313 - 1426.1615136784758 - -5.047420641134616'\n",
      "1578 - random_7 - lwr_k=10 - 939.6524128568551 - 1740.9675176471533 - 833.8186147960848 - 2722.5653668982377 - 1186.3780988367243 - 1484.629123203587 - -5.295343632527069'\n",
      "1579 - random_53 - lwr_k=30 - 29.338859881869386 - 42.814481160217 - 38.716123605574076 - 7438.782736997876 - 41.82595848290312 - 1517.8102156308935 - -5.436043000246459'\n",
      "1580 - random_53 - lwr_k=20 - 281.23549794422354 - 914.3324083278507 - 671.6985779432385 - 5034.996266900045 - 723.152109718295 - 1524.7792881912824 - -5.46559429078897'\n",
      "1581 - random_44 - lwr_k=50 - 48.41904980820877 - 94.84391165113163 - 2490.574697014863 - 4796.544775519907 - 229.23390315460145 - 1531.4450320321002 - -5.493859362104423'\n",
      "1582 - random_50 - lwr_k=100 - 11.325601366741793 - 11.841071087794674 - 7528.2840939776115 - 13.036772418326331 - 153.61657032849874 - 1543.1190902210549 - -5.54336142745998'\n",
      "1583 - random_80 - lwr_k=40 - 6986.33750237426 - 67.71976817704868 - 791.2958906085767 - 513.8859910820598 - 77.87121749548218 - 1688.0245322940807 - -6.157810880064345'\n",
      "1584 - random_82 - lwr_k=30 - 2434.438011709116 - 438.5445386244557 - 2482.045975257672 - 2329.482179279186 - 1047.733961883956 - 1746.3474243748876 - -6.405120219180093'\n",
      "1585 - random_53 - lwr_k=40 - 15.758081173598134 - 24.022008229835894 - 18.48795038974251 - 9194.194324454525 - 25.214680691720943 - 1854.9342479057539 - -6.865566102537346'\n",
      "1586 - random_79 - lwr_k=20 - 1745.6261869153434 - 179.01067208593423 - 47.45806600298173 - 6203.05815431508 - 1316.028985558193 - 1897.9299063491844 - -7.047882642323453'\n",
      "1587 - random_55 - lwr_k=100 - 8.93434598641994 - 9583.557913077308 - 8.013264928050507 - 11.25198981837799 - 21.349115612283658 - 1927.5611080373121 - -7.173529239144186'\n",
      "1588 - random_98 - lwr_k=10 - 25.42320675845656 - 60.47782697584074 - 20.418952168400725 - 34.65724391782994 - 9544.376125663764 - 1936.450359927961 - -7.211222757621696'\n",
      "1589 - random_90 - lwr_k=10 - 23.218823024238464 - 6561.052945090169 - 793.1216547133668 - 2259.1569839529207 - 112.79835891538258 - 1950.3093359559562 - -7.269989634229184'\n",
      "1590 - random_46 - lwr_k=20 - 223.61410594495146 - 365.03154081794844 - 311.0449392909734 - 735.4149173644294 - 8393.473120484568 - 2005.1552555153382 - -7.502555401040494'\n",
      "1591 - random_84 - lwr_k=50 - 426.0671866700107 - 7517.082785801099 - 406.43059302994385 - 1090.5403218433005 - 803.3507449311152 - 2049.3240564894095 - -7.689846473014397'\n",
      "1592 - random_55 - lwr_k=20 - 446.2275618855983 - 9444.493733142797 - 1955.5598863604582 - 437.1993222274038 - 133.09935480806865 - 2484.122274182271 - -9.533542079146947'\n",
      "1593 - random_74 - lwr_k=30 - 1043.0662982239608 - 612.1584667059241 - 9602.036123038582 - 533.54503201973 - 1566.05137244513 - 2670.7676402204065 - -10.324983320777323'\n",
      "1594 - random_6 - lwr_k=30 - 1150.2771028447607 - 2907.6419034174282 - 6054.619688960938 - 1898.006092897144 - 1897.0528626943767 - 2781.273071991313 - -10.793564770100833'\n",
      "1595 - random_26 - lwr_k=200 - 14887.599098354009 - 8.190631007491021 - 6.454950321419749 - 11.463827828032654 - 17.325868641830695 - 2987.6680469972002 - -11.668751219956512'\n",
      "1596 - random_63 - lwr_k=40 - 38.42890874443816 - 101.12947234577116 - 27.878760786640104 - 14980.143466846766 - 26.804986241303897 - 3033.906070056207 - -11.864816512961841'\n",
      "1597 - random_96 - lwr_k=20 - 519.5456196250105 - 12987.43603445682 - 213.34226139357864 - 183.91226062571155 - 1951.6700440836685 - 3172.3544254952844 - -12.451885673350205'\n",
      "1598 - random_40 - lwr_k=600 - 25.563232041356088 - 7.841836333401186 - 5.900534220993865 - 2526.673124851002 - 17283.84256344513 - 3968.669592475416 - -15.828538830413226'\n",
      "1599 - random_26 - lwr_k=10 - 90.223327048878 - 9.875871918640296 - 15.42753936418509 - 83.93509520014602 - 19788.628156912953 - 3996.3251969104417 - -15.945808208039367'\n",
      "1600 - random_73 - lwr_k=30 - 1051.547501381846 - 17248.305594921316 - 236.53758768174285 - 1077.1594164673645 - 447.4747221790828 - 4013.8875325903364 - -16.02027861709086'\n",
      "1601 - random_40 - lwr_k=700 - 20161.894539417022 - 7.915513028051031 - 5.8748742154507445 - 279.2289157755581 - 5199.966066900246 - 5132.598359153774 - -20.76400145572702'\n",
      "1602 - random_55 - lwr_k=50 - 1200.23105884435 - 9031.424086081894 - 8.66795497994088 - 17.163276821165958 - 17117.10580288264 - 5474.80083616321 - -22.215058929278225'\n",
      "1603 - random_27 - lwr_k=50 - 11.443485547614346 - 37.882195025174845 - 27632.97454503257 - 38.813104349638664 - 21.323247579000903 - 5546.678301432714 - -22.519844371133328'\n",
      "1604 - random_34 - lwr_k=40 - 500.4216941865959 - 268.09170959578046 - 478.3863765426286 - 27722.38617912207 - 3211.5315485405317 - 6434.181544237211 - -26.283166672382738'\n",
      "1605 - random_66 - lwr_k=10 - 1895.667701877923 - 8565.073061595576 - 6447.039435863752 - 12470.971090987343 - 4254.981808676827 - 6726.25656699594 - -27.521666312465726'\n",
      "1606 - random_29 - lwr_k=40 - 29.039917456772713 - 53.50510940348209 - 41.58580605726037 - 42.420515044574756 - 34495.99043473563 - 6930.2515247586025 - -28.386675854813696'\n",
      "1607 - random_40 - lwr_k=800 - 34796.9350423471 - 7.824195541938633 - 5.9906454599617005 - 56.878353430120086 - 2613.8560088164404 - 7499.541022287549 - -30.800661245050403'\n",
      "1608 - random_46 - lwr_k=40 - 18.170959268456233 - 66.53454571240984 - 14.804388015723944 - 63.55769625225884 - 37665.223566947316 - 7563.194401051336 - -31.070573700913286'\n",
      "1609 - random_46 - lwr_k=30 - 35.51011852503392 - 146.89692716638106 - 36.07055626613551 - 202.91860912798393 - 40169.22602039708 - 8115.495685555258 - -33.41252316175617'\n",
      "1610 - random_95 - lwr_k=20 - 5308.200681849702 - 1794.9978224319152 - 24214.829199711286 - 12626.149572649536 - 770.5344539815876 - 8941.176718781107 - -36.913697801116875'\n",
      "1611 - random_65 - lwr_k=40 - 38798.79800085733 - 4044.3835210139673 - 417.60924476440147 - 2309.898504600034 - 254.3656308771585 - 9169.024925004374 - -37.87985340982217'\n",
      "1612 - random_68 - lwr_k=200 - 19.909851247915192 - 8.65444989998733 - 50705.99132306921 - 13.366926281207403 - 111.80115448603765 - 10168.618177187986 - -42.118476320352386'\n",
      "1613 - random_27 - lwr_k=40 - 12.796179355456609 - 44.34380399023236 - 56334.91585627303 - 44.07777995890157 - 23.549749173463432 - 11288.247999546165 - -46.866096020658794'\n",
      "1614 - random_15 - lwr_k=30 - 7796.301917437114 - 16575.571643610205 - 14728.071532239675 - 6081.126024322677 - 12512.795717018274 - 11538.985308439786 - -47.929309382373226'\n",
      "1615 - random_55 - lwr_k=30 - 9668.080673093857 - 13841.210269289646 - 151.8711560615839 - 81.31869897417504 - 44034.275217014874 - 13554.761484291219 - -56.476901178136416'\n",
      "1616 - random_27 - lwr_k=100 - 9.733683453360086 - 29.12633383825337 - 69163.75172455273 - 24.520160966938466 - 19.013226818201996 - 13844.699862992444 - -57.706340704584186'\n",
      "1617 - random_98 - lwr_k=400 - 34.286885432461794 - 6.024156990526706 - 16.49470760253556 - 7.274855689292919 - 77259.61466487769 - 15459.681060493835 - -64.55442245068579'\n",
      "1618 - random_57 - lwr_k=10 - 1314.313549296183 - 224.33277787080735 - 13633.813462021202 - 261.99635116967863 - 62864.29879972451 - 15654.874476529842 - -65.38211039614514'\n",
      "1619 - random_34 - lwr_k=50 - 121.38623647891406 - 75.36112971571575 - 240.65254914071767 - 77264.21941185804 - 691.94726451172 - 15673.6108655376 - -65.4615591994813'\n",
      "1620 - random_40 - lwr_k=500 - 43189.35830120318 - 7.754717050626807 - 5.935799934550931 - 33928.16042758141 - 2521.6282027806915 - 15932.423716669131 - -66.55901566784914'\n",
      "1621 - random_80 - lwr_k=50 - 47979.436555980574 - 1484.0296880522903 - 18925.063509721564 - 16679.317940907873 - 497.24789451445685 - 17115.51419365136 - -71.57573060666296'\n",
      "1622 - random_55 - lwr_k=40 - 5921.85136142824 - 88665.8660738017 - 12.561568311733765 - 34.01522921278686 - 37.670603800101276 - 18943.680494984175 - -79.32779130951492'\n",
      "1623 - random_31 - lwr_k=50 - 29259.905786598312 - 133.31729622308052 - 69119.4445816318 - 96.26905991962113 - 4949.825427636284 - 20709.78251362141 - -86.81667787630259'\n",
      "1624 - random_3 - lwr_k=10 - 491.00137068933645 - 267.17271645570446 - 105389.73195699467 - 446.5201787828164 - 137.7403062821135 - 21339.566645856565 - -89.48718154942064'\n",
      "1625 - random_73 - lwr_k=40 - 71.76339156364051 - 776.6887755865929 - 36.27903377029253 - 128547.8279964753 - 127.66129277776804 - 25903.697015010497 - -108.84068109244186'\n",
      "1626 - random_61 - lwr_k=10 - 304.78238712168815 - 722.7483720849176 - 7788.954757984697 - 139097.20454252107 - 21509.784872427736 - 33873.76623921767 - -142.63654549872845'\n",
      "1627 - random_31 - lwr_k=40 - 955.551127998352 - 104826.77391910015 - 65554.69027506263 - 346.1059371789408 - 821.4578348196948 - 34506.93849664585 - -145.3214160005896'\n",
      "1628 - random_13 - lwr_k=10 - 15537.558482847378 - 2189.3745318503516 - 56640.57453903331 - 2596.557806730775 - 126637.48296712682 - 40709.87677108243 - -171.6240308143621'\n",
      "1629 - random_76 - lwr_k=20 - 87.1600027934639 - 149.95514918749973 - 219317.09820426878 - 387.90292695931396 - 59.26086015635224 - 43985.90447145916 - -185.51552721653852'\n",
      "1630 - random_50 - lwr_k=40 - 13.066750121081576 - 14.822426401351342 - 229890.74779354065 - 16.728669976511945 - 51.0090173634792 - 45982.21571080166 - -193.98058091427177'\n",
      "1631 - random_81 - lwr_k=50 - 235388.58492341932 - 29.261897245688388 - 10.94753893748728 - 12.574543296904737 - 22.824438116530118 - 47115.96494452287 - -198.78807182754989'\n",
      "1632 - random_76 - lwr_k=30 - 20.609246566484252 - 17.200366900147618 - 5306.856880956847 - 232870.56891976704 - 24.52940212318662 - 47632.35478145285 - -200.97774426560906'\n",
      "1633 - random_40 - lwr_k=400 - 50107.87235123967 - 7.7623954058561395 - 5.916644532914189 - 189474.74571232844 - 6576.860969043084 - 49226.71387609794 - -207.73838112606435'\n",
      "1634 - random_26 - lwr_k=20 - 19.560537609935352 - 8.829442976663215 - 11.450789474106179 - 29.742584531730653 - 301263.67694558634 - 60246.91979965595 - -254.46788555630616'\n",
      "1635 - random_52 - lwr_k=40 - 194.00275563594428 - 81681.7522555812 - 569.8357344175689 - 40245.88150507892 - 180124.32953613476 - 60556.733213812506 - -255.78160214289267'\n",
      "1636 - random_91 - lwr_k=30 - 34.64973082566324 - 116.18188555600977 - 304558.88118128007 - 30.864822469867903 - 47.714168368124426 - 60937.719846958345 - -257.397117922266'\n",
      "1637 - random_76 - lwr_k=10 - 97.27924133720558 - 169.15075515050023 - 135.33066416178585 - 315176.5328057622 - 157.07501980688346 - 63126.437097047565 - -266.67803999795524'\n",
      "1638 - random_29 - lwr_k=50 - 51.57381431096109 - 32.45674636388943 - 356107.97053009755 - 31.80252792987174 - 40.26283277062253 - 71229.49224951846 - -301.03781097432454'\n",
      "1639 - random_40 - lwr_k=300 - 54857.73504193288 - 7.706530737510746 - 5.985360057478309 - 295867.8615436973 - 12653.728989422647 - 72663.78578145946 - -307.11971420009155'\n",
      "1640 - random_23 - lwr_k=20 - 29884.477430740815 - 151.99882236176228 - 488.5905376225044 - 136.4866305802987 - 358170.13456736907 - 77745.78803183635 - -328.6691705642257'\n",
      "1641 - random_98 - lwr_k=500 - 22.88166524182636 - 5.969451759975839 - 16.2457164917467 - 7.4128789320958 - 425004.70271519537 - 84983.60655399413 - -359.36003741757935'\n",
      "1642 - random_86 - lwr_k=20 - 233.0563318318528 - 498606.8382430859 - 80.71757082482047 - 6019.3786971735 - 77.13737570547465 - 101052.03102169832 - -427.4957435522031'\n",
      "1643 - random_98 - lwr_k=30 - 565794.5775458602 - 194.52722621593986 - 16.46841982939577 - 840.8148504513558 - 1020.6035314559919 - 113628.88256368064 - -480.82596659215574'\n",
      "1644 - random_79 - lwr_k=10 - 2345.3802182424347 - 44.0516317902366 - 595883.6411175743 - 33.41464825769934 - 47.18020763488504 - 119631.93349342438 - -506.28098957106016'\n",
      "1645 - random_68 - lwr_k=300 - 19.299558485348356 - 7.967552024706201 - 2812.1156607614025 - 11.895260973295114 - 611942.2692456843 - 122918.44579883115 - -520.2169443436699'\n",
      "1646 - random_68 - lwr_k=100 - 98.63947660047485 - 11.729330653003123 - 149432.28644653343 - 16.8847265632276 - 561263.095104175 - 142117.98719617038 - -601.6296748324578'\n",
      "1647 - random_52 - lwr_k=50 - 750469.7430537426 - 14146.13338985797 - 347.8865565304527 - 213.74943101387447 - 61343.36645024229 - 165375.24299829986 - -700.2485251134325'\n",
      "1648 - random_40 - lwr_k=200 - 2751.3647350982833 - 7.734701931524693 - 6.041688966794517 - 853864.4209508192 - 23973.229756910066 - 176063.33190247524 - -745.5697379167979'\n",
      "1649 - random_10 - lwr_k=10 - 721014.24399585 - 198164.83378758695 - 128215.52138080653 - 23040.103983743913 - 16319.959177288389 - 217430.26392286643 - -920.9799114218199'\n",
      "1650 - random_54 - lwr_k=20 - 205.23032877175476 - 48342.08354182603 - 471.53307026914774 - 1116967.8957827815 - 175.87511798219015 - 233164.090993696 - -987.6968082666667'\n",
      "1651 - random_29 - lwr_k=30 - 185.59774053037705 - 152.53507260083128 - 385.11842885977495 - 1222394.624428675 - 490.7660450466891 - 244641.63904825572 - -1036.3656023331478'\n",
      "1652 - random_11 - lwr_k=50 - 5049.744351046352 - 23164.219495434576 - 1231733.628978081 - 915.9770023355045 - 8734.457552963418 - 253841.06851066923 - -1075.3743815521077'\n",
      "1653 - random_26 - lwr_k=100 - 47769.65555868963 - 8.366155888973788 - 1252618.4747012104 - 10.380755395265135 - 127.29702376382521 - 260029.47514552902 - -1101.615377161966'\n",
      "1654 - random_55 - lwr_k=10 - 5571.506739053013 - 15769.542109960279 - 1367009.7355172245 - 361.820215897147 - 7573.595289739603 - 279169.2796941883 - -1182.7748026440781'\n",
      "1655 - random_98 - lwr_k=300 - 20.302865876287353 - 6.328383054720777 - 16.56591674618478 - 7.136492139483197 - 1437972.8587072813 - 287510.4543162001 - -1218.1442829565544'\n",
      "1656 - random_27 - lwr_k=30 - 18.718502052132475 - 61.19013137637316 - 1567729.2064324943 - 88.27224151783956 - 28.621998400778565 - 313482.5179746301 - -1328.2748623856255'\n",
      "1657 - random_50 - lwr_k=50 - 12.340975595975115 - 13.782122018898674 - 1568691.441594768 - 21.420544008678153 - 48.53462963614111 - 313654.7548517304 - -1329.0052066248657'\n",
      "1658 - random_27 - lwr_k=10 - 22.751590474731007 - 169.55730709117614 - 1623137.3816345218 - 144.68368593426186 - 67.23798078953416 - 324602.014213795 - -1375.4253922735782'\n",
      "1659 - random_50 - lwr_k=30 - 15.139276970076388 - 20.171606421733852 - 1957230.7939737712 - 45.42100653652336 - 43.85199800203189 - 391342.87730146066 - -1658.4298550727906'\n",
      "1660 - random_9 - lwr_k=50 - 101.55208840790178 - 312.6308400368376 - 179.2102413899322 - 2023195.989173749 - 470.2080596131868 - 404719.39971593773 - -1715.1509606278419'\n",
      "1661 - random_89 - lwr_k=10 - 2070322.6554392742 - 29.172442713945173 - 28.817920212697324 - 53.525275789414046 - 101.80189849610497 - 414310.59027872473 - -1755.8209431131004'\n",
      "1662 - random_36 - lwr_k=200 - 9.75306318452913 - 6.890380992007934 - 7.3665866919000305 - 22422.730874481796 - 2181774.2537168586 - 440699.828242535 - -1867.7204866329223'\n",
      "1663 - random_40 - lwr_k=100 - 64306.35716958416 - 8.337880118317306 - 6.37709650508346 - 2282890.760556002 - 167.6772240461299 - 469332.68320206436 - -1989.1337462364968'\n",
      "1664 - random_68 - lwr_k=20 - 9721.211511824622 - 5927.843251175999 - 2868371.3801041855 - 507.48520372976805 - 1501.5461347347646 - 577019.4248062654 - -2445.7629692997916'\n",
      "1665 - random_63 - lwr_k=30 - 1622.6055108168205 - 100.76249919994633 - 3531041.5123887574 - 209760.44691532297 - 26.40436402410969 - 748265.4966017678 - -3171.9058495814525'\n",
      "1666 - random_98 - lwr_k=200 - 7.613789945572554 - 6.602480024472025 - 16.26201447850501 - 6.788745296728807 - 4001029.3610759177 - 799951.2636537224 - -3391.071471094173'\n",
      "1667 - random_50 - lwr_k=20 - 22.14756652564529 - 1491.6574614331164 - 4158351.214236483 - 28.9972293012702 - 41304.9650865701 - 839964.8715104944 - -3560.7430796451404'\n",
      "1668 - random_63 - lwr_k=100 - 5184698.83855954 - 100.05824912452904 - 26.206579678172673 - 14.423418310917155 - 28.776466540472395 - 1037483.0517755658 - -4398.288833669486'\n",
      "1669 - random_63 - lwr_k=20 - 1395.8831156325741 - 443914.0117217405 - 4849643.496231367 - 701959.8985323324 - 88.91742988876807 - 1199080.5640543052 - -5083.5184671559855'\n",
      "1670 - random_36 - lwr_k=100 - 9.467237427954613 - 4899313.28954176 - 7.331531837501471 - 2635047.8771883263 - 2183447.0068536517 - 1943730.7375142947 - -8241.094089703354'\n",
      "1671 - random_9 - lwr_k=40 - 215.88966431465238 - 239821.81236185442 - 235.85632564951223 - 10413432.873185642 - 504.22112710145046 - 2130183.599902645 - -9031.719048932306'\n",
      "1672 - random_27 - lwr_k=20 - 95.50578927595332 - 3315.424327644309 - 14360964.43613022 - 401.9981177145394 - 417.2525215399315 - 2872098.5816519414 - -12177.696507702249'\n",
      "1673 - random_86 - lwr_k=50 - 17.45831837913723 - 14645165.798817383 - 24.629836624681914 - 19.88307303259734 - 21.72400816359758 - 2930488.753151556 - -12425.291134944282'\n",
      "1674 - random_40 - lwr_k=20 - 1335174.3792972257 - 6377.831120866436 - 27.270247509100738 - 7614839.3273788905 - 6072117.2748516435 - 3004942.542889569 - -12741.00108823774'\n",
      "1675 - random_98 - lwr_k=50 - 393105.2138158226 - 21.56974863046317 - 478828.1060904154 - 15573625.501314562 - 297.60313226674214 - 3288162.7898447267 - -13941.953400438473'\n",
      "1676 - random_40 - lwr_k=10 - 839.2639458589955 - 3791.7394832991536 - 18806814.215977896 - 14383.659269572525 - 272.71129863420737 - 3763987.9928546296 - -15959.617687867061'\n",
      "1677 - random_68 - lwr_k=50 - 2886.562370718482 - 23.84164112291414 - 3221679.171516744 - 204.6699238595593 - 17382667.07365455 - 4120142.9803559287 - -17469.838656670996'\n",
      "1678 - random_86 - lwr_k=40 - 20.703244061016587 - 22246676.419842124 - 19.110191186490393 - 24.57282463102692 - 22.325217001802205 - 4451538.313486752 - -18875.070082936254'\n",
      "1679 - random_40 - lwr_k=50 - 208910.14936412824 - 13.07959182548237 - 7.7069506690306335 - 22655348.405797806 - 989337.2844903169 - 4769195.157002252 - -20222.04553687238'\n",
      "1680 - random_73 - lwr_k=50 - 19.9055045098031 - 529.6637914342793 - 64.00536622538726 - 3614477.857623826 - 23095369.927277643 - 5340342.863860569 - -22643.91038069566'\n",
      "1681 - random_68 - lwr_k=40 - 756.4001400568972 - 66.17211863057128 - 26304724.127571702 - 443.42809140740917 - 7454334.325958422 - 6749853.771206814 - -28620.72666218715'\n",
      "1682 - random_74 - lwr_k=50 - 37274.52187101014 - 146.8141307450654 - 96755.92162170344 - 15587207.706975454 - 20051977.18294896 - 7152335.4516934585 - -30327.38891531132'\n",
      "1683 - random_74 - lwr_k=100 - 8.789670808725754 - 26.010073383423695 - 6.154284038577415 - 8.559310715458317 - 42798823.40358482 - 8556971.322476448 - -36283.53334690632'\n",
      "1684 - random_40 - lwr_k=40 - 5850.371962781274 - 41.08162176402764 - 8.173318608391902 - 47481491.700264156 - 303290.4641530479 - 9555007.099829005 - -40515.552022676995'\n",
      "1685 - random_52 - lwr_k=30 - 3695.79368850329 - 64055.25534397396 - 19102487.539381802 - 39807256.86062592 - 2263117.0453744987 - 12244122.418000551 - -51918.335876770085'\n",
      "1686 - random_52 - lwr_k=20 - 248.00412166627058 - 1862.2914637885008 - 70287262.18525946 - 143.59235257586496 - 557.6237237133995 - 14053411.18294357 - -59590.34926231437'\n",
      "1687 - random_40 - lwr_k=30 - 120167.99555877039 - 18.248116334261976 - 12.468007640367844 - 110659691.26563194 - 23907065.175783236 - 26928588.902599685 - -114185.57901248318'\n",
      "1688 - random_68 - lwr_k=30 - 4378.721815944076 - 76.46965002062647 - 34645960.992862314 - 2033.2599638977915 - 127139669.56733267 - 32347827.373241425 - -137165.0341949893'\n",
      "1689 - random_98 - lwr_k=100 - 16.08031575389143 - 11.574538485935383 - 16.56020475915005 - 1072.9307004989896 - 185112088.68432373 - 37010516.51321213 - -156936.45719148032'\n",
      "1690 - random_74 - lwr_k=40 - 883.9463660924815 - 4901.071524091293 - 588524.0053269156 - 61451859.37290811 - 123699408.52285199 - 37136950.254312925 - -157472.58021004134'\n",
      "1691 - random_36 - lwr_k=50 - 9.686651705322227 - 1966247.1051946136 - 26.66294628949426 - 10.904055312346896 - 380822314.5152728 - 76532971.6214333 - -324525.4073331098'\n",
      "1692 - random_98 - lwr_k=40 - 10080057.421547133 - 288576.9383896877 - 43618.881543210875 - 88.73540210316075 - 677066560.2483157 - 137452449.3616058 - -582845.1723805041'\n",
      "1693 - random_63 - lwr_k=10 - 451018305.8361263 - 6427473.023553856 - 83840776.97198956 - 224095048.32786795 - 5467240.056343981 - 154194184.47133636 - -653835.9497223038'\n",
      "1694 - random_81 - lwr_k=40 - 801049119.3052845 - 30.168860171363313 - 11.817183063795138 - 12.64133671029413 - 23.077294861380132 - 160288540.80540928 - -679678.1393590457'\n",
      "1695 - random_90 - lwr_k=20 - 24.640301409532135 - 19.945109095778886 - 14.172501606267604 - 46.44664919749061 - 2067298843.8056452 - 413324384.60554236 - -1752638.0881918971'\n",
      "1696 - random_26 - lwr_k=50 - 224332.22767772587 - 8.490420474109202 - 20398619.33036238 - 327487904.4013173 - 3486997544.7140594 - 766770524.3048574 - -3251372.6005501277'\n",
      "1697 - random_36 - lwr_k=40 - 9.513278629796332 - 142.18678466372768 - 5025158998.703563 - 11985168.393984294 - 21.340045241775 - 1007098942.1090184 - -4270448.645262245'\n",
      "1698 - random_50 - lwr_k=10 - 3536.697060204987 - 5485779855.075804 - 29030181.207597006 - 193763461.73575616 - 226112.90594570828 - 1142284988.8060312 - -4843684.482401244'\n",
      "1699 - random_26 - lwr_k=40 - 734861.7563692656 - 8.80288780566588 - 36945131949.54942 - 29718.08573112392 - 138157938.13415268 - 7414382061.645282 - -31439556.64533447'\n"
     ]
    }
   ],
   "source": [
    "scores_df = pd.DataFrame(all_scores)\n",
    "scores_df.to_csv(log_dir/f\"scores.csv\",index=False)\n",
    "scores_df_final = pd.DataFrame(all_scores_final)\n",
    "scores_df_final.to_csv(log_dir/f\"test_scores.csv\",index=False)\n",
    "\n",
    "scores_df_sorted = pd.DataFrame(scores_df).sort_values(by='MSE')\n",
    "\n",
    "best_5 = []\n",
    "summary_logger.info(f\"Rank - \" +\" - \".join(list(scores_df_sorted.columns)))\n",
    "for i,(index,row) in enumerate(scores_df_sorted.iterrows()):\n",
    "    if i < 5:\n",
    "        best_5.append((row[\"model_num\"],row[\"predictor\"],row[\"MSE\"]))\n",
    "    s = f\"{i} - \" + \" - \".join([f\"{i}\" for i in row.tolist()])\n",
    "    summary_logger.info(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% save scores\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      " Best 5 on Test Sest \n",
      " ---------------------'\n",
      "Rank -  Deep Model - Predictor - Val Set - Test Set'\n",
      "0 - random_66 - lwr_k=50 - 7.679617927659398 - 6.216187925941892 - 0.9738311845034782'\n",
      "1 - random_94 - lwr_k=300 - 7.821441523019006 - 7.856251410664497 - 0.9669268696330771'\n",
      "2 - random_94 - lwr_k=400 - 7.864665219900357 - 7.9067304809475125 - 0.9667143635936185'\n",
      "3 - random_94 - lwr_k=200 - 7.866978199713183 - 8.26598289856386 - 0.9652019881079856'\n",
      "4 - random_78 - lwr_k=100 - 7.886886578073809 - 6.637566716984889 - 0.9720572703348069'\n"
     ]
    }
   ],
   "source": [
    "summary_logger.info(\"-----------------------\\n Best 5 on Test Sest \\n ---------------------\")\n",
    "summary_logger.info(f\"Rank -  Deep Model - Predictor - Val Set - Test Set\")\n",
    "for i, (j,k,v) in enumerate(best_5):\n",
    "\n",
    "    row = scores_df_final.loc[(scores_df_final['model_num']==j) & (scores_df_final['predictor'] == k)].iloc[0]\n",
    "    #print(row)\n",
    "    s = f\"{i} - {j} - {k} - {v} - {row['MSE']} - {row['R2']}\"\n",
    "    summary_logger.info(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote Summary Graph'\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#take 1 is a scatter plot - lets, for each dataset\n",
    "#graph our deep models by rank - plot - then overlay our knn moels\n",
    "#plot points\n",
    "\n",
    "deep_set = scores_df[scores_df[\"predictor\"]==\"deep\"].sort_values(\"R2\")\n",
    "deep_set[\"order\"] = [i for i in range(0,100)]\n",
    "deep_ordering = {row[\"model_num\"]:row[\"order\"] for index, row in deep_set.iterrows()}\n",
    "\n",
    "def order_models(x):\n",
    "    x = [deep_ordering[i] for i in x]\n",
    "    return x\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "set_deep = False\n",
    "knn_models = scores_df[\"predictor\"].unique()\n",
    "for knn_model in knn_models:\n",
    "    subset = scores_df[scores_df[\"predictor\"]==knn_model]\n",
    "    s=3\n",
    "    if knn_model == \"deep\":\n",
    "        s=10\n",
    "    ax.scatter(x=order_models(subset[\"model_num\"].tolist()), y=subset[\"R2\"], s=s, label=knn_model)\n",
    "\n",
    "#ax.set_ylim(0,scores_db[\"deep_mean\"].max())\n",
    "ax.set_ylim(0,1)\n",
    "# plot residuals\n",
    "ax.legend(loc='upper right',bbox_to_anchor=(1.4, 1))\n",
    "ax.set_ylabel(\"R^2 Score\")\n",
    "ax.set_xlabel(\"Deep Model Rank\")\n",
    "#ax.set_ylim(0,200)\n",
    "#ax.set_yscale(\"symlog\")\n",
    "ax.set_title(\"Summary of LWR improvements over Deep Models\")\n",
    "plt.savefig(log_dir/f\"summary_plot.png\", bbox_inches='tight')\n",
    "logging.getLogger().info(\"Wrote Summary Graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df[\"n_features\"] = [deep_models[i].n_features for i in scores_df[\"model_num\"]] \n",
    "from matplotlib.colors import Colormap\n",
    "import seaborn as sns #heatmap of features - pls model - score\n",
    "class nlcmap(Colormap):\n",
    "    def __init__(self, cmap, levels):\n",
    "        self.cmap = cmap\n",
    "        self.N = cmap.N\n",
    "        self.monochrome = self.cmap.monochrome\n",
    "        self.levels = np.asarray(levels, dtype='float64')\n",
    "        self._x = self.levels\n",
    "        self.levmax = self.levels.max()\n",
    "        self.levmin = self.levels.min()\n",
    "        self.transformed_levels = np.linspace(self.levmin, self.levmax, #uniform spacing along levels (colour segments)\n",
    "             len(self.levels))\n",
    "\n",
    "    def __call__(self, xi, alpha=1.0, **kw):\n",
    "        yi = np.interp(xi, self._x, self.transformed_levels)\n",
    "        return self.cmap((yi-self.levmin) / (self.levmax-self.levmin), alpha)\n",
    "    \n",
    "levels = np.concatenate((\n",
    "    [0, 1],\n",
    "    [0.6,0.8,0.9,0.95,0.98]\n",
    "    ))\n",
    "\n",
    "levels = levels[levels <= 1]\n",
    "levels.sort()\n",
    "cmap_nonlin = nlcmap(plt.cm.YlGnBu, levels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = scores_df[[\"predictor\",\"n_features\",\"R2\"]]\n",
    "subset = subset[np.logical_not(subset[\"predictor\"]==\"deep\")]\n",
    "subset = subset[np.logical_not(subset[\"predictor\"]==\"lr\")]\n",
    "trans = subset[\"predictor\"].transform(lambda x: int(x.replace(\"lwr_k=\",\"\"))).tolist()\n",
    "subset.loc[:,\"predictor\"]=trans\n",
    "subset=subset.sort_values(\"predictor\",ascending=False)\n",
    "\n",
    "def rand_jitter(arr):\n",
    "    stdev = .01 * (max(arr) - min(arr))\n",
    "    return arr + np.random.randn(len(arr)) * stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\AppData\\Local\\Temp\\ipykernel_9356\\3702862414.py:6: MatplotlibDeprecationWarning: Auto-removal of grids by pcolor() and pcolormesh() is deprecated since 3.5 and will be removed two minor releases later; please call grid(False) first.\n",
      "  cbar = fig.colorbar(sc,label=\"R2 Score\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sc = ax.scatter(x=rand_jitter(subset[\"n_features\"]), y=rand_jitter(subset[\"predictor\"]), s=20,c=subset[\"R2\"],cmap=cmap_nonlin,vmin=0)\n",
    "ax.set_xlabel(\"Number of Features\")\n",
    "ax.set_ylabel(\"Number of Neighbours\")\n",
    "\n",
    "cbar = fig.colorbar(sc,label=\"R2 Score\")\n",
    "\n",
    "ax.set_title(\"LWR performance as a function of the number of components\")\n",
    "plt.savefig(log_dir/f\"heat_scatter.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (lazydeep)",
   "language": "python",
   "name": "pycharm-12fcba0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
