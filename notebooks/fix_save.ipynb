{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU detected is GeForce GTX 970\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import logging\n",
    "# set seed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils as ut\n",
    "import experiment as ex\n",
    "from evaluation import *\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import random\n",
    "#define fixed_hyperparams and create a config gen\n",
    "from configurations import RandomConfigGen, Configuration\n",
    "from torch import nn\n",
    "from deep_net import RandomNet\n",
    "from experiment import run_experiment\n",
    "import regex as re\n",
    "from pathlib import *\n",
    "\n",
    "from river_models import StreamLocalWeightedRegression\n",
    "\n",
    "from river import stream, metrics\n",
    "from river.neighbors import KNNRegressor \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from river.utils import dict2numpy\n",
    "from sk_models import PLSRegression, StandardScaler,LocalWeightedRegression,PLSLWR,LinearRidge\n",
    "\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed + 1)\n",
    "np.random.seed(seed + 2)\n",
    "random_state = np.random.RandomState(seed)\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"GPU detected is {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory is D:\\workspace\\lazydeep\\experiments\\5.00\\PLN7\n"
     ]
    }
   ],
   "source": [
    "#setup input and output directories\n",
    "\n",
    "#setup input and outpu t formats, load data\n",
    "\n",
    "#we need to set parametesr\n",
    "file_name = \"PLN7.csv\" #\"mango_684_990.csv\" #\"mango_729_975.csv\" #fitlered=513-1050\n",
    "id_cols =[\"db_id\",\"sample_id\"] #\n",
    "output_cols = None\n",
    "data_path = Path('D:/workspace/lazydeep/data/soil_data/')\n",
    "log_path = Path(\"D:/workspace/lazydeep/experiments/5.00\") #1.01/\")\n",
    "if not log_path.exists():\n",
    "    log_path.mkdir()\n",
    "\n",
    "data_file = data_path / file_name\n",
    "log_dir = log_path / re.sub(r'\\.(?=csv$)[^.]+$', '',file_name)\n",
    "if not log_dir.exists():\n",
    "    log_dir.mkdir()\n",
    "print(f\"Output directory is {log_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(models,X,y):\n",
    "    preds = {name:[] for name in models.keys()}\n",
    "    for xi, yi in stream.iter_pandas(X,y):\n",
    "        for name,model in river_models.items():\n",
    "            pred = model.predict_one(xi)\n",
    "            preds[name].append(pred)\n",
    "    return preds\n",
    "        \n",
    "def batch_score(models,X,y):\n",
    "    preds = {name:[] for name in models.keys()}\n",
    "    for xi, yi in stream.iter_pandas(X,y):\n",
    "        for name,model in river_models.items():\n",
    "            pred = model.predict_one(xi)\n",
    "            preds[name].append(pred)\n",
    "            \n",
    "    scores = {name:r2_score(y,pred) for name,pred in preds.items()}\n",
    "    mse = {name:mean_squared_error(y,pred) for name,pred in preds.items()}\n",
    "    return scores, mse\n",
    "        \n",
    "\n",
    "def batch_learn(models,X,y):\n",
    "    for xi, yi in stream.iter_pandas(X,y):\n",
    "        for name,model in river_models.items():\n",
    "             model.learn_one(xi,yi)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sk_predict_one(models,xi,yi):\n",
    "    preds = {}\n",
    "    for name,model in models.items():\n",
    "        preds[name] = models.predict([dict2numpy(xi)])[0]\n",
    "    \n",
    "    return preds  \n",
    "\n",
    "def sk_predict(models,X,y):\n",
    "    preds = {}\n",
    "    for name,model in models.items():\n",
    "            pred = model.predict(X)\n",
    "            preds[name]=pred.tolist()\n",
    "    return preds\n",
    "        \n",
    "def sk_score(models,X,y):\n",
    "    preds = {}\n",
    "    for name,model in models.items():\n",
    "        pred = model.predict(X)\n",
    "        preds[name]=pred\n",
    "            \n",
    "    scores = {name:r2_score(y,pred) for name,pred in preds.items()}\n",
    "    mse = {name:mean_squared_error(y,pred) for name,pred in preds.items()}\n",
    "    return scores, mse\n",
    "        \n",
    "\n",
    "def sk_learn(models,X,y):\n",
    "    for name,model in models.items():\n",
    "        model.fit(X,y)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 8333, test: 1667, stream: 90000\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(data_file)\n",
    "data=data.sample(frac=1,random_state=random_state)\n",
    "\n",
    "pre_ind =[i for i in range(0,10000)]\n",
    "pretrain_ind,pretest_ind = train_test_split(pre_ind,train_size=5/6,random_state=random_state,shuffle=False)\n",
    "stream_ind = [i for i in range(10000,100000)]\n",
    "\n",
    "\n",
    "pretrain_data =  ut.TabularDataset(data.iloc[pretrain_ind,:],id_cols = id_cols, cat_cols=None, output_cols=output_cols, ignore_cols= None)\n",
    "pretest_data = ut.TabularDataset(data.iloc[pretest_ind,:],id_cols = id_cols, cat_cols=None, output_cols=output_cols, ignore_cols= None)\n",
    "stream_data = ut.TabularDataset(data.iloc[stream_ind,:],id_cols = id_cols, cat_cols=None, output_cols=output_cols, ignore_cols= None)\n",
    "\n",
    "nrow, ncol = data.shape\n",
    "nrow_train = len(pretrain_data)\n",
    "nrow_test = len(pretest_data)\n",
    "nrow_stream = len(stream_data)\n",
    "\n",
    "print(f\"train: {nrow_train}, test: {nrow_test}, stream: {nrow_stream}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__setstate__() argument 1 must be sequence of length 4, not 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(text)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mjsonpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\lazydeep3\\lib\\site-packages\\jsonpickle\\unpickler.py:65\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(string, backend, context, keys, reset, safe, classes, v1_decode)\u001b[0m\n\u001b[0;32m     61\u001b[0m context \u001b[38;5;241m=\u001b[39m context \u001b[38;5;129;01mor\u001b[39;00m Unpickler(\n\u001b[0;32m     62\u001b[0m     keys\u001b[38;5;241m=\u001b[39mkeys, backend\u001b[38;5;241m=\u001b[39mbackend, safe\u001b[38;5;241m=\u001b[39msafe, v1_decode\u001b[38;5;241m=\u001b[39mv1_decode\n\u001b[0;32m     63\u001b[0m )\n\u001b[0;32m     64\u001b[0m data \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mdecode(string)\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\lazydeep3\\lib\\site-packages\\jsonpickle\\unpickler.py:179\u001b[0m, in \u001b[0;36mUnpickler.restore\u001b[1;34m(self, obj, reset, classes)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m classes:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_classes(classes)\n\u001b[1;32m--> 179\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_restore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset:\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap_proxies()\n",
      "File \u001b[1;32m~\\.conda\\envs\\lazydeep3\\lib\\site-packages\\jsonpickle\\unpickler.py:212\u001b[0m, in \u001b[0;36mUnpickler._restore\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     restore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restore_tags(obj)\n\u001b[1;32m--> 212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\lazydeep3\\lib\\site-packages\\jsonpickle\\unpickler.py:603\u001b[0m, in \u001b[0;36mUnpickler._restore_dict\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    601\u001b[0m             str_k \u001b[38;5;241m=\u001b[39m k\n\u001b[0;32m    602\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_namestack\u001b[38;5;241m.\u001b[39mappend(str_k)\n\u001b[1;32m--> 603\u001b[0m         data[k] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_restore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    604\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_namestack\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\.conda\\envs\\lazydeep3\\lib\\site-packages\\jsonpickle\\unpickler.py:212\u001b[0m, in \u001b[0;36mUnpickler._restore\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     restore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restore_tags(obj)\n\u001b[1;32m--> 212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\lazydeep3\\lib\\site-packages\\jsonpickle\\unpickler.py:299\u001b[0m, in \u001b[0;36mUnpickler._restore_reduce\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state:\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 299\u001b[0m         \u001b[43mstage1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__setstate__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;66;03m# it's fine - we'll try the prescribed default methods\u001b[39;00m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    303\u001b[0m             \u001b[38;5;66;03m# we can't do a straight update here because we\u001b[39;00m\n\u001b[0;32m    304\u001b[0m             \u001b[38;5;66;03m# need object identity of the state dict to be\u001b[39;00m\n\u001b[0;32m    305\u001b[0m             \u001b[38;5;66;03m# preserved so that _swap_proxies works out\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: __setstate__() argument 1 must be sequence of length 4, not 5"
     ]
    }
   ],
   "source": [
    "with open(deep_model_dir/'preprocessing'/f\"_final\",'r') as file:\n",
    "    text = file.read()\n",
    "    jsonpickle.decode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_components': 34, 'deflation_mode': 'regression', 'mode': 'A', 'scale': True, 'algorithm': 'nipals', 'max_iter': 500, 'tol': 1e-06, 'copy': True}\n",
      "{'n_components': 34, 'deflation_mode': 'regression', 'mode': 'A', 'scale': True, 'algorithm': 'nipals', 'max_iter': 500, 'tol': 1e-06, 'copy': True}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__setstate__() argument 1 must be sequence of length 4, not 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m pls_deep_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(deep_model_dir\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_82\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m pls_deep_model\u001b[38;5;241m.\u001b[39mload_state(deep_model_dir\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_82\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_final\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m pls_scaler \u001b[38;5;241m=\u001b[39m PLSRegression(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m34\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_state(\u001b[43mPLSRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m34\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep_model_dir\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpreprocessing\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_final\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)       \n\u001b[0;32m     21\u001b[0m pls_deep_lwr \u001b[38;5;241m=\u001b[39m LocalWeightedRegression(n_neighbours\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     23\u001b[0m deep_model_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/workspace/lazydeep/experiments/2.00/PLN7\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\lazy_deep_v2\\lazydeep_src\\pipeline.py:53\u001b[0m, in \u001b[0;36mLearner.load_state\u001b[1;34m(self, fname)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(fname,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     52\u001b[0m     text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjsonpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\lazydeep3\\lib\\site-packages\\jsonpickle\\unpickler.py:65\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(string, backend, context, keys, reset, safe, classes, v1_decode)\u001b[0m\n\u001b[0;32m     61\u001b[0m context \u001b[38;5;241m=\u001b[39m context \u001b[38;5;129;01mor\u001b[39;00m Unpickler(\n\u001b[0;32m     62\u001b[0m     keys\u001b[38;5;241m=\u001b[39mkeys, backend\u001b[38;5;241m=\u001b[39mbackend, safe\u001b[38;5;241m=\u001b[39msafe, v1_decode\u001b[38;5;241m=\u001b[39mv1_decode\n\u001b[0;32m     63\u001b[0m )\n\u001b[0;32m     64\u001b[0m data \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mdecode(string)\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\lazydeep3\\lib\\site-packages\\jsonpickle\\unpickler.py:179\u001b[0m, in \u001b[0;36mUnpickler.restore\u001b[1;34m(self, obj, reset, classes)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m classes:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_classes(classes)\n\u001b[1;32m--> 179\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_restore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset:\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap_proxies()\n",
      "File \u001b[1;32m~\\.conda\\envs\\lazydeep3\\lib\\site-packages\\jsonpickle\\unpickler.py:212\u001b[0m, in \u001b[0;36mUnpickler._restore\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     restore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restore_tags(obj)\n\u001b[1;32m--> 212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\lazydeep3\\lib\\site-packages\\jsonpickle\\unpickler.py:603\u001b[0m, in \u001b[0;36mUnpickler._restore_dict\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    601\u001b[0m             str_k \u001b[38;5;241m=\u001b[39m k\n\u001b[0;32m    602\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_namestack\u001b[38;5;241m.\u001b[39mappend(str_k)\n\u001b[1;32m--> 603\u001b[0m         data[k] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_restore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    604\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_namestack\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\.conda\\envs\\lazydeep3\\lib\\site-packages\\jsonpickle\\unpickler.py:212\u001b[0m, in \u001b[0;36mUnpickler._restore\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     restore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restore_tags(obj)\n\u001b[1;32m--> 212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\lazydeep3\\lib\\site-packages\\jsonpickle\\unpickler.py:299\u001b[0m, in \u001b[0;36mUnpickler._restore_reduce\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state:\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 299\u001b[0m         \u001b[43mstage1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__setstate__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;66;03m# it's fine - we'll try the prescribed default methods\u001b[39;00m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    303\u001b[0m             \u001b[38;5;66;03m# we can't do a straight update here because we\u001b[39;00m\n\u001b[0;32m    304\u001b[0m             \u001b[38;5;66;03m# need object identity of the state dict to be\u001b[39;00m\n\u001b[0;32m    305\u001b[0m             \u001b[38;5;66;03m# preserved so that _swap_proxies works out\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: __setstate__() argument 1 must be sequence of length 4, not 5"
     ]
    }
   ],
   "source": [
    "#setup evaluation\n",
    "\n",
    "def setup_models():\n",
    "    return {'lr':LinearRidge(),\n",
    "            'plsr':PLSRegression(n_components=25),\n",
    "            'lwr':LocalWeightedRegression(n_neighbours=800,normalize=True),\n",
    "            'pls_lwr':PLSLWR(n_components=25, n_neighbours=300)\n",
    "           }\n",
    "\n",
    "#pls-deep - random_82, => random 24, random_10\n",
    "#pls-deep lwr - random_82 - lwr_k=1000 -\n",
    "\n",
    "#deep random 29 => random 60 => random 63\n",
    "#deep-lwr - random_13 - lwr_k=800\n",
    "\n",
    "deep_model_dir = Path(\"D:/workspace/lazydeep/experiments/1.01/PLN7\")\n",
    "pls_deep_model = torch.load(deep_model_dir/\"models\"/\"random_82\"/\"_model\")\n",
    "pls_deep_model.load_state(deep_model_dir/\"models\"/\"random_82\"/\"_final\")\n",
    "pls_scaler = PLSRegression(n_components=34).from_state(PLSRegression(n_components=34).load_state(deep_model_dir/'preprocessing'/f\"_final\"))       \n",
    "                          \n",
    "pls_deep_lwr = LocalWeightedRegression(n_neighbours=1000,normalize=True)\n",
    "                          \n",
    "deep_model_dir = Path(\"D:/workspace/lazydeep/experiments/2.00/PLN7\")\n",
    "deep_model = torch.load(deep_model_dir/\"models\"/\"random_29\"/\"_model\")\n",
    "deep_model.load_state(deep_model_dir/\"models\"/\"random_29\"/\"_final\")\n",
    "deep_scaler = StandardScaler().from_state(StandardScaler().load_state(deep_model_dir/'preprocessing'/f\"_final\"))                \n",
    "                      \n",
    "deep_lwr = LocalWeightedRegression(n_neighbours=1000,normalize=False)\n",
    "\n",
    "#ew have learnt - don't standardise lwr after deep\n",
    "#we can view features after pls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
