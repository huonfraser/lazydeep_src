{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "# set seed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import utils as ut\n",
    "import experiment as exp\n",
    "from evaluation import *\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import random\n",
    "#define fixed_hyperparams and create a config gen\n",
    "from configurations import RandomConfigGen, Configuration\n",
    "from torch import nn\n",
    "from deep_net import RandomNet\n",
    "from experiment import run_experiment\n",
    "import regex as re\n",
    "from pathlib import *\n",
    "from plot import *\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from sk_models import setup_pls_models_slim, StandardScaler, PLSRegression\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed + 1)\n",
    "np.random.seed(seed + 2)\n",
    "random_state = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\workspace\\lazydeep\\experiments\\1.05\\A_C_OF_SIWARE\n"
     ]
    }
   ],
   "source": [
    "#setup input and output formats, load data\n",
    "\n",
    "file_name = \"A_C_OF_SIWARE.csv\"\n",
    "id_cols =[]#[\"sample_id\"]\n",
    "\n",
    "data_path = Path('D:/workspace/lazydeep/data/soil_data/')\n",
    "model_path = Path('D:/workspace/lazydeep/experiments/1.01/')\n",
    "log_path = Path(\"D:/workspace/lazydeep/experiments/1.05\")\n",
    "n_components = 63\n",
    "\n",
    "data_file = data_path / file_name\n",
    "log_dir = log_path / re.sub(r'\\.(?=csv$)[^.]+$', '',file_name)\n",
    "model_dir = model_path / re.sub(r'\\.(?=csv$)[^.]+$', '',file_name)\n",
    "\n",
    "if not log_dir.exists():\n",
    "    log_dir.mkdir()\n",
    "print(log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% load data\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13916, 247)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(data_file)\n",
    "data = data.sample(frac=1)\n",
    "nrow, ncol = data.shape\n",
    "data = ut.sample_data(data,random_state)\n",
    "\n",
    "n_features = ncol - 1-len(id_cols)\n",
    "dataset = TabularDataset(data,id_cols = id_cols, cat_cols=None, output_cols=None, ignore_cols= None)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% load models\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 models\n"
     ]
    }
   ],
   "source": [
    "n_models = 100\n",
    "model_names = [f\"random_{i}\" for i in range(0,n_models)]\n",
    "deep_models = {name:torch.load(model_dir/\"models\"/name/\"_model\") for name in model_names}\n",
    "#for each model, load state\n",
    "print(f\"Loaded {len(deep_models)} models\")\n",
    "#print(deep_models)\n",
    "for name in model_names:\n",
    "    sub_path = log_dir / name\n",
    "    if not sub_path.exists():\n",
    "        sub_path.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% setup logging and tensorboard outputs\n"
    }
   },
   "outputs": [],
   "source": [
    "# set logging, in this case the root logger\n",
    "ut.setup_logger(logger_name=\"\",file_name=log_dir/\"log.txt\")\n",
    "ut.setup_logger(logger_name=\"summary\",file_name=log_dir/\"summary.txt\")\n",
    "summary_logger = logging.getLogger(\"summary\")\n",
    "tb = SummaryWriter(log_dir/\"tb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% setup experiment\n"
    }
   },
   "outputs": [],
   "source": [
    "fixed_hyperparams = {'bs': 32,'loss': nn.MSELoss(),'epochs': 100}\n",
    "preprocessing = PLSRegression(n_components=n_components)\n",
    "eval = CrossValEvaluation(preprocessing=preprocessing,tensorboard=None,time=True,random_state=random_state)\n",
    "scores={} #->model->knn:{fold_0:number,...,fold_n:number,mean:number,median:number\n",
    "preds={} #model-> foldsxknn_models\n",
    "deep_scores_dict={}\n",
    "deep_preds_dict={}\n",
    "actual_y = None\n",
    "\n",
    "device = \"cpu\" #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "load_fun_cv = lambda name,model, fold : model.load_state(model_dir/'models'/name/f\"_fold_{fold}\")\n",
    "load_fun_pp_cv = lambda fold : preprocessing.from_state(preprocessing.load_state(model_dir/'preprocessing'/f\"_fold_{fold}\"))\n",
    "load_fun_build = lambda name,model : model.load_state(model_dir/'models'/name/f\"_final\")\n",
    "load_fun_pp_build = lambda : preprocessing.from_state(preprocessing.load_state(model_dir/'preprocessing'/f\"_final\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% deep experiment\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Tested (test) on 2320 instances with mean losses of: random_0:173.7125,random_1:165.1398,random_2:117.45,random_3:106.0764,random_4:124.9805,random_5:126.037,random_6:118.5291,random_7:121.4108,random_8:126.8893,random_9:140.3154,random_10:120.2772,random_11:128.632,random_12:164.4252,random_13:133.3161,random_14:114.3982,random_15:124.4254,random_16:196.1024,random_17:161.4823,random_18:113.1342,random_19:135.0113,random_20:109.8883,random_21:317.3733,random_22:134.3421,random_23:241.6692,random_24:106.1707,random_25:104.9887,random_26:127.5515,random_27:130.8678,random_28:205.3853,random_29:153.6993,random_30:217.148,random_31:187.3168,random_32:221.4989,random_33:152.2349,random_34:343.5969,random_35:119.6449,random_36:161.4446,random_37:105.309,random_38:165.6674,random_39:103.303,random_40:109.138,random_41:670.9765,random_42:128.5039,random_43:118.1,random_44:230.5917,random_45:211.8003,random_46:343.5513,random_47:165.9895,random_48:123.4907,random_49:343.6559,random_50:121.8215,random_51:115.5526,random_52:195.8051,random_53:150.1331,random_54:122.6783,random_55:102.4699,random_56:343.3513,random_57:125.5266,random_58:143.4892,random_59:135.392,random_60:115.4959,random_61:165.4771,random_62:126.2594,random_63:162.3539,random_64:111.4345,random_65:111.6636,random_66:122.8927,random_67:122.0245,random_68:303.464,random_69:105.7877,random_70:142.4958,random_71:269.8639,random_72:244.8241,random_73:151.4524,random_74:160.7176,random_75:151.2897,random_76:195.0037,random_77:156.8685,random_78:113.4207,random_79:125.2806,random_80:137.3634,random_81:129.2206,random_82:226.9831,random_83:112.6084,random_84:135.9139,random_85:110.7167,random_86:125.9085,random_87:133.2782,random_88:343.5872,random_89:132.8863,random_90:324.4493,random_91:136.0524,random_92:113.8145,random_93:128.1666,random_94:129.6376,random_95:133.5544,random_96:131.4955,random_97:108.9951,random_98:132.1905,random_99:183.2659'\n",
      "Testing (test) took 0:00:03.166001'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Tested (test) on 2319 instances with mean losses of: random_0:417.4123,random_1:165.5658,random_2:178.6912,random_3:171.1348,random_4:160.6519,random_5:143.1112,random_6:160.9265,random_7:153.3123,random_8:183.6903,random_9:203.0522,random_10:227.7624,random_11:154.0714,random_12:162.2401,random_13:177.1157,random_14:206.6963,random_15:167.3774,random_16:184.1958,random_17:362.5226,random_18:169.7749,random_19:208.6975,random_20:160.0094,random_21:424.6596,random_22:425.3072,random_23:207.2316,random_24:164.6708,random_25:161.6788,random_26:176.5232,random_27:196.8148,random_28:195.7511,random_29:238.8836,random_30:179.6623,random_31:208.463,random_32:212.3037,random_33:252.2753,random_34:425.3313,random_35:155.1154,random_36:219.5113,random_37:148.897,random_38:206.4504,random_39:159.5369,random_40:155.045,random_41:424.1051,random_42:187.0429,random_43:134.5891,random_44:284.5265,random_45:283.7271,random_46:425.2051,random_47:204.9232,random_48:140.2973,random_49:425.3304,random_50:425.3135,random_51:425.2641,random_52:235.4424,random_53:177.7423,random_54:141.0993,random_55:751.1348,random_56:424.8138,random_57:159.4375,random_58:167.6658,random_59:131.549,random_60:147.7049,random_61:234.1249,random_62:180.2856,random_63:131.3451,random_64:147.458,random_65:151.7732,random_66:167.6374,random_67:149.9318,random_68:384.2864,random_69:155.2199,random_70:159.5303,random_71:329.2488,random_72:178.426,random_73:301.6401,random_74:230.1562,random_75:195.8779,random_76:277.9527,random_77:420.5475,random_78:144.1325,random_79:149.9306,random_80:167.8114,random_81:196.7049,random_82:311.8788,random_83:145.1887,random_84:173.148,random_85:155.8857,random_86:425.2438,random_87:425.2128,random_88:425.8024,random_89:167.0084,random_90:150.1219,random_91:154.9676,random_92:144.9818,random_93:140.636,random_94:154.5933,random_95:179.8454,random_96:156.0284,random_97:166.0928,random_98:154.0451,random_99:199.6666'\n",
      "Testing (test) took 0:00:03.245998'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Tested (test) on 2319 instances with mean losses of: random_0:371.9671,random_1:113.4009,random_2:122.4333,random_3:127.569,random_4:100.6992,random_5:111.7265,random_6:286.3153,random_7:141.5926,random_8:123.4986,random_9:123.1294,random_10:153.8905,random_11:101.9308,random_12:122.7807,random_13:151.5742,random_14:209.1268,random_15:112.3495,random_16:119.766,random_17:166.7848,random_18:116.0329,random_19:118.0121,random_20:185.166,random_21:272.8332,random_22:127.0966,random_23:165.8507,random_24:99.5915,random_25:112.4634,random_26:111.0955,random_27:128.9017,random_28:295.8908,random_29:167.9684,random_30:129.3721,random_31:139.8564,random_32:220.3935,random_33:134.1424,random_34:246.6501,random_35:109.1006,random_36:234.7988,random_37:618.7841,random_38:121.5641,random_39:119.0963,random_40:128.7887,random_41:313.6905,random_42:109.3426,random_43:119.0423,random_44:112.1612,random_45:129.1442,random_46:119.8399,random_47:132.8535,random_48:112.6637,random_49:286.9875,random_50:286.9506,random_51:364.7515,random_52:173.9695,random_53:125.1912,random_54:95.2822,random_55:614.375,random_56:286.5887,random_57:112.6001,random_58:115.5216,random_59:416.57,random_60:91.2359,random_61:139.7092,random_62:101.6695,random_63:280.7283,random_64:136.4569,random_65:113.6519,random_66:149.833,random_67:132.6062,random_68:254.6075,random_69:112.0024,random_70:122.5605,random_71:133.1364,random_72:116.0456,random_73:177.6075,random_74:120.0681,random_75:120.2018,random_76:145.2839,random_77:152.0867,random_78:135.107,random_79:176.3116,random_80:122.2772,random_81:123.642,random_82:173.8577,random_83:112.2898,random_84:105.9739,random_85:162.7769,random_86:98.2454,random_87:132.4666,random_88:178.4151,random_89:125.6558,random_90:259.6028,random_91:114.0838,random_92:127.7271,random_93:142.0107,random_94:107.6915,random_95:119.0637,random_96:123.7365,random_97:154.7584,random_98:122.5156,random_99:382.4909'\n",
      "Testing (test) took 0:00:03.114999'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Tested (test) on 2319 instances with mean losses of: random_0:132.7263,random_1:154.4692,random_2:150.6475,random_3:98.3469,random_4:238.1519,random_5:102.5186,random_6:652.5158,random_7:152.1863,random_8:98.8615,random_9:104.5608,random_10:131.9277,random_11:90.1242,random_12:175.8899,random_13:118.7002,random_14:137.3475,random_15:138.1492,random_16:184.6408,random_17:134.5732,random_18:142.6261,random_19:301.2543,random_20:123.9191,random_21:265.1102,random_22:301.2532,random_23:180.566,random_24:269.3766,random_25:110.2285,random_26:115.4113,random_27:112.2008,random_28:138.1199,random_29:112.5415,random_30:113.1606,random_31:199.7836,random_32:133.9257,random_33:111.7785,random_34:301.2184,random_35:103.4447,random_36:100.0514,random_37:632.666,random_38:121.2985,random_39:120.0043,random_40:101.6431,random_41:193.5194,random_42:104.1106,random_43:115.3249,random_44:151.1668,random_45:125.5551,random_46:301.2137,random_47:222.7092,random_48:188.4908,random_49:301.2164,random_50:301.2164,random_51:221.3253,random_52:118.5142,random_53:141.4884,random_54:84.5037,random_55:218.2958,random_56:300.9698,random_57:117.8662,random_58:109.5492,random_59:93.5949,random_60:92.3547,random_61:157.1789,random_62:147.3776,random_63:141.012,random_64:130.2747,random_65:103.1081,random_66:137.7001,random_67:121.7576,random_68:301.2165,random_69:101.7996,random_70:138.5043,random_71:136.1738,random_72:153.7133,random_73:181.6184,random_74:158.2393,random_75:119.5885,random_76:160.8729,random_77:168.0055,random_78:93.715,random_79:160.4645,random_80:127.049,random_81:111.0703,random_82:182.2648,random_83:98.9586,random_84:128.2853,random_85:148.7304,random_86:119.3834,random_87:176.3162,random_88:120.9623,random_89:118.1637,random_90:301.1541,random_91:252.2651,random_92:103.2652,random_93:102.4211,random_94:88.8937,random_95:169.9611,random_96:137.0093,random_97:148.2565,random_98:93.7078,random_99:187.1852'\n",
      "Testing (test) took 0:00:03.048004'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Tested (test) on 2319 instances with mean losses of: random_0:131.181,random_1:149.1386,random_2:131.398,random_3:132.1252,random_4:308.2364,random_5:113.2364,random_6:114.9021,random_7:133.0031,random_8:93.2369,random_9:146.0128,random_10:182.6574,random_11:116.1241,random_12:178.9654,random_13:117.2437,random_14:178.6635,random_15:160.9478,random_16:124.6863,random_17:300.1424,random_18:124.9359,random_19:153.31,random_20:142.6337,random_21:142.0375,random_22:172.7292,random_23:327.5456,random_24:221.9408,random_25:120.1366,random_26:199.8059,random_27:165.255,random_28:151.0619,random_29:175.4141,random_30:130.1532,random_31:238.4924,random_32:284.5227,random_33:267.6836,random_34:332.2626,random_35:126.9717,random_36:182.0345,random_37:141.2801,random_38:460.9773,random_39:110.6135,random_40:143.5288,random_41:379.4247,random_42:131.0317,random_43:121.9506,random_44:172.5848,random_45:680.5763,random_46:379.9147,random_47:679.287,random_48:113.5835,random_49:379.9161,random_50:379.9104,random_51:379.6607,random_52:462.1618,random_53:140.7366,random_54:97.6628,random_55:131.6169,random_56:379.3989,random_57:132.2108,random_58:145.8706,random_59:91.3797,random_60:85.1889,random_61:153.2445,random_62:173.5303,random_63:372.9491,random_64:125.3909,random_65:128.3902,random_66:155.4098,random_67:124.0334,random_68:379.9163,random_69:116.7492,random_70:127.6425,random_71:199.1735,random_72:187.5428,random_73:256.4443,random_74:158.1838,random_75:157.7771,random_76:207.7831,random_77:219.8124,random_78:120.2909,random_79:130.9018,random_80:125.1442,random_81:128.7203,random_82:261.1393,random_83:104.6625,random_84:151.119,random_85:138.1568,random_86:105.0938,random_87:139.0809,random_88:267.9228,random_89:379.845,random_90:128.2963,random_91:215.2113,random_92:125.3586,random_93:141.2922,random_94:117.8819,random_95:196.2482,random_96:104.2254,random_97:137.8246,random_98:95.1172,random_99:176.3186'\n",
      "Testing (test) took 0:00:03.039993'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Tested (test) on 2320 instances with mean losses of: random_0:450.0807,random_1:129.8369,random_2:125.2324,random_3:118.1483,random_4:241.9763,random_5:105.1087,random_6:86.766,random_7:79.4621,random_8:119.8713,random_9:130.4838,random_10:171.446,random_11:107.6532,random_12:169.5322,random_13:148.053,random_14:136.603,random_15:114.6636,random_16:211.525,random_17:155.1232,random_18:116.8623,random_19:329.0694,random_20:103.5083,random_21:124.5659,random_22:163.3598,random_23:296.2377,random_24:103.9057,random_25:121.3532,random_26:124.1409,random_27:137.0158,random_28:146.1499,random_29:151.8844,random_30:243.8608,random_31:139.6761,random_32:180.9157,random_33:140.7028,random_34:328.9936,random_35:133.8532,random_36:159.8115,random_37:651.9464,random_38:117.5037,random_39:117.8616,random_40:118.1536,random_41:328.7599,random_42:112.6944,random_43:94.818,random_44:242.5357,random_45:151.8369,random_46:328.939,random_47:131.1522,random_48:75.4515,random_49:328.9951,random_50:328.9833,random_51:670.4709,random_52:122.1249,random_53:151.4448,random_54:102.9354,random_55:647.586,random_56:328.6644,random_57:124.0119,random_58:120.0591,random_59:108.7136,random_60:112.6696,random_61:127.1754,random_62:125.9501,random_63:121.2932,random_64:147.4002,random_65:110.3283,random_66:143.1157,random_67:133.156,random_68:328.9958,random_69:109.3643,random_70:113.9639,random_71:221.1337,random_72:136.8657,random_73:211.8931,random_74:128.6438,random_75:147.5407,random_76:225.2088,random_77:125.3139,random_78:109.9104,random_79:126.9887,random_80:137.6375,random_81:136.2876,random_82:197.5841,random_83:115.4667,random_84:116.7414,random_85:136.8461,random_86:132.746,random_87:173.9728,random_88:328.9964,random_89:150.7682,random_90:111.268,random_91:193.5796,random_92:117.1439,random_93:132140.6576,random_94:105.4988,random_95:164.1177,random_96:82.0204,random_97:144.4811,random_98:97.5156,random_99:174.3494'\n",
      "Testing (test) took 0:00:03.081005'\n"
     ]
    }
   ],
   "source": [
    "deep_scheme = DeepScheme(None, fixed_hyperparams=fixed_hyperparams,loss_eval=loss_target,device=device,tensorboard=tb,adaptive_lr=True,update=False)\n",
    "deep_scores, deep_preds, _ , _, _,_ = eval.evaluate(deep_models,dataset,deep_scheme,logger_name=\"log\",load_fun=load_fun_cv,load_fun_pp = load_fun_pp_cv)\n",
    "deep_scores_final, deep_preds_final, _ ,_, _,_ = eval.build(deep_models,dataset,deep_scheme,logger_name=\"test_log\",load_fun=load_fun_build,load_fun_pp = load_fun_pp_build)\n",
    "\n",
    "all_scores = []\n",
    "for k,v in ut.flip_dicts(deep_scores).items():\n",
    "    dict1 = {'model_num':k,\"predictor\":\"deep\"}\n",
    "    all_scores.append({**dict1,**v})\n",
    "    \n",
    "\n",
    "\n",
    "all_scores_final = []\n",
    "for k,v in ut.flip_dicts(deep_scores_final).items():\n",
    "    dict1 = {'model_num':k,\"predictor\":\"deep\"}\n",
    "    all_scores_final.append({**dict1,**v})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - random_60 - deep - 115.49591637973128 - 147.70492251035517 - 91.23585914259792 - 92.35466049806267 - 85.18894709940355 - 106.39684576148638 - 0.6937172342672797\n",
      "1 - random_54 - deep - 122.67828775603196 - 141.09928568337898 - 95.2822361691546 - 84.50372972443166 - 97.66279256277015 - 108.24650952239685 - 0.6883926390848507\n",
      "2 - random_83 - deep - 112.60841353843952 - 145.18868143427932 - 112.28975962728917 - 98.95855506692996 - 104.6625034616856 - 114.7413996438966 - 0.669695910856624\n",
      "3 - random_11 - deep - 128.6319887884732 - 154.07143070143633 - 101.93082692490512 - 90.12416169765747 - 116.12413431540189 - 118.17740749700282 - 0.6598047342827631\n",
      "4 - random_69 - deep - 105.78770186325599 - 155.21988306543136 - 112.0024343822063 - 101.79959000837911 - 116.74917532463122 - 118.31067885912228 - 0.6594210883100767\n",
      "5 - random_5 - deep - 126.03696396926354 - 143.111247448841 - 111.72653537929547 - 102.51857789645373 - 113.2364223393041 - 119.32652774906697 - 0.656496781622675\n",
      "6 - random_98 - deep - 132.19047654250573 - 154.04513381359237 - 122.51563661187926 - 93.70780312002296 - 95.11717935901821 - 119.51633850293945 - 0.6559503766776242\n",
      "7 - random_94 - deep - 129.63758139774717 - 154.59333669013532 - 107.69154795473534 - 88.8937071709347 - 117.88189181072437 - 119.74046618949896 - 0.6553051841700324\n",
      "8 - random_78 - deep - 113.42071264858903 - 144.13250533050893 - 135.1070058509264 - 93.71504827139962 - 120.29090617164654 - 121.33255158960507 - 0.6507220753744922\n",
      "9 - random_65 - deep - 111.6635530406031 - 151.77320405829306 - 113.65190112667898 - 103.10806514251432 - 128.39023047904095 - 121.71652502480975 - 0.6496167376658348\n",
      "10 - random_43 - deep - 118.10001007606243 - 134.58910893962116 - 119.04225647578006 - 115.32494832771305 - 121.95063146448896 - 121.80107345459761 - 0.6493733495585234\n",
      "11 - random_25 - deep - 104.98873572513975 - 161.6788170252344 - 112.46336744735343 - 110.22852998727762 - 120.13657486402768 - 121.89774761500303 - 0.6490950553195172\n",
      "12 - random_39 - deep - 103.30295160227809 - 159.53693733692376 - 119.09634432149068 - 120.00433273035785 - 110.6135246043269 - 122.50916155338831 - 0.6473349885551699\n",
      "13 - random_35 - deep - 119.64489496165308 - 155.1154432440688 - 109.10057468258034 - 103.44470150607472 - 126.97173451452021 - 122.85519191067029 - 0.6463388769308329\n",
      "14 - random_92 - deep - 113.81454588791419 - 144.9818302629939 - 127.72709860067627 - 103.26520935427068 - 125.35857681202035 - 123.02865738954621 - 0.6458395248471408\n",
      "15 - random_8 - deep - 126.88925126174401 - 183.6903170790843 - 123.49864082616071 - 98.86146795693118 - 93.2369432338305 - 125.2354695153966 - 0.6394868128233917\n",
      "16 - random_3 - deep - 106.07641762042867 - 171.13481908669087 - 127.56898812396406 - 98.34691881835332 - 132.12519304769827 - 127.0486583358101 - 0.634267217423385\n",
      "17 - random_40 - deep - 109.13796973392881 - 155.0450106049571 - 128.78865159015402 - 101.64310977437364 - 143.52883310058908 - 127.62712073180771 - 0.6326020076960619\n",
      "18 - random_57 - deep - 125.52663224318931 - 159.43750461909008 - 112.60013957679554 - 117.86615945635916 - 132.21083750854334 - 129.52790872013682 - 0.6271302420814865\n",
      "19 - random_67 - deep - 122.0244542089002 - 149.93178301404498 - 132.60622681374693 - 121.75761722911787 - 124.03339718271299 - 130.0700032775867 - 0.6255697238240505\n",
      "20 - random_64 - deep - 111.43445726591965 - 147.4580487526404 - 136.4568878969996 - 130.2746971002063 - 125.39094962925682 - 130.20139052955597 - 0.6251915016067737\n",
      "21 - random_96 - deep - 131.4955086017477 - 156.02835883123703 - 123.7364508368528 - 137.0092915760746 - 104.22535886184673 - 130.49907995979936 - 0.62433454817576\n",
      "22 - random_93 - deep - 128.16660482472386 - 140.63603488121015 - 142.0107156606315 - 102.4210575148174 - 141.2921973389251 - 130.90508540388217 - 0.6231657873029518\n",
      "23 - random_42 - deep - 128.50389485852472 - 187.04285322010853 - 109.34258421272386 - 104.11064792121469 - 131.03169601854964 - 132.0060341593891 - 0.619996505099581\n",
      "24 - random_18 - deep - 113.13415971953293 - 169.77488611771557 - 116.03291010229482 - 142.6260836997285 - 124.93593408239973 - 133.29905802642386 - 0.6162742995838121\n",
      "25 - random_48 - deep - 123.49070792362608 - 140.29725822655996 - 112.66368087235583 - 188.49079118426783 - 113.58350256508206 - 135.70413576926856 - 0.6093508437462959\n",
      "26 - random_80 - deep - 137.36338040582064 - 167.81139590587014 - 122.2772018840571 - 127.04902627047612 - 125.14419715555643 - 135.92916416449458 - 0.6087030583841928\n",
      "27 - random_58 - deep - 143.48919285741346 - 167.66582944013786 - 115.52159550951801 - 109.54921443753327 - 145.8706377201319 - 136.41990259977865 - 0.6072903781103143\n",
      "28 - random_81 - deep - 129.22057884479392 - 196.70493244600482 - 123.64199983071232 - 111.07034724680912 - 128.72026506812986 - 137.87087761490125 - 0.6031134813474435\n",
      "29 - random_70 - deep - 142.49576055592505 - 159.53031665428594 - 122.56052303931075 - 138.5042744431325 - 127.64246337482672 - 138.14704203603563 - 0.602318491589085\n",
      "30 - random_84 - deep - 135.91391396358097 - 173.14800572693477 - 105.97390447026854 - 128.28531792802306 - 151.119014228403 - 138.8877751387994 - 0.600186155975557\n",
      "31 - random_13 - deep - 133.31610541508115 - 177.11568882240206 - 151.5741532174818 - 118.70020584198564 - 117.24367835080639 - 139.58942377366466 - 0.5981663321459966\n",
      "32 - random_2 - deep - 117.45002544008452 - 178.69123460671779 - 122.4333360843897 - 150.64754567674748 - 131.39804097217134 - 140.1220841832548 - 0.5966329718073704\n",
      "33 - random_7 - deep - 121.41075689381566 - 153.31234467487081 - 141.59264125388057 - 152.18629711195538 - 133.0030527694722 - 140.29938825964882 - 0.5961225696191759\n",
      "34 - random_15 - deep - 124.42538407424401 - 167.37744901261487 - 112.34945013154011 - 138.14923672431397 - 160.94782490666543 - 140.64846742844173 - 0.5951176814337005\n",
      "35 - random_97 - deep - 108.9950623084759 - 166.09283913122573 - 154.75843123764557 - 148.2564866023621 - 137.8246273190464 - 143.18253905036906 - 0.5878228931401771\n",
      "36 - random_85 - deep - 110.71670815040325 - 155.88570347136596 - 162.7768893058879 - 148.73040027297637 - 138.15683285744862 - 143.2505027105856 - 0.5876272473231281\n",
      "37 - random_9 - deep - 140.31542716190734 - 203.05215714665619 - 123.12938931009901 - 104.5608378414863 - 146.01282507142614 - 143.41385808324125 - 0.5871569990279255\n",
      "38 - random_20 - deep - 109.88826474156873 - 160.00942195680133 - 185.16602704441718 - 123.91914261328141 - 142.63371799628555 - 144.32034711806847 - 0.5845475046702107\n",
      "39 - random_62 - deep - 126.25941372575431 - 180.28556920455006 - 101.66947748168577 - 147.3776136384333 - 173.5302776848257 - 145.82278322524152 - 0.5802224677487849\n",
      "40 - random_26 - deep - 127.55148089178677 - 176.5231567817086 - 111.09548164884198 - 115.41127540716687 - 199.8058858314636 - 146.0758597117585 - 0.5794939408297999\n",
      "41 - random_66 - deep - 122.89265562912513 - 167.63739190013206 - 149.8330066445677 - 137.7001438149094 - 155.40982283986608 - 146.69255220626422 - 0.577718678776945\n",
      "42 - random_27 - deep - 130.86780474432584 - 196.8147749347695 - 128.90168632582993 - 112.20077126105318 - 165.25500759044152 - 146.80663487674798 - 0.5773902709600792\n",
      "43 - random_53 - deep - 150.13307881848564 - 177.74225764173843 - 125.19118338548299 - 141.48835254157603 - 140.73660352344191 - 147.05855864260533 - 0.5766650623581855\n",
      "44 - random_79 - deep - 125.28059942311253 - 149.93058366927673 - 176.31156317680856 - 160.46449549396576 - 130.90179721360167 - 148.5757991521192 - 0.5722974082589523\n",
      "45 - random_75 - deep - 151.28967648210195 - 195.87792877947373 - 120.2017876516904 - 119.58846613352235 - 157.77709674711832 - 148.94719411663385 - 0.5712282799770316\n",
      "46 - random_1 - deep - 165.13981959902006 - 165.56575204998114 - 113.40091965557328 - 154.46922518231324 - 149.1386085368786 - 149.5442116674082 - 0.5695096558455226\n",
      "47 - random_30 - deep - 217.14801433168608 - 179.66234828460418 - 129.3720605545077 - 113.16058784376295 - 130.15322198575814 - 153.90470524256912 - 0.5569571781606673\n",
      "48 - random_95 - deep - 133.55443096818595 - 179.84540387770903 - 119.06374822355849 - 169.9610668575522 - 196.2481991009961 - 159.7323121908573 - 0.5401813464999601\n",
      "49 - random_12 - deep - 164.4251945232523 - 162.24008735370512 - 122.78074503870245 - 175.88987395042693 - 178.96537085924234 - 160.8605617810045 - 0.5369334738545459\n",
      "50 - random_16 - deep - 196.1024364602977 - 184.1957801861617 - 119.7660444653615 - 184.64077214302304 - 124.68631976984246 - 161.88122236530958 - 0.5339953158255369\n",
      "51 - random_10 - deep - 120.27718469027815 - 227.762444565655 - 153.8904681285942 - 131.92766621545658 - 182.65744318927142 - 163.29932771346478 - 0.5299130404063294\n",
      "52 - random_74 - deep - 160.71764410610857 - 230.15619393924845 - 120.06812791055314 - 158.2393201077072 - 158.18377441432568 - 165.47260122569256 - 0.5236568754114481\n",
      "53 - random_14 - deep - 114.39822187752559 - 206.69627578426096 - 209.12676565047738 - 137.3474888014043 - 178.66349682596132 - 169.24171897226236 - 0.51280678112971\n",
      "54 - random_29 - deep - 153.69930041082975 - 238.88355704205156 - 167.96837913378295 - 112.54150763211861 - 175.41405130265446 - 169.69997972367014 - 0.5114875938045277\n",
      "55 - random_61 - deep - 165.47713788788894 - 234.12488623352183 - 139.70916953339767 - 157.17886245615682 - 153.24453567235108 - 169.94653132752322 - 0.5107778499527857\n",
      "56 - random_24 - deep - 106.17067202864023 - 164.6707920931495 - 99.59146685186981 - 269.3765573637297 - 221.94080679146265 - 172.3443520156352 - 0.5038752848736252\n",
      "57 - random_59 - deep - 135.3920064991918 - 131.54895198175547 - 416.56997455631466 - 93.59493808820362 - 91.37970112828974 - 173.69381277451902 - 0.4999906154501983\n",
      "58 - random_91 - deep - 136.05235290527344 - 154.96760158366095 - 114.08383511818396 - 252.2651417326341 - 215.2113026417037 - 174.51272758021884 - 0.497633219516192\n",
      "59 - random_86 - deep - 125.9085180611446 - 425.243785678852 - 98.24538721674318 - 119.38342576645839 - 105.09383507482242 - 174.77077649247045 - 0.49689037856096374\n",
      "60 - random_72 - deep - 244.82405421816068 - 178.42599792612066 - 116.04557136216519 - 153.7132869389819 - 187.54282474229953 - 176.11627256473528 - 0.4930171222125602\n",
      "61 - random_36 - deep - 161.4445924430058 - 219.51129758373423 - 234.7987843308278 - 100.05135039987108 - 182.03454765856085 - 179.5665499750281 - 0.48308486811041706\n",
      "62 - random_19 - deep - 135.01129034634295 - 208.69751230546657 - 118.012085852514 - 301.2543086767094 - 153.30997129492124 - 183.2528772137004 - 0.4724731014365491\n",
      "63 - random_33 - deep - 152.23486570160964 - 252.27530315180388 - 134.1424307286508 - 111.7785384329088 - 267.6836321437189 - 183.6202500322285 - 0.4714155516369427\n",
      "64 - random_89 - deep - 132.88629676555766 - 167.00844580225515 - 125.65583798966566 - 118.16372270078277 - 379.8450143903738 - 184.70738849322066 - 0.4682860248903178\n",
      "65 - random_4 - deep - 124.98053483634159 - 160.6519472386211 - 100.69921731229176 - 238.1518584560248 - 308.2364429640019 - 186.5386923837313 - 0.4630142819503056\n",
      "66 - random_44 - deep - 230.59168801143252 - 284.52646963654945 - 112.1612001084315 - 151.16683218077165 - 172.58480174338936 - 190.20968048443197 - 0.4524466717884872\n",
      "67 - random_31 - deep - 187.31683349609375 - 208.46300873421245 - 139.8563715880058 - 199.78356264582376 - 238.49235473474894 - 194.78178557749973 - 0.4392850316749759\n",
      "68 - random_28 - deep - 205.38525124911604 - 195.75110245573464 - 295.89078914877564 - 138.1199227465078 - 151.06189736666067 - 197.2424956104556 - 0.43220142812295415\n",
      "69 - random_76 - deep - 195.00365316456762 - 277.95266376214073 - 145.2838759823265 - 160.8728712336058 - 207.78310575563367 - 197.3790269503834 - 0.43180839770834933\n",
      "70 - random_87 - deep - 133.2781880872003 - 425.212820851736 - 132.46663454556682 - 176.31623100257733 - 139.08093381472938 - 201.2650998341275 - 0.4206216266894014\n",
      "71 - random_71 - deep - 269.863853059966 - 329.2487964177759 - 133.1363894078895 - 136.17379219298633 - 199.1735166950234 - 213.5241315721377 - 0.3853317633569857\n",
      "72 - random_73 - deep - 151.45242688409212 - 301.6401112282158 - 177.60751333232582 - 181.61841851218398 - 256.4443052521344 - 213.74718423820102 - 0.38468966549236405\n",
      "73 - random_32 - deep - 221.49887692681673 - 212.30369267169644 - 220.3934501657819 - 133.92565447752617 - 284.52272228785455 - 214.5294813059959 - 0.38243768040921655\n",
      "74 - random_38 - deep - 165.66735621485216 - 206.45037193676683 - 121.56405814344794 - 121.29849775807001 - 460.9773030677917 - 215.18724582856646 - 0.38054418501720655\n",
      "75 - random_63 - deep - 162.3539414241396 - 131.34509967575303 - 280.72828952646194 - 141.01198715565275 - 372.94907388033255 - 217.67290385125438 - 0.3733887641172182\n",
      "76 - random_77 - deep - 156.86845471612338 - 420.54748177209666 - 152.08674195862474 - 168.0054655515104 - 219.81237106685137 - 223.45835612030513 - 0.356734282404933\n",
      "77 - random_23 - deep - 241.66924393752527 - 207.23157748953955 - 165.85073224487567 - 180.56599973552747 - 327.54564692738654 - 224.57411319500324 - 0.3535223717483852\n",
      "78 - random_17 - deep - 161.4822677349222 - 362.5226207501213 - 166.78477221740224 - 134.5731945819205 - 300.14240592571616 - 225.09556787667265 - 0.35202126914555043\n",
      "79 - random_99 - deep - 183.26589316006365 - 199.6665765615104 - 382.49085117486493 - 187.18516304637913 - 176.3185925074516 - 225.78174585865295 - 0.35004598041775337\n",
      "80 - random_82 - deep - 226.98311020423625 - 311.878838990258 - 173.85766759973603 - 182.2648378432234 - 261.13926689868447 - 231.22437915436313 - 0.33437836577421365\n",
      "81 - random_22 - deep - 134.3421373301539 - 425.3072456863226 - 127.09661263831477 - 301.25317775632556 - 172.72920147433987 - 232.13723906102805 - 0.3317505316975661\n",
      "82 - random_90 - deep - 324.4493117233803 - 150.12187324448257 - 259.60278391375425 - 301.15412375782006 - 128.29626402663692 - 232.73277761950038 - 0.33003616511570066\n",
      "83 - random_52 - deep - 195.80513800259294 - 235.44244986168522 - 173.9695132585783 - 118.51421182985294 - 462.1618255707353 - 237.1750556136133 - 0.31724825603387885\n",
      "84 - random_0 - deep - 173.71247653303476 - 417.41226105486436 - 371.9671009321982 - 132.7262662087293 - 131.1809832501792 - 245.39363574541528 - 0.29358957108745465\n",
      "85 - random_6 - deep - 118.52910473264497 - 160.92647351474275 - 286.3153218889915 - 652.5157805751654 - 114.90207127303387 - 266.6249776320098 - 0.23247127320275351\n",
      "86 - random_88 - deep - 343.5871689368939 - 425.80236600585516 - 178.41506261196358 - 120.96233309523306 - 267.9228046708397 - 267.34451988097356 - 0.23039993933475333\n",
      "87 - random_47 - deep - 165.98947293511753 - 204.92318539374756 - 132.85353211423043 - 222.70919343488595 - 679.2869813696174 - 281.14253696621256 - 0.19067982541363082\n",
      "88 - random_21 - deep - 317.37327517805426 - 424.6596329899994 - 272.83320959830604 - 265.11021623039824 - 142.0375128248839 - 284.4056102943801 - 0.18128647247551588\n",
      "89 - random_45 - deep - 211.80033553550984 - 283.727095182418 - 129.14418567034025 - 125.55511441051883 - 680.5763326482868 - 286.15419837039724 - 0.17625284212475922\n",
      "90 - random_51 - deep - 115.55259533586174 - 425.26405885210323 - 364.75147534527105 - 221.32528613486542 - 379.6607437791779 - 301.2948128979279 - 0.13266781609136813\n",
      "91 - random_50 - deep - 121.82150527691019 - 425.3134516378372 - 286.9505926068871 - 301.21643222678887 - 379.9104133596087 - 303.02684695729516 - 0.12768183950296907\n",
      "92 - random_46 - deep - 343.55128973599136 - 425.205128869192 - 119.83991698429162 - 301.21374105080906 - 379.91472014130676 - 313.94751212117444 - 0.09624470895558335\n",
      "93 - random_68 - deep - 303.4639804182381 - 384.2863908498747 - 254.60753367737792 - 301.2164983416282 - 379.91632844991545 - 324.696313285115 - 0.06530232034208061\n",
      "94 - random_37 - deep - 105.30901952283136 - 148.89699438308116 - 618.7841123356394 - 632.6660194150227 - 141.2800672629004 - 329.36791645634486 - 0.051854256826198264\n",
      "95 - random_34 - deep - 343.5969034129176 - 425.33133464910287 - 246.65008929517055 - 301.2183632400335 - 332.26264000514703 - 329.81305335929585 - 0.05057284889732816\n",
      "96 - random_56 - deep - 343.35131583378234 - 424.81380903170816 - 286.58872546960487 - 300.96975005603446 - 379.39886416377254 - 347.02417345510145 - 0.0010274941777101976\n",
      "97 - random_49 - deep - 343.65586084826236 - 425.3303865380922 - 286.98748146310453 - 301.21640717052804 - 379.9160946825035 - 347.42092611385885 - -0.0001146308614894398\n",
      "98 - random_55 - deep - 102.4699027225889 - 751.1347900495904 - 614.3749610996288 - 218.2957524031902 - 131.616879399453 - 363.5559390903447 - -0.046562214567587024\n",
      "99 - random_41 - deep - 670.9764976764548 - 424.10509601180127 - 313.6904936901913 - 193.51943184086454 - 379.4247473008985 - 396.36693608971865 - -0.14101466600533397\n"
     ]
    }
   ],
   "source": [
    "scores_df_sorted = pd.DataFrame(all_scores).sort_values(by='MSE')\n",
    "\n",
    "for i,(index,row) in enumerate(scores_df_sorted.iterrows()):\n",
    "    s = f\"{i} - \" + \" - \".join([f\"{i}\" for i in row.tolist()])\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% lwr part\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73d276e7db7494390535e90d38520dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_0'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:48.7856,lwr_k=200:76.6793,lwr_k=300:90.7711,lwr_k=400:95.5119,lwr_k=500:99.7895,lwr_k=600:102.7974,lwr_k=700:104.9276,lwr_k=800:107.087,lwr_k=900:108.6229,lwr_k=1000:110.6536'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:290.9611,lwr_k=200:174.8763,lwr_k=300:123.4346,lwr_k=400:117.119,lwr_k=500:118.6586,lwr_k=600:122.5327,lwr_k=700:118.4797,lwr_k=800:116.0429,lwr_k=900:116.1993,lwr_k=1000:117.0096'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:66.7419,lwr_k=200:86.7079,lwr_k=300:90.1292,lwr_k=400:103.2727,lwr_k=500:104.8479,lwr_k=600:106.1255,lwr_k=700:153.0305,lwr_k=800:109.4459,lwr_k=900:103.529,lwr_k=1000:106.5148'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:359.1974,lwr_k=200:243.78,lwr_k=300:241.539,lwr_k=400:228.9006,lwr_k=500:206.4749,lwr_k=600:193.5907,lwr_k=700:220.1271,lwr_k=800:190.8496,lwr_k=900:180.0488,lwr_k=1000:175.8591'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:83.1192,lwr_k=200:114.3701,lwr_k=300:165.5645,lwr_k=400:135.4355,lwr_k=500:168.529,lwr_k=600:137.0875,lwr_k=700:117.5092,lwr_k=800:112.917,lwr_k=900:113.7138,lwr_k=1000:115.3414'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:247.6129,lwr_k=200:250.0673,lwr_k=300:234.2155,lwr_k=400:176.4047,lwr_k=500:177.1454,lwr_k=600:169.4413,lwr_k=700:136.4369,lwr_k=800:126.9784,lwr_k=900:129.1493,lwr_k=1000:121.4761'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:44.2245,lwr_k=200:73.9512,lwr_k=300:86.1042,lwr_k=400:94.4184,lwr_k=500:98.8794,lwr_k=600:103.4335,lwr_k=700:106.5073,lwr_k=800:109.4369,lwr_k=900:111.9221,lwr_k=1000:113.5135'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:176.5367,lwr_k=200:131.3077,lwr_k=300:113.8139,lwr_k=400:107.2359,lwr_k=500:105.2403,lwr_k=600:101.9074,lwr_k=700:101.7194,lwr_k=800:100.2225,lwr_k=900:100.4158,lwr_k=1000:100.3835'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:93.3548,lwr_k=200:93.95,lwr_k=300:95.4284,lwr_k=400:98.8759,lwr_k=500:101.6949,lwr_k=600:104.2038,lwr_k=700:106.0056,lwr_k=800:108.0155,lwr_k=900:108.6162,lwr_k=1000:110.0912'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:264.2883,lwr_k=200:160.3725,lwr_k=300:126.8979,lwr_k=400:120.9548,lwr_k=500:118.4404,lwr_k=600:115.5365,lwr_k=700:115.925,lwr_k=800:117.1573,lwr_k=900:116.7073,lwr_k=1000:116.792'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:89.7212,lwr_k=200:131.5041,lwr_k=300:145.3496,lwr_k=400:125.6563,lwr_k=500:123.9271,lwr_k=600:126.4465,lwr_k=700:128.1762,lwr_k=800:128.9236,lwr_k=900:132.2718,lwr_k=1000:132.1951'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:420.1397,lwr_k=200:251.3775,lwr_k=300:267.0442,lwr_k=400:165.109,lwr_k=500:162.4911,lwr_k=600:160.537,lwr_k=700:153.0528,lwr_k=800:147.3812,lwr_k=900:151.7407,lwr_k=1000:148.3552'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_1'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:103.2731,lwr_k=200:112.8284,lwr_k=300:117.786,lwr_k=400:120.865,lwr_k=500:123.0109,lwr_k=600:124.4096,lwr_k=700:125.8204,lwr_k=800:127.0741,lwr_k=900:128.3667,lwr_k=1000:129.2523'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:209.8304,lwr_k=200:168.4297,lwr_k=300:154.4334,lwr_k=400:139.0022,lwr_k=500:137.3535,lwr_k=600:138.4413,lwr_k=700:137.1722,lwr_k=800:136.8404,lwr_k=900:136.2962,lwr_k=1000:136.915'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:75.0447,lwr_k=200:84.3575,lwr_k=300:86.5827,lwr_k=400:89.3429,lwr_k=500:90.127,lwr_k=600:90.8033,lwr_k=700:91.2646,lwr_k=800:91.5054,lwr_k=900:92.0928,lwr_k=1000:92.4672'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:191.6585,lwr_k=200:182.5307,lwr_k=300:188.7411,lwr_k=400:185.2614,lwr_k=500:184.9387,lwr_k=600:184.8152,lwr_k=700:185.3258,lwr_k=800:186.3264,lwr_k=900:184.8047,lwr_k=1000:184.4281'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:83.5068,lwr_k=200:91.7426,lwr_k=300:96.6012,lwr_k=400:98.8485,lwr_k=500:99.5531,lwr_k=600:100.8582,lwr_k=700:102.6148,lwr_k=800:102.9196,lwr_k=900:103.5445,lwr_k=1000:104.4797'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:180.5576,lwr_k=200:141.019,lwr_k=300:145.8964,lwr_k=400:141.5597,lwr_k=500:140.2084,lwr_k=600:138.9041,lwr_k=700:107.901,lwr_k=800:108.2593,lwr_k=900:107.656,lwr_k=1000:107.6002'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:113.1907,lwr_k=200:122.0059,lwr_k=300:128.9187,lwr_k=400:138.8655,lwr_k=500:141.3812,lwr_k=600:143.4179,lwr_k=700:144.1778,lwr_k=800:144.8732,lwr_k=900:145.2401,lwr_k=1000:145.9079'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:211.6752,lwr_k=200:168.8425,lwr_k=300:181.0893,lwr_k=400:161.5669,lwr_k=500:162.5583,lwr_k=600:162.5173,lwr_k=700:158.5263,lwr_k=800:159.3673,lwr_k=900:159.6561,lwr_k=1000:159.3635'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:98.3903,lwr_k=200:106.884,lwr_k=300:111.6456,lwr_k=400:114.4403,lwr_k=500:117.6805,lwr_k=600:120.385,lwr_k=700:121.6418,lwr_k=800:122.2629,lwr_k=900:123.9145,lwr_k=1000:124.944'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:156.6165,lwr_k=200:123.8554,lwr_k=300:122.0081,lwr_k=400:123.0964,lwr_k=500:123.5563,lwr_k=600:126.4127,lwr_k=700:125.8451,lwr_k=800:126.1486,lwr_k=900:128.6465,lwr_k=1000:128.9863'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:96.975,lwr_k=200:129.1574,lwr_k=300:107.647,lwr_k=400:109.1103,lwr_k=500:110.6254,lwr_k=600:111.7755,lwr_k=700:113.1366,lwr_k=800:113.8619,lwr_k=900:114.2117,lwr_k=1000:114.5038'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:188.1327,lwr_k=200:209.1018,lwr_k=300:125.1934,lwr_k=400:124.322,lwr_k=500:120.7429,lwr_k=600:121.8621,lwr_k=700:124.7373,lwr_k=800:124.2952,lwr_k=900:124.5311,lwr_k=1000:124.9389'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_2'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:24.1998,lwr_k=200:52.6374,lwr_k=300:62.9934,lwr_k=400:70.4378,lwr_k=500:74.2041,lwr_k=600:76.6587,lwr_k=700:78.4264,lwr_k=800:79.8566,lwr_k=900:81.063,lwr_k=1000:82.1335'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:229.6508,lwr_k=200:135.9093,lwr_k=300:129.4083,lwr_k=400:122.3167,lwr_k=500:117.244,lwr_k=600:117.2863,lwr_k=700:118.6504,lwr_k=800:119.1933,lwr_k=900:116.7841,lwr_k=1000:114.9159'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:28.4596,lwr_k=200:46.7983,lwr_k=300:54.7197,lwr_k=400:59.5997,lwr_k=500:62.2302,lwr_k=600:64.7674,lwr_k=700:66.6877,lwr_k=800:68.1601,lwr_k=900:69.2408,lwr_k=1000:70.3927'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:259.4604,lwr_k=200:167.5222,lwr_k=300:157.7453,lwr_k=400:148.0558,lwr_k=500:145.4921,lwr_k=600:143.8567,lwr_k=700:143.1625,lwr_k=800:141.6385,lwr_k=900:142.3062,lwr_k=1000:141.1703'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:62.4389,lwr_k=200:52.8734,lwr_k=300:51.1849,lwr_k=400:53.6954,lwr_k=500:54.9938,lwr_k=600:57.6275,lwr_k=700:57.9728,lwr_k=800:59.8147,lwr_k=900:61.634,lwr_k=1000:62.7775'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:224.1392,lwr_k=200:117.2359,lwr_k=300:102.6569,lwr_k=400:93.9934,lwr_k=500:92.3677,lwr_k=600:91.2037,lwr_k=700:90.7695,lwr_k=800:89.2144,lwr_k=900:89.2875,lwr_k=1000:89.5398'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:25.1337,lwr_k=200:42.1582,lwr_k=300:49.2844,lwr_k=400:53.995,lwr_k=500:56.1861,lwr_k=600:58.9971,lwr_k=700:60.3674,lwr_k=800:61.3743,lwr_k=900:62.3866,lwr_k=1000:63.106'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:227.5855,lwr_k=200:138.6853,lwr_k=300:115.2248,lwr_k=400:108.2792,lwr_k=500:105.3413,lwr_k=600:100.4493,lwr_k=700:98.7642,lwr_k=800:97.602,lwr_k=900:97.8593,lwr_k=1000:97.5364'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:37.085,lwr_k=200:59.4243,lwr_k=300:67.1202,lwr_k=400:73.224,lwr_k=500:77.3897,lwr_k=600:80.4703,lwr_k=700:83.0943,lwr_k=800:84.6262,lwr_k=900:86.3042,lwr_k=1000:86.7519'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:233.5528,lwr_k=200:137.3755,lwr_k=300:120.1562,lwr_k=400:106.7297,lwr_k=500:102.5752,lwr_k=600:103.3215,lwr_k=700:102.4644,lwr_k=800:102.6497,lwr_k=900:99.1691,lwr_k=1000:98.4678'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:33.2457,lwr_k=200:55.5717,lwr_k=300:66.277,lwr_k=400:70.5818,lwr_k=500:72.5986,lwr_k=600:74.7269,lwr_k=700:76.4167,lwr_k=800:77.4631,lwr_k=900:78.6858,lwr_k=1000:79.557'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:191.9771,lwr_k=200:123.0998,lwr_k=300:129.5009,lwr_k=400:131.3007,lwr_k=500:129.7817,lwr_k=600:128.2715,lwr_k=700:124.3798,lwr_k=800:123.7211,lwr_k=900:123.6516,lwr_k=1000:121.6231'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_3'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:95.0726,lwr_k=200:96.4273,lwr_k=300:96.665,lwr_k=400:96.6669,lwr_k=500:96.7438,lwr_k=600:96.8959,lwr_k=700:96.803,lwr_k=800:96.7417,lwr_k=900:96.7184,lwr_k=1000:96.6483'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:110.2667,lwr_k=200:110.409,lwr_k=300:110.4156,lwr_k=400:110.5262,lwr_k=500:110.9481,lwr_k=600:110.9763,lwr_k=700:111.2206,lwr_k=800:111.1731,lwr_k=900:111.3411,lwr_k=1000:111.3861'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:73.9577,lwr_k=200:75.6547,lwr_k=300:76.2039,lwr_k=400:76.4375,lwr_k=500:76.61,lwr_k=600:76.8505,lwr_k=700:76.8781,lwr_k=800:76.8875,lwr_k=900:76.9188,lwr_k=1000:76.9917'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:156.9223,lwr_k=200:158.4944,lwr_k=300:159.3464,lwr_k=400:159.6368,lwr_k=500:160.0898,lwr_k=600:160.2567,lwr_k=700:160.3765,lwr_k=800:160.5404,lwr_k=900:160.7089,lwr_k=1000:160.9021'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:81.5184,lwr_k=200:85.0175,lwr_k=300:86.2029,lwr_k=400:86.6015,lwr_k=500:86.7437,lwr_k=600:86.9265,lwr_k=700:87.1084,lwr_k=800:87.2928,lwr_k=900:87.3517,lwr_k=1000:87.4835'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:105.5841,lwr_k=200:104.7901,lwr_k=300:105.0173,lwr_k=400:105.1703,lwr_k=500:104.9949,lwr_k=600:105.1006,lwr_k=700:105.4176,lwr_k=800:106.501,lwr_k=900:107.1423,lwr_k=1000:107.6366'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:87.9911,lwr_k=200:89.4871,lwr_k=300:89.9491,lwr_k=400:89.7183,lwr_k=500:89.5958,lwr_k=600:89.6668,lwr_k=700:89.6757,lwr_k=800:89.8082,lwr_k=900:89.8796,lwr_k=1000:89.8743'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:128.1649,lwr_k=200:129.0766,lwr_k=300:129.5764,lwr_k=400:129.4362,lwr_k=500:129.6597,lwr_k=600:129.4173,lwr_k=700:129.3986,lwr_k=800:129.5509,lwr_k=900:129.5083,lwr_k=1000:129.5483'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:98.0143,lwr_k=200:99.2316,lwr_k=300:99.8918,lwr_k=400:100.2427,lwr_k=500:100.6109,lwr_k=600:100.6316,lwr_k=700:100.737,lwr_k=800:100.8754,lwr_k=900:100.8511,lwr_k=1000:100.8607'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:109.3504,lwr_k=200:106.874,lwr_k=300:106.03,lwr_k=400:106.3585,lwr_k=500:106.2755,lwr_k=600:106.365,lwr_k=700:106.2989,lwr_k=800:106.356,lwr_k=900:106.3931,lwr_k=1000:106.4313'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:97.1789,lwr_k=200:101.3462,lwr_k=300:103.0985,lwr_k=400:104.211,lwr_k=500:104.6859,lwr_k=600:105.2978,lwr_k=700:105.6283,lwr_k=800:105.7617,lwr_k=900:105.8398,lwr_k=1000:105.9843'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:104.6822,lwr_k=200:109.4661,lwr_k=300:107.7221,lwr_k=400:107.1446,lwr_k=500:106.175,lwr_k=600:104.7692,lwr_k=700:105.6083,lwr_k=800:105.0406,lwr_k=900:104.9797,lwr_k=1000:104.7732'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_4'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:38.7735,lwr_k=200:65.1439,lwr_k=300:76.0517,lwr_k=400:80.1462,lwr_k=500:83.4882,lwr_k=600:85.2302,lwr_k=700:86.5474,lwr_k=800:87.7565,lwr_k=900:88.4778,lwr_k=1000:89.1878'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:179.6835,lwr_k=200:123.2678,lwr_k=300:118.7007,lwr_k=400:112.7386,lwr_k=500:113.4492,lwr_k=600:112.6619,lwr_k=700:112.0571,lwr_k=800:111.3216,lwr_k=900:110.9339,lwr_k=1000:111.291'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:31.5906,lwr_k=200:51.3232,lwr_k=300:57.0467,lwr_k=400:60.5135,lwr_k=500:63.0449,lwr_k=600:64.5167,lwr_k=700:65.7718,lwr_k=800:66.5841,lwr_k=900:67.297,lwr_k=1000:68.0905'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:219.6731,lwr_k=200:151.9709,lwr_k=300:148.7318,lwr_k=400:146.716,lwr_k=500:144.7816,lwr_k=600:143.8391,lwr_k=700:143.4456,lwr_k=800:142.1577,lwr_k=900:141.8178,lwr_k=1000:141.7033'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:33.4053,lwr_k=200:46.7282,lwr_k=300:54.3608,lwr_k=400:56.7515,lwr_k=500:58.3146,lwr_k=600:59.5705,lwr_k=700:60.5666,lwr_k=800:61.247,lwr_k=900:61.8041,lwr_k=1000:62.3067'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:142.0784,lwr_k=200:106.9403,lwr_k=300:97.1147,lwr_k=400:98.0542,lwr_k=500:97.9034,lwr_k=600:95.5311,lwr_k=700:96.1279,lwr_k=800:95.9637,lwr_k=900:95.8032,lwr_k=1000:95.9425'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:70.6953,lwr_k=200:106.964,lwr_k=300:94.876,lwr_k=400:110.2894,lwr_k=500:102.7238,lwr_k=600:120.3466,lwr_k=700:110.7073,lwr_k=800:117.3423,lwr_k=900:122.6076,lwr_k=1000:125.2247'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:387.33,lwr_k=200:306.8407,lwr_k=300:128.7144,lwr_k=400:185.998,lwr_k=500:115.9757,lwr_k=600:126.0581,lwr_k=700:108.6324,lwr_k=800:110.4929,lwr_k=900:109.6325,lwr_k=1000:107.7244'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:46.9662,lwr_k=200:76.9324,lwr_k=300:85.2836,lwr_k=400:89.3011,lwr_k=500:94.2957,lwr_k=600:97.4896,lwr_k=700:99.8413,lwr_k=800:103.3742,lwr_k=900:105.184,lwr_k=1000:107.664'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:244.8884,lwr_k=200:146.1938,lwr_k=300:103.0489,lwr_k=400:99.7893,lwr_k=500:96.9963,lwr_k=600:94.2088,lwr_k=700:94.1157,lwr_k=800:95.1502,lwr_k=900:95.9078,lwr_k=1000:97.6277'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:23.4654,lwr_k=200:38.0278,lwr_k=300:44.2912,lwr_k=400:46.6476,lwr_k=500:48.4364,lwr_k=600:49.7057,lwr_k=700:50.7404,lwr_k=800:51.5667,lwr_k=900:52.0023,lwr_k=1000:52.4362'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:206.0543,lwr_k=200:155.3163,lwr_k=300:237.9385,lwr_k=400:221.3082,lwr_k=500:215.96,lwr_k=600:213.6851,lwr_k=700:210.7613,lwr_k=800:208.3403,lwr_k=900:207.4565,lwr_k=1000:207.0856'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_5'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:119.7579,lwr_k=200:106.2456,lwr_k=300:99.2136,lwr_k=400:106.3114,lwr_k=500:102.4508,lwr_k=600:99.8728,lwr_k=700:97.2957,lwr_k=800:92.967,lwr_k=900:91.7791,lwr_k=1000:90.9847'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:212.7505,lwr_k=200:203.3905,lwr_k=300:183.9659,lwr_k=400:176.1099,lwr_k=500:160.6723,lwr_k=600:150.5808,lwr_k=700:144.2554,lwr_k=800:138.6327,lwr_k=900:137.1334,lwr_k=1000:134.7877'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:65.0708,lwr_k=200:72.1562,lwr_k=300:71.7407,lwr_k=400:69.7695,lwr_k=500:71.0435,lwr_k=600:68.7596,lwr_k=700:69.2446,lwr_k=800:67.8039,lwr_k=900:66.781,lwr_k=1000:66.2368'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:183.659,lwr_k=200:160.9512,lwr_k=300:161.7384,lwr_k=400:157.6468,lwr_k=500:148.9203,lwr_k=600:150.4422,lwr_k=700:147.1548,lwr_k=800:142.8342,lwr_k=900:150.9099,lwr_k=1000:149.6188'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:50.5799,lwr_k=200:59.7823,lwr_k=300:62.8284,lwr_k=400:64.8199,lwr_k=500:64.0528,lwr_k=600:64.4517,lwr_k=700:65.2964,lwr_k=800:64.9888,lwr_k=900:65.9582,lwr_k=1000:65.9313'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:154.705,lwr_k=200:125.9312,lwr_k=300:120.1953,lwr_k=400:114.7352,lwr_k=500:107.7072,lwr_k=600:106.9194,lwr_k=700:103.5387,lwr_k=800:100.9769,lwr_k=900:101.171,lwr_k=1000:100.2475'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:73.1388,lwr_k=200:78.2996,lwr_k=300:78.6479,lwr_k=400:80.4536,lwr_k=500:80.7288,lwr_k=600:78.796,lwr_k=700:77.8641,lwr_k=800:79.5176,lwr_k=900:79.7198,lwr_k=1000:79.2026'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:144.7886,lwr_k=200:125.8873,lwr_k=300:112.3666,lwr_k=400:106.2434,lwr_k=500:105.5313,lwr_k=600:105.1598,lwr_k=700:101.5345,lwr_k=800:103.0065,lwr_k=900:102.2694,lwr_k=1000:98.8158'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:62.2272,lwr_k=200:70.2295,lwr_k=300:71.6863,lwr_k=400:73.2775,lwr_k=500:75.5056,lwr_k=600:76.5837,lwr_k=700:77.5486,lwr_k=800:77.114,lwr_k=900:77.6195,lwr_k=1000:78.3571'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:158.6785,lwr_k=200:118.4431,lwr_k=300:110.3891,lwr_k=400:100.6134,lwr_k=500:101.1713,lwr_k=600:97.8416,lwr_k=700:99.2755,lwr_k=800:98.7466,lwr_k=900:96.0083,lwr_k=1000:97.868'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:81.2054,lwr_k=200:76.4954,lwr_k=300:74.3285,lwr_k=400:76.0389,lwr_k=500:77.1442,lwr_k=600:80.4573,lwr_k=700:79.0715,lwr_k=800:80.6627,lwr_k=900:79.7575,lwr_k=1000:80.3793'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:217.768,lwr_k=200:125.0282,lwr_k=300:114.3834,lwr_k=400:106.5941,lwr_k=500:96.7158,lwr_k=600:98.1667,lwr_k=700:101.8732,lwr_k=800:103.7924,lwr_k=900:124.0175,lwr_k=1000:101.2769'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_6'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:60.1864,lwr_k=200:54.0746,lwr_k=300:51.791,lwr_k=400:49.5202,lwr_k=500:50.706,lwr_k=600:50.8858,lwr_k=700:51.5324,lwr_k=800:51.7686,lwr_k=900:51.9275,lwr_k=1000:52.3813'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:206.934,lwr_k=200:124.3092,lwr_k=300:127.491,lwr_k=400:114.2716,lwr_k=500:126.3507,lwr_k=600:126.8505,lwr_k=700:126.4095,lwr_k=800:127.689,lwr_k=900:126.1811,lwr_k=1000:120.9839'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:75.3931,lwr_k=200:68.4945,lwr_k=300:67.7722,lwr_k=400:71.5786,lwr_k=500:68.2588,lwr_k=600:69.3925,lwr_k=700:70.5838,lwr_k=800:69.7659,lwr_k=900:71.9793,lwr_k=1000:71.3109'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:290.4135,lwr_k=200:198.9131,lwr_k=300:169.3282,lwr_k=400:153.3503,lwr_k=500:155.5393,lwr_k=600:154.3184,lwr_k=700:150.7049,lwr_k=800:148.336,lwr_k=900:147.7692,lwr_k=1000:147.5969'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:122.4355,lwr_k=200:100.7445,lwr_k=300:96.4841,lwr_k=400:95.3968,lwr_k=500:101.9005,lwr_k=600:106.7589,lwr_k=700:107.4068,lwr_k=800:108.1131,lwr_k=900:113.9322,lwr_k=1000:115.1604'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:256.546,lwr_k=200:200.3102,lwr_k=300:153.9008,lwr_k=400:129.607,lwr_k=500:138.3148,lwr_k=600:232.2882,lwr_k=700:128.0191,lwr_k=800:151.2815,lwr_k=900:135.1732,lwr_k=1000:123.547'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:187.7342,lwr_k=200:181.4379,lwr_k=300:209.0624,lwr_k=400:224.9987,lwr_k=500:229.168,lwr_k=600:240.239,lwr_k=700:247.1133,lwr_k=800:265.4822,lwr_k=900:273.9416,lwr_k=1000:296.7706'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:362.866,lwr_k=200:261.7714,lwr_k=300:214.421,lwr_k=400:222.8771,lwr_k=500:226.8348,lwr_k=600:230.0908,lwr_k=700:237.0086,lwr_k=800:234.7071,lwr_k=900:223.413,lwr_k=1000:245.5665'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:101.2978,lwr_k=200:86.5396,lwr_k=300:85.9567,lwr_k=400:82.42,lwr_k=500:81.5535,lwr_k=600:81.0527,lwr_k=700:85.444,lwr_k=800:81.6659,lwr_k=900:83.4783,lwr_k=1000:82.1799'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:310.523,lwr_k=200:196.5034,lwr_k=300:154.8259,lwr_k=400:153.7449,lwr_k=500:134.7551,lwr_k=600:131.2233,lwr_k=700:124.6721,lwr_k=800:125.9662,lwr_k=900:118.2197,lwr_k=1000:113.7556'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:84.2761,lwr_k=200:69.581,lwr_k=300:65.7175,lwr_k=400:67.2009,lwr_k=500:67.0224,lwr_k=600:65.8198,lwr_k=700:66.977,lwr_k=800:67.3932,lwr_k=900:68.0131,lwr_k=1000:64.3852'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:233.1424,lwr_k=200:138.9895,lwr_k=300:147.9853,lwr_k=400:138.7684,lwr_k=500:134.2888,lwr_k=600:130.3301,lwr_k=700:137.7797,lwr_k=800:139.7663,lwr_k=900:131.6135,lwr_k=1000:130.7199'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_7'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:107.1174,lwr_k=200:102.5699,lwr_k=300:99.4215,lwr_k=400:100.9232,lwr_k=500:100.7773,lwr_k=600:101.1828,lwr_k=700:102.6264,lwr_k=800:102.9449,lwr_k=900:103.2256,lwr_k=1000:102.9161'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:153.273,lwr_k=200:139.7113,lwr_k=300:135.7184,lwr_k=400:132.4846,lwr_k=500:130.6807,lwr_k=600:130.7687,lwr_k=700:130.3955,lwr_k=800:129.0104,lwr_k=900:129.1238,lwr_k=1000:129.8867'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:64.352,lwr_k=200:68.254,lwr_k=300:70.9961,lwr_k=400:71.422,lwr_k=500:71.3036,lwr_k=600:71.8598,lwr_k=700:72.1138,lwr_k=800:72.5723,lwr_k=900:72.9119,lwr_k=1000:72.965'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:173.8789,lwr_k=200:158.8944,lwr_k=300:159.0275,lwr_k=400:151.5996,lwr_k=500:149.9321,lwr_k=600:149.7242,lwr_k=700:149.3547,lwr_k=800:149.9498,lwr_k=900:149.5007,lwr_k=1000:149.1102'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:77.7159,lwr_k=200:79.3225,lwr_k=300:79.6613,lwr_k=400:80.044,lwr_k=500:80.7757,lwr_k=600:80.6492,lwr_k=700:80.7409,lwr_k=800:81.0103,lwr_k=900:80.9419,lwr_k=1000:81.4726'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:110.4562,lwr_k=200:104.5721,lwr_k=300:101.5123,lwr_k=400:98.4854,lwr_k=500:100.5284,lwr_k=600:100.6385,lwr_k=700:101.2793,lwr_k=800:99.5714,lwr_k=900:100.5137,lwr_k=1000:100.0952'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:107.3556,lwr_k=200:105.6968,lwr_k=300:106.0044,lwr_k=400:106.2645,lwr_k=500:106.7849,lwr_k=600:107.5248,lwr_k=700:108.9165,lwr_k=800:108.6932,lwr_k=900:108.3048,lwr_k=1000:110.0511'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:154.1268,lwr_k=200:137.2082,lwr_k=300:136.84,lwr_k=400:137.2709,lwr_k=500:137.9327,lwr_k=600:136.7863,lwr_k=700:139.2594,lwr_k=800:139.3945,lwr_k=900:137.4527,lwr_k=1000:138.4537'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:71.0856,lwr_k=200:77.9651,lwr_k=300:80.6851,lwr_k=400:82.0786,lwr_k=500:82.8554,lwr_k=600:83.6613,lwr_k=700:84.3672,lwr_k=800:84.6752,lwr_k=900:84.9894,lwr_k=1000:85.3189'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:115.2473,lwr_k=200:109.5889,lwr_k=300:106.5583,lwr_k=400:106.833,lwr_k=500:106.9698,lwr_k=600:108.1511,lwr_k=700:108.6967,lwr_k=800:108.8255,lwr_k=900:108.8712,lwr_k=1000:109.2512'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:68.456,lwr_k=200:74.6621,lwr_k=300:78.0949,lwr_k=400:79.7818,lwr_k=500:80.1627,lwr_k=600:80.9111,lwr_k=700:81.5477,lwr_k=800:81.931,lwr_k=900:82.1246,lwr_k=1000:82.4885'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:95.684,lwr_k=200:89.5916,lwr_k=300:86.7955,lwr_k=400:78.4783,lwr_k=500:77.1196,lwr_k=600:76.1053,lwr_k=700:76.468,lwr_k=800:76.8344,lwr_k=900:77.2334,lwr_k=1000:77.2354'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_8'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:30.6642,lwr_k=200:59.4204,lwr_k=300:69.9401,lwr_k=400:75.6793,lwr_k=500:77.9699,lwr_k=600:80.6778,lwr_k=700:81.8871,lwr_k=800:83.2021,lwr_k=900:84.2891,lwr_k=1000:85.073'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:221.353,lwr_k=200:116.6767,lwr_k=300:108.4018,lwr_k=400:101.4651,lwr_k=500:100.6372,lwr_k=600:101.1746,lwr_k=700:98.4399,lwr_k=800:96.5027,lwr_k=900:96.1411,lwr_k=1000:96.8672'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:35.1164,lwr_k=200:51.8375,lwr_k=300:59.2453,lwr_k=400:64.0866,lwr_k=500:66.7426,lwr_k=600:69.4149,lwr_k=700:71.0379,lwr_k=800:72.5658,lwr_k=900:73.4449,lwr_k=1000:74.5912'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:258.3872,lwr_k=200:160.2304,lwr_k=300:151.2345,lwr_k=400:135.8006,lwr_k=500:137.0276,lwr_k=600:138.1996,lwr_k=700:138.3452,lwr_k=800:140.0471,lwr_k=900:140.0828,lwr_k=1000:140.7199'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:37.4309,lwr_k=200:39.4031,lwr_k=300:44.7696,lwr_k=400:47.8587,lwr_k=500:50.0923,lwr_k=600:51.7245,lwr_k=700:52.7322,lwr_k=800:53.6253,lwr_k=900:54.2826,lwr_k=1000:54.8298'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:198.396,lwr_k=200:121.8599,lwr_k=300:106.8349,lwr_k=400:115.5845,lwr_k=500:103.041,lwr_k=600:99.5564,lwr_k=700:102.3127,lwr_k=800:101.6356,lwr_k=900:101.6426,lwr_k=1000:101.0462'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:61.4066,lwr_k=200:65.8717,lwr_k=300:74.9505,lwr_k=400:77.9412,lwr_k=500:78.7251,lwr_k=600:81.6607,lwr_k=700:82.5888,lwr_k=800:83.6316,lwr_k=900:83.4284,lwr_k=1000:83.8046'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:339.2932,lwr_k=200:181.9111,lwr_k=300:150.0514,lwr_k=400:140.9312,lwr_k=500:112.1706,lwr_k=600:124.6099,lwr_k=700:118.3803,lwr_k=800:106.8332,lwr_k=900:111.1609,lwr_k=1000:113.3214'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:27.9346,lwr_k=200:48.8177,lwr_k=300:55.9228,lwr_k=400:59.744,lwr_k=500:61.7175,lwr_k=600:64.014,lwr_k=700:65.6361,lwr_k=800:67.0148,lwr_k=900:67.7384,lwr_k=1000:68.7826'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:226.4346,lwr_k=200:114.4512,lwr_k=300:103.0297,lwr_k=400:93.2175,lwr_k=500:89.6804,lwr_k=600:87.4575,lwr_k=700:85.8408,lwr_k=800:83.7662,lwr_k=900:81.3862,lwr_k=1000:81.6731'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:38.7,lwr_k=200:55.2368,lwr_k=300:64.5694,lwr_k=400:70.8185,lwr_k=500:74.3466,lwr_k=600:76.7841,lwr_k=700:78.3721,lwr_k=800:80.1082,lwr_k=900:81.0642,lwr_k=1000:82.1141'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:256.4986,lwr_k=200:178.0035,lwr_k=300:149.6461,lwr_k=400:129.7358,lwr_k=500:117.7204,lwr_k=600:115.536,lwr_k=700:111.9849,lwr_k=800:113.8248,lwr_k=900:114.4617,lwr_k=1000:114.974'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_9'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:113.2897,lwr_k=200:110.1074,lwr_k=300:107.3034,lwr_k=400:109.946,lwr_k=500:111.6013,lwr_k=600:117.0143,lwr_k=700:112.5295,lwr_k=800:112.8969,lwr_k=900:113.8135,lwr_k=1000:113.9819'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:354.0716,lwr_k=200:162.7959,lwr_k=300:164.127,lwr_k=400:155.772,lwr_k=500:148.2583,lwr_k=600:139.7086,lwr_k=700:137.7224,lwr_k=800:138.3994,lwr_k=900:138.1463,lwr_k=1000:137.1802'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:36.068,lwr_k=200:62.728,lwr_k=300:72.8081,lwr_k=400:77.2333,lwr_k=500:80.8283,lwr_k=600:84.4814,lwr_k=700:86.5711,lwr_k=800:88.6321,lwr_k=900:90.6594,lwr_k=1000:91.8878'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:215.7486,lwr_k=200:168.2564,lwr_k=300:155.5597,lwr_k=400:158.7721,lwr_k=500:156.8729,lwr_k=600:160.4611,lwr_k=700:161.9406,lwr_k=800:162.8999,lwr_k=900:163.7902,lwr_k=1000:164.2426'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:67.0778,lwr_k=200:66.9194,lwr_k=300:78.4995,lwr_k=400:83.8958,lwr_k=500:85.8302,lwr_k=600:87.1556,lwr_k=700:88.6576,lwr_k=800:89.6444,lwr_k=900:90.6214,lwr_k=1000:91.6122'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:220.54,lwr_k=200:117.544,lwr_k=300:125.6007,lwr_k=400:112.1908,lwr_k=500:108.0798,lwr_k=600:105.0351,lwr_k=700:104.8204,lwr_k=800:104.7739,lwr_k=900:104.6711,lwr_k=1000:104.4792'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:100.6474,lwr_k=200:95.5396,lwr_k=300:95.0139,lwr_k=400:98.1698,lwr_k=500:99.381,lwr_k=600:101.1206,lwr_k=700:101.551,lwr_k=800:102.3325,lwr_k=900:102.8534,lwr_k=1000:104.0244'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:238.7667,lwr_k=200:151.1432,lwr_k=300:122.8505,lwr_k=400:115.4922,lwr_k=500:109.3093,lwr_k=600:110.3936,lwr_k=700:110.0284,lwr_k=800:109.8192,lwr_k=900:108.0545,lwr_k=1000:107.4137'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:43.7252,lwr_k=200:79.5664,lwr_k=300:92.085,lwr_k=400:98.0741,lwr_k=500:102.0639,lwr_k=600:104.5008,lwr_k=700:105.9483,lwr_k=800:107.0748,lwr_k=900:108.4819,lwr_k=1000:109.0287'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:159.1879,lwr_k=200:108.4587,lwr_k=300:103.7336,lwr_k=400:107.1783,lwr_k=500:108.7245,lwr_k=600:108.0224,lwr_k=700:108.3023,lwr_k=800:108.698,lwr_k=900:109.1221,lwr_k=1000:109.5014'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:48.5486,lwr_k=200:70.7022,lwr_k=300:84.0271,lwr_k=400:88.5241,lwr_k=500:93.6759,lwr_k=600:96.7318,lwr_k=700:98.7458,lwr_k=800:100.3623,lwr_k=900:101.7233,lwr_k=1000:102.763'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:146.0071,lwr_k=200:110.5184,lwr_k=300:106.1832,lwr_k=400:102.8865,lwr_k=500:105.8535,lwr_k=600:106.3816,lwr_k=700:107.8524,lwr_k=800:107.8919,lwr_k=900:107.7286,lwr_k=1000:108.2446'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_10'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:108.037,lwr_k=200:116.1676,lwr_k=300:118.4249,lwr_k=400:119.7811,lwr_k=500:120.53,lwr_k=600:121.3336,lwr_k=700:121.8089,lwr_k=800:122.2298,lwr_k=900:122.3843,lwr_k=1000:122.5395'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:154.2699,lwr_k=200:121.4014,lwr_k=300:124.8659,lwr_k=400:125.9203,lwr_k=500:121.3876,lwr_k=600:120.2796,lwr_k=700:119.3066,lwr_k=800:120.0596,lwr_k=900:119.8925,lwr_k=1000:119.447'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:85.7243,lwr_k=200:94.2936,lwr_k=300:97.2564,lwr_k=400:99.0707,lwr_k=500:100.399,lwr_k=600:101.2613,lwr_k=700:102.2741,lwr_k=800:102.7321,lwr_k=900:103.263,lwr_k=1000:103.6034'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:212.942,lwr_k=200:218.0494,lwr_k=300:216.0145,lwr_k=400:213.59,lwr_k=500:214.4464,lwr_k=600:214.6573,lwr_k=700:214.2411,lwr_k=800:214.2106,lwr_k=900:214.5581,lwr_k=1000:214.748'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:85.427,lwr_k=200:92.9072,lwr_k=300:95.5309,lwr_k=400:97.1205,lwr_k=500:98.6327,lwr_k=600:99.0955,lwr_k=700:99.8991,lwr_k=800:100.4003,lwr_k=900:100.6467,lwr_k=1000:100.8098'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:134.7259,lwr_k=200:135.8545,lwr_k=300:133.7936,lwr_k=400:132.569,lwr_k=500:134.8777,lwr_k=600:135.4053,lwr_k=700:135.0295,lwr_k=800:135.2878,lwr_k=900:135.2329,lwr_k=1000:135.6495'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:97.6062,lwr_k=200:116.2145,lwr_k=300:119.3242,lwr_k=400:120.8378,lwr_k=500:121.9157,lwr_k=600:122.6439,lwr_k=700:123.6326,lwr_k=800:124.1473,lwr_k=900:124.5907,lwr_k=1000:125.0381'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:129.5921,lwr_k=200:111.2214,lwr_k=300:109.0288,lwr_k=400:109.0118,lwr_k=500:109.3188,lwr_k=600:109.1643,lwr_k=700:108.607,lwr_k=800:108.7419,lwr_k=900:108.9483,lwr_k=1000:109.3131'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:103.8211,lwr_k=200:113.0654,lwr_k=300:116.3637,lwr_k=400:121.2202,lwr_k=500:123.3974,lwr_k=600:124.3366,lwr_k=700:124.6602,lwr_k=800:125.4769,lwr_k=900:126.0088,lwr_k=1000:126.564'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:123.0963,lwr_k=200:123.6108,lwr_k=300:125.2818,lwr_k=400:131.4391,lwr_k=500:130.6137,lwr_k=600:128.6447,lwr_k=700:128.6498,lwr_k=800:128.5197,lwr_k=900:128.5093,lwr_k=1000:128.5057'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:99.972,lwr_k=200:110.7454,lwr_k=300:117.4919,lwr_k=400:122.3562,lwr_k=500:124.5228,lwr_k=600:125.74,lwr_k=700:126.6025,lwr_k=800:127.5689,lwr_k=900:128.2735,lwr_k=1000:128.9088'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:126.1099,lwr_k=200:134.8528,lwr_k=300:120.8685,lwr_k=400:126.7663,lwr_k=500:125.658,lwr_k=600:124.8339,lwr_k=700:123.7721,lwr_k=800:124.2833,lwr_k=900:126.1551,lwr_k=1000:127.0597'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_11'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:86.1557,lwr_k=200:75.8729,lwr_k=300:80.8139,lwr_k=400:78.4847,lwr_k=500:80.5575,lwr_k=600:81.5443,lwr_k=700:80.0936,lwr_k=800:80.8774,lwr_k=900:80.0973,lwr_k=1000:79.8614'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:234.1077,lwr_k=200:138.2959,lwr_k=300:136.5831,lwr_k=400:159.7959,lwr_k=500:134.896,lwr_k=600:141.2945,lwr_k=700:143.3408,lwr_k=800:138.6858,lwr_k=900:131.039,lwr_k=1000:130.3498'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:55.6413,lwr_k=200:56.9447,lwr_k=300:59.9851,lwr_k=400:60.4997,lwr_k=500:60.8225,lwr_k=600:58.8878,lwr_k=700:59.9589,lwr_k=800:58.9787,lwr_k=900:58.8268,lwr_k=1000:59.4062'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:269.328,lwr_k=200:191.6968,lwr_k=300:170.9989,lwr_k=400:180.0253,lwr_k=500:161.6711,lwr_k=600:156.6783,lwr_k=700:159.1902,lwr_k=800:158.0222,lwr_k=900:155.0082,lwr_k=1000:156.2779'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:80.7415,lwr_k=200:75.3975,lwr_k=300:88.8883,lwr_k=400:82.7216,lwr_k=500:75.0998,lwr_k=600:81.4995,lwr_k=700:74.6901,lwr_k=800:71.4061,lwr_k=900:74.2003,lwr_k=1000:76.5841'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:199.1327,lwr_k=200:144.4779,lwr_k=300:134.6751,lwr_k=400:142.354,lwr_k=500:121.0765,lwr_k=600:128.6036,lwr_k=700:122.8183,lwr_k=800:113.2173,lwr_k=900:107.3097,lwr_k=1000:108.4551'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:66.3834,lwr_k=200:67.9333,lwr_k=300:70.2596,lwr_k=400:69.1636,lwr_k=500:71.171,lwr_k=600:71.8237,lwr_k=700:72.4813,lwr_k=800:72.0363,lwr_k=900:72.5191,lwr_k=1000:72.587'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:233.1394,lwr_k=200:141.7413,lwr_k=300:131.0165,lwr_k=400:118.6584,lwr_k=500:117.4037,lwr_k=600:110.302,lwr_k=700:105.2115,lwr_k=800:104.941,lwr_k=900:102.4208,lwr_k=1000:102.8962'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:48.7488,lwr_k=200:68.2858,lwr_k=300:66.8584,lwr_k=400:69.9332,lwr_k=500:70.9117,lwr_k=600:73.0145,lwr_k=700:73.6981,lwr_k=800:76.2665,lwr_k=900:77.35,lwr_k=1000:77.941'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:332.9379,lwr_k=200:255.4079,lwr_k=300:229.7458,lwr_k=400:195.7853,lwr_k=500:160.4459,lwr_k=600:163.0065,lwr_k=700:139.3799,lwr_k=800:120.6952,lwr_k=900:126.6404,lwr_k=1000:125.1604'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:41.2189,lwr_k=200:55.0259,lwr_k=300:61.7534,lwr_k=400:64.8258,lwr_k=500:66.5087,lwr_k=600:67.5243,lwr_k=700:68.3848,lwr_k=800:68.8907,lwr_k=900:70.1301,lwr_k=1000:70.754'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:178.1421,lwr_k=200:118.0843,lwr_k=300:105.5861,lwr_k=400:111.5029,lwr_k=500:109.9732,lwr_k=600:107.7451,lwr_k=700:116.4832,lwr_k=800:115.2115,lwr_k=900:115.8583,lwr_k=1000:120.1666'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_12'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:124.7857,lwr_k=200:126.2528,lwr_k=300:125.8697,lwr_k=400:125.3427,lwr_k=500:124.8775,lwr_k=600:125.0134,lwr_k=700:124.9892,lwr_k=800:125.1625,lwr_k=900:125.3,lwr_k=1000:125.5957'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:162.5322,lwr_k=200:161.8122,lwr_k=300:161.56,lwr_k=400:160.3634,lwr_k=500:160.7724,lwr_k=600:160.6486,lwr_k=700:160.8193,lwr_k=800:160.5507,lwr_k=900:160.6744,lwr_k=1000:160.4289'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:77.576,lwr_k=200:78.8053,lwr_k=300:80.117,lwr_k=400:80.0959,lwr_k=500:80.0791,lwr_k=600:80.069,lwr_k=700:80.0971,lwr_k=800:80.1201,lwr_k=900:80.1293,lwr_k=1000:80.1141'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:148.5329,lwr_k=200:150.6177,lwr_k=300:150.0256,lwr_k=400:150.0113,lwr_k=500:149.9156,lwr_k=600:149.8229,lwr_k=700:149.853,lwr_k=800:149.8308,lwr_k=900:149.8663,lwr_k=1000:149.846'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:92.5212,lwr_k=200:94.5054,lwr_k=300:95.5087,lwr_k=400:95.6545,lwr_k=500:95.81,lwr_k=600:95.8333,lwr_k=700:95.8602,lwr_k=800:95.9772,lwr_k=900:95.9662,lwr_k=1000:96.0756'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:117.3655,lwr_k=200:116.0005,lwr_k=300:115.4628,lwr_k=400:115.2833,lwr_k=500:115.1205,lwr_k=600:115.0877,lwr_k=700:115.1433,lwr_k=800:114.9937,lwr_k=900:115.0012,lwr_k=1000:115.1138'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:143.6841,lwr_k=200:150.929,lwr_k=300:154.0604,lwr_k=400:156.2156,lwr_k=500:158.2958,lwr_k=600:159.9154,lwr_k=700:161.997,lwr_k=800:163.7823,lwr_k=900:165.2979,lwr_k=1000:167.0158'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:117.7711,lwr_k=200:115.6042,lwr_k=300:115.5091,lwr_k=400:115.6438,lwr_k=500:115.4569,lwr_k=600:115.2825,lwr_k=700:116.3889,lwr_k=800:117.3676,lwr_k=900:118.018,lwr_k=1000:118.6163'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:121.1121,lwr_k=200:127.4666,lwr_k=300:131.2589,lwr_k=400:133.7725,lwr_k=500:136.055,lwr_k=600:137.8426,lwr_k=700:139.6168,lwr_k=800:140.713,lwr_k=900:141.5787,lwr_k=1000:142.3983'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:115.3595,lwr_k=200:122.3037,lwr_k=300:128.993,lwr_k=400:131.5607,lwr_k=500:133.4236,lwr_k=600:135.6515,lwr_k=700:134.1323,lwr_k=800:134.6634,lwr_k=900:135.3165,lwr_k=1000:135.9953'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:155.5679,lwr_k=200:156.9895,lwr_k=300:158.8499,lwr_k=400:160.0843,lwr_k=500:161.3775,lwr_k=600:161.6654,lwr_k=700:161.4762,lwr_k=800:161.3819,lwr_k=900:161.2453,lwr_k=1000:161.2407'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:157.3518,lwr_k=200:160.345,lwr_k=300:162.358,lwr_k=400:163.4675,lwr_k=500:164.8347,lwr_k=600:165.0728,lwr_k=700:164.9574,lwr_k=800:164.9685,lwr_k=900:164.8203,lwr_k=1000:164.8381'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_13'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:115.0851,lwr_k=200:119.0741,lwr_k=300:121.8899,lwr_k=400:122.5084,lwr_k=500:123.4084,lwr_k=600:123.5027,lwr_k=700:124.032,lwr_k=800:124.3319,lwr_k=900:124.5422,lwr_k=1000:124.8995'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:117.2467,lwr_k=200:114.4602,lwr_k=300:117.3408,lwr_k=400:115.7577,lwr_k=500:114.5644,lwr_k=600:114.3751,lwr_k=700:114.1829,lwr_k=800:114.4162,lwr_k=900:114.3387,lwr_k=1000:114.3713'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:88.6507,lwr_k=200:92.7427,lwr_k=300:94.9609,lwr_k=400:95.8716,lwr_k=500:96.8211,lwr_k=600:97.0021,lwr_k=700:97.3771,lwr_k=800:97.7246,lwr_k=900:97.9992,lwr_k=1000:98.2337'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:158.8087,lwr_k=200:156.4021,lwr_k=300:156.5808,lwr_k=400:155.154,lwr_k=500:155.4107,lwr_k=600:155.1854,lwr_k=700:155.0303,lwr_k=800:154.625,lwr_k=900:154.4211,lwr_k=1000:154.9307'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:91.5895,lwr_k=200:97.5399,lwr_k=300:99.5028,lwr_k=400:100.8489,lwr_k=500:101.3043,lwr_k=600:101.7454,lwr_k=700:101.973,lwr_k=800:102.2036,lwr_k=900:102.4589,lwr_k=1000:102.5278'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:118.4476,lwr_k=200:116.2158,lwr_k=300:117.7465,lwr_k=400:117.4711,lwr_k=500:117.4397,lwr_k=600:117.066,lwr_k=700:116.8605,lwr_k=800:116.597,lwr_k=900:116.2158,lwr_k=1000:116.5081'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:107.9042,lwr_k=200:111.7857,lwr_k=300:114.5881,lwr_k=400:116.6586,lwr_k=500:117.1981,lwr_k=600:117.8415,lwr_k=700:118.6046,lwr_k=800:119.1985,lwr_k=900:119.3694,lwr_k=1000:119.9911'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:127.6003,lwr_k=200:123.437,lwr_k=300:123.8882,lwr_k=400:124.6833,lwr_k=500:125.6333,lwr_k=600:126.3257,lwr_k=700:126.1656,lwr_k=800:125.9312,lwr_k=900:125.8182,lwr_k=1000:125.6048'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:109.6325,lwr_k=200:114.2457,lwr_k=300:117.0374,lwr_k=400:117.5446,lwr_k=500:119.3329,lwr_k=600:120.4543,lwr_k=700:121.3782,lwr_k=800:122.0264,lwr_k=900:122.6945,lwr_k=1000:123.3145'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:117.6664,lwr_k=200:107.8188,lwr_k=300:104.1401,lwr_k=400:104.4033,lwr_k=500:104.2815,lwr_k=600:104.4616,lwr_k=700:105.1064,lwr_k=800:105.4078,lwr_k=900:105.731,lwr_k=1000:106.153'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:108.5521,lwr_k=200:113.806,lwr_k=300:115.5857,lwr_k=400:116.5357,lwr_k=500:116.8981,lwr_k=600:117.7807,lwr_k=700:118.5515,lwr_k=800:118.9192,lwr_k=900:119.5876,lwr_k=1000:120.0804'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:137.1812,lwr_k=200:131.1396,lwr_k=300:129.1304,lwr_k=400:127.9833,lwr_k=500:126.5961,lwr_k=600:126.0664,lwr_k=700:125.6737,lwr_k=800:125.4427,lwr_k=900:125.6591,lwr_k=1000:125.323'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_14'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:55.8336,lwr_k=200:77.1855,lwr_k=300:85.9404,lwr_k=400:92.6786,lwr_k=500:93.3891,lwr_k=600:96.4453,lwr_k=700:97.2856,lwr_k=800:99.2063,lwr_k=900:100.1867,lwr_k=1000:100.804'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:252.7369,lwr_k=200:181.9861,lwr_k=300:132.7387,lwr_k=400:134.4894,lwr_k=500:139.8786,lwr_k=600:126.9255,lwr_k=700:124.4261,lwr_k=800:125.0479,lwr_k=900:128.2741,lwr_k=1000:118.4246'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:82.9123,lwr_k=200:87.1956,lwr_k=300:87.9532,lwr_k=400:88.7079,lwr_k=500:89.7533,lwr_k=600:90.8627,lwr_k=700:90.7669,lwr_k=800:91.2715,lwr_k=900:91.751,lwr_k=1000:91.5841'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:333.2605,lwr_k=200:232.8194,lwr_k=300:263.0811,lwr_k=400:259.7056,lwr_k=500:296.8919,lwr_k=600:349.31,lwr_k=700:293.6656,lwr_k=800:271.9949,lwr_k=900:264.9315,lwr_k=1000:321.2239'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:128.1253,lwr_k=200:126.3058,lwr_k=300:129.1565,lwr_k=400:130.9161,lwr_k=500:129.1598,lwr_k=600:132.0335,lwr_k=700:128.8051,lwr_k=800:133.454,lwr_k=900:134.2149,lwr_k=1000:132.8496'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:223.2595,lwr_k=200:129.3793,lwr_k=300:129.4894,lwr_k=400:222.841,lwr_k=500:221.0894,lwr_k=600:220.4399,lwr_k=700:127.7323,lwr_k=800:129.2606,lwr_k=900:130.6819,lwr_k=1000:130.2954'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:50.5019,lwr_k=200:68.2033,lwr_k=300:86.4448,lwr_k=400:98.4224,lwr_k=500:105.6003,lwr_k=600:110.0418,lwr_k=700:113.0146,lwr_k=800:115.3817,lwr_k=900:116.9701,lwr_k=1000:118.2201'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:398.1785,lwr_k=200:202.308,lwr_k=300:149.9464,lwr_k=400:129.2983,lwr_k=500:118.8863,lwr_k=600:115.5205,lwr_k=700:115.0019,lwr_k=800:112.7587,lwr_k=900:111.9939,lwr_k=1000:109.5002'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:100.7479,lwr_k=200:102.6619,lwr_k=300:104.5634,lwr_k=400:102.5127,lwr_k=500:105.3411,lwr_k=600:104.9018,lwr_k=700:106.4903,lwr_k=800:107.3561,lwr_k=900:107.457,lwr_k=1000:104.8611'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:264.5351,lwr_k=200:181.957,lwr_k=300:183.9255,lwr_k=400:185.4964,lwr_k=500:275.976,lwr_k=600:267.5661,lwr_k=700:275.5787,lwr_k=800:171.7255,lwr_k=900:178.9106,lwr_k=1000:172.0109'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:64.7238,lwr_k=200:81.4806,lwr_k=300:90.0557,lwr_k=400:95.0572,lwr_k=500:98.3561,lwr_k=600:99.5269,lwr_k=700:101.2444,lwr_k=800:102.289,lwr_k=900:103.1925,lwr_k=1000:103.5249'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:170.8099,lwr_k=200:172.2192,lwr_k=300:127.3423,lwr_k=400:118.667,lwr_k=500:108.9123,lwr_k=600:103.7797,lwr_k=700:103.8982,lwr_k=800:108.1353,lwr_k=900:108.6694,lwr_k=1000:107.5891'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_15'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:67.8728,lwr_k=200:84.061,lwr_k=300:90.067,lwr_k=400:93.667,lwr_k=500:95.5041,lwr_k=600:96.4347,lwr_k=700:97.2808,lwr_k=800:97.8176,lwr_k=900:98.2845,lwr_k=1000:98.4978'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:144.1839,lwr_k=200:122.6331,lwr_k=300:113.6304,lwr_k=400:107.8805,lwr_k=500:107.0909,lwr_k=600:106.4724,lwr_k=700:106.4603,lwr_k=800:106.1253,lwr_k=900:106.1217,lwr_k=1000:106.3242'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:53.979,lwr_k=200:66.394,lwr_k=300:72.1144,lwr_k=400:75.7232,lwr_k=500:77.2417,lwr_k=600:78.9165,lwr_k=700:79.8661,lwr_k=800:80.6811,lwr_k=900:81.1942,lwr_k=1000:81.765'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:174.3545,lwr_k=200:165.3958,lwr_k=300:163.3862,lwr_k=400:155.9703,lwr_k=500:153.2978,lwr_k=600:151.3935,lwr_k=700:149.8075,lwr_k=800:149.4446,lwr_k=900:149.3793,lwr_k=1000:149.4468'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:51.6077,lwr_k=200:61.203,lwr_k=300:65.0753,lwr_k=400:67.509,lwr_k=500:68.9776,lwr_k=600:70.0858,lwr_k=700:70.8334,lwr_k=800:72.0742,lwr_k=900:72.5512,lwr_k=1000:73.1863'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:196.5681,lwr_k=200:98.0392,lwr_k=300:96.2299,lwr_k=400:93.7212,lwr_k=500:105.0932,lwr_k=600:101.2835,lwr_k=700:110.3455,lwr_k=800:102.8675,lwr_k=900:107.5448,lwr_k=1000:104.3508'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:66.449,lwr_k=200:83.9543,lwr_k=300:87.1947,lwr_k=400:90.127,lwr_k=500:91.8893,lwr_k=600:93.2159,lwr_k=700:94.3824,lwr_k=800:95.621,lwr_k=900:96.3774,lwr_k=1000:97.0401'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:183.117,lwr_k=200:131.8004,lwr_k=300:127.7519,lwr_k=400:121.2989,lwr_k=500:118.8996,lwr_k=600:115.174,lwr_k=700:114.856,lwr_k=800:115.1055,lwr_k=900:116.3462,lwr_k=1000:116.0306'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:154.7892,lwr_k=200:87.5909,lwr_k=300:108.8147,lwr_k=400:103.1461,lwr_k=500:115.1408,lwr_k=600:114.595,lwr_k=700:107.3342,lwr_k=800:107.1997,lwr_k=900:107.0937,lwr_k=1000:106.2093'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:288.5252,lwr_k=200:121.008,lwr_k=300:100.6378,lwr_k=400:104.9312,lwr_k=500:120.9573,lwr_k=600:117.574,lwr_k=700:100.0181,lwr_k=800:99.8335,lwr_k=900:100.9962,lwr_k=1000:95.7809'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:53.5317,lwr_k=200:68.5337,lwr_k=300:72.5546,lwr_k=400:75.4832,lwr_k=500:77.228,lwr_k=600:78.3822,lwr_k=700:79.6282,lwr_k=800:80.0748,lwr_k=900:80.8489,lwr_k=1000:81.3876'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:96.21,lwr_k=200:92.0736,lwr_k=300:89.8057,lwr_k=400:97.4355,lwr_k=500:103.3659,lwr_k=600:93.3714,lwr_k=700:97.4849,lwr_k=800:100.1492,lwr_k=900:101.688,lwr_k=1000:103.8157'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_16'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:47.3219,lwr_k=200:76.6067,lwr_k=300:84.3844,lwr_k=400:90.2071,lwr_k=500:93.5464,lwr_k=600:96.2898,lwr_k=700:98.0551,lwr_k=800:100.2668,lwr_k=900:100.334,lwr_k=1000:101.5624'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:275.8845,lwr_k=200:142.2313,lwr_k=300:126.067,lwr_k=400:117.4647,lwr_k=500:116.3526,lwr_k=600:114.9669,lwr_k=700:114.6215,lwr_k=800:114.1653,lwr_k=900:114.2761,lwr_k=1000:113.8566'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:28.3458,lwr_k=200:46.3878,lwr_k=300:52.5078,lwr_k=400:56.0604,lwr_k=500:58.0033,lwr_k=600:59.6473,lwr_k=700:61.102,lwr_k=800:62.0829,lwr_k=900:63.1139,lwr_k=1000:63.9184'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:265.2937,lwr_k=200:179.1107,lwr_k=300:170.4957,lwr_k=400:165.8098,lwr_k=500:160.4011,lwr_k=600:158.9586,lwr_k=700:156.0052,lwr_k=800:154.8693,lwr_k=900:153.8421,lwr_k=1000:155.412'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:29.9299,lwr_k=200:55.4828,lwr_k=300:61.5337,lwr_k=400:65.5908,lwr_k=500:68.0134,lwr_k=600:70.4149,lwr_k=700:72.4978,lwr_k=800:73.9518,lwr_k=900:75.7018,lwr_k=1000:76.3776'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:181.9061,lwr_k=200:133.6078,lwr_k=300:106.4391,lwr_k=400:105.6611,lwr_k=500:104.4562,lwr_k=600:106.4303,lwr_k=700:106.9218,lwr_k=800:105.7122,lwr_k=900:105.828,lwr_k=1000:106.49'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:54.8304,lwr_k=200:77.4093,lwr_k=300:95.0234,lwr_k=400:105.8079,lwr_k=500:113.5812,lwr_k=600:118.6008,lwr_k=700:123.0217,lwr_k=800:125.4891,lwr_k=900:128.0298,lwr_k=1000:130.8053'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:378.3233,lwr_k=200:145.6853,lwr_k=300:132.0116,lwr_k=400:126.3633,lwr_k=500:115.1534,lwr_k=600:115.8645,lwr_k=700:109.6302,lwr_k=800:111.1996,lwr_k=900:107.5085,lwr_k=1000:108.3624'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:36.1223,lwr_k=200:61.8805,lwr_k=300:72.7844,lwr_k=400:77.7421,lwr_k=500:80.38,lwr_k=600:81.6579,lwr_k=700:82.7762,lwr_k=800:83.7668,lwr_k=900:84.4182,lwr_k=1000:85.2454'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:210.7305,lwr_k=200:116.2409,lwr_k=300:96.6071,lwr_k=400:89.9508,lwr_k=500:89.7112,lwr_k=600:93.9497,lwr_k=700:93.6333,lwr_k=800:91.5494,lwr_k=900:91.7133,lwr_k=1000:89.71'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:86.5305,lwr_k=200:88.484,lwr_k=300:92.3979,lwr_k=400:95.8628,lwr_k=500:99.6309,lwr_k=600:103.9974,lwr_k=700:107.8914,lwr_k=800:110.8178,lwr_k=900:112.602,lwr_k=1000:115.0105'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:308.4892,lwr_k=200:183.5419,lwr_k=300:168.9783,lwr_k=400:168.4725,lwr_k=500:135.5572,lwr_k=600:135.2196,lwr_k=700:128.765,lwr_k=800:128.1591,lwr_k=900:127.1069,lwr_k=1000:125.1899'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_17'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:90.1627,lwr_k=200:101.2965,lwr_k=300:107.0622,lwr_k=400:108.9708,lwr_k=500:109.3625,lwr_k=600:113.5712,lwr_k=700:112.8317,lwr_k=800:115.2944,lwr_k=900:117.6142,lwr_k=1000:119.8248'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:144.6643,lwr_k=200:128.2243,lwr_k=300:124.3199,lwr_k=400:138.8109,lwr_k=500:149.0712,lwr_k=600:144.4222,lwr_k=700:136.8805,lwr_k=800:134.1354,lwr_k=900:138.1249,lwr_k=1000:127.1879'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:200.5148,lwr_k=200:202.4654,lwr_k=300:203.0882,lwr_k=400:203.2456,lwr_k=500:203.4918,lwr_k=600:204.897,lwr_k=700:206.625,lwr_k=800:209.587,lwr_k=900:211.9076,lwr_k=1000:213.0543'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:310.1185,lwr_k=200:308.3197,lwr_k=300:306.6886,lwr_k=400:304.2155,lwr_k=500:304.9802,lwr_k=600:336.2788,lwr_k=700:391.1415,lwr_k=800:393.3732,lwr_k=900:314.8145,lwr_k=1000:316.2084'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:142.1042,lwr_k=200:150.7699,lwr_k=300:154.2452,lwr_k=400:155.5733,lwr_k=500:157.1143,lwr_k=600:157.9501,lwr_k=700:158.8891,lwr_k=800:159.7278,lwr_k=900:160.3085,lwr_k=1000:160.9802'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:150.5928,lwr_k=200:146.9923,lwr_k=300:145.202,lwr_k=400:146.1885,lwr_k=500:147.2239,lwr_k=600:147.6109,lwr_k=700:148.4447,lwr_k=800:149.4705,lwr_k=900:150.1861,lwr_k=1000:150.6416'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:87.3513,lwr_k=200:102.6742,lwr_k=300:109.5771,lwr_k=400:114.5697,lwr_k=500:117.8044,lwr_k=600:121.4137,lwr_k=700:124.103,lwr_k=800:125.9731,lwr_k=900:127.6065,lwr_k=1000:129.3997'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:126.5122,lwr_k=200:106.6749,lwr_k=300:104.3195,lwr_k=400:102.3102,lwr_k=500:101.2733,lwr_k=600:101.124,lwr_k=700:100.3329,lwr_k=800:100.9383,lwr_k=900:101.2224,lwr_k=1000:102.0685'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:247.3852,lwr_k=200:248.2896,lwr_k=300:395.1233,lwr_k=400:400.7261,lwr_k=500:250.5371,lwr_k=600:252.1888,lwr_k=700:254.297,lwr_k=800:255.9306,lwr_k=900:257.2082,lwr_k=1000:258.1548'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:280.6709,lwr_k=200:287.1001,lwr_k=300:459.9366,lwr_k=400:464.3362,lwr_k=500:278.5216,lwr_k=600:271.1864,lwr_k=700:278.1018,lwr_k=800:278.0432,lwr_k=900:278.1653,lwr_k=1000:277.9591'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:125.1931,lwr_k=200:130.5239,lwr_k=300:133.7392,lwr_k=400:135.5456,lwr_k=500:136.6953,lwr_k=600:137.8853,lwr_k=700:139.1159,lwr_k=800:140.5007,lwr_k=900:141.3282,lwr_k=1000:141.9688'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:135.6061,lwr_k=200:135.7929,lwr_k=300:138.0279,lwr_k=400:140.3947,lwr_k=500:139.6227,lwr_k=600:139.541,lwr_k=700:140.701,lwr_k=800:141.003,lwr_k=900:141.6146,lwr_k=1000:142.1372'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_18'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:93.0389,lwr_k=200:94.4385,lwr_k=300:95.5289,lwr_k=400:95.8306,lwr_k=500:96.7235,lwr_k=600:97.0227,lwr_k=700:96.9186,lwr_k=800:96.7154,lwr_k=900:96.9365,lwr_k=1000:96.752'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:135.7385,lwr_k=200:131.3822,lwr_k=300:123.252,lwr_k=400:122.1239,lwr_k=500:119.5546,lwr_k=600:117.5762,lwr_k=700:117.8524,lwr_k=800:118.3181,lwr_k=900:117.6995,lwr_k=1000:116.9222'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:76.3617,lwr_k=200:81.5832,lwr_k=300:84.8271,lwr_k=400:85.6755,lwr_k=500:86.2902,lwr_k=600:87.1575,lwr_k=700:87.5478,lwr_k=800:87.8433,lwr_k=900:87.9211,lwr_k=1000:88.1136'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:163.71,lwr_k=200:155.5376,lwr_k=300:154.7021,lwr_k=400:155.3468,lwr_k=500:154.2937,lwr_k=600:154.3155,lwr_k=700:155.2456,lwr_k=800:154.4404,lwr_k=900:155.6522,lwr_k=1000:154.8157'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:66.5537,lwr_k=200:70.3978,lwr_k=300:72.0567,lwr_k=400:73.0462,lwr_k=500:73.0379,lwr_k=600:73.3563,lwr_k=700:73.5868,lwr_k=800:73.7394,lwr_k=900:74.0393,lwr_k=1000:74.1147'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:137.9464,lwr_k=200:108.4473,lwr_k=300:120.1638,lwr_k=400:110.3997,lwr_k=500:109.4339,lwr_k=600:106.6383,lwr_k=700:106.1142,lwr_k=800:105.6089,lwr_k=900:107.9204,lwr_k=1000:107.9918'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:69.5702,lwr_k=200:76.5174,lwr_k=300:78.8874,lwr_k=400:79.6482,lwr_k=500:80.7025,lwr_k=600:81.1798,lwr_k=700:81.9246,lwr_k=800:82.2888,lwr_k=900:82.9492,lwr_k=1000:83.1922'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:97.6688,lwr_k=200:97.0485,lwr_k=300:96.1043,lwr_k=400:94.4144,lwr_k=500:95.0096,lwr_k=600:95.2238,lwr_k=700:94.8661,lwr_k=800:95.1686,lwr_k=900:95.4618,lwr_k=1000:95.3934'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:79.6231,lwr_k=200:85.9876,lwr_k=300:94.4949,lwr_k=400:95.1098,lwr_k=500:95.588,lwr_k=600:95.8892,lwr_k=700:96.7234,lwr_k=800:97.1185,lwr_k=900:97.4631,lwr_k=1000:97.7212'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:101.4614,lwr_k=200:101.1074,lwr_k=300:94.302,lwr_k=400:92.135,lwr_k=500:91.6995,lwr_k=600:91.549,lwr_k=700:91.1144,lwr_k=800:91.6043,lwr_k=900:92.089,lwr_k=1000:92.1347'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:74.3344,lwr_k=200:79.0549,lwr_k=300:80.9242,lwr_k=400:81.812,lwr_k=500:82.7244,lwr_k=600:83.2952,lwr_k=700:83.5602,lwr_k=800:83.6427,lwr_k=900:83.9403,lwr_k=1000:84.8536'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:95.741,lwr_k=200:92.4576,lwr_k=300:93.5706,lwr_k=400:96.7559,lwr_k=500:97.0448,lwr_k=600:96.3967,lwr_k=700:97.0073,lwr_k=800:96.7752,lwr_k=900:96.8029,lwr_k=1000:97.2393'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_19'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:109.4578,lwr_k=200:114.9032,lwr_k=300:117.9943,lwr_k=400:119.2614,lwr_k=500:120.2247,lwr_k=600:120.8646,lwr_k=700:121.4218,lwr_k=800:122.1581,lwr_k=900:122.7166,lwr_k=1000:123.0441'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:123.9509,lwr_k=200:121.2455,lwr_k=300:120.783,lwr_k=400:120.7168,lwr_k=500:122.1486,lwr_k=600:121.3391,lwr_k=700:121.5284,lwr_k=800:121.4019,lwr_k=900:121.3907,lwr_k=1000:121.4625'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:88.7862,lwr_k=200:96.8512,lwr_k=300:99.4658,lwr_k=400:102.7613,lwr_k=500:104.5505,lwr_k=600:105.8556,lwr_k=700:106.2953,lwr_k=800:106.9278,lwr_k=900:107.8385,lwr_k=1000:108.9295'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:156.7611,lwr_k=200:157.7432,lwr_k=300:160.2313,lwr_k=400:161.081,lwr_k=500:162.8258,lwr_k=600:165.2253,lwr_k=700:167.2582,lwr_k=800:168.6323,lwr_k=900:170.0415,lwr_k=1000:170.995'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:91.2191,lwr_k=200:97.325,lwr_k=300:100.4047,lwr_k=400:102.5136,lwr_k=500:103.634,lwr_k=600:104.3394,lwr_k=700:104.3912,lwr_k=800:104.9172,lwr_k=900:105.1293,lwr_k=1000:105.2213'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:121.9821,lwr_k=200:116.6192,lwr_k=300:116.8047,lwr_k=400:117.0842,lwr_k=500:117.5713,lwr_k=600:118.2011,lwr_k=700:117.7681,lwr_k=800:117.3045,lwr_k=900:118.0703,lwr_k=1000:118.2243'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:382.9008,lwr_k=200:383.0085,lwr_k=300:382.93,lwr_k=400:383.0885,lwr_k=500:383.0752,lwr_k=600:383.0732,lwr_k=700:382.9076,lwr_k=800:382.907,lwr_k=900:382.9353,lwr_k=1000:383.4738'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:301.2381,lwr_k=200:301.233,lwr_k=300:301.217,lwr_k=400:301.2706,lwr_k=500:301.2636,lwr_k=600:301.2626,lwr_k=700:301.2271,lwr_k=800:301.3075,lwr_k=900:301.3754,lwr_k=1000:302.1446'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:109.2087,lwr_k=200:118.669,lwr_k=300:121.1163,lwr_k=400:123.4952,lwr_k=500:124.7374,lwr_k=600:126.2666,lwr_k=700:127.5402,lwr_k=800:128.1324,lwr_k=900:129.2214,lwr_k=1000:130.2696'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:99.6776,lwr_k=200:110.0603,lwr_k=300:115.3819,lwr_k=400:117.1211,lwr_k=500:118.8725,lwr_k=600:120.7022,lwr_k=700:121.9423,lwr_k=800:123.3096,lwr_k=900:124.3425,lwr_k=1000:124.8986'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:358.9562,lwr_k=200:359.0375,lwr_k=300:358.9736,lwr_k=400:359.1076,lwr_k=500:359.0957,lwr_k=600:359.094,lwr_k=700:358.9589,lwr_k=800:358.9772,lwr_k=900:359.0148,lwr_k=1000:359.6073'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:329.085,lwr_k=200:328.9509,lwr_k=300:329.0064,lwr_k=400:328.94,lwr_k=500:328.9403,lwr_k=600:328.9403,lwr_k=700:329.0537,lwr_k=800:329.2266,lwr_k=900:329.3397,lwr_k=1000:330.3726'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_20'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:95.312,lwr_k=200:99.9266,lwr_k=300:101.5881,lwr_k=400:99.4641,lwr_k=500:98.9752,lwr_k=600:98.3676,lwr_k=700:98.0366,lwr_k=800:99.3063,lwr_k=900:100.488,lwr_k=1000:100.6554'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:108.054,lwr_k=200:112.2196,lwr_k=300:110.4827,lwr_k=400:103.8687,lwr_k=500:115.7084,lwr_k=600:113.9143,lwr_k=700:108.6797,lwr_k=800:111.5543,lwr_k=900:108.2434,lwr_k=1000:108.4326'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:84.3994,lwr_k=200:85.117,lwr_k=300:84.9371,lwr_k=400:84.9049,lwr_k=500:85.8215,lwr_k=600:84.816,lwr_k=700:85.3489,lwr_k=800:86.1559,lwr_k=900:85.2189,lwr_k=1000:85.2013'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:164.4507,lwr_k=200:167.6918,lwr_k=300:167.7765,lwr_k=400:171.3994,lwr_k=500:169.8196,lwr_k=600:167.666,lwr_k=700:169.4008,lwr_k=800:167.9024,lwr_k=900:167.1749,lwr_k=1000:169.214'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:82.3161,lwr_k=200:83.4936,lwr_k=300:85.2741,lwr_k=400:86.1686,lwr_k=500:87.7122,lwr_k=600:84.5353,lwr_k=700:86.0084,lwr_k=800:84.6359,lwr_k=900:84.5629,lwr_k=1000:85.47'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:179.4493,lwr_k=200:187.0995,lwr_k=300:184.9857,lwr_k=400:198.2165,lwr_k=500:206.6203,lwr_k=600:198.9661,lwr_k=700:163.9475,lwr_k=800:191.5386,lwr_k=900:189.6,lwr_k=1000:179.1892'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:107.9566,lwr_k=200:108.8578,lwr_k=300:107.8034,lwr_k=400:108.5366,lwr_k=500:110.0153,lwr_k=600:108.5138,lwr_k=700:107.7242,lwr_k=800:110.4558,lwr_k=900:110.6681,lwr_k=1000:111.7109'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:123.8026,lwr_k=200:120.9854,lwr_k=300:122.3032,lwr_k=400:122.9289,lwr_k=500:124.8113,lwr_k=600:124.04,lwr_k=700:123.6293,lwr_k=800:119.3409,lwr_k=900:123.2665,lwr_k=1000:122.7555'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:108.0248,lwr_k=200:107.0233,lwr_k=300:106.9614,lwr_k=400:107.8996,lwr_k=500:103.9348,lwr_k=600:111.5623,lwr_k=700:105.1368,lwr_k=800:106.4282,lwr_k=900:107.6399,lwr_k=1000:103.6263'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:122.5138,lwr_k=200:132.2603,lwr_k=300:135.2194,lwr_k=400:137.0745,lwr_k=500:133.809,lwr_k=600:126.9151,lwr_k=700:134.2733,lwr_k=800:129.8022,lwr_k=900:136.9945,lwr_k=1000:133.5485'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:100.723,lwr_k=200:103.4169,lwr_k=300:102.8348,lwr_k=400:102.4279,lwr_k=500:103.8164,lwr_k=600:101.3281,lwr_k=700:104.0576,lwr_k=800:104.4951,lwr_k=900:103.8309,lwr_k=1000:105.776'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:100.4385,lwr_k=200:98.1295,lwr_k=300:102.1873,lwr_k=400:101.2294,lwr_k=500:102.9413,lwr_k=600:107.8484,lwr_k=700:91.7342,lwr_k=800:94.3026,lwr_k=900:104.4508,lwr_k=1000:98.2841'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_21'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:56.1668,lwr_k=200:89.2492,lwr_k=300:100.1276,lwr_k=400:107.9548,lwr_k=500:111.8591,lwr_k=600:115.9907,lwr_k=700:119.0816,lwr_k=800:121.6179,lwr_k=900:124.1568,lwr_k=1000:126.0129'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:230.5625,lwr_k=200:162.0796,lwr_k=300:134.0008,lwr_k=400:126.9065,lwr_k=500:122.7041,lwr_k=600:123.3962,lwr_k=700:123.3184,lwr_k=800:125.6488,lwr_k=900:129.2318,lwr_k=1000:130.3722'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:38.0299,lwr_k=200:61.4178,lwr_k=300:75.7274,lwr_k=400:84.3667,lwr_k=500:90.8705,lwr_k=600:95.2617,lwr_k=700:99.2251,lwr_k=800:102.7335,lwr_k=900:105.4348,lwr_k=1000:107.7541'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:315.488,lwr_k=200:197.6,lwr_k=300:184.7265,lwr_k=400:185.3441,lwr_k=500:182.7594,lwr_k=600:184.594,lwr_k=700:187.3036,lwr_k=800:195.5942,lwr_k=900:199.9139,lwr_k=1000:205.0391'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:44.8223,lwr_k=200:73.2122,lwr_k=300:82.855,lwr_k=400:91.0678,lwr_k=500:98.7189,lwr_k=600:101.296,lwr_k=700:104.5051,lwr_k=800:107.641,lwr_k=900:111.6857,lwr_k=1000:114.176'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:232.6838,lwr_k=200:146.8526,lwr_k=300:222.808,lwr_k=400:217.0122,lwr_k=500:118.8447,lwr_k=600:116.4473,lwr_k=700:117.7025,lwr_k=800:116.2572,lwr_k=900:116.0247,lwr_k=1000:115.4561'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:75.046,lwr_k=200:91.3441,lwr_k=300:101.5276,lwr_k=400:107.8719,lwr_k=500:117.2528,lwr_k=600:125.4831,lwr_k=700:132.7286,lwr_k=800:137.3396,lwr_k=900:139.7115,lwr_k=1000:142.1181'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:272.5801,lwr_k=200:149.7401,lwr_k=300:133.7712,lwr_k=400:132.9503,lwr_k=500:126.0331,lwr_k=600:125.337,lwr_k=700:125.4292,lwr_k=800:124.3196,lwr_k=900:125.0115,lwr_k=1000:126.8324'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:45.408,lwr_k=200:71.2648,lwr_k=300:78.959,lwr_k=400:82.8732,lwr_k=500:86.6448,lwr_k=600:88.7621,lwr_k=700:89.8328,lwr_k=800:90.6835,lwr_k=900:91.7719,lwr_k=1000:92.5031'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:195.3471,lwr_k=200:125.7392,lwr_k=300:105.5092,lwr_k=400:97.783,lwr_k=500:96.4738,lwr_k=600:96.1853,lwr_k=700:94.1728,lwr_k=800:94.1515,lwr_k=900:94.1875,lwr_k=1000:95.2032'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:39.353,lwr_k=200:59.7027,lwr_k=300:65.4894,lwr_k=400:69.3073,lwr_k=500:71.7192,lwr_k=600:75.2452,lwr_k=700:77.4513,lwr_k=800:78.9056,lwr_k=900:79.8759,lwr_k=1000:80.6361'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:165.127,lwr_k=200:97.9444,lwr_k=300:113.8986,lwr_k=400:90.2342,lwr_k=500:88.9868,lwr_k=600:80.3708,lwr_k=700:78.0092,lwr_k=800:75.5528,lwr_k=900:75.7111,lwr_k=1000:74.9466'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_22'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:125.2554,lwr_k=200:126.6736,lwr_k=300:127.9857,lwr_k=400:130.1774,lwr_k=500:131.2559,lwr_k=600:131.7727,lwr_k=700:132.1403,lwr_k=800:132.3135,lwr_k=900:132.3513,lwr_k=1000:132.5519'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:130.8392,lwr_k=200:126.7854,lwr_k=300:124.2731,lwr_k=400:123.1109,lwr_k=500:122.2234,lwr_k=600:122.4286,lwr_k=700:121.958,lwr_k=800:121.4996,lwr_k=900:121.7813,lwr_k=1000:121.9429'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:322.5987,lwr_k=200:322.6803,lwr_k=300:322.6163,lwr_k=400:322.7505,lwr_k=500:322.7386,lwr_k=600:322.7369,lwr_k=700:322.6015,lwr_k=800:322.6196,lwr_k=900:322.6571,lwr_k=1000:323.249'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:425.3142,lwr_k=200:425.2916,lwr_k=300:425.2853,lwr_k=400:425.3226,lwr_k=500:425.3166,lwr_k=600:425.3158,lwr_k=700:425.3005,lwr_k=800:425.3934,lwr_k=900:425.4674,lwr_k=1000:426.2723'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:105.644,lwr_k=200:109.132,lwr_k=300:110.251,lwr_k=400:110.7276,lwr_k=500:111.3595,lwr_k=600:111.8102,lwr_k=700:112.1202,lwr_k=800:112.251,lwr_k=900:112.3214,lwr_k=1000:112.4131'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:144.3614,lwr_k=200:132.8311,lwr_k=300:119.5591,lwr_k=400:120.0492,lwr_k=500:120.6885,lwr_k=600:120.6306,lwr_k=700:121.7436,lwr_k=800:121.8838,lwr_k=900:122.0636,lwr_k=1000:121.9242'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:382.9008,lwr_k=200:383.0085,lwr_k=300:382.93,lwr_k=400:383.0885,lwr_k=500:383.0752,lwr_k=600:383.0732,lwr_k=700:382.9076,lwr_k=800:382.907,lwr_k=900:382.9353,lwr_k=1000:383.4738'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:301.2381,lwr_k=200:301.233,lwr_k=300:301.217,lwr_k=400:301.2706,lwr_k=500:301.2636,lwr_k=600:301.2626,lwr_k=700:301.2271,lwr_k=800:301.3075,lwr_k=900:301.3754,lwr_k=1000:302.1446'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:120.3964,lwr_k=200:122.9616,lwr_k=300:126.8253,lwr_k=400:127.5547,lwr_k=500:127.8793,lwr_k=600:128.2996,lwr_k=700:128.6957,lwr_k=800:129.0862,lwr_k=900:128.8873,lwr_k=1000:129.0323'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:133.2059,lwr_k=200:130.4705,lwr_k=300:136.1017,lwr_k=400:133.8369,lwr_k=500:133.6401,lwr_k=600:133.8239,lwr_k=700:132.9413,lwr_k=800:132.7406,lwr_k=900:132.6052,lwr_k=1000:133.5224'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:114.8876,lwr_k=200:120.115,lwr_k=300:121.0662,lwr_k=400:121.6904,lwr_k=500:122.2929,lwr_k=600:122.4753,lwr_k=700:122.5441,lwr_k=800:122.7501,lwr_k=900:122.9376,lwr_k=1000:123.3803'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:178.1419,lwr_k=200:173.6888,lwr_k=300:170.4803,lwr_k=400:168.9487,lwr_k=500:167.6199,lwr_k=600:167.1273,lwr_k=700:165.9543,lwr_k=800:165.7545,lwr_k=900:164.4158,lwr_k=1000:164.5074'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_23'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:144.7539,lwr_k=200:143.3214,lwr_k=300:143.1357,lwr_k=400:147.0007,lwr_k=500:148.7644,lwr_k=600:149.6165,lwr_k=700:150.6658,lwr_k=800:151.5209,lwr_k=900:152.2959,lwr_k=1000:153.067'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:169.6781,lwr_k=200:153.0745,lwr_k=300:164.4234,lwr_k=400:165.028,lwr_k=500:167.0986,lwr_k=600:168.3063,lwr_k=700:168.7426,lwr_k=800:169.1002,lwr_k=900:169.0973,lwr_k=1000:169.3948'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:95.4321,lwr_k=200:102.1476,lwr_k=300:104.8294,lwr_k=400:107.4791,lwr_k=500:108.8098,lwr_k=600:111.0126,lwr_k=700:111.6531,lwr_k=800:112.3576,lwr_k=900:112.9416,lwr_k=1000:113.3707'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:177.8659,lwr_k=200:177.1467,lwr_k=300:177.8938,lwr_k=400:176.2686,lwr_k=500:176.7257,lwr_k=600:177.0219,lwr_k=700:176.6369,lwr_k=800:176.9649,lwr_k=900:176.9443,lwr_k=1000:177.2072'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:115.5572,lwr_k=200:127.8089,lwr_k=300:135.062,lwr_k=400:138.5343,lwr_k=500:140.0466,lwr_k=600:141.295,lwr_k=700:143.2321,lwr_k=800:144.8479,lwr_k=900:145.7834,lwr_k=1000:146.8938'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:162.4982,lwr_k=200:157.5237,lwr_k=300:163.6581,lwr_k=400:167.118,lwr_k=500:165.6175,lwr_k=600:165.7037,lwr_k=700:163.3269,lwr_k=800:163.6444,lwr_k=900:163.5121,lwr_k=1000:162.9644'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:149.5195,lwr_k=200:162.8444,lwr_k=300:168.322,lwr_k=400:172.7572,lwr_k=500:176.7345,lwr_k=600:181.9905,lwr_k=700:183.3306,lwr_k=800:187.5693,lwr_k=900:189.3648,lwr_k=1000:190.8446'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:155.1166,lwr_k=200:152.3187,lwr_k=300:168.1558,lwr_k=400:166.6257,lwr_k=500:166.2747,lwr_k=600:167.0812,lwr_k=700:167.7138,lwr_k=800:134.7101,lwr_k=900:136.3153,lwr_k=1000:137.1739'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:300.9905,lwr_k=200:5122.8594,lwr_k=300:4574.9594,lwr_k=400:27966.0228,lwr_k=500:13566.6122,lwr_k=600:550.828,lwr_k=700:13565.5688,lwr_k=800:557.1982,lwr_k=900:558.1866,lwr_k=1000:616.3966'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:338.034,lwr_k=200:5631.877,lwr_k=300:4874.569,lwr_k=400:31332.7691,lwr_k=500:14299.3786,lwr_k=600:648.0409,lwr_k=700:14280.1137,lwr_k=800:600.8718,lwr_k=900:600.5938,lwr_k=1000:680.1087'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:316.6979,lwr_k=200:11289.6689,lwr_k=300:554.087,lwr_k=400:554.1712,lwr_k=500:1814.4943,lwr_k=600:554.4686,lwr_k=700:556.465,lwr_k=800:32404.5772,lwr_k=900:32404.6446,lwr_k=1000:491.9314'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:291.7312,lwr_k=200:12192.2286,lwr_k=300:607.5023,lwr_k=400:607.6286,lwr_k=500:1884.4458,lwr_k=600:607.6836,lwr_k=700:517.7982,lwr_k=800:34763.8971,lwr_k=900:34587.9005,lwr_k=1000:526.2071'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_24'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:64.8004,lwr_k=200:59.7411,lwr_k=300:61.5677,lwr_k=400:61.8899,lwr_k=500:59.8238,lwr_k=600:60.225,lwr_k=700:60.2213,lwr_k=800:60.1807,lwr_k=900:59.3567,lwr_k=1000:59.7625'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:269.4543,lwr_k=200:155.7237,lwr_k=300:130.4692,lwr_k=400:122.8448,lwr_k=500:120.3277,lwr_k=600:115.683,lwr_k=700:116.0759,lwr_k=800:114.2715,lwr_k=900:111.7626,lwr_k=1000:110.9098'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:110.8911,lwr_k=200:93.8437,lwr_k=300:85.3664,lwr_k=400:85.7385,lwr_k=500:84.7831,lwr_k=600:82.4765,lwr_k=700:80.322,lwr_k=800:80.2525,lwr_k=900:84.2669,lwr_k=1000:81.7398'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:368.0298,lwr_k=200:220.9183,lwr_k=300:188.5669,lwr_k=400:164.0617,lwr_k=500:163.3642,lwr_k=600:157.441,lwr_k=700:157.315,lwr_k=800:155.3416,lwr_k=900:152.3902,lwr_k=1000:153.6622'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:45.1181,lwr_k=200:49.0225,lwr_k=300:50.2813,lwr_k=400:52.2354,lwr_k=500:50.5504,lwr_k=600:52.0545,lwr_k=700:51.3347,lwr_k=800:50.9289,lwr_k=900:50.1845,lwr_k=1000:50.089'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:178.4934,lwr_k=200:126.8054,lwr_k=300:115.5308,lwr_k=400:113.779,lwr_k=500:107.668,lwr_k=600:108.6387,lwr_k=700:104.5853,lwr_k=800:104.9569,lwr_k=900:102.0863,lwr_k=1000:101.4101'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:123.3085,lwr_k=200:139.6535,lwr_k=300:158.1516,lwr_k=400:158.9638,lwr_k=500:157.9357,lwr_k=600:154.0596,lwr_k=700:159.4514,lwr_k=800:166.0893,lwr_k=900:171.6479,lwr_k=1000:158.4022'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:354.3345,lwr_k=200:247.0537,lwr_k=300:222.7809,lwr_k=400:225.0741,lwr_k=500:190.8527,lwr_k=600:191.7859,lwr_k=700:179.4126,lwr_k=800:163.4827,lwr_k=900:168.0697,lwr_k=1000:170.3612'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:93.3061,lwr_k=200:92.5568,lwr_k=300:107.2221,lwr_k=400:105.0375,lwr_k=500:109.2151,lwr_k=600:110.6771,lwr_k=700:109.8869,lwr_k=800:111.5931,lwr_k=900:110.4179,lwr_k=1000:111.6845'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:268.264,lwr_k=200:152.7954,lwr_k=300:172.3409,lwr_k=400:134.015,lwr_k=500:140.8866,lwr_k=600:115.2036,lwr_k=700:133.799,lwr_k=800:127.6274,lwr_k=900:122.9001,lwr_k=1000:125.776'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:38.8954,lwr_k=200:50.7549,lwr_k=300:57.5834,lwr_k=400:63.8005,lwr_k=500:66.1728,lwr_k=600:67.9498,lwr_k=700:69.4507,lwr_k=800:70.1805,lwr_k=900:70.9847,lwr_k=1000:71.6271'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:315.8831,lwr_k=200:146.3331,lwr_k=300:117.9763,lwr_k=400:103.6267,lwr_k=500:105.8173,lwr_k=600:102.1636,lwr_k=700:101.2462,lwr_k=800:95.4915,lwr_k=900:94.1368,lwr_k=1000:93.2632'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_25'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:121.4756,lwr_k=200:103.5727,lwr_k=300:100.8637,lwr_k=400:97.8034,lwr_k=500:96.3211,lwr_k=600:94.9315,lwr_k=700:95.4657,lwr_k=800:93.037,lwr_k=900:93.4107,lwr_k=1000:92.7465'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:190.0573,lwr_k=200:153.6708,lwr_k=300:139.2383,lwr_k=400:129.2281,lwr_k=500:122.2002,lwr_k=600:119.5218,lwr_k=700:115.86,lwr_k=800:116.4234,lwr_k=900:116.219,lwr_k=1000:113.6666'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:95.0169,lwr_k=200:85.2238,lwr_k=300:87.279,lwr_k=400:86.6682,lwr_k=500:86.9704,lwr_k=600:88.4079,lwr_k=700:86.4873,lwr_k=800:85.2718,lwr_k=900:89.4377,lwr_k=1000:86.9291'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:202.3386,lwr_k=200:174.6061,lwr_k=300:170.3457,lwr_k=400:165.9823,lwr_k=500:165.8433,lwr_k=600:167.1203,lwr_k=700:172.5984,lwr_k=800:179.5699,lwr_k=900:175.8841,lwr_k=1000:171.331'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:93.4558,lwr_k=200:77.4721,lwr_k=300:72.8329,lwr_k=400:73.3651,lwr_k=500:72.4646,lwr_k=600:73.1973,lwr_k=700:73.2715,lwr_k=800:73.007,lwr_k=900:73.7981,lwr_k=1000:73.7674'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:165.3513,lwr_k=200:132.3095,lwr_k=300:114.2573,lwr_k=400:108.2587,lwr_k=500:104.7704,lwr_k=600:102.2979,lwr_k=700:102.9233,lwr_k=800:102.2311,lwr_k=900:101.27,lwr_k=1000:99.7944'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:109.9687,lwr_k=200:97.1276,lwr_k=300:92.6157,lwr_k=400:96.9074,lwr_k=500:93.4175,lwr_k=600:99.1605,lwr_k=700:92.7456,lwr_k=800:89.2847,lwr_k=900:86.9071,lwr_k=1000:90.8961'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:186.0529,lwr_k=200:161.1149,lwr_k=300:160.6524,lwr_k=400:150.2434,lwr_k=500:151.9732,lwr_k=600:148.879,lwr_k=700:138.1635,lwr_k=800:137.3501,lwr_k=900:140.2732,lwr_k=1000:146.1337'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:113.3292,lwr_k=200:98.872,lwr_k=300:95.148,lwr_k=400:96.5612,lwr_k=500:91.1746,lwr_k=600:90.5034,lwr_k=700:90.6318,lwr_k=800:91.7208,lwr_k=900:122.1502,lwr_k=1000:111.596'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:178.8898,lwr_k=200:134.6534,lwr_k=300:112.8118,lwr_k=400:109.2519,lwr_k=500:104.085,lwr_k=600:101.2447,lwr_k=700:99.5464,lwr_k=800:100.9383,lwr_k=900:121.2074,lwr_k=1000:108.7515'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:125.6974,lwr_k=200:106.4403,lwr_k=300:105.6629,lwr_k=400:106.4756,lwr_k=500:108.7772,lwr_k=600:109.4368,lwr_k=700:108.0893,lwr_k=800:109.795,lwr_k=900:113.1126,lwr_k=1000:110.6835'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:206.4636,lwr_k=200:154.3184,lwr_k=300:138.2402,lwr_k=400:135.3762,lwr_k=500:136.1849,lwr_k=600:139.8458,lwr_k=700:135.1269,lwr_k=800:134.8082,lwr_k=900:150.564,lwr_k=1000:147.6316'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_26'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:50.2212,lwr_k=200:65.5526,lwr_k=300:70.7492,lwr_k=400:73.5188,lwr_k=500:76.5475,lwr_k=600:77.7891,lwr_k=700:79.0463,lwr_k=800:79.6227,lwr_k=900:80.5423,lwr_k=1000:81.3102'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:188.4114,lwr_k=200:142.5305,lwr_k=300:117.7264,lwr_k=400:114.5227,lwr_k=500:113.3659,lwr_k=600:112.036,lwr_k=700:109.3628,lwr_k=800:108.6273,lwr_k=900:104.3079,lwr_k=1000:103.6199'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:47.9464,lwr_k=200:58.2031,lwr_k=300:62.1797,lwr_k=400:64.811,lwr_k=500:66.8215,lwr_k=600:67.7705,lwr_k=700:69.5188,lwr_k=800:70.8627,lwr_k=900:71.8089,lwr_k=1000:72.6911'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:276.9646,lwr_k=200:193.4459,lwr_k=300:174.0973,lwr_k=400:165.0468,lwr_k=500:161.2221,lwr_k=600:158.7536,lwr_k=700:157.127,lwr_k=800:155.6446,lwr_k=900:155.92,lwr_k=1000:155.919'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:38.411,lwr_k=200:50.384,lwr_k=300:57.3157,lwr_k=400:62.0821,lwr_k=500:64.7467,lwr_k=600:66.4282,lwr_k=700:68.2954,lwr_k=800:69.0595,lwr_k=900:70.0407,lwr_k=1000:70.5076'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:204.7237,lwr_k=200:130.7665,lwr_k=300:106.8256,lwr_k=400:102.4429,lwr_k=500:100.7585,lwr_k=600:101.1857,lwr_k=700:100.3078,lwr_k=800:100.6491,lwr_k=900:99.5024,lwr_k=1000:99.2244'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:36.1639,lwr_k=200:51.052,lwr_k=300:58.4393,lwr_k=400:61.8776,lwr_k=500:64.2421,lwr_k=600:64.9503,lwr_k=700:65.7349,lwr_k=800:66.7589,lwr_k=900:67.419,lwr_k=1000:68.1043'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:183.517,lwr_k=200:152.383,lwr_k=300:144.5904,lwr_k=400:141.7707,lwr_k=500:140.9588,lwr_k=600:137.8548,lwr_k=700:138.4161,lwr_k=800:140.9167,lwr_k=900:138.7641,lwr_k=1000:138.373'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:52.844,lwr_k=200:74.2966,lwr_k=300:82.417,lwr_k=400:88.5215,lwr_k=500:92.2558,lwr_k=600:93.8444,lwr_k=700:94.9472,lwr_k=800:95.8195,lwr_k=900:96.9323,lwr_k=1000:97.8417'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:338.3318,lwr_k=200:167.8201,lwr_k=300:136.5427,lwr_k=400:120.1356,lwr_k=500:117.4817,lwr_k=600:116.4465,lwr_k=700:116.6329,lwr_k=800:116.0566,lwr_k=900:114.214,lwr_k=1000:112.344'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:40.5645,lwr_k=200:57.7611,lwr_k=300:64.319,lwr_k=400:68.3699,lwr_k=500:70.9511,lwr_k=600:72.846,lwr_k=700:74.0213,lwr_k=800:75.2982,lwr_k=900:76.1889,lwr_k=1000:77.2776'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:229.3162,lwr_k=200:159.4349,lwr_k=300:133.9583,lwr_k=400:132.1185,lwr_k=500:125.3113,lwr_k=600:123.2778,lwr_k=700:125.7581,lwr_k=800:125.96,lwr_k=900:124.5355,lwr_k=1000:124.0479'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_27'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:101.333,lwr_k=200:109.4333,lwr_k=300:113.0349,lwr_k=400:115.0247,lwr_k=500:116.5407,lwr_k=600:117.2432,lwr_k=700:118.0592,lwr_k=800:118.5228,lwr_k=900:119.0814,lwr_k=1000:119.7175'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:119.5089,lwr_k=200:123.8652,lwr_k=300:122.8898,lwr_k=400:123.1744,lwr_k=500:123.1814,lwr_k=600:122.9415,lwr_k=700:123.553,lwr_k=800:123.6993,lwr_k=900:123.218,lwr_k=1000:122.9166'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:80.9167,lwr_k=200:92.8916,lwr_k=300:97.9297,lwr_k=400:100.0176,lwr_k=500:102.3585,lwr_k=600:103.7025,lwr_k=700:105.3327,lwr_k=800:105.8744,lwr_k=900:106.4667,lwr_k=1000:106.8445'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:173.7335,lwr_k=200:161.8089,lwr_k=300:160.9433,lwr_k=400:166.2479,lwr_k=500:166.6862,lwr_k=600:167.3136,lwr_k=700:167.6085,lwr_k=800:167.2422,lwr_k=900:167.6255,lwr_k=1000:168.3035'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:82.8771,lwr_k=200:90.7393,lwr_k=300:93.8074,lwr_k=400:96.1443,lwr_k=500:98.1426,lwr_k=600:99.114,lwr_k=700:99.7916,lwr_k=800:100.0983,lwr_k=900:100.5611,lwr_k=1000:100.9975'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:120.3792,lwr_k=200:111.7257,lwr_k=300:109.8352,lwr_k=400:109.9529,lwr_k=500:110.7513,lwr_k=600:110.8978,lwr_k=700:110.2574,lwr_k=800:111.194,lwr_k=900:111.1678,lwr_k=1000:111.3944'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:104.012,lwr_k=200:111.7804,lwr_k=300:117.2796,lwr_k=400:119.8582,lwr_k=500:121.6466,lwr_k=600:123.0235,lwr_k=700:124.1071,lwr_k=800:124.5296,lwr_k=900:125.116,lwr_k=1000:125.7041'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:131.9295,lwr_k=200:119.8046,lwr_k=300:113.4852,lwr_k=400:113.3104,lwr_k=500:111.3159,lwr_k=600:110.7861,lwr_k=700:109.828,lwr_k=800:110.1682,lwr_k=900:110.0547,lwr_k=1000:109.5652'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:110.439,lwr_k=200:122.303,lwr_k=300:124.2792,lwr_k=400:125.9023,lwr_k=500:127.2925,lwr_k=600:128.4272,lwr_k=700:129.1625,lwr_k=800:129.7685,lwr_k=900:130.0443,lwr_k=1000:130.1949'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:129.616,lwr_k=200:131.8622,lwr_k=300:133.9717,lwr_k=400:135.6911,lwr_k=500:135.5299,lwr_k=600:133.7329,lwr_k=700:135.9689,lwr_k=800:135.1041,lwr_k=900:135.5499,lwr_k=1000:136.1891'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:95.2911,lwr_k=200:104.3096,lwr_k=300:107.345,lwr_k=400:111.455,lwr_k=500:113.3307,lwr_k=600:114.8068,lwr_k=700:115.8187,lwr_k=800:116.8774,lwr_k=900:117.6406,lwr_k=1000:118.416'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:121.3473,lwr_k=200:120.098,lwr_k=300:122.6347,lwr_k=400:119.4897,lwr_k=500:120.8919,lwr_k=600:121.0764,lwr_k=700:120.1556,lwr_k=800:119.1357,lwr_k=900:119.1211,lwr_k=1000:116.6484'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_28'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:119.116,lwr_k=200:118.9649,lwr_k=300:116.5325,lwr_k=400:117.9069,lwr_k=500:125.6789,lwr_k=600:124.3742,lwr_k=700:121.8154,lwr_k=800:121.7177,lwr_k=900:120.2613,lwr_k=1000:120.8803'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:133.0731,lwr_k=200:131.6961,lwr_k=300:133.8098,lwr_k=400:124.5713,lwr_k=500:142.8353,lwr_k=600:138.7349,lwr_k=700:149.5912,lwr_k=800:133.9223,lwr_k=900:133.2526,lwr_k=1000:131.9572'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:102.5333,lwr_k=200:103.9285,lwr_k=300:104.7506,lwr_k=400:103.9274,lwr_k=500:105.1529,lwr_k=600:103.925,lwr_k=700:105.7285,lwr_k=800:105.8038,lwr_k=900:104.7923,lwr_k=1000:105.8781'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:188.1385,lwr_k=200:186.5371,lwr_k=300:184.7137,lwr_k=400:183.0595,lwr_k=500:183.825,lwr_k=600:182.3059,lwr_k=700:187.0425,lwr_k=800:182.8772,lwr_k=900:183.3606,lwr_k=1000:183.6234'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:180.374,lwr_k=200:181.2304,lwr_k=300:180.7058,lwr_k=400:179.832,lwr_k=500:182.8797,lwr_k=600:181.6568,lwr_k=700:186.2764,lwr_k=800:185.9719,lwr_k=900:181.2212,lwr_k=1000:185.6466'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:293.5349,lwr_k=200:200.5228,lwr_k=300:194.5337,lwr_k=400:197.3285,lwr_k=500:195.4639,lwr_k=600:192.2047,lwr_k=700:190.8465,lwr_k=800:189.5618,lwr_k=900:189.1549,lwr_k=1000:190.9017'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:110.0026,lwr_k=200:104.4922,lwr_k=300:104.1067,lwr_k=400:111.7263,lwr_k=500:108.5683,lwr_k=600:108.0226,lwr_k=700:108.7948,lwr_k=800:109.6448,lwr_k=900:108.3654,lwr_k=1000:113.5471'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:123.4055,lwr_k=200:124.0035,lwr_k=300:123.154,lwr_k=400:123.6907,lwr_k=500:127.2205,lwr_k=600:125.336,lwr_k=700:121.7661,lwr_k=800:118.9272,lwr_k=900:119.723,lwr_k=1000:119.544'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:94.8692,lwr_k=200:96.1522,lwr_k=300:98.4656,lwr_k=400:97.9029,lwr_k=500:98.8337,lwr_k=600:99.3274,lwr_k=700:97.2532,lwr_k=800:97.0396,lwr_k=900:98.6334,lwr_k=1000:97.5003'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:170.8147,lwr_k=200:174.6276,lwr_k=300:175.9235,lwr_k=400:177.7609,lwr_k=500:176.0655,lwr_k=600:177.0691,lwr_k=700:175.4616,lwr_k=800:177.2332,lwr_k=900:178.0581,lwr_k=1000:178.3137'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:92.7628,lwr_k=200:89.6708,lwr_k=300:101.7817,lwr_k=400:97.183,lwr_k=500:97.8109,lwr_k=600:93.3894,lwr_k=700:95.4416,lwr_k=800:99.064,lwr_k=900:96.1884,lwr_k=1000:94.4424'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:150.1273,lwr_k=200:145.817,lwr_k=300:154.2297,lwr_k=400:170.8498,lwr_k=500:160.9233,lwr_k=600:160.7091,lwr_k=700:150.2093,lwr_k=800:169.319,lwr_k=900:163.6941,lwr_k=1000:152.7144'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_29'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:111.718,lwr_k=200:111.5474,lwr_k=300:111.3466,lwr_k=400:115.5749,lwr_k=500:114.706,lwr_k=600:114.9756,lwr_k=700:113.7789,lwr_k=800:109.0386,lwr_k=900:115.0296,lwr_k=1000:118.7965'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:128.809,lwr_k=200:128.5628,lwr_k=300:129.3994,lwr_k=400:135.7054,lwr_k=500:133.3635,lwr_k=600:134.4291,lwr_k=700:132.3669,lwr_k=800:123.9418,lwr_k=900:133.6285,lwr_k=1000:138.5755'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:123.7764,lwr_k=200:120.4163,lwr_k=300:120.8212,lwr_k=400:121.5805,lwr_k=500:122.4577,lwr_k=600:123.7823,lwr_k=700:122.5238,lwr_k=800:124.6611,lwr_k=900:126.9079,lwr_k=1000:126.329'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:237.0704,lwr_k=200:245.6623,lwr_k=300:238.5217,lwr_k=400:255.4179,lwr_k=500:237.9751,lwr_k=600:226.659,lwr_k=700:233.5327,lwr_k=800:237.6753,lwr_k=900:228.6142,lwr_k=1000:235.4412'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:93.9627,lwr_k=200:93.3658,lwr_k=300:93.1179,lwr_k=400:93.8645,lwr_k=500:94.3181,lwr_k=600:94.4975,lwr_k=700:95.3977,lwr_k=800:95.3515,lwr_k=900:95.2552,lwr_k=1000:95.4374'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:140.9943,lwr_k=200:151.1073,lwr_k=300:148.7994,lwr_k=400:148.2865,lwr_k=500:124.6599,lwr_k=600:124.8152,lwr_k=700:126.258,lwr_k=800:146.984,lwr_k=900:124.9498,lwr_k=1000:125.8053'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:110.6212,lwr_k=200:114.0462,lwr_k=300:115.5838,lwr_k=400:115.8004,lwr_k=500:116.364,lwr_k=600:115.1792,lwr_k=700:118.2059,lwr_k=800:117.6761,lwr_k=900:119.0766,lwr_k=1000:117.4439'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:131.4816,lwr_k=200:139.8984,lwr_k=300:142.9882,lwr_k=400:140.8832,lwr_k=500:132.3748,lwr_k=600:153.1513,lwr_k=700:154.3376,lwr_k=800:154.4211,lwr_k=900:143.4416,lwr_k=1000:134.2033'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:113.1491,lwr_k=200:116.5524,lwr_k=300:118.1556,lwr_k=400:118.7747,lwr_k=500:119.7226,lwr_k=600:120.3093,lwr_k=700:120.9667,lwr_k=800:121.3106,lwr_k=900:121.5691,lwr_k=1000:122.7306'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:125.7052,lwr_k=200:130.8578,lwr_k=300:134.0777,lwr_k=400:135.4639,lwr_k=500:136.3533,lwr_k=600:136.8052,lwr_k=700:137.422,lwr_k=800:137.6798,lwr_k=900:138.2223,lwr_k=1000:139.6092'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:111.6548,lwr_k=200:118.6082,lwr_k=300:127.6965,lwr_k=400:119.6097,lwr_k=500:114.4619,lwr_k=600:125.7693,lwr_k=700:126.0773,lwr_k=800:144.6688,lwr_k=900:128.3768,lwr_k=1000:125.9503'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:143.071,lwr_k=200:148.5596,lwr_k=300:152.7895,lwr_k=400:147.2769,lwr_k=500:144.6684,lwr_k=600:151.2318,lwr_k=700:148.509,lwr_k=800:171.9341,lwr_k=900:152.7546,lwr_k=1000:151.0038'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_30'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:135.2744,lwr_k=200:139.4885,lwr_k=300:146.7516,lwr_k=400:150.3728,lwr_k=500:157.8851,lwr_k=600:158.1349,lwr_k=700:161.2354,lwr_k=800:163.4179,lwr_k=900:164.1677,lwr_k=1000:164.8683'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:234.9769,lwr_k=200:170.4006,lwr_k=300:168.8039,lwr_k=400:175.091,lwr_k=500:181.7351,lwr_k=600:181.4152,lwr_k=700:184.3921,lwr_k=800:188.3206,lwr_k=900:188.3665,lwr_k=1000:188.1674'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:87.7223,lwr_k=200:86.1069,lwr_k=300:83.1208,lwr_k=400:83.5352,lwr_k=500:83.6275,lwr_k=600:84.4701,lwr_k=700:84.7834,lwr_k=800:85.8493,lwr_k=900:86.1331,lwr_k=1000:86.1981'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:222.4373,lwr_k=200:185.7372,lwr_k=300:180.8356,lwr_k=400:179.2808,lwr_k=500:178.3613,lwr_k=600:176.1917,lwr_k=700:175.8232,lwr_k=800:174.2719,lwr_k=900:176.0785,lwr_k=1000:176.1039'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:77.0384,lwr_k=200:92.0181,lwr_k=300:98.824,lwr_k=400:105.0581,lwr_k=500:107.7628,lwr_k=600:109.1431,lwr_k=700:104.0132,lwr_k=800:104.8293,lwr_k=900:101.7157,lwr_k=1000:101.0395'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:135.0545,lwr_k=200:120.5328,lwr_k=300:117.4106,lwr_k=400:114.758,lwr_k=500:114.1314,lwr_k=600:113.7126,lwr_k=700:124.1254,lwr_k=800:117.1294,lwr_k=900:112.5949,lwr_k=1000:116.5479'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:80.1693,lwr_k=200:89.2082,lwr_k=300:92.0232,lwr_k=400:91.6038,lwr_k=500:75.2435,lwr_k=600:77.5219,lwr_k=700:76.1666,lwr_k=800:75.2868,lwr_k=900:76.1339,lwr_k=1000:76.3557'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:142.1067,lwr_k=200:127.189,lwr_k=300:122.7767,lwr_k=400:133.2925,lwr_k=500:159.769,lwr_k=600:173.5355,lwr_k=700:167.1446,lwr_k=800:159.9314,lwr_k=900:143.7906,lwr_k=1000:163.0187'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:40.9595,lwr_k=200:83.7261,lwr_k=300:103.1085,lwr_k=400:109.8511,lwr_k=500:111.1443,lwr_k=600:112.8736,lwr_k=700:113.5341,lwr_k=800:114.0276,lwr_k=900:114.2964,lwr_k=1000:114.5939'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:227.9846,lwr_k=200:138.0348,lwr_k=300:120.8396,lwr_k=400:117.2774,lwr_k=500:118.0573,lwr_k=600:117.173,lwr_k=700:118.287,lwr_k=800:117.5406,lwr_k=900:117.6728,lwr_k=1000:118.6877'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:93.3653,lwr_k=200:112.7598,lwr_k=300:122.9175,lwr_k=400:128.9185,lwr_k=500:129.0723,lwr_k=600:135.5125,lwr_k=700:138.2394,lwr_k=800:140.6806,lwr_k=900:142.6462,lwr_k=1000:144.4941'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:190.974,lwr_k=200:155.8079,lwr_k=300:152.5001,lwr_k=400:156.5785,lwr_k=500:160.6859,lwr_k=600:151.7979,lwr_k=700:157.0698,lwr_k=800:164.068,lwr_k=900:153.5355,lwr_k=1000:151.3824'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_31'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:46.4789,lwr_k=200:70.6305,lwr_k=300:84.1069,lwr_k=400:92.4517,lwr_k=500:97.3069,lwr_k=600:100.5406,lwr_k=700:103.6929,lwr_k=800:105.9643,lwr_k=900:107.5452,lwr_k=1000:109.5064'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:332.8987,lwr_k=200:142.6377,lwr_k=300:120.6611,lwr_k=400:120.6607,lwr_k=500:115.187,lwr_k=600:114.5102,lwr_k=700:115.254,lwr_k=800:115.9712,lwr_k=900:117.1698,lwr_k=1000:116.7493'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:35.7172,lwr_k=200:56.3132,lwr_k=300:66.1981,lwr_k=400:71.8616,lwr_k=500:75.5638,lwr_k=600:79.4794,lwr_k=700:82.0139,lwr_k=800:84.5797,lwr_k=900:85.7185,lwr_k=1000:87.2372'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:271.5049,lwr_k=200:176.4163,lwr_k=300:164.6091,lwr_k=400:157.2244,lwr_k=500:154.1768,lwr_k=600:154.3146,lwr_k=700:154.9901,lwr_k=800:154.9952,lwr_k=900:155.7957,lwr_k=1000:156.9938'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:64.9386,lwr_k=200:78.0523,lwr_k=300:80.1956,lwr_k=400:85.3582,lwr_k=500:87.496,lwr_k=600:91.1646,lwr_k=700:93.446,lwr_k=800:96.6885,lwr_k=900:97.9489,lwr_k=1000:99.4391'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:278.2279,lwr_k=200:149.5306,lwr_k=300:135.9916,lwr_k=400:123.2675,lwr_k=500:124.8182,lwr_k=600:123.1468,lwr_k=700:120.1131,lwr_k=800:121.6692,lwr_k=900:120.5832,lwr_k=1000:118.6456'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:83.53,lwr_k=200:125.9174,lwr_k=300:95.2567,lwr_k=400:101.107,lwr_k=500:111.8605,lwr_k=600:118.0942,lwr_k=700:116.1504,lwr_k=800:114.0813,lwr_k=900:129.1692,lwr_k=1000:125.6826'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:352.9724,lwr_k=200:228.1043,lwr_k=300:138.0561,lwr_k=400:143.2229,lwr_k=500:159.765,lwr_k=600:146.6379,lwr_k=700:150.9501,lwr_k=800:160.1576,lwr_k=900:159.0369,lwr_k=1000:154.8516'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:143.6302,lwr_k=200:101.433,lwr_k=300:107.611,lwr_k=400:103.707,lwr_k=500:104.7381,lwr_k=600:108.8197,lwr_k=700:112.8703,lwr_k=800:116.5519,lwr_k=900:120.0845,lwr_k=1000:124.121'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:345.6153,lwr_k=200:195.4742,lwr_k=300:171.7498,lwr_k=400:158.9705,lwr_k=500:131.0366,lwr_k=600:155.5661,lwr_k=700:121.3675,lwr_k=800:123.9185,lwr_k=900:129.0396,lwr_k=1000:133.2957'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:42.2908,lwr_k=200:68.9657,lwr_k=300:78.1917,lwr_k=400:83.9689,lwr_k=500:86.6646,lwr_k=600:89.6657,lwr_k=700:93.0628,lwr_k=800:95.469,lwr_k=900:97.5701,lwr_k=1000:99.6995'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:222.3035,lwr_k=200:153.6171,lwr_k=300:127.7515,lwr_k=400:128.9263,lwr_k=500:126.9511,lwr_k=600:126.4491,lwr_k=700:124.8075,lwr_k=800:120.7322,lwr_k=900:118.8941,lwr_k=1000:115.5441'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_32'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:104.3513,lwr_k=200:115.7531,lwr_k=300:120.7291,lwr_k=400:123.5357,lwr_k=500:125.1758,lwr_k=600:126.1208,lwr_k=700:127.5418,lwr_k=800:129.0334,lwr_k=900:130.1181,lwr_k=1000:131.0456'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:232.2195,lwr_k=200:180.0341,lwr_k=300:169.9712,lwr_k=400:167.7614,lwr_k=500:164.453,lwr_k=600:163.6074,lwr_k=700:163.807,lwr_k=800:164.1968,lwr_k=900:163.435,lwr_k=1000:163.6272'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:95.7412,lwr_k=200:94.5937,lwr_k=300:97.5322,lwr_k=400:99.0991,lwr_k=500:99.6214,lwr_k=600:100.3913,lwr_k=700:100.9601,lwr_k=800:100.8037,lwr_k=900:101.2461,lwr_k=1000:101.7436'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:221.5608,lwr_k=200:172.0791,lwr_k=300:160.2157,lwr_k=400:156.9972,lwr_k=500:153.4579,lwr_k=600:153.0458,lwr_k=700:151.7881,lwr_k=800:152.4414,lwr_k=900:150.4716,lwr_k=1000:150.3687'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:80.4696,lwr_k=200:104.0363,lwr_k=300:114.0639,lwr_k=400:120.6408,lwr_k=500:124.9643,lwr_k=600:129.1991,lwr_k=700:133.8508,lwr_k=800:135.9816,lwr_k=900:136.9486,lwr_k=1000:138.6109'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:205.4214,lwr_k=200:159.9189,lwr_k=300:141.8595,lwr_k=400:142.0848,lwr_k=500:152.5756,lwr_k=600:154.7704,lwr_k=700:149.7221,lwr_k=800:145.4419,lwr_k=900:145.768,lwr_k=1000:145.1944'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:81.981,lwr_k=200:99.5972,lwr_k=300:109.3855,lwr_k=400:113.1797,lwr_k=500:116.1709,lwr_k=600:117.7528,lwr_k=700:119.1788,lwr_k=800:120.3513,lwr_k=900:121.5126,lwr_k=1000:122.4938'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:153.8597,lwr_k=200:126.4647,lwr_k=300:123.3455,lwr_k=400:124.6046,lwr_k=500:125.388,lwr_k=600:126.1546,lwr_k=700:124.8333,lwr_k=800:126.3536,lwr_k=900:125.9975,lwr_k=1000:124.7703'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:219.1093,lwr_k=200:216.5092,lwr_k=300:222.7359,lwr_k=400:219.1948,lwr_k=500:222.3939,lwr_k=600:224.5146,lwr_k=700:225.0899,lwr_k=800:226.1044,lwr_k=900:226.2087,lwr_k=1000:226.984'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:534.8783,lwr_k=200:414.8431,lwr_k=300:426.7906,lwr_k=400:252.596,lwr_k=500:254.5238,lwr_k=600:256.2281,lwr_k=700:255.6196,lwr_k=800:253.9734,lwr_k=900:253.2114,lwr_k=1000:253.0044'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:192.1898,lwr_k=200:153.9357,lwr_k=300:141.1343,lwr_k=400:135.6302,lwr_k=500:132.713,lwr_k=600:132.3077,lwr_k=700:129.0654,lwr_k=800:128.077,lwr_k=900:127.701,lwr_k=1000:126.4742'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:220.5875,lwr_k=200:157.8508,lwr_k=300:136.4996,lwr_k=400:124.1531,lwr_k=500:117.5792,lwr_k=600:114.4678,lwr_k=700:111.4927,lwr_k=800:105.5581,lwr_k=900:104.0779,lwr_k=1000:104.1996'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_33'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:59.8811,lwr_k=200:79.6162,lwr_k=300:86.1407,lwr_k=400:91.2687,lwr_k=500:94.2386,lwr_k=600:96.4034,lwr_k=700:98.9885,lwr_k=800:100.9507,lwr_k=900:102.9146,lwr_k=1000:104.2518'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:227.4888,lwr_k=200:152.0238,lwr_k=300:146.5592,lwr_k=400:138.2258,lwr_k=500:122.3437,lwr_k=600:118.7837,lwr_k=700:119.5985,lwr_k=800:119.3216,lwr_k=900:123.5937,lwr_k=1000:120.045'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:50.7992,lwr_k=200:69.9639,lwr_k=300:77.4292,lwr_k=400:81.1061,lwr_k=500:84.0645,lwr_k=600:86.7999,lwr_k=700:88.5865,lwr_k=800:91.4064,lwr_k=900:92.9958,lwr_k=1000:94.383'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:191.1569,lwr_k=200:161.0563,lwr_k=300:156.8364,lwr_k=400:157.6874,lwr_k=500:158.6876,lwr_k=600:159.0916,lwr_k=700:160.6439,lwr_k=800:160.7569,lwr_k=900:164.1616,lwr_k=1000:163.7487'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:54.9352,lwr_k=200:64.4498,lwr_k=300:69.6508,lwr_k=400:72.7096,lwr_k=500:74.7069,lwr_k=600:76.1606,lwr_k=700:77.3447,lwr_k=800:78.6173,lwr_k=900:79.3398,lwr_k=1000:80.4108'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:235.4441,lwr_k=200:208.4292,lwr_k=300:179.3516,lwr_k=400:132.5481,lwr_k=500:139.2234,lwr_k=600:128.8818,lwr_k=700:129.1928,lwr_k=800:110.9369,lwr_k=900:110.0486,lwr_k=1000:107.5204'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:52.942,lwr_k=200:65.4193,lwr_k=300:69.7547,lwr_k=400:72.9327,lwr_k=500:74.986,lwr_k=600:76.4554,lwr_k=700:77.5085,lwr_k=800:78.5687,lwr_k=900:79.4712,lwr_k=1000:80.0374'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:168.8458,lwr_k=200:153.1161,lwr_k=300:147.5819,lwr_k=400:144.0987,lwr_k=500:144.0127,lwr_k=600:142.3112,lwr_k=700:142.7807,lwr_k=800:142.3381,lwr_k=900:142.3859,lwr_k=1000:142.8899'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:69.5767,lwr_k=200:85.7686,lwr_k=300:93.5945,lwr_k=400:99.1482,lwr_k=500:101.9375,lwr_k=600:104.5651,lwr_k=700:106.0194,lwr_k=800:107.7265,lwr_k=900:108.8836,lwr_k=1000:109.928'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:139.7652,lwr_k=200:119.3627,lwr_k=300:123.2495,lwr_k=400:123.3909,lwr_k=500:119.6167,lwr_k=600:117.8694,lwr_k=700:113.8966,lwr_k=800:113.4168,lwr_k=900:114.0734,lwr_k=1000:113.7122'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:61.9518,lwr_k=200:78.2034,lwr_k=300:85.4337,lwr_k=400:90.0536,lwr_k=500:93.2605,lwr_k=600:95.6881,lwr_k=700:97.1538,lwr_k=800:98.2832,lwr_k=900:99.1231,lwr_k=1000:100.039'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:108.7418,lwr_k=200:91.0508,lwr_k=300:89.1874,lwr_k=400:89.8233,lwr_k=500:93.1677,lwr_k=600:94.1846,lwr_k=700:94.3179,lwr_k=800:94.0549,lwr_k=900:94.5353,lwr_k=1000:95.047'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_34'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:339.6882,lwr_k=200:341.2668,lwr_k=300:337.8672,lwr_k=400:337.8628,lwr_k=500:337.8682,lwr_k=600:337.9922,lwr_k=700:338.3777,lwr_k=800:338.1711,lwr_k=900:338.1347,lwr_k=1000:338.1206'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:345.9773,lwr_k=200:347.7481,lwr_k=300:343.6623,lwr_k=400:343.6451,lwr_k=500:343.6658,lwr_k=600:343.8963,lwr_k=700:344.4204,lwr_k=800:344.1508,lwr_k=900:344.1014,lwr_k=1000:344.0819'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:322.6336,lwr_k=200:322.714,lwr_k=300:322.6506,lwr_k=400:322.7837,lwr_k=500:322.7719,lwr_k=600:322.7702,lwr_k=700:322.6362,lwr_k=800:322.6552,lwr_k=900:322.6931,lwr_k=1000:323.2876'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:425.3142,lwr_k=200:425.2916,lwr_k=300:425.2853,lwr_k=400:425.3226,lwr_k=500:425.3166,lwr_k=600:425.3158,lwr_k=700:425.3005,lwr_k=800:425.3934,lwr_k=900:425.4674,lwr_k=1000:426.2723'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:299.6213,lwr_k=200:1718.6391,lwr_k=300:362.856,lwr_k=400:333.6393,lwr_k=500:1715.379,lwr_k=600:362.1048,lwr_k=700:363.5554,lwr_k=800:366.6575,lwr_k=900:366.4871,lwr_k=1000:366.3475'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:249.157,lwr_k=200:1669.0657,lwr_k=300:316.7247,lwr_k=400:280.1045,lwr_k=500:1667.7834,lwr_k=600:316.8544,lwr_k=700:318.5031,lwr_k=800:321.9689,lwr_k=900:321.6669,lwr_k=1000:321.3095'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:382.9008,lwr_k=200:383.0085,lwr_k=300:382.93,lwr_k=400:383.0885,lwr_k=500:383.0752,lwr_k=600:383.0732,lwr_k=700:382.9076,lwr_k=800:382.907,lwr_k=900:382.9353,lwr_k=1000:383.4738'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:301.2381,lwr_k=200:301.233,lwr_k=300:301.217,lwr_k=400:301.2706,lwr_k=500:301.2636,lwr_k=600:301.2626,lwr_k=700:301.2271,lwr_k=800:301.3075,lwr_k=900:301.3754,lwr_k=1000:302.1446'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:299.8775,lwr_k=200:299.1772,lwr_k=300:298.9441,lwr_k=400:300.4751,lwr_k=500:300.6953,lwr_k=600:300.7896,lwr_k=700:300.8344,lwr_k=800:300.8013,lwr_k=900:300.7997,lwr_k=1000:300.8341'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:332.5276,lwr_k=200:332.4613,lwr_k=300:331.4949,lwr_k=400:331.3666,lwr_k=500:331.5758,lwr_k=600:331.5258,lwr_k=700:331.6022,lwr_k=800:331.7474,lwr_k=900:331.9232,lwr_k=1000:332.244'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:358.9562,lwr_k=200:359.0375,lwr_k=300:358.9736,lwr_k=400:359.1076,lwr_k=500:359.0957,lwr_k=600:359.094,lwr_k=700:358.9589,lwr_k=800:358.9772,lwr_k=900:359.0148,lwr_k=1000:359.6073'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:329.085,lwr_k=200:328.9509,lwr_k=300:329.0064,lwr_k=400:328.94,lwr_k=500:328.9403,lwr_k=600:328.9403,lwr_k=700:329.0537,lwr_k=800:329.2266,lwr_k=900:329.3397,lwr_k=1000:330.3726'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_35'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:105.0266,lwr_k=200:104.9444,lwr_k=300:105.1402,lwr_k=400:105.0347,lwr_k=500:105.19,lwr_k=600:105.2089,lwr_k=700:105.5231,lwr_k=800:105.5987,lwr_k=900:105.7141,lwr_k=1000:105.6975'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:121.3558,lwr_k=200:120.784,lwr_k=300:120.1288,lwr_k=400:120.2342,lwr_k=500:120.5738,lwr_k=600:120.7316,lwr_k=700:120.7387,lwr_k=800:120.7512,lwr_k=900:120.7349,lwr_k=1000:120.7782'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:81.4342,lwr_k=200:82.6429,lwr_k=300:83.5901,lwr_k=400:84.0111,lwr_k=500:84.3506,lwr_k=600:84.5004,lwr_k=700:84.584,lwr_k=800:84.661,lwr_k=900:84.7444,lwr_k=1000:84.7206'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:143.0671,lwr_k=200:145.089,lwr_k=300:147.3261,lwr_k=400:148.495,lwr_k=500:148.7765,lwr_k=600:148.7958,lwr_k=700:148.8961,lwr_k=800:148.9629,lwr_k=900:148.924,lwr_k=1000:148.9309'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:86.912,lwr_k=200:87.6653,lwr_k=300:87.9308,lwr_k=400:88.1522,lwr_k=500:88.1427,lwr_k=600:88.1853,lwr_k=700:88.345,lwr_k=800:88.4165,lwr_k=900:88.3986,lwr_k=1000:88.3983'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:109.3377,lwr_k=200:109.2903,lwr_k=300:108.5539,lwr_k=400:108.3956,lwr_k=500:108.2767,lwr_k=600:108.2637,lwr_k=700:108.0291,lwr_k=800:108.1017,lwr_k=900:108.103,lwr_k=1000:108.1969'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:90.8944,lwr_k=200:91.5895,lwr_k=300:91.8905,lwr_k=400:92.2461,lwr_k=500:92.5466,lwr_k=600:92.652,lwr_k=700:92.7394,lwr_k=800:92.7458,lwr_k=900:92.7772,lwr_k=1000:92.7866'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:101.4173,lwr_k=200:101.6342,lwr_k=300:102.1157,lwr_k=400:102.6682,lwr_k=500:102.5963,lwr_k=600:103.0394,lwr_k=700:103.1055,lwr_k=800:103.1312,lwr_k=900:103.0409,lwr_k=1000:102.9936'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:111.2512,lwr_k=200:111.4959,lwr_k=300:112.7059,lwr_k=400:112.9272,lwr_k=500:113.1003,lwr_k=600:113.083,lwr_k=700:113.1804,lwr_k=800:113.2171,lwr_k=900:113.3374,lwr_k=1000:113.4041'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:98.3708,lwr_k=200:97.457,lwr_k=300:97.5571,lwr_k=400:97.4433,lwr_k=500:97.548,lwr_k=600:97.5058,lwr_k=700:97.5064,lwr_k=800:97.5798,lwr_k=900:97.3547,lwr_k=1000:97.4485'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:92.477,lwr_k=200:92.5523,lwr_k=300:92.5279,lwr_k=400:92.7001,lwr_k=500:92.8722,lwr_k=600:92.9058,lwr_k=700:92.9614,lwr_k=800:93.0621,lwr_k=900:93.0882,lwr_k=1000:93.1446'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:131.7534,lwr_k=200:133.0586,lwr_k=300:133.3961,lwr_k=400:133.4261,lwr_k=500:133.6165,lwr_k=600:133.6787,lwr_k=700:133.7964,lwr_k=800:133.7943,lwr_k=900:133.8021,lwr_k=1000:133.8789'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_36'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:141.0416,lwr_k=200:141.4823,lwr_k=300:142.2865,lwr_k=400:142.4002,lwr_k=500:142.7564,lwr_k=600:142.4287,lwr_k=700:142.3687,lwr_k=800:144.3538,lwr_k=900:145.0825,lwr_k=1000:144.8246'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:147.9584,lwr_k=200:147.9857,lwr_k=300:147.8408,lwr_k=400:148.4556,lwr_k=500:148.5657,lwr_k=600:150.217,lwr_k=700:149.7757,lwr_k=800:150.6198,lwr_k=900:152.1496,lwr_k=1000:151.9514'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:122.4223,lwr_k=200:113.59,lwr_k=300:115.6842,lwr_k=400:115.6955,lwr_k=500:114.5702,lwr_k=600:116.4061,lwr_k=700:115.5867,lwr_k=800:113.0831,lwr_k=900:115.5204,lwr_k=1000:114.8076'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:186.0793,lwr_k=200:180.9098,lwr_k=300:179.0058,lwr_k=400:177.7679,lwr_k=500:178.0101,lwr_k=600:176.8404,lwr_k=700:178.7661,lwr_k=800:176.0144,lwr_k=900:179.2509,lwr_k=1000:177.1293'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:107.1641,lwr_k=200:106.4773,lwr_k=300:106.7727,lwr_k=400:106.8118,lwr_k=500:106.8277,lwr_k=600:106.9638,lwr_k=700:107.3679,lwr_k=800:106.8284,lwr_k=900:107.1588,lwr_k=1000:107.2222'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:213.8889,lwr_k=200:214.7834,lwr_k=300:216.6798,lwr_k=400:217.7251,lwr_k=500:219.697,lwr_k=600:221.1944,lwr_k=700:221.7993,lwr_k=800:221.6549,lwr_k=900:221.9275,lwr_k=1000:221.6824'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:123.8837,lwr_k=200:129.907,lwr_k=300:132.6984,lwr_k=400:135.4441,lwr_k=500:134.0685,lwr_k=600:132.5641,lwr_k=700:134.1013,lwr_k=800:134.4727,lwr_k=900:133.7394,lwr_k=1000:131.862'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:97.7375,lwr_k=200:94.0459,lwr_k=300:92.7746,lwr_k=400:92.2299,lwr_k=500:91.1518,lwr_k=600:90.7933,lwr_k=700:90.6618,lwr_k=800:91.9226,lwr_k=900:92.0857,lwr_k=1000:91.7003'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:158.2275,lwr_k=200:160.5536,lwr_k=300:163.3922,lwr_k=400:161.9575,lwr_k=500:162.9722,lwr_k=600:162.5479,lwr_k=700:164.6811,lwr_k=800:163.5041,lwr_k=900:164.6342,lwr_k=1000:165.3593'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:150.1867,lwr_k=200:153.2592,lwr_k=300:151.5847,lwr_k=400:156.0882,lwr_k=500:157.3015,lwr_k=600:159.6416,lwr_k=700:159.6596,lwr_k=800:160.667,lwr_k=900:162.6877,lwr_k=1000:162.8001'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:116.5002,lwr_k=200:120.2681,lwr_k=300:121.0891,lwr_k=400:122.2535,lwr_k=500:122.9511,lwr_k=600:123.5191,lwr_k=700:123.3416,lwr_k=800:123.6059,lwr_k=900:123.7926,lwr_k=1000:123.948'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:134.5176,lwr_k=200:136.5342,lwr_k=300:136.1596,lwr_k=400:136.1105,lwr_k=500:135.9726,lwr_k=600:136.5506,lwr_k=700:136.4152,lwr_k=800:136.4482,lwr_k=900:136.6515,lwr_k=1000:136.2527'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_37'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:78.8418,lwr_k=200:92.2034,lwr_k=300:98.376,lwr_k=400:100.9844,lwr_k=500:102.5998,lwr_k=600:103.5371,lwr_k=700:104.9482,lwr_k=800:105.6626,lwr_k=900:106.2299,lwr_k=1000:106.7435'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:125.6011,lwr_k=200:104.0672,lwr_k=300:104.5415,lwr_k=400:100.6461,lwr_k=500:100.2503,lwr_k=600:100.0194,lwr_k=700:100.814,lwr_k=800:100.4897,lwr_k=900:100.4201,lwr_k=1000:100.0942'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:47.2239,lwr_k=200:64.4136,lwr_k=300:74.4576,lwr_k=400:79.2322,lwr_k=500:81.8189,lwr_k=600:83.099,lwr_k=700:83.9896,lwr_k=800:84.9486,lwr_k=900:85.4467,lwr_k=1000:86.0266'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:228.0692,lwr_k=200:176.3324,lwr_k=300:155.3754,lwr_k=400:156.2011,lwr_k=500:151.519,lwr_k=600:150.6697,lwr_k=700:151.1916,lwr_k=800:151.2122,lwr_k=900:151.7473,lwr_k=1000:150.6869'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:95.4787,lwr_k=200:119.515,lwr_k=300:130.6821,lwr_k=400:139.453,lwr_k=500:144.3592,lwr_k=600:147.7546,lwr_k=700:150.0719,lwr_k=800:153.2996,lwr_k=900:156.0275,lwr_k=1000:159.8842'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:157.4911,lwr_k=200:238.9538,lwr_k=300:241.1435,lwr_k=400:243.9362,lwr_k=500:245.3996,lwr_k=600:245.7823,lwr_k=700:246.9002,lwr_k=800:232.0843,lwr_k=900:155.9991,lwr_k=1000:157.8166'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:121.9493,lwr_k=200:148.7822,lwr_k=300:163.7058,lwr_k=400:176.8279,lwr_k=500:184.1526,lwr_k=600:190.8995,lwr_k=700:194.9556,lwr_k=800:198.0843,lwr_k=900:201.2804,lwr_k=1000:204.0253'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:163.7163,lwr_k=200:154.5347,lwr_k=300:155.7494,lwr_k=400:162.663,lwr_k=500:161.8648,lwr_k=600:163.5564,lwr_k=700:164.8864,lwr_k=800:163.8832,lwr_k=900:165.5136,lwr_k=1000:168.065'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:84.2668,lwr_k=200:95.3793,lwr_k=300:99.1305,lwr_k=400:101.9112,lwr_k=500:103.31,lwr_k=600:105.2719,lwr_k=700:107.0489,lwr_k=800:108.659,lwr_k=900:109.4935,lwr_k=1000:110.1225'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:128.1989,lwr_k=200:114.5518,lwr_k=300:107.6834,lwr_k=400:108.3787,lwr_k=500:111.6516,lwr_k=600:110.0919,lwr_k=700:109.2349,lwr_k=800:107.8029,lwr_k=900:108.2037,lwr_k=1000:108.608'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:104.006,lwr_k=200:131.7293,lwr_k=300:145.1722,lwr_k=400:150.9749,lwr_k=500:157.8521,lwr_k=600:162.3723,lwr_k=700:166.668,lwr_k=800:171.3138,lwr_k=900:173.2933,lwr_k=1000:175.8192'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:186.1835,lwr_k=200:167.8537,lwr_k=300:172.6787,lwr_k=400:173.9358,lwr_k=500:178.3612,lwr_k=600:180.4767,lwr_k=700:181.1556,lwr_k=800:183.4553,lwr_k=900:184.4981,lwr_k=1000:186.3284'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_38'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:62.3594,lwr_k=200:81.2806,lwr_k=300:89.7056,lwr_k=400:95.0026,lwr_k=500:99.4555,lwr_k=600:103.5422,lwr_k=700:107.0216,lwr_k=800:110.3478,lwr_k=900:112.1442,lwr_k=1000:114.5705'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:143.7278,lwr_k=200:117.7712,lwr_k=300:116.2374,lwr_k=400:115.005,lwr_k=500:116.3165,lwr_k=600:111.6563,lwr_k=700:112.3807,lwr_k=800:111.2671,lwr_k=900:112.693,lwr_k=1000:116.0198'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:47.9668,lwr_k=200:65.6536,lwr_k=300:73.2206,lwr_k=400:79.3256,lwr_k=500:82.8456,lwr_k=600:86.2746,lwr_k=700:89.4425,lwr_k=800:91.3992,lwr_k=900:92.8245,lwr_k=1000:94.5061'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:195.1146,lwr_k=200:168.8506,lwr_k=300:161.556,lwr_k=400:159.3176,lwr_k=500:161.0656,lwr_k=600:160.3856,lwr_k=700:159.6366,lwr_k=800:160.9305,lwr_k=900:160.7631,lwr_k=1000:161.081'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:49.568,lwr_k=200:67.5113,lwr_k=300:75.5662,lwr_k=400:84.3808,lwr_k=500:89.5539,lwr_k=600:93.1841,lwr_k=700:95.4251,lwr_k=800:96.5828,lwr_k=900:97.9433,lwr_k=1000:98.6845'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:134.8592,lwr_k=200:113.2663,lwr_k=300:115.7446,lwr_k=400:105.4448,lwr_k=500:105.1823,lwr_k=600:103.5499,lwr_k=700:103.9038,lwr_k=800:103.201,lwr_k=900:103.7529,lwr_k=1000:105.1263'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:56.6132,lwr_k=200:82.0999,lwr_k=300:90.2946,lwr_k=400:96.4042,lwr_k=500:102.6465,lwr_k=600:107.1851,lwr_k=700:111.2569,lwr_k=800:114.3928,lwr_k=900:116.6186,lwr_k=1000:118.6352'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:127.0561,lwr_k=200:100.5894,lwr_k=300:95.2215,lwr_k=400:96.5362,lwr_k=500:95.9279,lwr_k=600:96.3285,lwr_k=700:96.6462,lwr_k=800:97.9184,lwr_k=900:98.6395,lwr_k=1000:99.2183'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:66.5772,lwr_k=200:98.2666,lwr_k=300:112.3081,lwr_k=400:117.5463,lwr_k=500:122.5722,lwr_k=600:126.2748,lwr_k=700:130.3823,lwr_k=800:132.9955,lwr_k=900:135.8766,lwr_k=1000:138.3728'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:216.9169,lwr_k=200:208.9987,lwr_k=300:165.5583,lwr_k=400:137.4874,lwr_k=500:140.7787,lwr_k=600:139.6589,lwr_k=700:138.9833,lwr_k=800:140.3046,lwr_k=900:142.3508,lwr_k=1000:144.8132'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:58.4795,lwr_k=200:80.8354,lwr_k=300:91.533,lwr_k=400:96.8133,lwr_k=500:100.0101,lwr_k=600:102.3956,lwr_k=700:103.3899,lwr_k=800:104.2307,lwr_k=900:105.1666,lwr_k=1000:106.2259'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:123.2965,lwr_k=200:117.2018,lwr_k=300:101.5792,lwr_k=400:97.658,lwr_k=500:97.158,lwr_k=600:98.3737,lwr_k=700:100.815,lwr_k=800:101.2359,lwr_k=900:102.2151,lwr_k=1000:103.287'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_39'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:82.573,lwr_k=200:93.129,lwr_k=300:99.6377,lwr_k=400:101.9336,lwr_k=500:105.5994,lwr_k=600:107.362,lwr_k=700:108.0976,lwr_k=800:107.8545,lwr_k=900:108.3129,lwr_k=1000:107.8061'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:293.1745,lwr_k=200:165.731,lwr_k=300:145.2947,lwr_k=400:135.1136,lwr_k=500:130.7281,lwr_k=600:133.487,lwr_k=700:131.3012,lwr_k=800:128.2057,lwr_k=900:125.4019,lwr_k=1000:124.5273'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:30.4107,lwr_k=200:53.894,lwr_k=300:62.8981,lwr_k=400:68.3429,lwr_k=500:72.9099,lwr_k=600:75.3008,lwr_k=700:76.6667,lwr_k=800:77.7436,lwr_k=900:78.0706,lwr_k=1000:79.0226'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:230.9456,lwr_k=200:159.1858,lwr_k=300:152.2035,lwr_k=400:149.0554,lwr_k=500:146.1507,lwr_k=600:145.5258,lwr_k=700:145.2915,lwr_k=800:145.0372,lwr_k=900:144.1196,lwr_k=1000:143.8443'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:39.0807,lwr_k=200:62.0559,lwr_k=300:70.0674,lwr_k=400:73.9678,lwr_k=500:77.1453,lwr_k=600:78.7618,lwr_k=700:80.1854,lwr_k=800:81.2821,lwr_k=900:82.3902,lwr_k=1000:83.5947'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:173.2141,lwr_k=200:129.9559,lwr_k=300:116.3737,lwr_k=400:112.3137,lwr_k=500:110.643,lwr_k=600:110.4178,lwr_k=700:109.1166,lwr_k=800:108.4804,lwr_k=900:107.9561,lwr_k=1000:108.5267'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:123.1775,lwr_k=200:100.3879,lwr_k=300:98.327,lwr_k=400:97.3378,lwr_k=500:96.78,lwr_k=600:97.2398,lwr_k=700:96.4126,lwr_k=800:96.8256,lwr_k=900:96.6126,lwr_k=1000:96.549'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:340.7992,lwr_k=200:189.2307,lwr_k=300:147.6971,lwr_k=400:136.6232,lwr_k=500:132.011,lwr_k=600:127.8885,lwr_k=700:123.6816,lwr_k=800:119.3285,lwr_k=900:118.422,lwr_k=1000:118.2454'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:76.8673,lwr_k=200:79.9659,lwr_k=300:84.4946,lwr_k=400:87.591,lwr_k=500:90.5428,lwr_k=600:92.6031,lwr_k=700:93.8676,lwr_k=800:95.1348,lwr_k=900:96.8445,lwr_k=1000:97.7724'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:200.6929,lwr_k=200:136.276,lwr_k=300:112.4291,lwr_k=400:101.5608,lwr_k=500:99.9033,lwr_k=600:97.8104,lwr_k=700:98.713,lwr_k=800:99.1166,lwr_k=900:97.158,lwr_k=1000:95.766'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:43.1954,lwr_k=200:68.9826,lwr_k=300:78.8432,lwr_k=400:82.8536,lwr_k=500:85.1012,lwr_k=600:87.1952,lwr_k=700:88.9545,lwr_k=800:89.9857,lwr_k=900:90.9609,lwr_k=1000:91.6658'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:163.6566,lwr_k=200:142.1655,lwr_k=300:126.3035,lwr_k=400:121.3914,lwr_k=500:118.2581,lwr_k=600:118.3328,lwr_k=700:117.3402,lwr_k=800:115.8967,lwr_k=900:114.2529,lwr_k=1000:113.2017'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_40'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:87.5512,lwr_k=200:92.2791,lwr_k=300:93.5073,lwr_k=400:94.1736,lwr_k=500:95.0716,lwr_k=600:95.9888,lwr_k=700:96.0463,lwr_k=800:96.522,lwr_k=900:96.7768,lwr_k=1000:97.3242'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:118.6717,lwr_k=200:103.3835,lwr_k=300:99.9625,lwr_k=400:99.4497,lwr_k=500:99.3924,lwr_k=600:99.9866,lwr_k=700:100.6163,lwr_k=800:100.2511,lwr_k=900:100.8141,lwr_k=1000:100.7808'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:65.9314,lwr_k=200:69.6077,lwr_k=300:71.7448,lwr_k=400:72.7473,lwr_k=500:72.7977,lwr_k=600:73.169,lwr_k=700:73.4169,lwr_k=800:73.8382,lwr_k=900:73.8318,lwr_k=1000:73.9165'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:163.3445,lwr_k=200:151.7495,lwr_k=300:152.5709,lwr_k=400:153.2218,lwr_k=500:152.9049,lwr_k=600:152.6681,lwr_k=700:152.8417,lwr_k=800:152.6936,lwr_k=900:152.7444,lwr_k=1000:152.7544'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:73.2243,lwr_k=200:75.8856,lwr_k=300:77.1602,lwr_k=400:78.2124,lwr_k=500:78.521,lwr_k=600:78.9334,lwr_k=700:79.0093,lwr_k=800:79.2722,lwr_k=900:79.4984,lwr_k=1000:79.7623'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:108.8472,lwr_k=200:104.7126,lwr_k=300:104.3336,lwr_k=400:104.87,lwr_k=500:104.4439,lwr_k=600:105.3191,lwr_k=700:106.1219,lwr_k=800:106.1539,lwr_k=900:106.2204,lwr_k=1000:106.3434'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:77.3222,lwr_k=200:80.2385,lwr_k=300:81.2996,lwr_k=400:82.2656,lwr_k=500:83.3183,lwr_k=600:83.4935,lwr_k=700:83.7178,lwr_k=800:84.3855,lwr_k=900:84.4289,lwr_k=1000:84.5416'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:97.7109,lwr_k=200:93.0944,lwr_k=300:92.2014,lwr_k=400:92.0895,lwr_k=500:92.4642,lwr_k=600:92.7148,lwr_k=700:92.3423,lwr_k=800:92.6657,lwr_k=900:92.303,lwr_k=1000:92.1395'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:90.4955,lwr_k=200:97.4011,lwr_k=300:101.0974,lwr_k=400:101.2916,lwr_k=500:102.2743,lwr_k=600:103.4769,lwr_k=700:103.7328,lwr_k=800:103.9421,lwr_k=900:104.5824,lwr_k=1000:104.8107'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:104.7484,lwr_k=200:92.6364,lwr_k=300:91.4685,lwr_k=400:91.3172,lwr_k=500:90.243,lwr_k=600:91.4003,lwr_k=700:90.9126,lwr_k=800:91.46,lwr_k=900:91.337,lwr_k=1000:92.3416'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:81.0091,lwr_k=200:85.1977,lwr_k=300:87.5824,lwr_k=400:88.5616,lwr_k=500:89.5725,lwr_k=600:89.5193,lwr_k=700:89.975,lwr_k=800:90.423,lwr_k=900:90.7306,lwr_k=1000:90.9129'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:119.0986,lwr_k=200:111.7025,lwr_k=300:112.6633,lwr_k=400:108.6922,lwr_k=500:108.2623,lwr_k=600:108.2683,lwr_k=700:109.3018,lwr_k=800:109.5292,lwr_k=900:109.99,lwr_k=1000:110.0927'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_41'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:99.2733,lwr_k=200:133.6037,lwr_k=300:147.9097,lwr_k=400:151.5758,lwr_k=500:149.9559,lwr_k=600:153.5358,lwr_k=700:154.694,lwr_k=800:160.3028,lwr_k=900:162.7077,lwr_k=1000:163.6951'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:209.5956,lwr_k=200:236.2964,lwr_k=300:278.466,lwr_k=400:209.9111,lwr_k=500:174.0509,lwr_k=600:180.6393,lwr_k=700:174.7791,lwr_k=800:171.5583,lwr_k=900:173.3745,lwr_k=1000:177.1153'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:131.8969,lwr_k=200:154.0146,lwr_k=300:151.6202,lwr_k=400:146.7336,lwr_k=500:178.0518,lwr_k=600:199.8668,lwr_k=700:155.5475,lwr_k=800:157.6143,lwr_k=900:171.2942,lwr_k=1000:162.2748'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:374.8457,lwr_k=200:266.8435,lwr_k=300:266.9115,lwr_k=400:246.1925,lwr_k=500:263.9473,lwr_k=600:294.088,lwr_k=700:254.8623,lwr_k=800:249.2864,lwr_k=900:275.405,lwr_k=1000:262.0502'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:171.6425,lwr_k=200:142.7761,lwr_k=300:135.7549,lwr_k=400:139.6223,lwr_k=500:148.144,lwr_k=600:142.5385,lwr_k=700:146.5511,lwr_k=800:143.7448,lwr_k=900:145.4261,lwr_k=1000:148.1095'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:320.4835,lwr_k=200:313.8371,lwr_k=300:281.8885,lwr_k=400:259.7459,lwr_k=500:268.6542,lwr_k=600:154.8548,lwr_k=700:242.5747,lwr_k=800:145.4308,lwr_k=900:142.8225,lwr_k=1000:147.3367'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:126.6303,lwr_k=200:113.788,lwr_k=300:119.8084,lwr_k=400:127.435,lwr_k=500:136.5668,lwr_k=600:132.3099,lwr_k=700:141.1773,lwr_k=800:154.0115,lwr_k=900:159.48,lwr_k=1000:162.8203'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:245.9757,lwr_k=200:166.1626,lwr_k=300:149.882,lwr_k=400:145.2813,lwr_k=500:146.2245,lwr_k=600:138.4922,lwr_k=700:147.411,lwr_k=800:154.5065,lwr_k=900:161.568,lwr_k=1000:150.8736'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:193.8206,lwr_k=200:188.0805,lwr_k=300:187.7152,lwr_k=400:213.1495,lwr_k=500:251.5342,lwr_k=600:220.7443,lwr_k=700:248.3717,lwr_k=800:207.9978,lwr_k=900:209.0715,lwr_k=1000:201.8234'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:356.0161,lwr_k=200:334.6039,lwr_k=300:239.4384,lwr_k=400:258.5514,lwr_k=500:269.8964,lwr_k=600:356.1994,lwr_k=700:270.4225,lwr_k=800:263.4273,lwr_k=900:273.6571,lwr_k=1000:198.0561'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:244.0207,lwr_k=200:219.3877,lwr_k=300:231.7141,lwr_k=400:231.1457,lwr_k=500:244.9462,lwr_k=600:246.42,lwr_k=700:257.3944,lwr_k=800:273.0775,lwr_k=900:282.6996,lwr_k=1000:292.6836'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:468.4558,lwr_k=200:393.868,lwr_k=300:434.3036,lwr_k=400:413.3422,lwr_k=500:341.658,lwr_k=600:413.6755,lwr_k=700:403.2208,lwr_k=800:374.431,lwr_k=900:328.9383,lwr_k=1000:364.8452'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_42'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:103.9498,lwr_k=200:88.6076,lwr_k=300:88.8172,lwr_k=400:84.6795,lwr_k=500:84.2197,lwr_k=600:87.3313,lwr_k=700:82.757,lwr_k=800:83.7184,lwr_k=900:86.1497,lwr_k=1000:88.6398'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:224.4487,lwr_k=200:150.34,lwr_k=300:140.0435,lwr_k=400:118.5069,lwr_k=500:118.069,lwr_k=600:104.655,lwr_k=700:113.5922,lwr_k=800:164.1544,lwr_k=900:231.0506,lwr_k=1000:132.6746'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:49.9094,lwr_k=200:55.2026,lwr_k=300:58.606,lwr_k=400:62.4624,lwr_k=500:64.1585,lwr_k=600:65.404,lwr_k=700:66.5247,lwr_k=800:67.832,lwr_k=900:68.5392,lwr_k=1000:69.142'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:225.3342,lwr_k=200:183.7332,lwr_k=300:174.3272,lwr_k=400:163.803,lwr_k=500:160.136,lwr_k=600:158.1097,lwr_k=700:159.162,lwr_k=800:157.9632,lwr_k=900:157.2935,lwr_k=1000:156.446'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:50.9786,lwr_k=200:59.644,lwr_k=300:64.0844,lwr_k=400:67.2147,lwr_k=500:69.0222,lwr_k=600:70.1524,lwr_k=700:71.5002,lwr_k=800:72.689,lwr_k=900:73.6844,lwr_k=1000:74.2529'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:160.5568,lwr_k=200:110.3496,lwr_k=300:104.5961,lwr_k=400:104.8448,lwr_k=500:102.3265,lwr_k=600:99.8371,lwr_k=700:99.9163,lwr_k=800:99.5272,lwr_k=900:98.7346,lwr_k=1000:99.1334'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:91.5702,lwr_k=200:79.1564,lwr_k=300:74.6497,lwr_k=400:73.3168,lwr_k=500:74.3762,lwr_k=600:73.4254,lwr_k=700:73.615,lwr_k=800:73.7456,lwr_k=900:74.0038,lwr_k=1000:74.7492'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:232.7596,lwr_k=200:141.0234,lwr_k=300:116.6666,lwr_k=400:109.7297,lwr_k=500:105.3716,lwr_k=600:100.3725,lwr_k=700:101.2262,lwr_k=800:99.7594,lwr_k=900:98.8699,lwr_k=1000:98.8446'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:67.6477,lwr_k=200:80.038,lwr_k=300:83.679,lwr_k=400:85.6754,lwr_k=500:88.965,lwr_k=600:90.0945,lwr_k=700:91.0306,lwr_k=800:91.91,lwr_k=900:93.2739,lwr_k=1000:93.8548'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:156.6857,lwr_k=200:130.6385,lwr_k=300:111.2917,lwr_k=400:101.3065,lwr_k=500:105.0191,lwr_k=600:101.2029,lwr_k=700:100.5182,lwr_k=800:99.8788,lwr_k=900:100.0565,lwr_k=1000:99.2724'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:96.2273,lwr_k=200:88.9921,lwr_k=300:90.9249,lwr_k=400:89.8691,lwr_k=500:90.2825,lwr_k=600:92.0211,lwr_k=700:86.8537,lwr_k=800:85.2053,lwr_k=900:89.8512,lwr_k=1000:90.682'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:238.2134,lwr_k=200:163.8782,lwr_k=300:152.6368,lwr_k=400:135.0957,lwr_k=500:129.1181,lwr_k=600:124.9449,lwr_k=700:122.9886,lwr_k=800:117.5996,lwr_k=900:121.2799,lwr_k=1000:116.7371'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_43'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:72.7449,lwr_k=200:80.826,lwr_k=300:71.8675,lwr_k=400:76.8475,lwr_k=500:81.4333,lwr_k=600:78.6398,lwr_k=700:81.6339,lwr_k=800:91.522,lwr_k=900:85.2352,lwr_k=1000:85.8336'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:144.7748,lwr_k=200:113.9854,lwr_k=300:114.0505,lwr_k=400:143.4805,lwr_k=500:145.4229,lwr_k=600:142.0313,lwr_k=700:167.859,lwr_k=800:186.5205,lwr_k=900:166.4119,lwr_k=1000:144.852'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:54.5524,lwr_k=200:61.6599,lwr_k=300:65.2991,lwr_k=400:66.4538,lwr_k=500:68.916,lwr_k=600:69.4116,lwr_k=700:68.5267,lwr_k=800:69.5182,lwr_k=900:70.7891,lwr_k=1000:72.8882'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:165.6298,lwr_k=200:139.7437,lwr_k=300:146.0238,lwr_k=400:135.0287,lwr_k=500:142.2909,lwr_k=600:140.9174,lwr_k=700:137.2476,lwr_k=800:138.746,lwr_k=900:134.7276,lwr_k=1000:136.6689'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:80.0328,lwr_k=200:81.0214,lwr_k=300:76.6445,lwr_k=400:85.3159,lwr_k=500:89.5931,lwr_k=600:83.9442,lwr_k=700:82.7428,lwr_k=800:81.2252,lwr_k=900:84.122,lwr_k=1000:88.4208'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:170.7506,lwr_k=200:143.3264,lwr_k=300:131.2422,lwr_k=400:128.9537,lwr_k=500:126.097,lwr_k=600:120.054,lwr_k=700:125.0127,lwr_k=800:125.4586,lwr_k=900:126.7817,lwr_k=1000:128.6033'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:53.2859,lwr_k=200:59.6432,lwr_k=300:64.4521,lwr_k=400:66.5376,lwr_k=500:68.6446,lwr_k=600:71.9572,lwr_k=700:69.4954,lwr_k=800:69.2451,lwr_k=900:70.18,lwr_k=1000:73.568'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:121.7911,lwr_k=200:122.2794,lwr_k=300:105.4738,lwr_k=400:113.0954,lwr_k=500:104.0879,lwr_k=600:110.4443,lwr_k=700:101.8383,lwr_k=800:101.3661,lwr_k=900:106.5221,lwr_k=1000:103.904'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:90.3781,lwr_k=200:103.3436,lwr_k=300:86.8581,lwr_k=400:90.7121,lwr_k=500:90.6711,lwr_k=600:89.5625,lwr_k=700:89.2818,lwr_k=800:92.5708,lwr_k=900:91.1468,lwr_k=1000:92.0235'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:146.1822,lwr_k=200:115.6393,lwr_k=300:123.2965,lwr_k=400:99.9935,lwr_k=500:105.5127,lwr_k=600:107.3818,lwr_k=700:101.6422,lwr_k=800:104.7398,lwr_k=900:103.6141,lwr_k=1000:106.054'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:67.0065,lwr_k=200:70.0902,lwr_k=300:69.4397,lwr_k=400:69.6523,lwr_k=500:70.184,lwr_k=600:71.0078,lwr_k=700:72.1903,lwr_k=800:73.628,lwr_k=900:73.2254,lwr_k=1000:72.8113'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:175.7409,lwr_k=200:113.6358,lwr_k=300:97.6303,lwr_k=400:89.0882,lwr_k=500:90.3005,lwr_k=600:88.8751,lwr_k=700:88.4315,lwr_k=800:81.4194,lwr_k=900:84.082,lwr_k=1000:80.0851'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_44'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:79.2733,lwr_k=200:104.4412,lwr_k=300:112.8149,lwr_k=400:117.8857,lwr_k=500:121.5475,lwr_k=600:125.1098,lwr_k=700:127.2274,lwr_k=800:128.4467,lwr_k=900:129.6129,lwr_k=1000:130.5603'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:230.0768,lwr_k=200:179.8817,lwr_k=300:168.562,lwr_k=400:163.7529,lwr_k=500:154.613,lwr_k=600:148.6994,lwr_k=700:145.0183,lwr_k=800:145.2711,lwr_k=900:143.8454,lwr_k=1000:143.1304'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:70.2543,lwr_k=200:89.9997,lwr_k=300:96.2341,lwr_k=400:100.4376,lwr_k=500:101.959,lwr_k=600:102.5673,lwr_k=700:103.8869,lwr_k=800:104.4446,lwr_k=900:105.1773,lwr_k=1000:105.7829'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:238.1633,lwr_k=200:194.1151,lwr_k=300:186.067,lwr_k=400:193.7552,lwr_k=500:180.3409,lwr_k=600:173.9793,lwr_k=700:173.9527,lwr_k=800:175.1357,lwr_k=900:175.5775,lwr_k=1000:175.7592'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:98.3745,lwr_k=200:88.594,lwr_k=300:86.2508,lwr_k=400:87.4825,lwr_k=500:87.3086,lwr_k=600:88.0445,lwr_k=700:88.3992,lwr_k=800:89.7017,lwr_k=900:90.2344,lwr_k=1000:90.4858'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:231.172,lwr_k=200:184.1519,lwr_k=300:168.2237,lwr_k=400:144.7782,lwr_k=500:142.6485,lwr_k=600:140.7611,lwr_k=700:138.4567,lwr_k=800:138.2911,lwr_k=900:138.1665,lwr_k=1000:138.9588'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:77.1431,lwr_k=200:96.6745,lwr_k=300:102.8841,lwr_k=400:107.5462,lwr_k=500:110.46,lwr_k=600:112.7554,lwr_k=700:113.6729,lwr_k=800:114.5225,lwr_k=900:115.8586,lwr_k=1000:117.0707'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:172.1164,lwr_k=200:129.6403,lwr_k=300:122.0912,lwr_k=400:120.527,lwr_k=500:121.7234,lwr_k=600:115.5695,lwr_k=700:118.5688,lwr_k=800:113.3419,lwr_k=900:112.7254,lwr_k=1000:112.1141'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:80.7831,lwr_k=200:99.287,lwr_k=300:107.0873,lwr_k=400:112.0978,lwr_k=500:114.881,lwr_k=600:116.9743,lwr_k=700:118.4657,lwr_k=800:120.7097,lwr_k=900:122.729,lwr_k=1000:124.9259'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:139.6079,lwr_k=200:122.8403,lwr_k=300:119.8641,lwr_k=400:121.9576,lwr_k=500:121.4795,lwr_k=600:119.7787,lwr_k=700:121.9794,lwr_k=800:122.295,lwr_k=900:123.1401,lwr_k=1000:122.8129'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:69.937,lwr_k=200:91.5032,lwr_k=300:102.6099,lwr_k=400:110.0664,lwr_k=500:114.0895,lwr_k=600:117.4346,lwr_k=700:121.2794,lwr_k=800:123.8936,lwr_k=900:128.3203,lwr_k=1000:131.0997'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:140.7358,lwr_k=200:126.6148,lwr_k=300:120.4649,lwr_k=400:119.3003,lwr_k=500:121.203,lwr_k=600:123.885,lwr_k=700:122.0841,lwr_k=800:123.1411,lwr_k=900:125.2308,lwr_k=1000:128.2242'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_45'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:136.0944,lwr_k=200:142.749,lwr_k=300:142.867,lwr_k=400:143.8466,lwr_k=500:144.0627,lwr_k=600:144.6658,lwr_k=700:144.9318,lwr_k=800:145.641,lwr_k=900:146.2889,lwr_k=1000:147.1047'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:176.7743,lwr_k=200:177.7784,lwr_k=300:174.6676,lwr_k=400:175.3038,lwr_k=500:176.6354,lwr_k=600:176.7419,lwr_k=700:177.2406,lwr_k=800:181.2334,lwr_k=900:185.5668,lwr_k=1000:187.2302'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:120.7005,lwr_k=200:131.8023,lwr_k=300:139.4721,lwr_k=400:142.248,lwr_k=500:146.53,lwr_k=600:156.4961,lwr_k=700:159.1059,lwr_k=800:160.4814,lwr_k=900:160.9432,lwr_k=1000:161.5439'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:194.5226,lwr_k=200:212.7276,lwr_k=300:220.2276,lwr_k=400:221.3003,lwr_k=500:227.0888,lwr_k=600:240.4363,lwr_k=700:243.3024,lwr_k=800:244.1796,lwr_k=900:244.6211,lwr_k=1000:244.5211'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:99.6867,lwr_k=200:106.7592,lwr_k=300:110.8043,lwr_k=400:112.8053,lwr_k=500:115.5692,lwr_k=600:117.0375,lwr_k=700:117.9269,lwr_k=800:118.3638,lwr_k=900:118.4745,lwr_k=1000:118.7891'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:121.1988,lwr_k=200:118.9131,lwr_k=300:119.0241,lwr_k=400:120.4544,lwr_k=500:124.9513,lwr_k=600:126.794,lwr_k=700:125.9016,lwr_k=800:125.2587,lwr_k=900:124.9341,lwr_k=1000:124.4649'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:117.6153,lwr_k=200:121.2706,lwr_k=300:123.2506,lwr_k=400:124.5368,lwr_k=500:124.8001,lwr_k=600:125.469,lwr_k=700:126.1909,lwr_k=800:126.7183,lwr_k=900:126.9313,lwr_k=1000:127.232'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:128.3194,lwr_k=200:120.7284,lwr_k=300:120.6918,lwr_k=400:123.1456,lwr_k=500:123.7912,lwr_k=600:124.1645,lwr_k=700:124.4117,lwr_k=800:124.1753,lwr_k=900:124.5068,lwr_k=1000:124.5049'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:161.1109,lwr_k=200:169.7455,lwr_k=300:177.2284,lwr_k=400:183.4857,lwr_k=500:188.1308,lwr_k=600:192.1801,lwr_k=700:195.4051,lwr_k=800:206.7184,lwr_k=900:211.9907,lwr_k=1000:215.4105'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:163.3319,lwr_k=200:165.29,lwr_k=300:166.5624,lwr_k=400:174.7885,lwr_k=500:180.0395,lwr_k=600:186.2335,lwr_k=700:192.4994,lwr_k=800:200.462,lwr_k=900:202.6372,lwr_k=1000:206.3179'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:113.7268,lwr_k=200:123.3567,lwr_k=300:127.0335,lwr_k=400:128.4693,lwr_k=500:128.9557,lwr_k=600:129.6551,lwr_k=700:130.1543,lwr_k=800:130.4386,lwr_k=900:130.8055,lwr_k=1000:131.0943'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:123.9343,lwr_k=200:125.7467,lwr_k=300:127.5438,lwr_k=400:126.9912,lwr_k=500:127.8399,lwr_k=600:127.9303,lwr_k=700:127.9783,lwr_k=800:127.8215,lwr_k=900:128.1282,lwr_k=1000:128.4077'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_46'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:135.9337,lwr_k=200:153.348,lwr_k=300:163.8122,lwr_k=400:169.8927,lwr_k=500:173.1514,lwr_k=600:176.4879,lwr_k=700:179.0517,lwr_k=800:180.9166,lwr_k=900:182.4225,lwr_k=1000:184.8417'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:174.0038,lwr_k=200:182.9971,lwr_k=300:175.0959,lwr_k=400:179.5579,lwr_k=500:181.4584,lwr_k=600:184.3059,lwr_k=700:185.5718,lwr_k=800:186.418,lwr_k=900:186.5929,lwr_k=1000:187.0425'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:104.0362,lwr_k=200:121.0091,lwr_k=300:130.7106,lwr_k=400:137.7743,lwr_k=500:142.2488,lwr_k=600:146.6637,lwr_k=700:150.8308,lwr_k=800:153.4259,lwr_k=900:156.8291,lwr_k=1000:160.6468'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:211.6752,lwr_k=200:215.266,lwr_k=300:218.0723,lwr_k=400:226.6481,lwr_k=500:234.4917,lwr_k=600:239.0214,lwr_k=700:244.8527,lwr_k=800:247.1946,lwr_k=900:249.6938,lwr_k=1000:250.1889'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:77.4383,lwr_k=200:85.1454,lwr_k=300:86.8208,lwr_k=400:89.0176,lwr_k=500:90.629,lwr_k=600:91.0818,lwr_k=700:91.4437,lwr_k=800:91.7159,lwr_k=900:91.9039,lwr_k=1000:92.3125'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:130.7857,lwr_k=200:123.9867,lwr_k=300:124.5866,lwr_k=400:118.3457,lwr_k=500:114.1101,lwr_k=600:113.5255,lwr_k=700:114.0321,lwr_k=800:113.594,lwr_k=900:114.3871,lwr_k=1000:114.5672'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:137.3762,lwr_k=200:156.9724,lwr_k=300:169.1641,lwr_k=400:177.1127,lwr_k=500:182.3277,lwr_k=600:187.998,lwr_k=700:193.2009,lwr_k=800:201.4174,lwr_k=900:204.4177,lwr_k=1000:207.7813'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:151.7536,lwr_k=200:144.6656,lwr_k=300:147.1595,lwr_k=400:146.3439,lwr_k=500:148.366,lwr_k=600:150.1911,lwr_k=700:153.1424,lwr_k=800:152.8116,lwr_k=900:154.8743,lwr_k=1000:156.5473'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:139.4798,lwr_k=200:154.8014,lwr_k=300:166.4443,lwr_k=400:172.3679,lwr_k=500:178.9625,lwr_k=600:184.0791,lwr_k=700:187.2846,lwr_k=800:191.5274,lwr_k=900:195.6784,lwr_k=1000:201.6022'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:172.9641,lwr_k=200:170.1803,lwr_k=300:169.5881,lwr_k=400:170.917,lwr_k=500:175.4556,lwr_k=600:182.531,lwr_k=700:183.7903,lwr_k=800:187.4602,lwr_k=900:191.9499,lwr_k=1000:208.0996'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:120.7865,lwr_k=200:137.6404,lwr_k=300:144.9248,lwr_k=400:149.7409,lwr_k=500:153.8448,lwr_k=600:156.6265,lwr_k=700:160.902,lwr_k=800:164.4088,lwr_k=900:166.7517,lwr_k=1000:168.7698'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:151.3984,lwr_k=200:154.7087,lwr_k=300:158.2341,lwr_k=400:156.8913,lwr_k=500:158.4111,lwr_k=600:161.1902,lwr_k=700:161.7583,lwr_k=800:162.6889,lwr_k=900:163.6388,lwr_k=1000:164.9836'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_47'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:117.0974,lwr_k=200:144.4109,lwr_k=300:143.7082,lwr_k=400:114.6572,lwr_k=500:117.8357,lwr_k=600:118.8795,lwr_k=700:118.0665,lwr_k=800:120.9915,lwr_k=900:136.1298,lwr_k=1000:124.0267'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:418.6212,lwr_k=200:275.8672,lwr_k=300:270.7956,lwr_k=400:172.3937,lwr_k=500:148.1114,lwr_k=600:135.1786,lwr_k=700:130.7099,lwr_k=800:134.7793,lwr_k=900:150.5047,lwr_k=1000:139.4575'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:86.7814,lwr_k=200:75.3926,lwr_k=300:84.5525,lwr_k=400:83.7769,lwr_k=500:86.745,lwr_k=600:86.2993,lwr_k=700:89.7227,lwr_k=800:90.4557,lwr_k=900:91.1145,lwr_k=1000:93.8054'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:353.0827,lwr_k=200:225.8866,lwr_k=300:190.1446,lwr_k=400:170.044,lwr_k=500:180.5894,lwr_k=600:166.4947,lwr_k=700:164.7552,lwr_k=800:165.7995,lwr_k=900:165.3935,lwr_k=1000:164.7448'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:98.4646,lwr_k=200:99.9576,lwr_k=300:98.7669,lwr_k=400:105.2596,lwr_k=500:109.7302,lwr_k=600:114.8258,lwr_k=700:118.6963,lwr_k=800:117.223,lwr_k=900:119.0928,lwr_k=1000:120.3164'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:468.7174,lwr_k=200:191.9613,lwr_k=300:157.5908,lwr_k=400:148.224,lwr_k=500:137.0194,lwr_k=600:140.9648,lwr_k=700:154.3777,lwr_k=800:153.9602,lwr_k=900:148.4842,lwr_k=1000:142.2455'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:216.1786,lwr_k=200:175.8147,lwr_k=300:332.2651,lwr_k=400:292.8526,lwr_k=500:187.3278,lwr_k=600:209.778,lwr_k=700:212.2962,lwr_k=800:248.5705,lwr_k=900:246.8646,lwr_k=1000:263.8504'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:448.8658,lwr_k=200:253.872,lwr_k=300:293.4027,lwr_k=400:279.891,lwr_k=500:231.3822,lwr_k=600:257.5271,lwr_k=700:350.4498,lwr_k=800:365.239,lwr_k=900:303.2931,lwr_k=1000:275.9209'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:83.7656,lwr_k=200:113.2726,lwr_k=300:127.6741,lwr_k=400:143.054,lwr_k=500:165.8408,lwr_k=600:139.6495,lwr_k=700:151.9402,lwr_k=800:155.4303,lwr_k=900:179.4208,lwr_k=1000:163.9395'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:230.2741,lwr_k=200:304.9747,lwr_k=300:207.1726,lwr_k=400:157.1906,lwr_k=500:241.0719,lwr_k=600:163.1286,lwr_k=700:163.7094,lwr_k=800:173.552,lwr_k=900:188.5738,lwr_k=1000:169.4271'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:83.962,lwr_k=200:88.7319,lwr_k=300:89.8476,lwr_k=400:94.961,lwr_k=500:96.8105,lwr_k=600:99.198,lwr_k=700:99.8382,lwr_k=800:100.6404,lwr_k=900:101.1953,lwr_k=1000:101.7207'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:283.4197,lwr_k=200:189.5633,lwr_k=300:140.4672,lwr_k=400:119.5517,lwr_k=500:126.382,lwr_k=600:117.1252,lwr_k=700:116.2259,lwr_k=800:117.0951,lwr_k=900:116.8702,lwr_k=1000:117.8441'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_48'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:92.4067,lwr_k=200:95.0891,lwr_k=300:97.6133,lwr_k=400:94.0767,lwr_k=500:93.1008,lwr_k=600:96.8858,lwr_k=700:92.8694,lwr_k=800:94.2121,lwr_k=900:97.8684,lwr_k=1000:91.5766'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:113.0868,lwr_k=200:123.0494,lwr_k=300:122.3589,lwr_k=400:119.9824,lwr_k=500:119.875,lwr_k=600:120.295,lwr_k=700:114.5675,lwr_k=800:115.2193,lwr_k=900:119.4043,lwr_k=1000:118.067'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:73.2275,lwr_k=200:76.0866,lwr_k=300:76.4711,lwr_k=400:77.1162,lwr_k=500:77.7928,lwr_k=600:77.6608,lwr_k=700:78.2875,lwr_k=800:78.2824,lwr_k=900:78.9257,lwr_k=1000:79.3769'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:150.4421,lwr_k=200:150.3413,lwr_k=300:149.2751,lwr_k=400:153.1358,lwr_k=500:150.6949,lwr_k=600:149.7643,lwr_k=700:149.4191,lwr_k=800:151.8361,lwr_k=900:150.5168,lwr_k=1000:150.0466'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:67.1231,lwr_k=200:71.2032,lwr_k=300:70.9867,lwr_k=400:71.9364,lwr_k=500:70.5311,lwr_k=600:70.7775,lwr_k=700:70.3731,lwr_k=800:70.9481,lwr_k=900:70.7258,lwr_k=1000:70.2583'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:113.0132,lwr_k=200:116.5247,lwr_k=300:119.0849,lwr_k=400:112.1998,lwr_k=500:113.6601,lwr_k=600:122.2132,lwr_k=700:117.7437,lwr_k=800:118.5252,lwr_k=900:118.1021,lwr_k=1000:118.8673'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:87.7287,lwr_k=200:87.2298,lwr_k=300:89.9058,lwr_k=400:90.5824,lwr_k=500:88.9127,lwr_k=600:90.3626,lwr_k=700:89.816,lwr_k=800:90.0751,lwr_k=900:91.2289,lwr_k=1000:91.564'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:120.171,lwr_k=200:114.5171,lwr_k=300:111.4586,lwr_k=400:112.3715,lwr_k=500:115.6091,lwr_k=600:115.6345,lwr_k=700:114.1356,lwr_k=800:114.175,lwr_k=900:117.1879,lwr_k=1000:114.7074'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:88.7508,lwr_k=200:88.3608,lwr_k=300:91.3623,lwr_k=400:89.2058,lwr_k=500:90.0518,lwr_k=600:93.5228,lwr_k=700:90.2755,lwr_k=800:88.9786,lwr_k=900:90.8173,lwr_k=1000:91.6117'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:94.4414,lwr_k=200:97.7831,lwr_k=300:104.8831,lwr_k=400:96.1738,lwr_k=500:95.1322,lwr_k=600:93.8053,lwr_k=700:97.2248,lwr_k=800:96.652,lwr_k=900:95.3645,lwr_k=1000:96.3519'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:80.666,lwr_k=200:81.0583,lwr_k=300:80.7784,lwr_k=400:81.6292,lwr_k=500:82.4343,lwr_k=600:82.0968,lwr_k=700:80.3852,lwr_k=800:81.3333,lwr_k=900:82.5451,lwr_k=1000:81.8117'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:81.9221,lwr_k=200:86.2762,lwr_k=300:84.4217,lwr_k=400:90.6963,lwr_k=500:90.8163,lwr_k=600:83.0735,lwr_k=700:87.8965,lwr_k=800:86.6623,lwr_k=900:84.6501,lwr_k=1000:84.5018'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_49'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:355.0121,lwr_k=200:341.8789,lwr_k=300:340.4468,lwr_k=400:338.5653,lwr_k=500:337.9369,lwr_k=600:337.9893,lwr_k=700:337.883,lwr_k=800:337.872,lwr_k=900:338.0717,lwr_k=1000:338.3747'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:359.1625,lwr_k=200:346.8612,lwr_k=300:345.5834,lwr_k=400:344.0011,lwr_k=500:343.5918,lwr_k=600:343.6121,lwr_k=700:343.5869,lwr_k=800:343.6768,lwr_k=900:344.0134,lwr_k=1000:344.4167'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:342.141,lwr_k=200:325.3458,lwr_k=300:325.0387,lwr_k=400:323.1007,lwr_k=500:322.7478,lwr_k=600:322.7162,lwr_k=700:322.6374,lwr_k=800:322.6665,lwr_k=900:322.7149,lwr_k=1000:322.9889'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:446.4701,lwr_k=200:428.6428,lwr_k=300:428.2999,lwr_k=400:426.0387,lwr_k=500:425.557,lwr_k=600:425.5066,lwr_k=700:425.3437,lwr_k=800:425.2822,lwr_k=900:425.2919,lwr_k=1000:425.4501'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:349.9649,lwr_k=200:341.8788,lwr_k=300:343.2463,lwr_k=400:341.905,lwr_k=500:341.8376,lwr_k=600:341.744,lwr_k=700:341.5638,lwr_k=800:341.5574,lwr_k=900:341.5919,lwr_k=1000:341.5757'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:296.597,lwr_k=200:287.5631,lwr_k=300:289.2268,lwr_k=400:287.5983,lwr_k=500:287.5068,lwr_k=600:287.3746,lwr_k=700:287.07,lwr_k=800:286.9572,lwr_k=900:286.9501,lwr_k=1000:286.9504'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:388.5402,lwr_k=200:383.0828,lwr_k=300:383.9959,lwr_k=400:383.0632,lwr_k=500:382.9135,lwr_k=600:382.9949,lwr_k=700:383.6642,lwr_k=800:382.999,lwr_k=900:382.905,lwr_k=1000:382.907'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:307.8712,lwr_k=200:301.6193,lwr_k=300:302.7847,lwr_k=400:301.5901,lwr_k=500:301.3258,lwr_k=600:301.483,lwr_k=700:302.3826,lwr_k=800:301.4897,lwr_k=900:301.3008,lwr_k=1000:301.3072'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:352.26,lwr_k=200:352.0487,lwr_k=300:351.9734,lwr_k=400:352.3741,lwr_k=500:352.2368,lwr_k=600:352.4188,lwr_k=700:352.2593,lwr_k=800:352.3445,lwr_k=900:352.0386,lwr_k=1000:352.1666'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:379.7377,lwr_k=200:379.7178,lwr_k=300:379.7723,lwr_k=400:379.7775,lwr_k=500:379.7312,lwr_k=600:379.7959,lwr_k=700:379.7375,lwr_k=800:379.7661,lwr_k=900:379.7208,lwr_k=1000:379.7162'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:359.9774,lwr_k=200:361.1123,lwr_k=300:360.7826,lwr_k=400:359.4066,lwr_k=500:359.0701,lwr_k=600:359.1555,lwr_k=700:359.0456,lwr_k=800:358.968,lwr_k=900:358.9739,lwr_k=1000:358.9884'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:329.3252,lwr_k=200:330.1032,lwr_k=300:329.8646,lwr_k=400:329.0191,lwr_k=500:328.9427,lwr_k=600:328.9432,lwr_k=700:329.4148,lwr_k=800:329.1892,lwr_k=900:329.0057,lwr_k=1000:328.9841'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_50'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:78.1845,lwr_k=200:82.9368,lwr_k=300:84.7553,lwr_k=400:85.9909,lwr_k=500:86.9122,lwr_k=600:87.2273,lwr_k=700:87.5063,lwr_k=800:87.9541,lwr_k=900:88.132,lwr_k=1000:88.6088'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:123.247,lwr_k=200:116.0592,lwr_k=300:115.4634,lwr_k=400:119.2755,lwr_k=500:118.8921,lwr_k=600:118.3881,lwr_k=700:117.8136,lwr_k=800:117.0984,lwr_k=900:116.8341,lwr_k=1000:115.9621'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:342.141,lwr_k=200:325.3458,lwr_k=300:325.0387,lwr_k=400:323.1007,lwr_k=500:322.7478,lwr_k=600:322.7162,lwr_k=700:322.6374,lwr_k=800:322.6665,lwr_k=900:322.7149,lwr_k=1000:322.9889'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:446.4701,lwr_k=200:428.6428,lwr_k=300:428.2999,lwr_k=400:426.0387,lwr_k=500:425.557,lwr_k=600:425.5066,lwr_k=700:425.3437,lwr_k=800:425.2822,lwr_k=900:425.2919,lwr_k=1000:425.4501'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:349.9649,lwr_k=200:341.8788,lwr_k=300:343.2463,lwr_k=400:341.905,lwr_k=500:341.8376,lwr_k=600:341.744,lwr_k=700:341.5638,lwr_k=800:341.5574,lwr_k=900:341.5919,lwr_k=1000:341.5757'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:296.597,lwr_k=200:287.5631,lwr_k=300:289.2268,lwr_k=400:287.5983,lwr_k=500:287.5068,lwr_k=600:287.3746,lwr_k=700:287.07,lwr_k=800:286.9572,lwr_k=900:286.9501,lwr_k=1000:286.9504'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:388.5402,lwr_k=200:383.0828,lwr_k=300:383.9959,lwr_k=400:383.0632,lwr_k=500:382.9135,lwr_k=600:382.9949,lwr_k=700:383.6642,lwr_k=800:382.999,lwr_k=900:382.905,lwr_k=1000:382.907'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:307.8712,lwr_k=200:301.6193,lwr_k=300:302.7847,lwr_k=400:301.5901,lwr_k=500:301.3258,lwr_k=600:301.483,lwr_k=700:302.3826,lwr_k=800:301.4897,lwr_k=900:301.3008,lwr_k=1000:301.3072'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:352.26,lwr_k=200:352.0487,lwr_k=300:351.9734,lwr_k=400:352.3741,lwr_k=500:352.2368,lwr_k=600:352.4188,lwr_k=700:352.2593,lwr_k=800:352.3445,lwr_k=900:352.0386,lwr_k=1000:352.1666'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:379.7377,lwr_k=200:379.7178,lwr_k=300:379.7723,lwr_k=400:379.7775,lwr_k=500:379.7312,lwr_k=600:379.7959,lwr_k=700:379.7375,lwr_k=800:379.7661,lwr_k=900:379.7208,lwr_k=1000:379.7162'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:359.9774,lwr_k=200:361.1123,lwr_k=300:360.7826,lwr_k=400:359.4066,lwr_k=500:359.0701,lwr_k=600:359.1555,lwr_k=700:359.0456,lwr_k=800:358.968,lwr_k=900:358.9739,lwr_k=1000:358.9884'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:329.3252,lwr_k=200:330.1032,lwr_k=300:329.8646,lwr_k=400:329.0191,lwr_k=500:328.9427,lwr_k=600:328.9432,lwr_k=700:329.4148,lwr_k=800:329.1892,lwr_k=900:329.0057,lwr_k=1000:328.9841'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_51'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:90.9683,lwr_k=200:95.0276,lwr_k=300:96.5919,lwr_k=400:96.9864,lwr_k=500:97.9227,lwr_k=600:97.9445,lwr_k=700:97.6404,lwr_k=800:98.0207,lwr_k=900:97.9389,lwr_k=1000:98.2637'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:116.1078,lwr_k=200:113.653,lwr_k=300:114.5427,lwr_k=400:113.4875,lwr_k=500:113.3353,lwr_k=600:113.0608,lwr_k=700:112.2626,lwr_k=800:112.3544,lwr_k=900:112.817,lwr_k=1000:112.3562'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:134.5809,lwr_k=200:157.7381,lwr_k=300:170.5568,lwr_k=400:177.5235,lwr_k=500:182.5549,lwr_k=600:187.5497,lwr_k=700:191.1655,lwr_k=800:194.1891,lwr_k=900:198.1719,lwr_k=1000:203.6049'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:255.0503,lwr_k=200:263.4096,lwr_k=300:265.1534,lwr_k=400:277.7651,lwr_k=500:284.2082,lwr_k=600:289.6208,lwr_k=700:293.7411,lwr_k=800:297.2088,lwr_k=900:300.3357,lwr_k=1000:304.2916'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:176.0498,lwr_k=200:202.2933,lwr_k=300:232.5003,lwr_k=400:242.0063,lwr_k=500:241.7834,lwr_k=600:246.6752,lwr_k=700:248.3677,lwr_k=800:246.2959,lwr_k=900:247.8735,lwr_k=1000:249.598'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:260.5109,lwr_k=200:265.1172,lwr_k=300:294.6219,lwr_k=400:301.6953,lwr_k=500:296.5668,lwr_k=600:278.9756,lwr_k=700:276.8127,lwr_k=800:277.0554,lwr_k=900:277.0854,lwr_k=1000:275.1697'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:130.5887,lwr_k=200:141.3909,lwr_k=300:145.9994,lwr_k=400:148.3866,lwr_k=500:151.6925,lwr_k=600:153.9132,lwr_k=700:155.0446,lwr_k=800:156.3934,lwr_k=900:157.8494,lwr_k=1000:159.4305'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:143.8816,lwr_k=200:137.1569,lwr_k=300:137.4342,lwr_k=400:138.1689,lwr_k=500:138.3239,lwr_k=600:138.1799,lwr_k=700:137.7636,lwr_k=800:137.7669,lwr_k=900:136.9196,lwr_k=1000:137.1314'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:174.6073,lwr_k=200:187.1916,lwr_k=300:197.2856,lwr_k=400:202.3857,lwr_k=500:205.8827,lwr_k=600:206.4987,lwr_k=700:209.5314,lwr_k=800:210.8258,lwr_k=900:213.2616,lwr_k=1000:215.4528'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:206.8662,lwr_k=200:196.8741,lwr_k=300:194.2571,lwr_k=400:198.8906,lwr_k=500:201.6023,lwr_k=600:205.5976,lwr_k=700:207.5098,lwr_k=800:207.4587,lwr_k=900:210.2079,lwr_k=1000:212.3378'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:165.8445,lwr_k=200:183.8703,lwr_k=300:190.2794,lwr_k=400:193.3065,lwr_k=500:195.7987,lwr_k=600:198.1565,lwr_k=700:201.2724,lwr_k=800:203.2833,lwr_k=900:205.8929,lwr_k=1000:207.9044'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:200.5814,lwr_k=200:191.6953,lwr_k=300:193.4152,lwr_k=400:195.899,lwr_k=500:198.1945,lwr_k=600:197.6353,lwr_k=700:199.142,lwr_k=800:200.7699,lwr_k=900:199.8078,lwr_k=1000:201.0376'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_52'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:61.8625,lwr_k=200:84.4691,lwr_k=300:94.7509,lwr_k=400:98.4972,lwr_k=500:102.1644,lwr_k=600:105.6168,lwr_k=700:107.8329,lwr_k=800:110.5956,lwr_k=900:112.0247,lwr_k=1000:113.3905'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:228.9469,lwr_k=200:137.2661,lwr_k=300:126.4286,lwr_k=400:125.2731,lwr_k=500:126.7326,lwr_k=600:122.1199,lwr_k=700:121.52,lwr_k=800:122.301,lwr_k=900:123.0159,lwr_k=1000:122.6028'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:45.5365,lwr_k=200:68.2004,lwr_k=300:77.7495,lwr_k=400:83.5164,lwr_k=500:87.3083,lwr_k=600:90.6988,lwr_k=700:93.9261,lwr_k=800:96.1181,lwr_k=900:97.5641,lwr_k=1000:98.8279'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:266.7875,lwr_k=200:189.7764,lwr_k=300:167.8041,lwr_k=400:166.1793,lwr_k=500:166.3444,lwr_k=600:167.7424,lwr_k=700:166.5387,lwr_k=800:167.1219,lwr_k=900:167.8976,lwr_k=1000:168.4334'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:53.8223,lwr_k=200:76.3763,lwr_k=300:85.5352,lwr_k=400:91.1255,lwr_k=500:95.9568,lwr_k=600:98.3168,lwr_k=700:101.1317,lwr_k=800:103.6681,lwr_k=900:105.6069,lwr_k=1000:106.9435'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:213.9295,lwr_k=200:138.4409,lwr_k=300:119.2912,lwr_k=400:113.549,lwr_k=500:111.5558,lwr_k=600:112.1014,lwr_k=700:113.515,lwr_k=800:112.6581,lwr_k=900:112.5418,lwr_k=1000:112.0815'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:57.9098,lwr_k=200:89.1821,lwr_k=300:101.0069,lwr_k=400:107.2244,lwr_k=500:110.9067,lwr_k=600:113.9534,lwr_k=700:116.152,lwr_k=800:117.0434,lwr_k=900:118.0514,lwr_k=1000:118.4871'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:189.653,lwr_k=200:128.9919,lwr_k=300:119.4533,lwr_k=400:111.1888,lwr_k=500:108.6767,lwr_k=600:106.0129,lwr_k=700:105.8507,lwr_k=800:105.3849,lwr_k=900:105.6141,lwr_k=1000:104.7276'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:147.7834,lwr_k=200:154.5742,lwr_k=300:150.6035,lwr_k=400:149.1439,lwr_k=500:144.5832,lwr_k=600:140.8279,lwr_k=700:139.7667,lwr_k=800:148.2275,lwr_k=900:153.1054,lwr_k=1000:158.2833'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:386.8729,lwr_k=200:221.157,lwr_k=300:311.4903,lwr_k=400:183.0569,lwr_k=500:162.8148,lwr_k=600:163.8404,lwr_k=700:156.7472,lwr_k=800:165.0851,lwr_k=900:155.0444,lwr_k=1000:173.4119'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:58.1522,lwr_k=200:81.9821,lwr_k=300:89.0448,lwr_k=400:93.135,lwr_k=500:95.5425,lwr_k=600:98.2337,lwr_k=700:100.0214,lwr_k=800:101.4827,lwr_k=900:102.5114,lwr_k=1000:103.2235'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:160.9646,lwr_k=200:121.1773,lwr_k=300:113.7642,lwr_k=400:110.044,lwr_k=500:109.1608,lwr_k=600:109.3608,lwr_k=700:109.8573,lwr_k=800:109.2301,lwr_k=900:107.7854,lwr_k=1000:109.574'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_53'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:98.7595,lwr_k=200:133.8479,lwr_k=300:190.1131,lwr_k=400:129.4405,lwr_k=500:215.5706,lwr_k=600:226.482,lwr_k=700:137.9572,lwr_k=800:224.606,lwr_k=900:194.5158,lwr_k=1000:147.6035'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:184.2433,lwr_k=200:194.2224,lwr_k=300:255.3482,lwr_k=400:138.0768,lwr_k=500:412.0348,lwr_k=600:423.0542,lwr_k=700:165.3308,lwr_k=800:400.2924,lwr_k=900:301.6395,lwr_k=1000:150.7837'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:72.262,lwr_k=200:84.3638,lwr_k=300:90.9275,lwr_k=400:93.2347,lwr_k=500:95.1871,lwr_k=600:96.0097,lwr_k=700:96.3182,lwr_k=800:97.0779,lwr_k=900:97.3198,lwr_k=1000:97.6335'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:206.3218,lwr_k=200:168.8903,lwr_k=300:163.8648,lwr_k=400:160.9181,lwr_k=500:159.736,lwr_k=600:159.3549,lwr_k=700:159.6377,lwr_k=800:159.2765,lwr_k=900:159.3102,lwr_k=1000:159.3301'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:67.6057,lwr_k=200:79.3583,lwr_k=300:84.7106,lwr_k=400:88.0959,lwr_k=500:91.0212,lwr_k=600:92.4212,lwr_k=700:93.3298,lwr_k=800:93.8738,lwr_k=900:94.221,lwr_k=1000:94.6764'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:157.3523,lwr_k=200:121.9168,lwr_k=300:116.5862,lwr_k=400:116.584,lwr_k=500:115.6705,lwr_k=600:115.8605,lwr_k=700:116.2485,lwr_k=800:114.5986,lwr_k=900:115.3071,lwr_k=1000:115.7068'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:113.4842,lwr_k=200:123.058,lwr_k=300:127.4904,lwr_k=400:130.2285,lwr_k=500:130.2117,lwr_k=600:130.9432,lwr_k=700:131.0554,lwr_k=800:131.9868,lwr_k=900:131.7016,lwr_k=1000:133.6524'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:195.892,lwr_k=200:139.8543,lwr_k=300:133.8168,lwr_k=400:135.7789,lwr_k=500:128.2873,lwr_k=600:127.0875,lwr_k=700:127.7026,lwr_k=800:127.2714,lwr_k=900:124.3103,lwr_k=1000:124.1433'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:110.8739,lwr_k=200:100.6602,lwr_k=300:103.4486,lwr_k=400:106.1318,lwr_k=500:108.7213,lwr_k=600:110.3348,lwr_k=700:112.3624,lwr_k=800:112.9071,lwr_k=900:113.3694,lwr_k=1000:114.0695'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:142.7431,lwr_k=200:129.0602,lwr_k=300:123.1102,lwr_k=400:118.7065,lwr_k=500:114.8913,lwr_k=600:112.903,lwr_k=700:111.9575,lwr_k=800:112.0801,lwr_k=900:111.3554,lwr_k=1000:110.3956'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:87.3992,lwr_k=200:107.2379,lwr_k=300:110.5401,lwr_k=400:108.2787,lwr_k=500:107.0815,lwr_k=600:109.2438,lwr_k=700:111.1942,lwr_k=800:113.3702,lwr_k=900:115.1486,lwr_k=1000:116.6942'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:130.7114,lwr_k=200:126.63,lwr_k=300:173.9329,lwr_k=400:156.1769,lwr_k=500:132.137,lwr_k=600:132.711,lwr_k=700:128.7547,lwr_k=800:126.7331,lwr_k=900:126.4859,lwr_k=1000:126.0891'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_54'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:66.5248,lwr_k=200:73.7564,lwr_k=300:75.9462,lwr_k=400:77.1909,lwr_k=500:78.0484,lwr_k=600:78.6245,lwr_k=700:78.8799,lwr_k=800:78.9995,lwr_k=900:79.4095,lwr_k=1000:79.6397'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:126.9954,lwr_k=200:123.5315,lwr_k=300:122.8829,lwr_k=400:124.2276,lwr_k=500:125.317,lwr_k=600:124.5944,lwr_k=700:124.6585,lwr_k=800:124.6195,lwr_k=900:124.5917,lwr_k=1000:123.9435'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:54.7234,lwr_k=200:61.8154,lwr_k=300:64.4887,lwr_k=400:66.4034,lwr_k=500:67.4353,lwr_k=600:67.9727,lwr_k=700:68.4765,lwr_k=800:69.1187,lwr_k=900:69.6645,lwr_k=1000:69.9225'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:151.4959,lwr_k=200:146.404,lwr_k=300:141.8066,lwr_k=400:141.1833,lwr_k=500:141.2301,lwr_k=600:140.8699,lwr_k=700:141.0471,lwr_k=800:139.1953,lwr_k=900:139.318,lwr_k=1000:138.9594'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:55.2043,lwr_k=200:60.8131,lwr_k=300:63.4683,lwr_k=400:64.8549,lwr_k=500:65.7343,lwr_k=600:66.1495,lwr_k=700:66.8633,lwr_k=800:67.2691,lwr_k=900:67.4953,lwr_k=1000:67.9694'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:103.1342,lwr_k=200:99.3168,lwr_k=300:97.5342,lwr_k=400:98.2113,lwr_k=500:98.3453,lwr_k=600:97.5678,lwr_k=700:95.7221,lwr_k=800:95.818,lwr_k=900:96.1729,lwr_k=1000:95.3801'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:66.8853,lwr_k=200:79.0385,lwr_k=300:82.0423,lwr_k=400:83.6068,lwr_k=500:84.2538,lwr_k=600:85.1816,lwr_k=700:85.8867,lwr_k=800:86.4865,lwr_k=900:86.918,lwr_k=1000:87.4333'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:105.1422,lwr_k=200:90.9683,lwr_k=300:88.1107,lwr_k=400:87.5861,lwr_k=500:88.0284,lwr_k=600:87.3291,lwr_k=700:86.6736,lwr_k=800:86.7007,lwr_k=900:86.4335,lwr_k=1000:86.6143'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:66.613,lwr_k=200:74.7569,lwr_k=300:78.731,lwr_k=400:80.8481,lwr_k=500:82.1811,lwr_k=600:82.7031,lwr_k=700:83.4328,lwr_k=800:84.0495,lwr_k=900:84.3438,lwr_k=1000:84.8533'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:129.4348,lwr_k=200:121.098,lwr_k=300:118.6854,lwr_k=400:119.4331,lwr_k=500:118.993,lwr_k=600:118.7234,lwr_k=700:118.6544,lwr_k=800:119.0236,lwr_k=900:119.5866,lwr_k=1000:119.3861'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:68.2788,lwr_k=200:77.2236,lwr_k=300:79.6148,lwr_k=400:81.6401,lwr_k=500:83.0064,lwr_k=600:83.818,lwr_k=700:84.5798,lwr_k=800:85.1169,lwr_k=900:85.5727,lwr_k=1000:86.7928'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:140.3186,lwr_k=200:141.0456,lwr_k=300:122.5319,lwr_k=400:122.6428,lwr_k=500:122.0104,lwr_k=600:122.8691,lwr_k=700:124.1977,lwr_k=800:124.2458,lwr_k=900:124.1496,lwr_k=1000:124.2297'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_55'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:111.308,lwr_k=200:113.1807,lwr_k=300:114.9691,lwr_k=400:115.8276,lwr_k=500:116.2767,lwr_k=600:116.3737,lwr_k=700:116.6512,lwr_k=800:116.9689,lwr_k=900:117.2458,lwr_k=1000:117.3804'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:108.5785,lwr_k=200:105.2113,lwr_k=300:104.1112,lwr_k=400:102.9024,lwr_k=500:102.4069,lwr_k=600:102.6271,lwr_k=700:102.8694,lwr_k=800:103.2505,lwr_k=900:103.2969,lwr_k=1000:103.0215'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:115.5663,lwr_k=200:133.2564,lwr_k=300:145.5348,lwr_k=400:152.5803,lwr_k=500:156.5465,lwr_k=600:160.2772,lwr_k=700:163.3467,lwr_k=800:166.3632,lwr_k=900:169.0318,lwr_k=1000:171.9999'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:227.4704,lwr_k=200:237.012,lwr_k=300:242.4987,lwr_k=400:245.5841,lwr_k=500:247.6038,lwr_k=600:248.3893,lwr_k=700:253.5328,lwr_k=800:256.1696,lwr_k=900:259.7729,lwr_k=1000:262.5839'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:168.7701,lwr_k=200:195.6731,lwr_k=300:207.6371,lwr_k=400:214.2043,lwr_k=500:220.236,lwr_k=600:223.1883,lwr_k=700:224.9674,lwr_k=800:227.2475,lwr_k=900:229.8633,lwr_k=1000:232.1619'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:223.5713,lwr_k=200:241.3125,lwr_k=300:236.9796,lwr_k=400:208.4696,lwr_k=500:204.3338,lwr_k=600:201.005,lwr_k=700:202.2056,lwr_k=800:203.154,lwr_k=900:203.3518,lwr_k=1000:204.7149'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:111.3116,lwr_k=200:118.983,lwr_k=300:121.8476,lwr_k=400:123.9889,lwr_k=500:126.3674,lwr_k=600:127.1661,lwr_k=700:128.2828,lwr_k=800:129.5277,lwr_k=900:130.5754,lwr_k=1000:131.205'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:264.0612,lwr_k=200:180.702,lwr_k=300:169.1114,lwr_k=400:175.4172,lwr_k=500:179.3607,lwr_k=600:156.5825,lwr_k=700:138.6245,lwr_k=800:134.0158,lwr_k=900:131.5207,lwr_k=1000:130.4967'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:92.7112,lwr_k=200:100.1762,lwr_k=300:103.1522,lwr_k=400:104.4209,lwr_k=500:105.3704,lwr_k=600:105.9585,lwr_k=700:106.9457,lwr_k=800:107.5495,lwr_k=900:107.9118,lwr_k=1000:108.602'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:112.1339,lwr_k=200:105.6671,lwr_k=300:103.3164,lwr_k=400:102.1498,lwr_k=500:102.3702,lwr_k=600:102.4406,lwr_k=700:102.4244,lwr_k=800:102.1345,lwr_k=900:101.9335,lwr_k=1000:101.7478'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:156.4381,lwr_k=200:195.6123,lwr_k=300:206.8005,lwr_k=400:214.3732,lwr_k=500:217.6772,lwr_k=600:220.8912,lwr_k=700:224.3302,lwr_k=800:226.354,lwr_k=900:228.7712,lwr_k=1000:230.3277'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:212.7067,lwr_k=200:204.7569,lwr_k=300:210.6828,lwr_k=400:215.6872,lwr_k=500:217.4502,lwr_k=600:221.4269,lwr_k=700:222.3102,lwr_k=800:221.2987,lwr_k=900:222.2804,lwr_k=1000:225.54'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_56'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:70.062,lwr_k=200:80.7029,lwr_k=300:89.2783,lwr_k=400:95.6804,lwr_k=500:100.8547,lwr_k=600:103.7327,lwr_k=700:107.0407,lwr_k=800:109.5363,lwr_k=900:113.2713,lwr_k=1000:114.9783'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:351.7315,lwr_k=200:225.9043,lwr_k=300:131.5989,lwr_k=400:131.8603,lwr_k=500:127.3796,lwr_k=600:132.118,lwr_k=700:132.455,lwr_k=800:131.0217,lwr_k=900:134.3416,lwr_k=1000:135.2566'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:70.9067,lwr_k=200:67.726,lwr_k=300:73.3594,lwr_k=400:78.6419,lwr_k=500:83.2404,lwr_k=600:87.8128,lwr_k=700:90.562,lwr_k=800:92.9993,lwr_k=900:94.8012,lwr_k=1000:97.3308'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:356.0272,lwr_k=200:229.1455,lwr_k=300:203.8463,lwr_k=400:196.9339,lwr_k=500:194.481,lwr_k=600:188.7023,lwr_k=700:187.3466,lwr_k=800:185.107,lwr_k=900:184.3949,lwr_k=1000:188.1846'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:74.8746,lwr_k=200:75.0124,lwr_k=300:82.0574,lwr_k=400:89.0631,lwr_k=500:92.697,lwr_k=600:96.4838,lwr_k=700:100.3052,lwr_k=800:103.584,lwr_k=900:106.7046,lwr_k=1000:109.5251'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:246.894,lwr_k=200:394.6397,lwr_k=300:467.3012,lwr_k=400:467.0948,lwr_k=500:457.448,lwr_k=600:457.871,lwr_k=700:456.1328,lwr_k=800:463.4229,lwr_k=900:464.3,lwr_k=1000:433.2349'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:71.6036,lwr_k=200:84.1246,lwr_k=300:93.7953,lwr_k=400:100.3087,lwr_k=500:105.7035,lwr_k=600:109.4626,lwr_k=700:113.3293,lwr_k=800:116.6215,lwr_k=900:119.0459,lwr_k=1000:121.6159'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:266.3047,lwr_k=200:182.7391,lwr_k=300:134.6996,lwr_k=400:120.1588,lwr_k=500:118.3647,lwr_k=600:118.1225,lwr_k=700:117.4065,lwr_k=800:114.0743,lwr_k=900:113.7786,lwr_k=1000:113.3927'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:94.0815,lwr_k=200:94.6674,lwr_k=300:103.1213,lwr_k=400:105.1763,lwr_k=500:109.8849,lwr_k=600:114.7509,lwr_k=700:118.7035,lwr_k=800:122.699,lwr_k=900:126.6605,lwr_k=1000:128.5607'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:310.8698,lwr_k=200:154.6216,lwr_k=300:137.2428,lwr_k=400:118.0438,lwr_k=500:119.4796,lwr_k=600:123.3678,lwr_k=700:126.8314,lwr_k=800:127.1046,lwr_k=900:129.093,lwr_k=1000:130.6796'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:103.1482,lwr_k=200:88.4034,lwr_k=300:94.7699,lwr_k=400:95.856,lwr_k=500:99.7452,lwr_k=600:101.9199,lwr_k=700:104.3917,lwr_k=800:106.5019,lwr_k=900:109.2429,lwr_k=1000:111.9578'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:356.9408,lwr_k=200:217.5629,lwr_k=300:261.704,lwr_k=400:212.2491,lwr_k=500:222.3921,lwr_k=600:150.1157,lwr_k=700:133.7252,lwr_k=800:133.4276,lwr_k=900:127.8505,lwr_k=1000:131.3232'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_57'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:93.6431,lwr_k=200:106.4577,lwr_k=300:111.3536,lwr_k=400:113.9542,lwr_k=500:115.1778,lwr_k=600:116.0553,lwr_k=700:116.931,lwr_k=800:117.2469,lwr_k=900:118.0531,lwr_k=1000:118.4712'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:141.4982,lwr_k=200:122.3043,lwr_k=300:119.4704,lwr_k=400:120.1212,lwr_k=500:120.2378,lwr_k=600:120.0781,lwr_k=700:119.801,lwr_k=800:119.1218,lwr_k=900:119.285,lwr_k=1000:119.2465'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:72.673,lwr_k=200:84.0973,lwr_k=300:89.8804,lwr_k=400:91.8153,lwr_k=500:93.2914,lwr_k=600:94.6069,lwr_k=700:94.964,lwr_k=800:95.4061,lwr_k=900:95.8721,lwr_k=1000:96.1897'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:169.4862,lwr_k=200:155.1606,lwr_k=300:154.757,lwr_k=400:154.1411,lwr_k=500:153.0717,lwr_k=600:153.4154,lwr_k=700:152.4794,lwr_k=800:152.353,lwr_k=900:152.7255,lwr_k=1000:152.6623'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:64.6858,lwr_k=200:78.7758,lwr_k=300:84.9601,lwr_k=400:88.3525,lwr_k=500:90.1426,lwr_k=600:91.8421,lwr_k=700:93.0117,lwr_k=800:93.5746,lwr_k=900:94.3179,lwr_k=1000:94.7643'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:114.9144,lwr_k=200:100.3304,lwr_k=300:99.5076,lwr_k=400:100.4282,lwr_k=500:101.2138,lwr_k=600:100.786,lwr_k=700:101.7189,lwr_k=800:101.9407,lwr_k=900:102.9206,lwr_k=1000:104.2364'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:86.2089,lwr_k=200:109.989,lwr_k=300:113.8779,lwr_k=400:116.6125,lwr_k=500:117.9904,lwr_k=600:119.3117,lwr_k=700:120.0379,lwr_k=800:120.6177,lwr_k=900:121.2262,lwr_k=1000:121.6917'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:137.812,lwr_k=200:116.7157,lwr_k=300:114.0142,lwr_k=400:111.6248,lwr_k=500:110.9386,lwr_k=600:110.8853,lwr_k=700:109.9137,lwr_k=800:110.0105,lwr_k=900:109.4196,lwr_k=1000:109.1694'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:93.0851,lwr_k=200:106.315,lwr_k=300:111.277,lwr_k=400:113.9545,lwr_k=500:114.941,lwr_k=600:115.9382,lwr_k=700:116.6313,lwr_k=800:117.1812,lwr_k=900:117.718,lwr_k=1000:118.3076'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:112.7917,lwr_k=200:104.9469,lwr_k=300:106.2106,lwr_k=400:104.3945,lwr_k=500:105.8509,lwr_k=600:106.4285,lwr_k=700:106.8153,lwr_k=800:108.3683,lwr_k=900:109.3829,lwr_k=1000:109.979'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:84.4625,lwr_k=200:99.0112,lwr_k=300:103.5555,lwr_k=400:106.2873,lwr_k=500:107.981,lwr_k=600:109.209,lwr_k=700:109.9608,lwr_k=800:110.7052,lwr_k=900:110.722,lwr_k=1000:111.1161'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:132.9457,lwr_k=200:112.6195,lwr_k=300:117.0116,lwr_k=400:114.9312,lwr_k=500:117.0369,lwr_k=600:117.7237,lwr_k=700:117.1165,lwr_k=800:116.9392,lwr_k=900:117.1338,lwr_k=1000:117.2037'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_58'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:104.8279,lwr_k=200:110.1778,lwr_k=300:113.6137,lwr_k=400:115.7437,lwr_k=500:116.6854,lwr_k=600:117.5066,lwr_k=700:118.494,lwr_k=800:119.1241,lwr_k=900:119.6797,lwr_k=1000:120.1082'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:111.3426,lwr_k=200:109.8313,lwr_k=300:108.7919,lwr_k=400:109.2515,lwr_k=500:111.8035,lwr_k=600:113.4687,lwr_k=700:114.4592,lwr_k=800:115.521,lwr_k=900:116.6615,lwr_k=1000:117.3227'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:93.4738,lwr_k=200:96.3498,lwr_k=300:98.477,lwr_k=400:99.4475,lwr_k=500:99.9678,lwr_k=600:100.5894,lwr_k=700:101.0583,lwr_k=800:101.0536,lwr_k=900:101.2066,lwr_k=1000:101.5074'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:158.1255,lwr_k=200:159.8554,lwr_k=300:158.287,lwr_k=400:158.7283,lwr_k=500:158.671,lwr_k=600:158.6452,lwr_k=700:158.8472,lwr_k=800:158.7402,lwr_k=900:158.9648,lwr_k=1000:159.1837'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:90.4672,lwr_k=200:96.0305,lwr_k=300:96.8588,lwr_k=400:97.5155,lwr_k=500:98.8467,lwr_k=600:99.0789,lwr_k=700:99.4999,lwr_k=800:99.7623,lwr_k=900:99.9421,lwr_k=1000:100.2102'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:117.3305,lwr_k=200:112.2397,lwr_k=300:111.8789,lwr_k=400:112.9349,lwr_k=500:110.662,lwr_k=600:111.0239,lwr_k=700:110.6723,lwr_k=800:110.7238,lwr_k=900:110.7177,lwr_k=1000:110.9645'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:102.2093,lwr_k=200:114.1447,lwr_k=300:117.7829,lwr_k=400:121.3689,lwr_k=500:123.689,lwr_k=600:125.0,lwr_k=700:126.8039,lwr_k=800:128.0239,lwr_k=900:129.0035,lwr_k=1000:129.7971'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:123.7664,lwr_k=200:108.9287,lwr_k=300:107.3434,lwr_k=400:105.3429,lwr_k=500:105.1871,lwr_k=600:104.8027,lwr_k=700:104.3828,lwr_k=800:103.4258,lwr_k=900:103.2356,lwr_k=1000:103.3936'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:105.0644,lwr_k=200:111.8566,lwr_k=300:117.7982,lwr_k=400:119.5175,lwr_k=500:120.6013,lwr_k=600:121.3383,lwr_k=700:121.7304,lwr_k=800:121.9708,lwr_k=900:122.3945,lwr_k=1000:122.3955'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:100.9794,lwr_k=200:104.547,lwr_k=300:108.7868,lwr_k=400:112.1258,lwr_k=500:113.2105,lwr_k=600:113.8941,lwr_k=700:114.3365,lwr_k=800:115.0272,lwr_k=900:115.6593,lwr_k=1000:115.879'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:100.8658,lwr_k=200:107.5916,lwr_k=300:111.8879,lwr_k=400:113.0181,lwr_k=500:114.2257,lwr_k=600:115.026,lwr_k=700:115.5149,lwr_k=800:116.0891,lwr_k=900:116.4661,lwr_k=1000:116.6788'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:115.3529,lwr_k=200:110.5719,lwr_k=300:108.4882,lwr_k=400:109.7399,lwr_k=500:108.8921,lwr_k=600:108.6809,lwr_k=700:109.3281,lwr_k=800:109.2122,lwr_k=900:109.0549,lwr_k=1000:109.2814'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_59'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:79.5377,lwr_k=200:92.2547,lwr_k=300:97.2943,lwr_k=400:100.3772,lwr_k=500:102.6706,lwr_k=600:104.0109,lwr_k=700:106.2156,lwr_k=800:106.2513,lwr_k=900:107.1122,lwr_k=1000:107.7968'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:136.6226,lwr_k=200:115.4381,lwr_k=300:109.8701,lwr_k=400:107.5666,lwr_k=500:107.2352,lwr_k=600:109.3244,lwr_k=700:108.5949,lwr_k=800:109.3719,lwr_k=900:110.2958,lwr_k=1000:109.5824'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:40.5892,lwr_k=200:47.0301,lwr_k=300:49.1663,lwr_k=400:50.3955,lwr_k=500:50.7423,lwr_k=600:51.3996,lwr_k=700:51.8531,lwr_k=800:52.1232,lwr_k=900:52.3785,lwr_k=1000:52.5637'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:150.015,lwr_k=200:136.7441,lwr_k=300:135.7277,lwr_k=400:134.3807,lwr_k=500:133.0042,lwr_k=600:132.6784,lwr_k=700:132.6967,lwr_k=800:132.7259,lwr_k=900:132.4099,lwr_k=1000:132.228'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:151.6097,lwr_k=200:169.2291,lwr_k=300:179.7018,lwr_k=400:191.565,lwr_k=500:196.1297,lwr_k=600:201.0459,lwr_k=700:201.9768,lwr_k=800:205.8582,lwr_k=900:207.744,lwr_k=1000:212.3038'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:208.1821,lwr_k=200:180.2811,lwr_k=300:182.398,lwr_k=400:191.9955,lwr_k=500:196.2798,lwr_k=600:199.9079,lwr_k=700:195.1167,lwr_k=800:198.2581,lwr_k=900:197.4403,lwr_k=1000:203.9233'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:59.1724,lwr_k=200:64.4211,lwr_k=300:65.3372,lwr_k=400:66.1619,lwr_k=500:67.4439,lwr_k=600:67.2564,lwr_k=700:67.3258,lwr_k=800:68.1776,lwr_k=900:68.4058,lwr_k=1000:68.3834'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:105.8459,lwr_k=200:99.0296,lwr_k=300:93.9089,lwr_k=400:94.0225,lwr_k=500:94.2504,lwr_k=600:101.61,lwr_k=700:101.0367,lwr_k=800:100.3968,lwr_k=900:98.9422,lwr_k=1000:99.8543'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:64.3833,lwr_k=200:74.1204,lwr_k=300:78.2526,lwr_k=400:80.2844,lwr_k=500:81.549,lwr_k=600:82.3888,lwr_k=700:82.7545,lwr_k=800:83.2165,lwr_k=900:83.9862,lwr_k=1000:84.2654'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:111.5819,lwr_k=200:87.6855,lwr_k=300:83.6885,lwr_k=400:83.414,lwr_k=500:82.457,lwr_k=600:82.3623,lwr_k=700:82.798,lwr_k=800:83.3935,lwr_k=900:83.3137,lwr_k=1000:82.814'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:67.0299,lwr_k=200:75.8441,lwr_k=300:80.2843,lwr_k=400:82.0428,lwr_k=500:83.2766,lwr_k=600:84.5484,lwr_k=700:84.9737,lwr_k=800:85.4002,lwr_k=900:85.5812,lwr_k=1000:85.9908'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:135.4724,lwr_k=200:131.511,lwr_k=300:99.803,lwr_k=400:99.9522,lwr_k=500:101.6277,lwr_k=600:101.6721,lwr_k=700:102.2905,lwr_k=800:101.7494,lwr_k=900:101.4818,lwr_k=1000:101.4335'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_60'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:45.5539,lwr_k=200:55.3679,lwr_k=300:58.515,lwr_k=400:60.2521,lwr_k=500:61.6092,lwr_k=600:62.3993,lwr_k=700:63.0391,lwr_k=800:63.7197,lwr_k=900:64.1916,lwr_k=1000:64.5635'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:168.8889,lwr_k=200:124.6201,lwr_k=300:119.9273,lwr_k=400:120.549,lwr_k=500:122.3664,lwr_k=600:121.8887,lwr_k=700:121.223,lwr_k=800:120.9671,lwr_k=900:121.6404,lwr_k=1000:121.5989'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:35.764,lwr_k=200:40.973,lwr_k=300:43.6874,lwr_k=400:44.9275,lwr_k=500:45.5692,lwr_k=600:46.2563,lwr_k=700:46.6506,lwr_k=800:46.9615,lwr_k=900:47.1992,lwr_k=1000:47.3092'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:158.2353,lwr_k=200:148.6857,lwr_k=300:149.564,lwr_k=400:148.8243,lwr_k=500:147.9921,lwr_k=600:147.098,lwr_k=700:147.5882,lwr_k=800:147.6782,lwr_k=900:147.5907,lwr_k=1000:147.317'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:32.0321,lwr_k=200:38.9699,lwr_k=300:41.3745,lwr_k=400:42.4396,lwr_k=500:43.3301,lwr_k=600:43.8662,lwr_k=700:44.2819,lwr_k=800:44.5649,lwr_k=900:44.8069,lwr_k=1000:45.047'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:101.4839,lwr_k=200:93.817,lwr_k=300:91.4926,lwr_k=400:91.1097,lwr_k=500:89.7373,lwr_k=600:89.487,lwr_k=700:89.4808,lwr_k=800:90.0522,lwr_k=900:90.2167,lwr_k=1000:90.2084'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:39.3503,lwr_k=200:46.8539,lwr_k=300:49.4548,lwr_k=400:50.9203,lwr_k=500:51.8925,lwr_k=600:52.479,lwr_k=700:53.0126,lwr_k=800:53.4505,lwr_k=900:53.6546,lwr_k=1000:53.9264'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:118.8822,lwr_k=200:104.1466,lwr_k=300:102.1913,lwr_k=400:98.5227,lwr_k=500:97.7221,lwr_k=600:97.6981,lwr_k=700:97.8646,lwr_k=800:98.2228,lwr_k=900:98.1485,lwr_k=1000:97.9654'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:53.4168,lwr_k=200:61.7133,lwr_k=300:64.7414,lwr_k=400:67.0932,lwr_k=500:68.7415,lwr_k=600:69.8487,lwr_k=700:70.532,lwr_k=800:71.0339,lwr_k=900:71.456,lwr_k=1000:71.8307'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:102.4613,lwr_k=200:86.7733,lwr_k=300:82.8752,lwr_k=400:80.3953,lwr_k=500:80.0085,lwr_k=600:79.7468,lwr_k=700:79.9396,lwr_k=800:80.2047,lwr_k=900:79.6509,lwr_k=1000:79.4347'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:38.3286,lwr_k=200:43.9287,lwr_k=300:46.457,lwr_k=400:47.8192,lwr_k=500:48.8421,lwr_k=600:49.6514,lwr_k=700:50.2669,lwr_k=800:50.6467,lwr_k=900:51.0222,lwr_k=1000:51.4111'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:130.3486,lwr_k=200:140.4936,lwr_k=300:135.7729,lwr_k=400:133.0157,lwr_k=500:135.6008,lwr_k=600:137.8117,lwr_k=700:135.8163,lwr_k=800:136.3102,lwr_k=900:137.3637,lwr_k=1000:136.7857'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_61'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:118.7029,lwr_k=200:126.7281,lwr_k=300:130.1006,lwr_k=400:131.8579,lwr_k=500:132.5791,lwr_k=600:133.9531,lwr_k=700:134.2699,lwr_k=800:134.7202,lwr_k=900:135.192,lwr_k=1000:135.404'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:135.3854,lwr_k=200:140.1993,lwr_k=300:142.6982,lwr_k=400:144.7247,lwr_k=500:145.7581,lwr_k=600:147.0459,lwr_k=700:145.8435,lwr_k=800:145.0135,lwr_k=900:145.5699,lwr_k=1000:146.1914'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:109.3957,lwr_k=200:118.0047,lwr_k=300:122.2559,lwr_k=400:125.0951,lwr_k=500:126.8109,lwr_k=600:128.013,lwr_k=700:128.6476,lwr_k=800:129.4604,lwr_k=900:130.5058,lwr_k=1000:131.6012'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:192.629,lwr_k=200:186.1416,lwr_k=300:190.0062,lwr_k=400:192.1823,lwr_k=500:193.0996,lwr_k=600:194.9889,lwr_k=700:197.2562,lwr_k=800:198.8614,lwr_k=900:199.7086,lwr_k=1000:200.4931'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:111.0915,lwr_k=200:116.8159,lwr_k=300:120.2942,lwr_k=400:124.2892,lwr_k=500:126.1041,lwr_k=600:127.7345,lwr_k=700:128.4406,lwr_k=800:128.9732,lwr_k=900:129.2388,lwr_k=1000:129.5247'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:126.0735,lwr_k=200:118.7726,lwr_k=300:117.0018,lwr_k=400:115.9259,lwr_k=500:115.6048,lwr_k=600:116.2724,lwr_k=700:116.1193,lwr_k=800:116.2374,lwr_k=900:116.7175,lwr_k=1000:117.42'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:128.7968,lwr_k=200:139.2492,lwr_k=300:145.2953,lwr_k=400:149.4305,lwr_k=500:151.6935,lwr_k=600:152.5767,lwr_k=700:154.2773,lwr_k=800:155.8688,lwr_k=900:157.0739,lwr_k=1000:157.9703'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:128.513,lwr_k=200:116.6283,lwr_k=300:115.7068,lwr_k=400:115.3578,lwr_k=500:114.927,lwr_k=600:114.8886,lwr_k=700:114.9923,lwr_k=800:115.6199,lwr_k=900:115.9514,lwr_k=1000:115.6515'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:119.0387,lwr_k=200:128.2879,lwr_k=300:131.3795,lwr_k=400:132.3848,lwr_k=500:133.3661,lwr_k=600:134.2111,lwr_k=700:134.3981,lwr_k=800:134.778,lwr_k=900:135.2041,lwr_k=1000:135.4542'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:115.607,lwr_k=200:121.4089,lwr_k=300:121.9325,lwr_k=400:122.7607,lwr_k=500:123.2231,lwr_k=600:124.1319,lwr_k=700:124.3021,lwr_k=800:124.6022,lwr_k=900:124.7173,lwr_k=1000:124.8201'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:113.5226,lwr_k=200:119.4539,lwr_k=300:121.4007,lwr_k=400:122.4506,lwr_k=500:123.3468,lwr_k=600:123.7466,lwr_k=700:124.0092,lwr_k=800:124.179,lwr_k=900:124.3202,lwr_k=1000:124.283'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:124.0453,lwr_k=200:133.7474,lwr_k=300:136.0675,lwr_k=400:134.7089,lwr_k=500:133.0053,lwr_k=600:132.2853,lwr_k=700:131.3826,lwr_k=800:130.6176,lwr_k=900:129.9056,lwr_k=1000:129.7687'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_62'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:79.0599,lwr_k=200:84.7945,lwr_k=300:86.7189,lwr_k=400:88.0383,lwr_k=500:89.1827,lwr_k=600:90.21,lwr_k=700:90.7896,lwr_k=800:91.3042,lwr_k=900:91.7262,lwr_k=1000:91.9679'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:120.5262,lwr_k=200:113.0913,lwr_k=300:110.0277,lwr_k=400:109.0669,lwr_k=500:109.3576,lwr_k=600:109.6285,lwr_k=700:110.4747,lwr_k=800:110.5055,lwr_k=900:110.8449,lwr_k=1000:110.6264'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:63.0547,lwr_k=200:68.8137,lwr_k=300:71.3555,lwr_k=400:72.5825,lwr_k=500:73.9998,lwr_k=600:74.5729,lwr_k=700:75.1487,lwr_k=800:75.7097,lwr_k=900:76.1702,lwr_k=1000:76.4714'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:149.554,lwr_k=200:143.8326,lwr_k=300:141.4032,lwr_k=400:141.2954,lwr_k=500:141.6361,lwr_k=600:141.1741,lwr_k=700:141.0514,lwr_k=800:141.3192,lwr_k=900:141.0834,lwr_k=1000:141.3153'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:69.6434,lwr_k=200:74.1695,lwr_k=300:76.252,lwr_k=400:77.1961,lwr_k=500:77.9464,lwr_k=600:79.0675,lwr_k=700:79.3429,lwr_k=800:79.6269,lwr_k=900:79.9155,lwr_k=1000:80.2368'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:94.8444,lwr_k=200:95.0217,lwr_k=300:94.7118,lwr_k=400:94.4296,lwr_k=500:93.0082,lwr_k=600:92.1548,lwr_k=700:92.6818,lwr_k=800:91.1323,lwr_k=900:91.0536,lwr_k=1000:91.4154'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:77.7282,lwr_k=200:84.7624,lwr_k=300:87.2293,lwr_k=400:88.6863,lwr_k=500:89.9938,lwr_k=600:90.5374,lwr_k=700:91.2582,lwr_k=800:91.5405,lwr_k=900:91.8226,lwr_k=1000:92.0178'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:87.6637,lwr_k=200:84.3177,lwr_k=300:83.4762,lwr_k=400:84.9093,lwr_k=500:85.957,lwr_k=600:86.3252,lwr_k=700:86.6798,lwr_k=800:87.1738,lwr_k=900:86.9711,lwr_k=1000:86.7722'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:84.8657,lwr_k=200:93.0791,lwr_k=300:96.8768,lwr_k=400:100.1357,lwr_k=500:102.7616,lwr_k=600:105.4038,lwr_k=700:106.4972,lwr_k=800:107.0736,lwr_k=900:107.8695,lwr_k=1000:108.0425'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:105.3769,lwr_k=200:109.6212,lwr_k=300:108.7858,lwr_k=400:112.6774,lwr_k=500:115.4653,lwr_k=600:118.0184,lwr_k=700:117.8587,lwr_k=800:118.9043,lwr_k=900:119.4557,lwr_k=1000:119.8768'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:78.2096,lwr_k=200:84.517,lwr_k=300:86.9908,lwr_k=400:88.7821,lwr_k=500:89.7844,lwr_k=600:90.5321,lwr_k=700:91.3219,lwr_k=800:92.0112,lwr_k=900:92.7358,lwr_k=1000:93.1812'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:96.7112,lwr_k=200:92.0554,lwr_k=300:91.3657,lwr_k=400:90.7315,lwr_k=500:91.1818,lwr_k=600:92.0657,lwr_k=700:92.1351,lwr_k=800:93.5312,lwr_k=900:93.1696,lwr_k=1000:93.1598'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_63'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:76.2655,lwr_k=200:87.7588,lwr_k=300:93.0078,lwr_k=400:96.6698,lwr_k=500:99.6597,lwr_k=600:101.9437,lwr_k=700:103.1863,lwr_k=800:105.4882,lwr_k=900:107.0025,lwr_k=1000:108.0612'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:122.943,lwr_k=200:107.9135,lwr_k=300:106.4417,lwr_k=400:107.2365,lwr_k=500:106.2043,lwr_k=600:107.0255,lwr_k=700:108.4566,lwr_k=800:109.8652,lwr_k=900:109.3601,lwr_k=1000:107.9933'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:29.0729,lwr_k=200:34.6991,lwr_k=300:36.5923,lwr_k=400:37.359,lwr_k=500:37.9569,lwr_k=600:38.3855,lwr_k=700:38.6301,lwr_k=800:38.9299,lwr_k=900:39.1137,lwr_k=1000:39.3185'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:150.5956,lwr_k=200:142.3007,lwr_k=300:140.5481,lwr_k=400:140.359,lwr_k=500:136.6,lwr_k=600:135.3906,lwr_k=700:134.7172,lwr_k=800:133.989,lwr_k=900:133.5559,lwr_k=1000:133.2934'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:103.0967,lwr_k=200:124.9647,lwr_k=300:132.9703,lwr_k=400:137.8197,lwr_k=500:146.9344,lwr_k=600:158.4019,lwr_k=700:166.5043,lwr_k=800:170.2729,lwr_k=900:172.3567,lwr_k=1000:176.9102'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:229.3455,lwr_k=200:256.0941,lwr_k=300:163.5967,lwr_k=400:167.5384,lwr_k=500:161.3582,lwr_k=600:173.1913,lwr_k=700:165.531,lwr_k=800:162.9698,lwr_k=900:167.071,lwr_k=1000:166.2336'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:79.1997,lwr_k=200:94.332,lwr_k=300:100.1132,lwr_k=400:105.378,lwr_k=500:109.0418,lwr_k=600:112.0045,lwr_k=700:114.7108,lwr_k=800:117.044,lwr_k=900:118.9469,lwr_k=1000:120.9467'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:143.1431,lwr_k=200:114.709,lwr_k=300:108.6163,lwr_k=400:108.4057,lwr_k=500:106.7977,lwr_k=600:106.8885,lwr_k=700:106.5713,lwr_k=800:105.9292,lwr_k=900:106.9609,lwr_k=1000:106.021'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:89.7052,lwr_k=200:119.3908,lwr_k=300:128.4015,lwr_k=400:136.8904,lwr_k=500:143.3506,lwr_k=600:148.9744,lwr_k=700:153.2063,lwr_k=800:155.945,lwr_k=900:159.611,lwr_k=1000:162.9158'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:212.7713,lwr_k=200:149.6966,lwr_k=300:145.7857,lwr_k=400:153.9578,lwr_k=500:153.8921,lwr_k=600:158.399,lwr_k=700:161.9574,lwr_k=800:164.4053,lwr_k=900:165.3528,lwr_k=1000:170.5414'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:56.5454,lwr_k=200:70.5284,lwr_k=300:73.8658,lwr_k=400:75.7174,lwr_k=500:76.8736,lwr_k=600:77.6338,lwr_k=700:78.1279,lwr_k=800:78.6679,lwr_k=900:79.0677,lwr_k=1000:79.4552'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:230.3873,lwr_k=200:102.4697,lwr_k=300:103.0356,lwr_k=400:143.3298,lwr_k=500:100.5187,lwr_k=600:100.7937,lwr_k=700:134.9284,lwr_k=800:120.1114,lwr_k=900:124.0381,lwr_k=1000:125.7753'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_64'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:70.0149,lwr_k=200:84.1636,lwr_k=300:89.5807,lwr_k=400:92.4353,lwr_k=500:97.2152,lwr_k=600:99.2052,lwr_k=700:100.1748,lwr_k=800:103.3398,lwr_k=900:104.8564,lwr_k=1000:105.641'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:212.7669,lwr_k=200:159.1806,lwr_k=300:149.0212,lwr_k=400:142.5363,lwr_k=500:137.1636,lwr_k=600:132.698,lwr_k=700:128.3873,lwr_k=800:124.8987,lwr_k=900:122.0295,lwr_k=1000:120.9736'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:61.7936,lwr_k=200:73.2669,lwr_k=300:77.1147,lwr_k=400:78.9551,lwr_k=500:79.6889,lwr_k=600:80.745,lwr_k=700:81.95,lwr_k=800:82.9725,lwr_k=900:83.5002,lwr_k=1000:83.9549'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:209.2344,lwr_k=200:185.7208,lwr_k=300:180.5034,lwr_k=400:178.8343,lwr_k=500:170.7716,lwr_k=600:168.7859,lwr_k=700:171.1903,lwr_k=800:168.102,lwr_k=900:169.3242,lwr_k=1000:168.7909'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:92.3383,lwr_k=200:92.7364,lwr_k=300:94.5782,lwr_k=400:94.8309,lwr_k=500:94.2743,lwr_k=600:95.137,lwr_k=700:96.0257,lwr_k=800:94.9633,lwr_k=900:95.4198,lwr_k=1000:94.8147'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:133.3494,lwr_k=200:121.4984,lwr_k=300:138.4743,lwr_k=400:121.9534,lwr_k=500:141.2376,lwr_k=600:140.2891,lwr_k=700:144.3588,lwr_k=800:127.6554,lwr_k=900:129.9591,lwr_k=1000:132.2997'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:58.3301,lwr_k=200:78.495,lwr_k=300:88.085,lwr_k=400:97.2461,lwr_k=500:102.5116,lwr_k=600:107.0419,lwr_k=700:109.3764,lwr_k=800:111.1584,lwr_k=900:112.4037,lwr_k=1000:113.1824'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:275.4597,lwr_k=200:155.8228,lwr_k=300:142.2448,lwr_k=400:125.2796,lwr_k=500:113.9468,lwr_k=600:106.1607,lwr_k=700:108.1367,lwr_k=800:106.0226,lwr_k=900:106.9293,lwr_k=1000:107.106'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:78.7985,lwr_k=200:87.179,lwr_k=300:94.9085,lwr_k=400:99.5488,lwr_k=500:103.128,lwr_k=600:104.6362,lwr_k=700:105.8595,lwr_k=800:107.1935,lwr_k=900:108.2281,lwr_k=1000:109.7737'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:205.3149,lwr_k=200:177.2501,lwr_k=300:128.856,lwr_k=400:119.4444,lwr_k=500:118.1197,lwr_k=600:119.243,lwr_k=700:117.7308,lwr_k=800:113.3745,lwr_k=900:112.5427,lwr_k=1000:110.1436'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:117.4428,lwr_k=200:119.1887,lwr_k=300:116.8825,lwr_k=400:118.1958,lwr_k=500:117.9378,lwr_k=600:118.2195,lwr_k=700:117.9868,lwr_k=800:118.2188,lwr_k=900:122.1351,lwr_k=1000:120.3387'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:166.6218,lwr_k=200:163.487,lwr_k=300:175.3152,lwr_k=400:161.1787,lwr_k=500:178.1899,lwr_k=600:166.457,lwr_k=700:189.3811,lwr_k=800:175.5471,lwr_k=900:162.2545,lwr_k=1000:223.8816'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_65'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:84.8701,lwr_k=200:87.8906,lwr_k=300:88.7359,lwr_k=400:88.9516,lwr_k=500:89.1151,lwr_k=600:89.5505,lwr_k=700:89.6847,lwr_k=800:89.9888,lwr_k=900:89.9619,lwr_k=1000:90.0284'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:107.7456,lwr_k=200:107.8669,lwr_k=300:109.4117,lwr_k=400:108.7634,lwr_k=500:108.0698,lwr_k=600:107.9502,lwr_k=700:107.8363,lwr_k=800:107.6797,lwr_k=900:107.357,lwr_k=1000:107.325'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:68.8122,lwr_k=200:71.6879,lwr_k=300:72.4287,lwr_k=400:72.8433,lwr_k=500:73.1706,lwr_k=600:73.3496,lwr_k=700:73.5304,lwr_k=800:73.8093,lwr_k=900:73.9122,lwr_k=1000:74.0001'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:148.8249,lwr_k=200:142.5512,lwr_k=300:142.9337,lwr_k=400:143.2861,lwr_k=500:143.4032,lwr_k=600:143.3868,lwr_k=700:143.3735,lwr_k=800:143.6109,lwr_k=900:144.4797,lwr_k=1000:144.6131'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:69.1197,lwr_k=200:72.1068,lwr_k=300:73.1576,lwr_k=400:73.8646,lwr_k=500:73.9726,lwr_k=600:74.1926,lwr_k=700:74.433,lwr_k=800:74.5114,lwr_k=900:74.54,lwr_k=1000:74.6791'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:103.6553,lwr_k=200:103.7849,lwr_k=300:103.2066,lwr_k=400:104.2943,lwr_k=500:104.152,lwr_k=600:104.178,lwr_k=700:104.0794,lwr_k=800:104.3344,lwr_k=900:104.0978,lwr_k=1000:104.0215'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:82.4982,lwr_k=200:84.472,lwr_k=300:85.1836,lwr_k=400:85.3209,lwr_k=500:85.6155,lwr_k=600:85.8807,lwr_k=700:86.0408,lwr_k=800:86.3148,lwr_k=900:86.5051,lwr_k=1000:86.66'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:94.6571,lwr_k=200:90.566,lwr_k=300:89.4068,lwr_k=400:88.7656,lwr_k=500:88.4581,lwr_k=600:88.4043,lwr_k=700:88.5337,lwr_k=800:88.3738,lwr_k=900:88.5758,lwr_k=1000:88.5341'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:85.0562,lwr_k=200:87.7135,lwr_k=300:88.832,lwr_k=400:89.4776,lwr_k=500:89.509,lwr_k=600:90.0355,lwr_k=700:90.371,lwr_k=800:90.7895,lwr_k=900:90.789,lwr_k=1000:91.0473'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:143.2212,lwr_k=200:133.9865,lwr_k=300:132.56,lwr_k=400:132.2733,lwr_k=500:131.4255,lwr_k=600:130.4224,lwr_k=700:129.8232,lwr_k=800:129.6219,lwr_k=900:129.3983,lwr_k=1000:128.6887'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:80.8274,lwr_k=200:83.4028,lwr_k=300:84.1618,lwr_k=400:84.5423,lwr_k=500:84.7094,lwr_k=600:85.0804,lwr_k=700:85.3223,lwr_k=800:85.4363,lwr_k=900:85.295,lwr_k=1000:85.436'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:105.298,lwr_k=200:104.6617,lwr_k=300:105.5501,lwr_k=400:103.9642,lwr_k=500:103.2702,lwr_k=600:103.0694,lwr_k=700:102.9341,lwr_k=800:102.9853,lwr_k=900:102.6086,lwr_k=1000:102.5523'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_66'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:70.2334,lwr_k=200:92.1897,lwr_k=300:101.3299,lwr_k=400:105.3223,lwr_k=500:107.1004,lwr_k=600:108.7736,lwr_k=700:110.3526,lwr_k=800:111.136,lwr_k=900:112.0162,lwr_k=1000:112.8729'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:186.6041,lwr_k=200:134.4595,lwr_k=300:120.9459,lwr_k=400:112.9257,lwr_k=500:109.412,lwr_k=600:109.259,lwr_k=700:107.84,lwr_k=800:107.4705,lwr_k=900:107.0816,lwr_k=1000:106.5752'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:50.2367,lwr_k=200:70.6943,lwr_k=300:78.6162,lwr_k=400:83.3024,lwr_k=500:86.1804,lwr_k=600:88.1359,lwr_k=700:89.6259,lwr_k=800:90.6794,lwr_k=900:91.4806,lwr_k=1000:91.8853'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:214.1699,lwr_k=200:171.7363,lwr_k=300:163.342,lwr_k=400:158.2605,lwr_k=500:159.3241,lwr_k=600:158.0853,lwr_k=700:158.7546,lwr_k=800:157.294,lwr_k=900:157.0891,lwr_k=1000:156.6546'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:62.8549,lwr_k=200:75.3067,lwr_k=300:81.8363,lwr_k=400:84.7967,lwr_k=500:86.6595,lwr_k=600:88.1815,lwr_k=700:88.9378,lwr_k=800:89.4674,lwr_k=900:90.7421,lwr_k=1000:91.5088'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:153.2628,lwr_k=200:120.5276,lwr_k=300:113.5877,lwr_k=400:111.5242,lwr_k=500:112.8514,lwr_k=600:111.67,lwr_k=700:111.5459,lwr_k=800:111.7438,lwr_k=900:110.9552,lwr_k=1000:111.532'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:118.3144,lwr_k=200:103.9613,lwr_k=300:108.3373,lwr_k=400:109.7502,lwr_k=500:112.7699,lwr_k=600:114.6783,lwr_k=700:116.3012,lwr_k=800:117.6073,lwr_k=900:118.3134,lwr_k=1000:119.1558'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:232.144,lwr_k=200:145.495,lwr_k=300:129.6521,lwr_k=400:130.9538,lwr_k=500:129.1539,lwr_k=600:126.7688,lwr_k=700:127.4802,lwr_k=800:126.5799,lwr_k=900:126.8019,lwr_k=1000:126.215'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:157.8967,lwr_k=200:133.4953,lwr_k=300:135.7454,lwr_k=400:135.683,lwr_k=500:132.6784,lwr_k=600:131.524,lwr_k=700:132.6917,lwr_k=800:129.1811,lwr_k=900:127.966,lwr_k=1000:126.5801'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:346.3006,lwr_k=200:149.7363,lwr_k=300:140.098,lwr_k=400:133.4527,lwr_k=500:129.791,lwr_k=600:129.5537,lwr_k=700:129.0258,lwr_k=800:125.784,lwr_k=900:125.0029,lwr_k=1000:121.4563'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:114.7754,lwr_k=200:109.2341,lwr_k=300:110.5741,lwr_k=400:112.1426,lwr_k=500:114.2665,lwr_k=600:115.4622,lwr_k=700:115.4189,lwr_k=800:113.8735,lwr_k=900:113.4239,lwr_k=1000:113.8841'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:304.1531,lwr_k=200:173.1899,lwr_k=300:142.0544,lwr_k=400:127.8077,lwr_k=500:130.1111,lwr_k=600:124.7139,lwr_k=700:124.2732,lwr_k=800:119.2931,lwr_k=900:118.4778,lwr_k=1000:119.3863'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_67'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:102.3721,lwr_k=200:107.0679,lwr_k=300:108.4406,lwr_k=400:109.1835,lwr_k=500:110.2889,lwr_k=600:110.4637,lwr_k=700:110.779,lwr_k=800:111.0184,lwr_k=900:111.2798,lwr_k=1000:111.5952'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:106.0222,lwr_k=200:103.1844,lwr_k=300:102.5701,lwr_k=400:102.1598,lwr_k=500:101.5632,lwr_k=600:101.1094,lwr_k=700:100.927,lwr_k=800:100.8431,lwr_k=900:100.542,lwr_k=1000:100.6781'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:89.5054,lwr_k=200:93.5569,lwr_k=300:94.8823,lwr_k=400:96.4881,lwr_k=500:97.3298,lwr_k=600:97.429,lwr_k=700:97.8771,lwr_k=800:98.0176,lwr_k=900:98.4019,lwr_k=1000:98.3443'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:159.0095,lwr_k=200:149.7943,lwr_k=300:151.8278,lwr_k=400:152.1358,lwr_k=500:153.0123,lwr_k=600:152.454,lwr_k=700:151.9366,lwr_k=800:151.3748,lwr_k=900:151.1926,lwr_k=1000:151.0233'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:85.6685,lwr_k=200:90.1095,lwr_k=300:91.1442,lwr_k=400:92.3471,lwr_k=500:92.9853,lwr_k=600:93.5636,lwr_k=700:93.8948,lwr_k=800:93.9526,lwr_k=900:93.8573,lwr_k=1000:93.9089'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:114.984,lwr_k=200:111.9634,lwr_k=300:111.3228,lwr_k=400:111.2913,lwr_k=500:110.7419,lwr_k=600:110.9957,lwr_k=700:111.0321,lwr_k=800:110.9863,lwr_k=900:111.0463,lwr_k=1000:110.9146'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:101.991,lwr_k=200:105.9834,lwr_k=300:108.6659,lwr_k=400:109.503,lwr_k=500:110.2694,lwr_k=600:110.6161,lwr_k=700:111.2799,lwr_k=800:111.4475,lwr_k=900:111.7524,lwr_k=1000:112.066'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:117.8754,lwr_k=200:116.3586,lwr_k=300:116.379,lwr_k=400:116.4865,lwr_k=500:116.2739,lwr_k=600:116.3302,lwr_k=700:116.4497,lwr_k=800:116.7691,lwr_k=900:117.141,lwr_k=1000:117.1515'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:106.7325,lwr_k=200:111.5018,lwr_k=300:114.1853,lwr_k=400:114.7663,lwr_k=500:115.6162,lwr_k=600:116.25,lwr_k=700:116.5612,lwr_k=800:116.3953,lwr_k=900:116.9129,lwr_k=1000:117.4386'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:126.2746,lwr_k=200:123.6243,lwr_k=300:113.2003,lwr_k=400:111.5268,lwr_k=500:108.8794,lwr_k=600:109.0192,lwr_k=700:108.7291,lwr_k=800:108.7563,lwr_k=900:108.8587,lwr_k=1000:108.8098'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:104.1902,lwr_k=200:107.7762,lwr_k=300:109.461,lwr_k=400:110.9434,lwr_k=500:111.468,lwr_k=600:112.135,lwr_k=700:112.6139,lwr_k=800:112.6928,lwr_k=900:112.8815,lwr_k=1000:112.9148'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:127.9134,lwr_k=200:125.5455,lwr_k=300:126.8796,lwr_k=400:126.3445,lwr_k=500:125.9267,lwr_k=600:125.4946,lwr_k=700:125.8228,lwr_k=800:125.6594,lwr_k=900:125.2713,lwr_k=1000:124.9195'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_68'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:305.5973,lwr_k=200:301.448,lwr_k=300:300.912,lwr_k=400:13942.1971,lwr_k=500:539.0124,lwr_k=600:539.0114,lwr_k=700:539.0176,lwr_k=800:300.8555,lwr_k=900:300.9103,lwr_k=1000:47846.6325'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:309.3398,lwr_k=200:304.4534,lwr_k=300:303.4787,lwr_k=400:14350.0357,lwr_k=500:555.3637,lwr_k=600:555.3653,lwr_k=700:555.3596,lwr_k=800:303.5106,lwr_k=900:303.6253,lwr_k=1000:50840.8644'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:283.9452,lwr_k=200:284.3309,lwr_k=300:284.1179,lwr_k=400:284.307,lwr_k=500:283.9035,lwr_k=600:284.0577,lwr_k=700:283.8833,lwr_k=800:283.8723,lwr_k=900:283.9337,lwr_k=1000:284.0507'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:382.8287,lwr_k=200:383.3641,lwr_k=300:383.2773,lwr_k=400:383.4859,lwr_k=500:382.9695,lwr_k=600:382.8814,lwr_k=700:382.9121,lwr_k=800:382.9063,lwr_k=900:383.0854,lwr_k=1000:383.2956'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:303.2817,lwr_k=200:302.243,lwr_k=300:373.2207,lwr_k=400:1278.4763,lwr_k=500:374.1146,lwr_k=600:302.5614,lwr_k=700:373.4162,lwr_k=800:373.2128,lwr_k=900:373.2144,lwr_k=1000:373.2403'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:254.6872,lwr_k=200:254.4557,lwr_k=300:331.0675,lwr_k=400:1226.6282,lwr_k=500:332.842,lwr_k=600:255.8591,lwr_k=700:331.6328,lwr_k=800:330.9991,lwr_k=900:330.9944,lwr_k=1000:330.8173'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:388.5402,lwr_k=200:383.0828,lwr_k=300:383.9959,lwr_k=400:383.0632,lwr_k=500:382.9135,lwr_k=600:382.9949,lwr_k=700:383.6642,lwr_k=800:382.999,lwr_k=900:382.905,lwr_k=1000:382.907'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:307.8712,lwr_k=200:301.6193,lwr_k=300:302.7847,lwr_k=400:301.5901,lwr_k=500:301.3258,lwr_k=600:301.483,lwr_k=700:302.3826,lwr_k=800:301.4897,lwr_k=900:301.3008,lwr_k=1000:301.3072'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:352.26,lwr_k=200:352.0487,lwr_k=300:351.9734,lwr_k=400:352.3741,lwr_k=500:352.2368,lwr_k=600:352.4188,lwr_k=700:352.2593,lwr_k=800:352.3445,lwr_k=900:352.0386,lwr_k=1000:352.1666'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:379.7377,lwr_k=200:379.7178,lwr_k=300:379.7723,lwr_k=400:379.7775,lwr_k=500:379.7312,lwr_k=600:379.7959,lwr_k=700:379.7375,lwr_k=800:379.7661,lwr_k=900:379.7208,lwr_k=1000:379.7162'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:359.9774,lwr_k=200:361.1123,lwr_k=300:360.7826,lwr_k=400:359.4066,lwr_k=500:359.0701,lwr_k=600:359.1555,lwr_k=700:359.0456,lwr_k=800:358.968,lwr_k=900:358.9739,lwr_k=1000:358.9884'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:329.3252,lwr_k=200:330.1032,lwr_k=300:329.8646,lwr_k=400:329.0191,lwr_k=500:328.9427,lwr_k=600:328.9432,lwr_k=700:329.4148,lwr_k=800:329.1892,lwr_k=900:329.0057,lwr_k=1000:328.9841'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_69'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:83.4752,lwr_k=200:77.995,lwr_k=300:78.2433,lwr_k=400:77.1408,lwr_k=500:79.0443,lwr_k=600:78.1821,lwr_k=700:78.2001,lwr_k=800:77.1434,lwr_k=900:77.2741,lwr_k=1000:83.8686'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:222.2229,lwr_k=200:135.6355,lwr_k=300:117.314,lwr_k=400:113.0663,lwr_k=500:112.2025,lwr_k=600:110.2863,lwr_k=700:109.9515,lwr_k=800:110.0855,lwr_k=900:109.7952,lwr_k=1000:113.9468'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:98.4471,lwr_k=200:92.2246,lwr_k=300:84.8362,lwr_k=400:81.8128,lwr_k=500:78.9026,lwr_k=600:75.7206,lwr_k=700:76.7606,lwr_k=800:80.9138,lwr_k=900:77.058,lwr_k=1000:76.8401'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:210.6301,lwr_k=200:194.286,lwr_k=300:181.4774,lwr_k=400:167.6804,lwr_k=500:159.8321,lwr_k=600:155.4317,lwr_k=700:155.0574,lwr_k=800:150.8732,lwr_k=900:152.8869,lwr_k=1000:150.1323'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:91.552,lwr_k=200:93.3509,lwr_k=300:95.3895,lwr_k=400:95.3653,lwr_k=500:94.8428,lwr_k=600:87.0807,lwr_k=700:88.3917,lwr_k=800:84.4659,lwr_k=900:81.2962,lwr_k=1000:79.7386'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:209.5067,lwr_k=200:164.2861,lwr_k=300:166.9178,lwr_k=400:150.4299,lwr_k=500:147.6501,lwr_k=600:145.9075,lwr_k=700:136.8435,lwr_k=800:139.5341,lwr_k=900:130.5302,lwr_k=1000:130.7605'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:91.1591,lwr_k=200:78.6046,lwr_k=300:83.2289,lwr_k=400:91.5241,lwr_k=500:84.275,lwr_k=600:83.0638,lwr_k=700:103.7593,lwr_k=800:83.1421,lwr_k=900:92.3757,lwr_k=1000:92.3859'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:193.7667,lwr_k=200:140.8991,lwr_k=300:123.836,lwr_k=400:132.7457,lwr_k=500:127.3147,lwr_k=600:123.5032,lwr_k=700:131.9826,lwr_k=800:109.1815,lwr_k=900:130.7243,lwr_k=1000:122.5804'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:98.3288,lwr_k=200:104.2438,lwr_k=300:103.2596,lwr_k=400:107.8308,lwr_k=500:103.0329,lwr_k=600:105.5988,lwr_k=700:123.6694,lwr_k=800:125.9319,lwr_k=900:137.7279,lwr_k=1000:137.017'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:153.6045,lwr_k=200:152.0484,lwr_k=300:139.0912,lwr_k=400:154.8512,lwr_k=500:133.6023,lwr_k=600:121.1337,lwr_k=700:152.1787,lwr_k=800:140.9744,lwr_k=900:157.7948,lwr_k=1000:150.8099'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:105.2171,lwr_k=200:91.9766,lwr_k=300:91.487,lwr_k=400:91.9172,lwr_k=500:95.0914,lwr_k=600:95.6173,lwr_k=700:95.296,lwr_k=800:98.3387,lwr_k=900:106.6935,lwr_k=1000:101.5477'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:156.7074,lwr_k=200:119.0886,lwr_k=300:108.3703,lwr_k=400:91.911,lwr_k=500:117.3339,lwr_k=600:106.5523,lwr_k=700:88.5978,lwr_k=800:104.4957,lwr_k=900:112.6168,lwr_k=1000:100.9399'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_70'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:124.1693,lwr_k=200:129.2767,lwr_k=300:130.9424,lwr_k=400:132.5102,lwr_k=500:133.3433,lwr_k=600:146.8321,lwr_k=700:134.261,lwr_k=800:147.2405,lwr_k=900:134.6846,lwr_k=1000:134.4866'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:119.6705,lwr_k=200:120.3694,lwr_k=300:120.5154,lwr_k=400:120.4584,lwr_k=500:120.8187,lwr_k=600:121.1206,lwr_k=700:120.7595,lwr_k=800:121.5455,lwr_k=900:121.575,lwr_k=1000:121.6862'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:79.6892,lwr_k=200:81.245,lwr_k=300:81.8651,lwr_k=400:81.9187,lwr_k=500:82.009,lwr_k=600:81.9945,lwr_k=700:82.0274,lwr_k=800:82.0682,lwr_k=900:82.0965,lwr_k=1000:82.2944'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:149.4941,lwr_k=200:148.422,lwr_k=300:149.1643,lwr_k=400:149.0455,lwr_k=500:148.9446,lwr_k=600:149.0625,lwr_k=700:149.122,lwr_k=800:149.1335,lwr_k=900:149.1005,lwr_k=1000:149.1444'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:100.9535,lwr_k=200:101.7881,lwr_k=300:101.7347,lwr_k=400:101.9753,lwr_k=500:101.8594,lwr_k=600:102.0059,lwr_k=700:102.0144,lwr_k=800:102.1067,lwr_k=900:102.2147,lwr_k=1000:102.2039'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:153.455,lwr_k=200:152.69,lwr_k=300:152.6187,lwr_k=400:144.5505,lwr_k=500:145.2823,lwr_k=600:153.673,lwr_k=700:194.8301,lwr_k=800:194.4834,lwr_k=900:194.1143,lwr_k=1000:193.2727'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:103.0808,lwr_k=200:101.2304,lwr_k=300:101.0997,lwr_k=400:101.1284,lwr_k=500:101.1745,lwr_k=600:101.2088,lwr_k=700:101.4002,lwr_k=800:101.2762,lwr_k=900:101.6823,lwr_k=1000:101.9017'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:121.9984,lwr_k=200:119.4156,lwr_k=300:119.0268,lwr_k=400:119.2513,lwr_k=500:119.4168,lwr_k=600:119.5385,lwr_k=700:119.5943,lwr_k=800:119.9222,lwr_k=900:120.1026,lwr_k=1000:120.1877'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:111.6621,lwr_k=200:113.8869,lwr_k=300:113.7409,lwr_k=400:113.6017,lwr_k=500:113.61,lwr_k=600:113.6939,lwr_k=700:113.7096,lwr_k=800:113.6867,lwr_k=900:113.7254,lwr_k=1000:113.7192'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:101.1378,lwr_k=200:107.5057,lwr_k=300:107.3697,lwr_k=400:107.247,lwr_k=500:107.1884,lwr_k=600:107.3548,lwr_k=700:107.4013,lwr_k=800:107.4162,lwr_k=900:107.449,lwr_k=1000:107.3626'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:110.3048,lwr_k=200:111.8161,lwr_k=300:111.5474,lwr_k=400:111.5136,lwr_k=500:111.591,lwr_k=600:111.5754,lwr_k=700:111.5945,lwr_k=800:111.5929,lwr_k=900:111.5911,lwr_k=1000:111.5784'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:99.9166,lwr_k=200:103.279,lwr_k=300:102.6911,lwr_k=400:102.4624,lwr_k=500:102.5488,lwr_k=600:102.4676,lwr_k=700:102.5209,lwr_k=800:102.4685,lwr_k=900:102.4215,lwr_k=1000:102.3782'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_71'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:147.4196,lwr_k=200:152.6206,lwr_k=300:154.9328,lwr_k=400:155.8208,lwr_k=500:155.3608,lwr_k=600:155.2156,lwr_k=700:155.1601,lwr_k=800:155.0175,lwr_k=900:154.9418,lwr_k=1000:154.1474'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:138.5966,lwr_k=200:139.344,lwr_k=300:137.4065,lwr_k=400:136.967,lwr_k=500:137.6064,lwr_k=600:137.5138,lwr_k=700:138.4579,lwr_k=800:139.4423,lwr_k=900:140.0905,lwr_k=1000:140.8943'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:140.5904,lwr_k=200:143.5863,lwr_k=300:144.5917,lwr_k=400:145.0669,lwr_k=500:146.0137,lwr_k=600:146.0939,lwr_k=700:146.4898,lwr_k=800:145.6194,lwr_k=900:145.7413,lwr_k=1000:145.7274'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:192.9218,lwr_k=200:194.3291,lwr_k=300:195.6676,lwr_k=400:198.4594,lwr_k=500:199.0858,lwr_k=600:199.9932,lwr_k=700:201.7814,lwr_k=800:202.9464,lwr_k=900:204.666,lwr_k=1000:206.7359'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:119.8839,lwr_k=200:137.8999,lwr_k=300:138.381,lwr_k=400:138.7584,lwr_k=500:139.1698,lwr_k=600:139.4152,lwr_k=700:139.6005,lwr_k=800:139.7559,lwr_k=900:139.7433,lwr_k=1000:139.8423'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:115.4324,lwr_k=200:124.375,lwr_k=300:125.5481,lwr_k=400:125.4593,lwr_k=500:125.8369,lwr_k=600:125.3786,lwr_k=700:125.5304,lwr_k=800:124.6608,lwr_k=900:124.2394,lwr_k=1000:123.7314'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:163.5571,lwr_k=200:171.9082,lwr_k=300:173.1138,lwr_k=400:174.2044,lwr_k=500:174.8199,lwr_k=600:175.2754,lwr_k=700:175.5032,lwr_k=800:176.1684,lwr_k=900:176.7657,lwr_k=1000:177.2579'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:118.4514,lwr_k=200:116.0863,lwr_k=300:116.5757,lwr_k=400:117.2543,lwr_k=500:117.5904,lwr_k=600:118.207,lwr_k=700:118.1818,lwr_k=800:118.4416,lwr_k=900:118.4878,lwr_k=1000:118.6069'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:145.1226,lwr_k=200:149.9273,lwr_k=300:150.3672,lwr_k=400:150.955,lwr_k=500:151.0017,lwr_k=600:151.0927,lwr_k=700:151.5263,lwr_k=800:151.5558,lwr_k=900:151.8787,lwr_k=1000:152.0308'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:160.4524,lwr_k=200:165.0416,lwr_k=300:166.4525,lwr_k=400:166.8168,lwr_k=500:167.9353,lwr_k=600:168.3044,lwr_k=700:168.5092,lwr_k=800:168.2799,lwr_k=900:168.1267,lwr_k=1000:168.2356'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:104.3337,lwr_k=200:106.7266,lwr_k=300:109.6762,lwr_k=400:111.8758,lwr_k=500:113.9836,lwr_k=600:124.2328,lwr_k=700:176.6172,lwr_k=800:191.6739,lwr_k=900:196.1578,lwr_k=1000:198.5797'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:107.5698,lwr_k=200:108.6665,lwr_k=300:110.9392,lwr_k=400:112.7375,lwr_k=500:114.8114,lwr_k=600:126.0958,lwr_k=700:174.6067,lwr_k=800:187.9452,lwr_k=900:191.4953,lwr_k=1000:193.2612'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_72'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:57.4904,lwr_k=200:86.4941,lwr_k=300:98.4419,lwr_k=400:107.1999,lwr_k=500:113.9373,lwr_k=600:118.7194,lwr_k=700:122.0841,lwr_k=800:125.1328,lwr_k=900:127.7069,lwr_k=1000:130.0114'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:205.5097,lwr_k=200:139.2933,lwr_k=300:145.5152,lwr_k=400:148.3096,lwr_k=500:153.463,lwr_k=600:154.7432,lwr_k=700:156.8126,lwr_k=800:157.8376,lwr_k=900:160.8317,lwr_k=1000:163.1228'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:31.7704,lwr_k=200:47.6279,lwr_k=300:53.5474,lwr_k=400:58.4203,lwr_k=500:60.868,lwr_k=600:62.6496,lwr_k=700:64.0113,lwr_k=800:64.9564,lwr_k=900:65.9241,lwr_k=1000:66.4481'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:199.8318,lwr_k=200:169.2895,lwr_k=300:158.9644,lwr_k=400:156.3579,lwr_k=500:154.7175,lwr_k=600:155.6387,lwr_k=700:154.6774,lwr_k=800:154.5896,lwr_k=900:154.3364,lwr_k=1000:153.6591'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:42.8871,lwr_k=200:52.3492,lwr_k=300:59.0677,lwr_k=400:62.4497,lwr_k=500:64.158,lwr_k=600:65.3636,lwr_k=700:66.3769,lwr_k=800:67.3564,lwr_k=900:67.8855,lwr_k=1000:68.185'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:241.0312,lwr_k=200:114.5963,lwr_k=300:108.6825,lwr_k=400:98.6973,lwr_k=500:95.9489,lwr_k=600:95.8194,lwr_k=700:95.5203,lwr_k=800:96.6643,lwr_k=900:97.0328,lwr_k=1000:97.2652'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:73.114,lwr_k=200:92.0442,lwr_k=300:100.0973,lwr_k=400:106.2861,lwr_k=500:109.9252,lwr_k=600:113.4535,lwr_k=700:117.0194,lwr_k=800:118.6511,lwr_k=900:120.5867,lwr_k=1000:122.194'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:220.6209,lwr_k=200:137.8079,lwr_k=300:127.198,lwr_k=400:130.3356,lwr_k=500:129.6474,lwr_k=600:121.7767,lwr_k=700:120.5213,lwr_k=800:118.7148,lwr_k=900:120.5585,lwr_k=1000:121.1484'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:41.2139,lwr_k=200:69.6249,lwr_k=300:78.7551,lwr_k=400:84.6595,lwr_k=500:88.4126,lwr_k=600:90.5153,lwr_k=700:92.8227,lwr_k=800:95.035,lwr_k=900:96.4848,lwr_k=1000:97.8994'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:182.4077,lwr_k=200:139.5569,lwr_k=300:131.9013,lwr_k=400:125.4279,lwr_k=500:124.3284,lwr_k=600:124.6451,lwr_k=700:124.2104,lwr_k=800:120.4401,lwr_k=900:119.3254,lwr_k=1000:118.0385'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:43.9495,lwr_k=200:68.7969,lwr_k=300:77.8849,lwr_k=400:82.8807,lwr_k=500:86.5724,lwr_k=600:88.5661,lwr_k=700:90.7219,lwr_k=800:92.319,lwr_k=900:93.7856,lwr_k=1000:94.9821'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:151.9349,lwr_k=200:106.4556,lwr_k=300:96.9121,lwr_k=400:96.5012,lwr_k=500:95.2472,lwr_k=600:96.1289,lwr_k=700:95.8826,lwr_k=800:96.0822,lwr_k=900:96.8434,lwr_k=1000:97.3288'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_73'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:111.553,lwr_k=200:112.0522,lwr_k=300:111.7441,lwr_k=400:111.7579,lwr_k=500:111.8006,lwr_k=600:111.8913,lwr_k=700:112.1993,lwr_k=800:112.887,lwr_k=900:113.0052,lwr_k=1000:113.2185'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:172.4443,lwr_k=200:172.9322,lwr_k=300:178.2111,lwr_k=400:178.305,lwr_k=500:178.2849,lwr_k=600:176.3955,lwr_k=700:179.0214,lwr_k=800:178.43,lwr_k=900:182.6884,lwr_k=1000:183.6533'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:110.8893,lwr_k=200:110.8744,lwr_k=300:111.2131,lwr_k=400:112.9882,lwr_k=500:114.6912,lwr_k=600:116.434,lwr_k=700:116.3213,lwr_k=800:116.087,lwr_k=900:116.7152,lwr_k=1000:116.3686'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:192.4809,lwr_k=200:193.3182,lwr_k=300:193.5738,lwr_k=400:193.6771,lwr_k=500:195.0684,lwr_k=600:194.679,lwr_k=700:199.4761,lwr_k=800:199.4361,lwr_k=900:202.9948,lwr_k=1000:203.0735'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:105.3116,lwr_k=200:106.0818,lwr_k=300:106.6427,lwr_k=400:106.9691,lwr_k=500:107.3203,lwr_k=600:107.6016,lwr_k=700:107.7505,lwr_k=800:107.9665,lwr_k=900:108.0591,lwr_k=1000:108.1202'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:116.5556,lwr_k=200:115.418,lwr_k=300:115.4185,lwr_k=400:114.6463,lwr_k=500:114.4719,lwr_k=600:114.3876,lwr_k=700:114.4408,lwr_k=800:114.4068,lwr_k=900:114.3359,lwr_k=1000:114.3194'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:219.9321,lwr_k=200:220.1084,lwr_k=300:220.7034,lwr_k=400:220.6971,lwr_k=500:220.8359,lwr_k=600:221.673,lwr_k=700:222.6443,lwr_k=800:223.5894,lwr_k=900:224.3569,lwr_k=1000:225.1336'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:156.2515,lwr_k=200:156.8616,lwr_k=300:158.3322,lwr_k=400:157.8426,lwr_k=500:157.9099,lwr_k=600:159.0164,lwr_k=700:160.3963,lwr_k=800:161.3846,lwr_k=900:162.2271,lwr_k=1000:163.1576'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:198.6179,lwr_k=200:194.1062,lwr_k=300:193.9083,lwr_k=400:194.0752,lwr_k=500:196.1223,lwr_k=600:197.2239,lwr_k=700:198.1608,lwr_k=800:198.8041,lwr_k=900:199.2592,lwr_k=1000:199.6176'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:219.1451,lwr_k=200:214.0055,lwr_k=300:213.6454,lwr_k=400:213.7021,lwr_k=500:216.0193,lwr_k=600:217.3882,lwr_k=700:218.5822,lwr_k=800:219.5237,lwr_k=900:220.1819,lwr_k=1000:220.335'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:201.7105,lwr_k=200:201.0852,lwr_k=300:200.7915,lwr_k=400:200.4636,lwr_k=500:201.9678,lwr_k=600:201.8295,lwr_k=700:202.3788,lwr_k=800:202.8597,lwr_k=900:203.1186,lwr_k=1000:203.1544'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:200.463,lwr_k=200:198.9956,lwr_k=300:196.0527,lwr_k=400:193.7335,lwr_k=500:193.9388,lwr_k=600:194.1826,lwr_k=700:193.6778,lwr_k=800:193.457,lwr_k=900:193.5818,lwr_k=1000:193.3916'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_74'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:85.6304,lwr_k=200:97.7715,lwr_k=300:101.8208,lwr_k=400:105.0856,lwr_k=500:106.7513,lwr_k=600:109.3545,lwr_k=700:111.3757,lwr_k=800:112.7033,lwr_k=900:113.7612,lwr_k=1000:114.7065'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:119.7794,lwr_k=200:116.734,lwr_k=300:109.9649,lwr_k=400:113.5432,lwr_k=500:114.1137,lwr_k=600:115.3631,lwr_k=700:117.5205,lwr_k=800:118.6749,lwr_k=900:120.1187,lwr_k=1000:121.7044'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:72.167,lwr_k=200:84.2127,lwr_k=300:90.2865,lwr_k=400:92.6272,lwr_k=500:96.2411,lwr_k=600:97.9394,lwr_k=700:98.9714,lwr_k=800:100.4448,lwr_k=900:101.811,lwr_k=1000:102.8142'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:178.0464,lwr_k=200:170.5981,lwr_k=300:172.3319,lwr_k=400:164.3581,lwr_k=500:166.3572,lwr_k=600:166.1075,lwr_k=700:179.5148,lwr_k=800:175.9218,lwr_k=900:176.357,lwr_k=1000:175.4497'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:76.9054,lwr_k=200:86.8133,lwr_k=300:92.2421,lwr_k=400:95.7481,lwr_k=500:97.8569,lwr_k=600:99.3449,lwr_k=700:100.1765,lwr_k=800:101.1814,lwr_k=900:102.0524,lwr_k=1000:102.3581'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:142.9022,lwr_k=200:112.3751,lwr_k=300:104.4837,lwr_k=400:102.7722,lwr_k=500:103.4776,lwr_k=600:103.3574,lwr_k=700:103.384,lwr_k=800:103.8834,lwr_k=900:104.8187,lwr_k=1000:104.8059'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:89.5015,lwr_k=200:103.9583,lwr_k=300:111.5685,lwr_k=400:115.8318,lwr_k=500:119.7371,lwr_k=600:122.2596,lwr_k=700:125.9769,lwr_k=800:128.7621,lwr_k=900:130.9057,lwr_k=1000:133.9274'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:141.104,lwr_k=200:138.9204,lwr_k=300:129.9492,lwr_k=400:125.6034,lwr_k=500:113.6862,lwr_k=600:113.788,lwr_k=700:114.6962,lwr_k=800:114.6936,lwr_k=900:114.2353,lwr_k=1000:115.3167'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:88.7686,lwr_k=200:101.2076,lwr_k=300:107.4576,lwr_k=400:111.3286,lwr_k=500:113.3604,lwr_k=600:115.5977,lwr_k=700:116.4661,lwr_k=800:117.6814,lwr_k=900:119.3771,lwr_k=1000:120.6945'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:122.8912,lwr_k=200:118.3493,lwr_k=300:115.0436,lwr_k=400:114.946,lwr_k=500:116.8624,lwr_k=600:117.2502,lwr_k=700:118.1259,lwr_k=800:119.4769,lwr_k=900:120.7133,lwr_k=1000:120.955'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:78.8984,lwr_k=200:92.9933,lwr_k=300:98.8561,lwr_k=400:102.137,lwr_k=500:104.6664,lwr_k=600:106.6497,lwr_k=700:108.1118,lwr_k=800:109.3045,lwr_k=900:110.4499,lwr_k=1000:111.2429'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:115.6302,lwr_k=200:120.0091,lwr_k=300:113.6688,lwr_k=400:112.318,lwr_k=500:110.1692,lwr_k=600:108.7187,lwr_k=700:106.8084,lwr_k=800:105.4584,lwr_k=900:105.2856,lwr_k=1000:105.7458'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_75'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:95.4871,lwr_k=200:102.7389,lwr_k=300:109.8681,lwr_k=400:113.5004,lwr_k=500:115.7241,lwr_k=600:117.3526,lwr_k=700:118.4557,lwr_k=800:119.2684,lwr_k=900:119.9282,lwr_k=1000:120.7256'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:225.732,lwr_k=200:114.2725,lwr_k=300:125.0213,lwr_k=400:120.5461,lwr_k=500:120.0394,lwr_k=600:120.4354,lwr_k=700:120.633,lwr_k=800:120.9557,lwr_k=900:121.062,lwr_k=1000:120.8451'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:59.3475,lwr_k=200:74.448,lwr_k=300:83.4378,lwr_k=400:89.3036,lwr_k=500:92.6589,lwr_k=600:94.6148,lwr_k=700:96.2125,lwr_k=800:97.7109,lwr_k=900:98.5308,lwr_k=1000:99.325'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:186.3907,lwr_k=200:166.3188,lwr_k=300:164.5664,lwr_k=400:161.7229,lwr_k=500:161.794,lwr_k=600:162.5118,lwr_k=700:162.2039,lwr_k=800:162.2465,lwr_k=900:162.4493,lwr_k=1000:163.1151'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:75.0075,lwr_k=200:84.2326,lwr_k=300:87.8763,lwr_k=400:90.4815,lwr_k=500:92.6446,lwr_k=600:93.3144,lwr_k=700:94.2385,lwr_k=800:94.9277,lwr_k=900:95.369,lwr_k=1000:95.7116'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:126.8831,lwr_k=200:117.5584,lwr_k=300:112.3555,lwr_k=400:112.5205,lwr_k=500:112.809,lwr_k=600:114.2086,lwr_k=700:114.4961,lwr_k=800:115.8048,lwr_k=900:114.7039,lwr_k=1000:114.1452'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:113.172,lwr_k=200:119.1138,lwr_k=300:120.9519,lwr_k=400:121.3699,lwr_k=500:121.8094,lwr_k=600:121.9268,lwr_k=700:122.091,lwr_k=800:122.3563,lwr_k=900:122.4229,lwr_k=1000:122.7117'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:157.7003,lwr_k=200:148.799,lwr_k=300:146.9967,lwr_k=400:151.0062,lwr_k=500:152.5221,lwr_k=600:151.9778,lwr_k=700:152.1065,lwr_k=800:152.74,lwr_k=900:151.982,lwr_k=1000:152.9002'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:131.0417,lwr_k=200:136.5109,lwr_k=300:138.1049,lwr_k=400:139.696,lwr_k=500:140.5093,lwr_k=600:141.5413,lwr_k=700:141.871,lwr_k=800:142.5397,lwr_k=900:143.4367,lwr_k=1000:143.6737'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:139.1164,lwr_k=200:137.4304,lwr_k=300:138.7452,lwr_k=400:140.2463,lwr_k=500:141.2375,lwr_k=600:141.2188,lwr_k=700:141.7605,lwr_k=800:141.9925,lwr_k=900:142.2604,lwr_k=1000:142.4633'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:80.0475,lwr_k=200:90.0914,lwr_k=300:97.0355,lwr_k=400:102.256,lwr_k=500:106.3067,lwr_k=600:108.8176,lwr_k=700:111.4975,lwr_k=800:112.8376,lwr_k=900:114.8184,lwr_k=1000:116.111'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:108.431,lwr_k=200:95.6892,lwr_k=300:94.4633,lwr_k=400:99.4938,lwr_k=500:105.5926,lwr_k=600:113.3769,lwr_k=700:115.5856,lwr_k=800:115.6476,lwr_k=900:117.2583,lwr_k=1000:118.1272'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_76'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:78.4562,lwr_k=200:97.5499,lwr_k=300:107.7405,lwr_k=400:113.8447,lwr_k=500:117.153,lwr_k=600:119.8095,lwr_k=700:120.95,lwr_k=800:121.7945,lwr_k=900:123.2097,lwr_k=1000:124.9862'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:254.4743,lwr_k=200:148.2285,lwr_k=300:142.6832,lwr_k=400:143.6708,lwr_k=500:140.4726,lwr_k=600:146.6829,lwr_k=700:146.7759,lwr_k=800:145.4738,lwr_k=900:146.3418,lwr_k=1000:146.907'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:78.3001,lwr_k=200:99.7286,lwr_k=300:108.6006,lwr_k=400:113.4287,lwr_k=500:116.3038,lwr_k=600:118.4859,lwr_k=700:120.7203,lwr_k=800:121.8936,lwr_k=900:123.0465,lwr_k=1000:123.5912'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:234.2311,lwr_k=200:187.622,lwr_k=300:182.0889,lwr_k=400:187.3059,lwr_k=500:188.9149,lwr_k=600:192.5703,lwr_k=700:192.2286,lwr_k=800:193.7964,lwr_k=900:194.8329,lwr_k=1000:195.0601'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:71.5927,lwr_k=200:85.326,lwr_k=300:92.9755,lwr_k=400:96.7648,lwr_k=500:100.1329,lwr_k=600:103.3062,lwr_k=700:105.6816,lwr_k=800:108.7351,lwr_k=900:110.6121,lwr_k=1000:112.5646'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:259.8319,lwr_k=200:211.5738,lwr_k=300:207.8366,lwr_k=400:172.113,lwr_k=500:130.3355,lwr_k=600:129.7388,lwr_k=700:130.729,lwr_k=800:126.692,lwr_k=900:122.9442,lwr_k=1000:121.3639'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:88.4812,lwr_k=200:104.6724,lwr_k=300:111.6786,lwr_k=400:114.8765,lwr_k=500:121.384,lwr_k=600:123.0915,lwr_k=700:124.6475,lwr_k=800:126.6903,lwr_k=900:127.852,lwr_k=1000:128.5929'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:147.2838,lwr_k=200:118.2487,lwr_k=300:119.5553,lwr_k=400:119.929,lwr_k=500:124.9159,lwr_k=600:126.5179,lwr_k=700:127.2606,lwr_k=800:127.5245,lwr_k=900:127.6427,lwr_k=1000:127.8272'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:90.6679,lwr_k=200:108.8486,lwr_k=300:118.3261,lwr_k=400:127.8127,lwr_k=500:133.425,lwr_k=600:136.4763,lwr_k=700:139.4246,lwr_k=800:145.3197,lwr_k=900:149.6556,lwr_k=1000:153.0501'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:197.9609,lwr_k=200:146.8418,lwr_k=300:114.7987,lwr_k=400:119.6941,lwr_k=500:125.539,lwr_k=600:127.3929,lwr_k=700:132.5025,lwr_k=800:138.9216,lwr_k=900:147.0475,lwr_k=1000:151.601'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:102.4145,lwr_k=200:127.8742,lwr_k=300:147.9556,lwr_k=400:159.493,lwr_k=500:163.0404,lwr_k=600:167.2026,lwr_k=700:172.8034,lwr_k=800:176.2786,lwr_k=900:178.0732,lwr_k=1000:181.8851'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:250.1065,lwr_k=200:211.4704,lwr_k=300:170.6421,lwr_k=400:172.0263,lwr_k=500:171.8411,lwr_k=600:174.1773,lwr_k=700:179.1349,lwr_k=800:182.3095,lwr_k=900:181.0272,lwr_k=1000:183.9254'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_77'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:101.9091,lwr_k=200:111.6863,lwr_k=300:114.6079,lwr_k=400:117.6549,lwr_k=500:119.243,lwr_k=600:121.237,lwr_k=700:122.5025,lwr_k=800:123.2856,lwr_k=900:123.9263,lwr_k=1000:124.7887'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:115.8662,lwr_k=200:110.3828,lwr_k=300:112.5162,lwr_k=400:115.5552,lwr_k=500:117.4543,lwr_k=600:118.7569,lwr_k=700:119.5557,lwr_k=800:120.2107,lwr_k=900:120.9681,lwr_k=1000:121.9002'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:267.7606,lwr_k=200:269.6712,lwr_k=300:270.0157,lwr_k=400:270.5444,lwr_k=500:270.9437,lwr_k=600:271.1977,lwr_k=700:271.9967,lwr_k=800:272.7188,lwr_k=900:272.8429,lwr_k=1000:273.3679'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:361.342,lwr_k=200:359.7286,lwr_k=300:360.053,lwr_k=400:362.3982,lwr_k=500:365.3157,lwr_k=600:366.201,lwr_k=700:366.9668,lwr_k=800:367.9878,lwr_k=900:368.1088,lwr_k=1000:368.5764'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:139.3822,lwr_k=200:141.2547,lwr_k=300:143.0036,lwr_k=400:143.8677,lwr_k=500:144.6877,lwr_k=600:145.4659,lwr_k=700:146.2283,lwr_k=800:146.9789,lwr_k=900:146.9728,lwr_k=1000:147.3938'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:143.4388,lwr_k=200:139.0882,lwr_k=300:139.0182,lwr_k=400:138.5361,lwr_k=500:137.5554,lwr_k=600:137.0443,lwr_k=700:137.3367,lwr_k=800:137.0526,lwr_k=900:136.7244,lwr_k=1000:137.0364'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:104.859,lwr_k=200:118.6505,lwr_k=300:125.9803,lwr_k=400:129.1785,lwr_k=500:132.4801,lwr_k=600:135.8029,lwr_k=700:137.4853,lwr_k=800:138.8799,lwr_k=900:140.3212,lwr_k=1000:142.062'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:135.7845,lwr_k=200:119.058,lwr_k=300:112.6502,lwr_k=400:111.7006,lwr_k=500:109.6414,lwr_k=600:109.222,lwr_k=700:108.8135,lwr_k=800:108.8911,lwr_k=900:109.4027,lwr_k=1000:109.7182'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:124.8613,lwr_k=200:131.9433,lwr_k=300:134.8291,lwr_k=400:135.8511,lwr_k=500:136.7811,lwr_k=600:137.5817,lwr_k=700:138.2408,lwr_k=800:138.1379,lwr_k=900:138.5112,lwr_k=1000:138.6781'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:109.6063,lwr_k=200:110.2901,lwr_k=300:112.4182,lwr_k=400:113.737,lwr_k=500:113.8868,lwr_k=600:113.7038,lwr_k=700:113.8547,lwr_k=800:113.4343,lwr_k=900:113.4631,lwr_k=1000:113.2139'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:85.2796,lwr_k=200:97.7549,lwr_k=300:101.9178,lwr_k=400:103.8387,lwr_k=500:105.1562,lwr_k=600:106.063,lwr_k=700:106.4862,lwr_k=800:106.8898,lwr_k=900:107.4421,lwr_k=1000:108.0365'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:120.6126,lwr_k=200:125.9685,lwr_k=300:127.7102,lwr_k=400:127.9631,lwr_k=500:127.7563,lwr_k=600:128.0863,lwr_k=700:127.8403,lwr_k=800:127.6533,lwr_k=900:127.1897,lwr_k=1000:127.3811'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_78'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:61.7054,lwr_k=200:68.7633,lwr_k=300:72.4154,lwr_k=400:74.5604,lwr_k=500:75.8669,lwr_k=600:76.7856,lwr_k=700:77.2312,lwr_k=800:77.7357,lwr_k=900:78.1211,lwr_k=1000:78.3898'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:134.9727,lwr_k=200:125.4041,lwr_k=300:121.6187,lwr_k=400:117.3007,lwr_k=500:117.1577,lwr_k=600:116.3435,lwr_k=700:116.8824,lwr_k=800:116.3685,lwr_k=900:116.2381,lwr_k=1000:116.8293'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:47.1942,lwr_k=200:54.1701,lwr_k=300:56.287,lwr_k=400:57.8614,lwr_k=500:58.7807,lwr_k=600:59.6051,lwr_k=700:60.0042,lwr_k=800:60.1651,lwr_k=900:60.5889,lwr_k=1000:61.0112'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:142.116,lwr_k=200:147.8158,lwr_k=300:145.8662,lwr_k=400:144.3211,lwr_k=500:144.2057,lwr_k=600:143.5582,lwr_k=700:143.8382,lwr_k=800:143.3987,lwr_k=900:143.43,lwr_k=1000:141.8208'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:52.0682,lwr_k=200:57.5327,lwr_k=300:60.4122,lwr_k=400:61.8106,lwr_k=500:62.2061,lwr_k=600:62.7282,lwr_k=700:63.1796,lwr_k=800:63.3392,lwr_k=900:63.675,lwr_k=1000:63.9417'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:104.1943,lwr_k=200:105.2122,lwr_k=300:105.074,lwr_k=400:104.8324,lwr_k=500:103.3396,lwr_k=600:105.3729,lwr_k=700:104.9735,lwr_k=800:103.802,lwr_k=900:103.7277,lwr_k=1000:103.7091'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:60.6484,lwr_k=200:67.1427,lwr_k=300:69.37,lwr_k=400:70.3856,lwr_k=500:71.0192,lwr_k=600:71.7333,lwr_k=700:72.0402,lwr_k=800:72.4491,lwr_k=900:72.7808,lwr_k=1000:73.0654'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:109.1873,lwr_k=200:100.282,lwr_k=300:98.0844,lwr_k=400:97.0861,lwr_k=500:96.2648,lwr_k=600:96.7598,lwr_k=700:96.2481,lwr_k=800:95.2393,lwr_k=900:95.1646,lwr_k=1000:95.2001'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:73.5504,lwr_k=200:80.5854,lwr_k=300:83.5568,lwr_k=400:85.7208,lwr_k=500:87.1323,lwr_k=600:88.3619,lwr_k=700:89.0457,lwr_k=800:89.8327,lwr_k=900:90.4276,lwr_k=1000:91.0259'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:147.7644,lwr_k=200:132.6529,lwr_k=300:125.8883,lwr_k=400:125.133,lwr_k=500:123.2077,lwr_k=600:121.4952,lwr_k=700:121.8578,lwr_k=800:122.0286,lwr_k=900:120.9983,lwr_k=1000:119.0354'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:70.1156,lwr_k=200:79.4924,lwr_k=300:81.9755,lwr_k=400:83.91,lwr_k=500:85.2607,lwr_k=600:86.183,lwr_k=700:86.9241,lwr_k=800:86.9466,lwr_k=900:87.1395,lwr_k=1000:87.9267'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:110.5519,lwr_k=200:111.8983,lwr_k=300:104.3168,lwr_k=400:104.5268,lwr_k=500:105.1344,lwr_k=600:104.1593,lwr_k=700:106.9534,lwr_k=800:107.0616,lwr_k=900:104.627,lwr_k=1000:103.1832'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_79'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:98.4955,lwr_k=200:105.9146,lwr_k=300:110.1126,lwr_k=400:112.4344,lwr_k=500:113.7511,lwr_k=600:114.8258,lwr_k=700:115.601,lwr_k=800:116.6473,lwr_k=900:117.3247,lwr_k=1000:117.8805'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:113.3574,lwr_k=200:107.9854,lwr_k=300:106.7028,lwr_k=400:106.3301,lwr_k=500:106.2758,lwr_k=600:106.16,lwr_k=700:106.2281,lwr_k=800:106.8099,lwr_k=900:106.7583,lwr_k=1000:106.8346'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:79.7826,lwr_k=200:86.6183,lwr_k=300:89.7138,lwr_k=400:90.8766,lwr_k=500:91.8445,lwr_k=600:92.4066,lwr_k=700:92.8828,lwr_k=800:93.6044,lwr_k=900:93.9191,lwr_k=1000:94.4071'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:159.8354,lwr_k=200:153.074,lwr_k=300:153.0074,lwr_k=400:152.3141,lwr_k=500:151.1101,lwr_k=600:150.6554,lwr_k=700:150.5487,lwr_k=800:150.2741,lwr_k=900:150.6729,lwr_k=1000:150.3683'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:80.4365,lwr_k=200:87.1873,lwr_k=300:90.8918,lwr_k=400:95.237,lwr_k=500:97.1683,lwr_k=600:98.4008,lwr_k=700:99.3699,lwr_k=800:99.6382,lwr_k=900:99.93,lwr_k=1000:100.5814'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:108.8777,lwr_k=200:105.5562,lwr_k=300:106.1507,lwr_k=400:107.0658,lwr_k=500:108.2977,lwr_k=600:108.1876,lwr_k=700:108.715,lwr_k=800:108.7145,lwr_k=900:108.9034,lwr_k=1000:108.6916'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:110.7845,lwr_k=200:121.327,lwr_k=300:123.5054,lwr_k=400:125.8613,lwr_k=500:128.6393,lwr_k=600:131.1504,lwr_k=700:132.3137,lwr_k=800:133.1453,lwr_k=900:133.7426,lwr_k=1000:134.222'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:133.89,lwr_k=200:116.8807,lwr_k=300:110.9617,lwr_k=400:109.9614,lwr_k=500:112.2778,lwr_k=600:112.2632,lwr_k=700:111.9768,lwr_k=800:112.0243,lwr_k=900:112.2342,lwr_k=1000:112.033'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:105.5991,lwr_k=200:112.0606,lwr_k=300:114.7093,lwr_k=400:115.653,lwr_k=500:116.6107,lwr_k=600:117.172,lwr_k=700:117.6533,lwr_k=800:117.7481,lwr_k=900:117.8579,lwr_k=1000:118.0747'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:116.7956,lwr_k=200:110.6132,lwr_k=300:109.3794,lwr_k=400:109.5723,lwr_k=500:109.2735,lwr_k=600:109.5516,lwr_k=700:110.0717,lwr_k=800:109.9486,lwr_k=900:110.0076,lwr_k=1000:109.5553'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:96.6176,lwr_k=200:103.0784,lwr_k=300:105.6117,lwr_k=400:106.6344,lwr_k=500:107.3305,lwr_k=600:108.0099,lwr_k=700:108.2898,lwr_k=800:108.8577,lwr_k=900:109.079,lwr_k=1000:109.03'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:112.4798,lwr_k=200:116.6333,lwr_k=300:115.719,lwr_k=400:114.0494,lwr_k=500:112.7838,lwr_k=600:113.5804,lwr_k=700:114.2974,lwr_k=800:115.2212,lwr_k=900:115.7753,lwr_k=1000:116.1785'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_80'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:129.3301,lwr_k=200:120.8457,lwr_k=300:121.1861,lwr_k=400:121.6439,lwr_k=500:122.2086,lwr_k=600:122.6889,lwr_k=700:123.4084,lwr_k=800:123.8975,lwr_k=900:124.2971,lwr_k=1000:124.4836'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:170.1503,lwr_k=200:132.6039,lwr_k=300:123.3925,lwr_k=400:118.9748,lwr_k=500:118.3156,lwr_k=600:117.0032,lwr_k=700:117.1475,lwr_k=800:116.8769,lwr_k=900:116.2264,lwr_k=1000:116.049'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:73.1685,lwr_k=200:85.7623,lwr_k=300:90.1025,lwr_k=400:92.327,lwr_k=500:93.9295,lwr_k=600:95.1089,lwr_k=700:96.0877,lwr_k=800:96.8157,lwr_k=900:97.3309,lwr_k=1000:97.5826'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:189.8363,lwr_k=200:163.0232,lwr_k=300:159.9295,lwr_k=400:159.6776,lwr_k=500:159.1899,lwr_k=600:158.2853,lwr_k=700:156.1974,lwr_k=800:155.9693,lwr_k=900:156.1635,lwr_k=1000:155.8256'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:89.5305,lwr_k=200:91.7767,lwr_k=300:94.6021,lwr_k=400:96.283,lwr_k=500:97.721,lwr_k=600:99.0086,lwr_k=700:99.8403,lwr_k=800:100.2613,lwr_k=900:100.7765,lwr_k=1000:101.0418'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:142.4181,lwr_k=200:122.2926,lwr_k=300:117.5514,lwr_k=400:115.9546,lwr_k=500:115.4579,lwr_k=600:115.3794,lwr_k=700:115.8033,lwr_k=800:116.0345,lwr_k=900:116.473,lwr_k=1000:116.7204'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:146.3807,lwr_k=200:149.7157,lwr_k=300:149.7968,lwr_k=400:149.6652,lwr_k=500:150.1318,lwr_k=600:150.3998,lwr_k=700:150.416,lwr_k=800:150.5158,lwr_k=900:150.7225,lwr_k=1000:150.4672'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:130.3496,lwr_k=200:125.6882,lwr_k=300:124.5419,lwr_k=400:124.2552,lwr_k=500:124.2016,lwr_k=600:123.8447,lwr_k=700:123.5103,lwr_k=800:123.5495,lwr_k=900:123.399,lwr_k=1000:123.5871'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:99.457,lwr_k=200:109.2412,lwr_k=300:114.162,lwr_k=400:116.8441,lwr_k=500:117.781,lwr_k=600:118.4809,lwr_k=700:119.4015,lwr_k=800:119.8002,lwr_k=900:120.0585,lwr_k=1000:120.8452'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:128.6437,lwr_k=200:124.8359,lwr_k=300:122.2684,lwr_k=400:121.4924,lwr_k=500:119.7163,lwr_k=600:118.6829,lwr_k=700:116.7449,lwr_k=800:116.4159,lwr_k=900:117.2605,lwr_k=1000:117.2124'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:94.5597,lwr_k=200:103.8625,lwr_k=300:109.0585,lwr_k=400:113.1193,lwr_k=500:116.1558,lwr_k=600:118.1616,lwr_k=700:119.2713,lwr_k=800:120.3256,lwr_k=900:121.0453,lwr_k=1000:121.6942'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:102.4219,lwr_k=200:196.8083,lwr_k=300:112.3771,lwr_k=400:120.1656,lwr_k=500:122.4764,lwr_k=600:120.672,lwr_k=700:119.4436,lwr_k=800:119.7117,lwr_k=900:120.0438,lwr_k=1000:119.7361'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_81'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:176.7618,lwr_k=200:169.1626,lwr_k=300:152.3178,lwr_k=400:152.6176,lwr_k=500:152.8413,lwr_k=600:168.6046,lwr_k=700:173.7109,lwr_k=800:178.9326,lwr_k=900:172.3139,lwr_k=1000:164.4897'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:318.64,lwr_k=200:220.1876,lwr_k=300:180.2374,lwr_k=400:161.8823,lwr_k=500:155.5425,lwr_k=600:165.6555,lwr_k=700:167.8836,lwr_k=800:149.5391,lwr_k=900:151.5712,lwr_k=1000:140.9535'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:55.8325,lwr_k=200:64.6621,lwr_k=300:76.3419,lwr_k=400:82.9386,lwr_k=500:86.5548,lwr_k=600:89.725,lwr_k=700:91.1262,lwr_k=800:92.7961,lwr_k=900:93.5942,lwr_k=1000:94.7024'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:247.909,lwr_k=200:207.0201,lwr_k=300:184.935,lwr_k=400:154.7079,lwr_k=500:156.001,lwr_k=600:155.9647,lwr_k=700:156.9325,lwr_k=800:157.343,lwr_k=900:158.5345,lwr_k=1000:158.8095'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:98.6308,lwr_k=200:94.7696,lwr_k=300:94.4655,lwr_k=400:96.4359,lwr_k=500:92.3729,lwr_k=600:94.4936,lwr_k=700:95.0147,lwr_k=800:91.8683,lwr_k=900:92.7792,lwr_k=1000:92.1042'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:216.4044,lwr_k=200:175.7375,lwr_k=300:169.6785,lwr_k=400:155.8489,lwr_k=500:119.5532,lwr_k=600:118.1451,lwr_k=700:117.8126,lwr_k=800:111.5504,lwr_k=900:112.7527,lwr_k=1000:112.5833'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:87.2052,lwr_k=200:74.1675,lwr_k=300:92.4775,lwr_k=400:103.0689,lwr_k=500:105.6819,lwr_k=600:108.037,lwr_k=700:110.2876,lwr_k=800:111.7448,lwr_k=900:112.8562,lwr_k=1000:114.1856'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:246.7149,lwr_k=200:148.1106,lwr_k=300:112.1624,lwr_k=400:107.0032,lwr_k=500:103.2886,lwr_k=600:103.1364,lwr_k=700:102.4634,lwr_k=800:101.5365,lwr_k=900:101.1011,lwr_k=1000:100.754'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:100.4729,lwr_k=200:94.558,lwr_k=300:96.9436,lwr_k=400:99.1762,lwr_k=500:100.2282,lwr_k=600:100.7126,lwr_k=700:101.9691,lwr_k=800:100.4514,lwr_k=900:100.5734,lwr_k=1000:100.5665'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:243.3225,lwr_k=200:141.8379,lwr_k=300:129.4495,lwr_k=400:115.7758,lwr_k=500:117.3517,lwr_k=600:113.3879,lwr_k=700:113.5476,lwr_k=800:113.5315,lwr_k=900:111.8562,lwr_k=1000:113.45'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:158.3904,lwr_k=200:133.0167,lwr_k=300:130.602,lwr_k=400:129.5828,lwr_k=500:130.4,lwr_k=600:129.3114,lwr_k=700:127.9076,lwr_k=800:128.9237,lwr_k=900:131.2155,lwr_k=1000:126.9058'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:306.229,lwr_k=200:141.2228,lwr_k=300:200.0534,lwr_k=400:149.2374,lwr_k=500:134.8159,lwr_k=600:131.7282,lwr_k=700:132.0539,lwr_k=800:136.5612,lwr_k=900:122.6706,lwr_k=1000:123.3194'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_82'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:155.1947,lwr_k=200:156.6796,lwr_k=300:157.6224,lwr_k=400:158.8153,lwr_k=500:160.027,lwr_k=600:161.7349,lwr_k=700:161.6718,lwr_k=800:163.1835,lwr_k=900:162.652,lwr_k=1000:164.6807'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:123.0633,lwr_k=200:119.2882,lwr_k=300:121.4525,lwr_k=400:121.4784,lwr_k=500:130.0176,lwr_k=600:130.3426,lwr_k=700:130.2116,lwr_k=800:136.5533,lwr_k=900:136.5658,lwr_k=1000:147.1486'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:147.5509,lwr_k=200:144.1662,lwr_k=300:144.3214,lwr_k=400:144.305,lwr_k=500:144.9779,lwr_k=600:150.2613,lwr_k=700:150.6964,lwr_k=800:151.1675,lwr_k=900:151.7823,lwr_k=1000:152.1051'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:306.6356,lwr_k=200:299.0356,lwr_k=300:298.0412,lwr_k=400:298.5558,lwr_k=500:298.9135,lwr_k=600:299.8589,lwr_k=700:300.6404,lwr_k=800:301.2975,lwr_k=900:302.0171,lwr_k=1000:302.2594'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:178.8498,lwr_k=200:178.5272,lwr_k=300:178.8738,lwr_k=400:179.0209,lwr_k=500:180.0325,lwr_k=600:181.4399,lwr_k=700:182.6967,lwr_k=800:183.6939,lwr_k=900:184.7225,lwr_k=1000:185.8873'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:144.8125,lwr_k=200:146.8773,lwr_k=300:147.2223,lwr_k=400:143.8265,lwr_k=500:144.5647,lwr_k=600:145.2998,lwr_k=700:145.9542,lwr_k=800:146.5254,lwr_k=900:147.1424,lwr_k=1000:147.7389'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:181.9521,lwr_k=200:182.275,lwr_k=300:182.7565,lwr_k=400:180.7238,lwr_k=500:182.6537,lwr_k=600:183.3039,lwr_k=700:183.7954,lwr_k=800:185.4863,lwr_k=900:186.4713,lwr_k=1000:189.3068'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:193.6721,lwr_k=200:194.1356,lwr_k=300:194.5415,lwr_k=400:194.9595,lwr_k=500:195.1551,lwr_k=600:195.2711,lwr_k=700:210.0679,lwr_k=800:210.4455,lwr_k=900:208.6524,lwr_k=1000:210.0009'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:165.5506,lwr_k=200:165.3821,lwr_k=300:165.6664,lwr_k=400:166.1718,lwr_k=500:167.4572,lwr_k=600:173.5037,lwr_k=700:186.8834,lwr_k=800:170.7398,lwr_k=900:172.3049,lwr_k=1000:173.8385'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:243.9742,lwr_k=200:312.1863,lwr_k=300:288.6248,lwr_k=400:242.3283,lwr_k=500:234.8252,lwr_k=600:233.441,lwr_k=700:260.4975,lwr_k=800:237.1186,lwr_k=900:242.0208,lwr_k=1000:261.6469'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:133.316,lwr_k=200:134.6678,lwr_k=300:136.3518,lwr_k=400:141.78,lwr_k=500:141.5572,lwr_k=600:145.0534,lwr_k=700:141.981,lwr_k=800:136.8516,lwr_k=900:141.6258,lwr_k=1000:145.5297'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:148.4004,lwr_k=200:158.2116,lwr_k=300:155.9188,lwr_k=400:146.9123,lwr_k=500:145.8699,lwr_k=600:151.9703,lwr_k=700:150.6316,lwr_k=800:142.3484,lwr_k=900:131.18,lwr_k=1000:131.2104'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_83'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:75.0855,lwr_k=200:74.6705,lwr_k=300:73.9367,lwr_k=400:69.7196,lwr_k=500:70.1153,lwr_k=600:70.5977,lwr_k=700:70.8899,lwr_k=800:71.0552,lwr_k=900:69.9482,lwr_k=1000:69.7821'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:267.9628,lwr_k=200:188.892,lwr_k=300:160.6635,lwr_k=400:143.2941,lwr_k=500:130.2094,lwr_k=600:127.3935,lwr_k=700:123.82,lwr_k=800:123.5319,lwr_k=900:122.4812,lwr_k=1000:116.4274'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:105.3802,lwr_k=200:89.698,lwr_k=300:82.63,lwr_k=400:86.2306,lwr_k=500:80.5862,lwr_k=600:80.0202,lwr_k=700:79.7147,lwr_k=800:80.3619,lwr_k=900:80.1357,lwr_k=1000:80.0259'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:260.2824,lwr_k=200:213.8822,lwr_k=300:181.6441,lwr_k=400:180.7306,lwr_k=500:191.2546,lwr_k=600:174.8419,lwr_k=700:171.607,lwr_k=800:174.9376,lwr_k=900:176.8744,lwr_k=1000:173.5245'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:74.8972,lwr_k=200:82.8889,lwr_k=300:71.3973,lwr_k=400:69.8141,lwr_k=500:73.0223,lwr_k=600:73.0806,lwr_k=700:74.1101,lwr_k=800:71.9635,lwr_k=900:70.7162,lwr_k=1000:70.8178'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:175.4585,lwr_k=200:148.8369,lwr_k=300:126.4701,lwr_k=400:115.5542,lwr_k=500:117.9771,lwr_k=600:120.2398,lwr_k=700:126.3967,lwr_k=800:122.7355,lwr_k=900:122.9949,lwr_k=1000:120.2289'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:68.1383,lwr_k=200:67.602,lwr_k=300:67.4207,lwr_k=400:68.8714,lwr_k=500:69.5723,lwr_k=600:70.0793,lwr_k=700:70.1055,lwr_k=800:70.996,lwr_k=900:70.5357,lwr_k=1000:70.7699'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:167.1317,lwr_k=200:140.1261,lwr_k=300:114.361,lwr_k=400:114.5939,lwr_k=500:109.2187,lwr_k=600:107.8475,lwr_k=700:104.1197,lwr_k=800:102.6362,lwr_k=900:102.5297,lwr_k=1000:96.527'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:135.3935,lwr_k=200:112.7171,lwr_k=300:111.6011,lwr_k=400:113.0568,lwr_k=500:97.1901,lwr_k=600:97.1926,lwr_k=700:100.9984,lwr_k=800:103.995,lwr_k=900:100.5933,lwr_k=1000:99.5353'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:246.4413,lwr_k=200:159.9966,lwr_k=300:174.3062,lwr_k=400:149.6323,lwr_k=500:123.0112,lwr_k=600:120.4716,lwr_k=700:125.7977,lwr_k=800:120.0295,lwr_k=900:117.4231,lwr_k=1000:106.468'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:165.0089,lwr_k=200:117.3275,lwr_k=300:107.1929,lwr_k=400:93.1539,lwr_k=500:89.4183,lwr_k=600:92.7728,lwr_k=700:92.0721,lwr_k=800:94.7122,lwr_k=900:92.8936,lwr_k=1000:88.8348'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:294.1544,lwr_k=200:235.7513,lwr_k=300:130.4572,lwr_k=400:129.5336,lwr_k=500:121.0464,lwr_k=600:117.6688,lwr_k=700:100.6213,lwr_k=800:97.0899,lwr_k=900:107.7312,lwr_k=1000:137.4351'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_84'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:43.8838,lwr_k=200:68.6766,lwr_k=300:76.5963,lwr_k=400:81.4432,lwr_k=500:83.9707,lwr_k=600:85.1628,lwr_k=700:86.0288,lwr_k=800:86.8743,lwr_k=900:87.6679,lwr_k=1000:88.1208'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:154.9113,lwr_k=200:109.3499,lwr_k=300:119.8931,lwr_k=400:113.0637,lwr_k=500:114.8889,lwr_k=600:113.479,lwr_k=700:115.0636,lwr_k=800:114.7382,lwr_k=900:113.1728,lwr_k=1000:112.7344'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:65.6142,lwr_k=200:60.3175,lwr_k=300:63.5561,lwr_k=400:64.1402,lwr_k=500:65.4121,lwr_k=600:66.5875,lwr_k=700:67.337,lwr_k=800:68.0399,lwr_k=900:68.7216,lwr_k=1000:68.9664'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:272.4051,lwr_k=200:176.2561,lwr_k=300:172.4556,lwr_k=400:167.9102,lwr_k=500:164.0432,lwr_k=600:161.8089,lwr_k=700:163.4629,lwr_k=800:163.0148,lwr_k=900:162.8503,lwr_k=1000:160.7173'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:43.8001,lwr_k=200:57.6887,lwr_k=300:61.3092,lwr_k=400:63.9924,lwr_k=500:65.61,lwr_k=600:67.213,lwr_k=700:68.1415,lwr_k=800:69.0774,lwr_k=900:69.9346,lwr_k=1000:70.3484'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:149.761,lwr_k=200:107.7695,lwr_k=300:100.6534,lwr_k=400:99.8724,lwr_k=500:98.0582,lwr_k=600:98.8193,lwr_k=700:99.927,lwr_k=800:101.1098,lwr_k=900:101.1805,lwr_k=1000:101.8142'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:41.4524,lwr_k=200:64.0798,lwr_k=300:74.1391,lwr_k=400:78.0777,lwr_k=500:80.0083,lwr_k=600:82.5923,lwr_k=700:84.8252,lwr_k=800:86.1819,lwr_k=900:87.0129,lwr_k=1000:87.781'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:133.9228,lwr_k=200:109.3981,lwr_k=300:108.4514,lwr_k=400:107.2452,lwr_k=500:106.7416,lwr_k=600:106.7869,lwr_k=700:107.4749,lwr_k=800:106.1547,lwr_k=900:106.1055,lwr_k=1000:106.3051'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:65.7027,lwr_k=200:87.514,lwr_k=300:83.9067,lwr_k=400:89.8889,lwr_k=500:92.6684,lwr_k=600:96.1908,lwr_k=700:97.2826,lwr_k=800:99.5865,lwr_k=900:100.5418,lwr_k=1000:101.5927'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:236.3197,lwr_k=200:214.6279,lwr_k=300:111.9055,lwr_k=400:111.9241,lwr_k=500:107.4415,lwr_k=600:108.3894,lwr_k=700:111.1956,lwr_k=800:110.0224,lwr_k=900:110.2571,lwr_k=1000:109.8267'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:42.9594,lwr_k=200:62.1255,lwr_k=300:69.7961,lwr_k=400:74.8466,lwr_k=500:77.3497,lwr_k=600:78.4917,lwr_k=700:79.7357,lwr_k=800:80.822,lwr_k=900:81.4217,lwr_k=1000:81.7944'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:209.4115,lwr_k=200:130.5098,lwr_k=300:115.7671,lwr_k=400:111.0917,lwr_k=500:111.9073,lwr_k=600:112.806,lwr_k=700:112.665,lwr_k=800:112.6982,lwr_k=900:112.6574,lwr_k=1000:111.763'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_85'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:60.1967,lwr_k=200:84.2647,lwr_k=300:92.7294,lwr_k=400:96.7148,lwr_k=500:98.8595,lwr_k=600:100.7774,lwr_k=700:101.9359,lwr_k=800:102.7692,lwr_k=900:103.3195,lwr_k=1000:103.5351'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:219.3405,lwr_k=200:120.3506,lwr_k=300:107.5005,lwr_k=400:103.4212,lwr_k=500:102.282,lwr_k=600:101.8659,lwr_k=700:101.0396,lwr_k=800:100.8328,lwr_k=900:100.1258,lwr_k=1000:99.0234'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:52.7322,lwr_k=200:65.6208,lwr_k=300:70.6652,lwr_k=400:73.0788,lwr_k=500:76.3199,lwr_k=600:78.4888,lwr_k=700:80.0185,lwr_k=800:80.7229,lwr_k=900:81.4664,lwr_k=1000:82.2557'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:204.1004,lwr_k=200:178.1694,lwr_k=300:164.2759,lwr_k=400:169.9617,lwr_k=500:166.6992,lwr_k=600:168.0552,lwr_k=700:167.8744,lwr_k=800:167.6507,lwr_k=900:167.7172,lwr_k=1000:161.5084'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:46.6979,lwr_k=200:62.2562,lwr_k=300:69.1622,lwr_k=400:73.2429,lwr_k=500:75.3182,lwr_k=600:77.1826,lwr_k=700:79.0865,lwr_k=800:80.5659,lwr_k=900:81.6781,lwr_k=1000:82.356'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:177.0565,lwr_k=200:142.5438,lwr_k=300:136.0551,lwr_k=400:132.3296,lwr_k=500:127.7561,lwr_k=600:125.2779,lwr_k=700:122.1717,lwr_k=800:119.7419,lwr_k=900:117.0779,lwr_k=1000:116.9624'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:90.266,lwr_k=200:99.484,lwr_k=300:158.478,lwr_k=400:120.8355,lwr_k=500:111.6257,lwr_k=600:110.391,lwr_k=700:110.6969,lwr_k=800:111.5084,lwr_k=900:113.8616,lwr_k=1000:115.8837'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:209.6002,lwr_k=200:157.994,lwr_k=300:173.2449,lwr_k=400:134.0312,lwr_k=500:134.9693,lwr_k=600:140.9495,lwr_k=700:132.2889,lwr_k=800:131.6276,lwr_k=900:129.993,lwr_k=1000:122.1588'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:138.5313,lwr_k=200:118.2406,lwr_k=300:116.3492,lwr_k=400:117.6672,lwr_k=500:122.344,lwr_k=600:116.9183,lwr_k=700:132.3716,lwr_k=800:139.8233,lwr_k=900:136.5401,lwr_k=1000:118.6036'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:268.8846,lwr_k=200:145.7451,lwr_k=300:149.6059,lwr_k=400:123.1825,lwr_k=500:137.1352,lwr_k=600:119.4433,lwr_k=700:130.6454,lwr_k=800:143.6761,lwr_k=900:137.6202,lwr_k=1000:122.9914'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:52.6665,lwr_k=200:73.8079,lwr_k=300:82.2145,lwr_k=400:87.4271,lwr_k=500:90.4213,lwr_k=600:93.0507,lwr_k=700:94.7243,lwr_k=800:96.1142,lwr_k=900:97.5348,lwr_k=1000:98.3491'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:195.952,lwr_k=200:114.2313,lwr_k=300:122.5909,lwr_k=400:118.6547,lwr_k=500:123.0305,lwr_k=600:120.8194,lwr_k=700:127.3684,lwr_k=800:130.0247,lwr_k=900:130.5331,lwr_k=1000:133.4215'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_86'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:58.9521,lwr_k=200:66.1746,lwr_k=300:73.4014,lwr_k=400:78.9652,lwr_k=500:82.2707,lwr_k=600:84.4345,lwr_k=700:86.6217,lwr_k=800:88.342,lwr_k=900:89.3391,lwr_k=1000:90.7522'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:345.7145,lwr_k=200:150.9144,lwr_k=300:124.3387,lwr_k=400:118.665,lwr_k=500:105.7101,lwr_k=600:103.3447,lwr_k=700:102.2081,lwr_k=800:101.3525,lwr_k=900:102.1611,lwr_k=1000:100.1862'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:83.0495,lwr_k=200:65.3605,lwr_k=300:73.3358,lwr_k=400:78.7563,lwr_k=500:82.4789,lwr_k=600:86.2124,lwr_k=700:89.0116,lwr_k=800:91.4112,lwr_k=900:93.7921,lwr_k=1000:96.5068'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:378.923,lwr_k=200:241.747,lwr_k=300:199.3992,lwr_k=400:196.1288,lwr_k=500:190.9702,lwr_k=600:188.6703,lwr_k=700:192.1791,lwr_k=800:195.3767,lwr_k=900:194.6625,lwr_k=1000:198.2654'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:38.2837,lwr_k=200:45.6446,lwr_k=300:48.7466,lwr_k=400:50.6601,lwr_k=500:53.5754,lwr_k=600:54.3671,lwr_k=700:55.8094,lwr_k=800:56.7642,lwr_k=900:56.7477,lwr_k=1000:57.7477'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:201.6873,lwr_k=200:137.2946,lwr_k=300:126.2878,lwr_k=400:108.5067,lwr_k=500:105.3434,lwr_k=600:102.5076,lwr_k=700:102.9646,lwr_k=800:103.878,lwr_k=900:105.4127,lwr_k=1000:105.2149'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:104.4233,lwr_k=200:76.2526,lwr_k=300:69.1434,lwr_k=400:69.2967,lwr_k=500:68.4195,lwr_k=600:69.7711,lwr_k=700:70.2884,lwr_k=800:72.2649,lwr_k=900:73.1147,lwr_k=1000:73.7597'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:347.4418,lwr_k=200:194.4051,lwr_k=300:155.5235,lwr_k=400:131.2698,lwr_k=500:122.4078,lwr_k=600:122.3857,lwr_k=700:119.836,lwr_k=800:122.1455,lwr_k=900:120.8056,lwr_k=1000:117.378'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:55.5435,lwr_k=200:50.7602,lwr_k=300:58.1392,lwr_k=400:61.9326,lwr_k=500:63.3954,lwr_k=600:65.5954,lwr_k=700:67.2393,lwr_k=800:68.5558,lwr_k=900:69.3605,lwr_k=1000:70.1861'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:357.8874,lwr_k=200:248.1381,lwr_k=300:226.62,lwr_k=400:215.5949,lwr_k=500:198.8007,lwr_k=600:197.189,lwr_k=700:184.9283,lwr_k=800:184.756,lwr_k=900:184.8024,lwr_k=1000:186.394'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:71.2733,lwr_k=200:68.3462,lwr_k=300:67.5033,lwr_k=400:69.9983,lwr_k=500:70.068,lwr_k=600:70.6164,lwr_k=700:72.467,lwr_k=800:73.4511,lwr_k=900:74.3133,lwr_k=1000:74.952'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:347.6019,lwr_k=200:171.2066,lwr_k=300:226.4882,lwr_k=400:129.4989,lwr_k=500:125.6244,lwr_k=600:122.0649,lwr_k=700:115.9057,lwr_k=800:111.6115,lwr_k=900:114.3915,lwr_k=1000:114.4561'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_87'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:78.9192,lwr_k=200:87.6726,lwr_k=300:91.3279,lwr_k=400:92.1788,lwr_k=500:92.8155,lwr_k=600:93.7097,lwr_k=700:94.6134,lwr_k=800:95.1441,lwr_k=900:95.6312,lwr_k=1000:95.8733'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:117.7407,lwr_k=200:115.9634,lwr_k=300:115.1637,lwr_k=400:113.2522,lwr_k=500:112.9103,lwr_k=600:113.3233,lwr_k=700:115.0136,lwr_k=800:114.0285,lwr_k=900:113.8069,lwr_k=1000:112.9323'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:94.2528,lwr_k=200:113.7693,lwr_k=300:125.0064,lwr_k=400:133.6366,lwr_k=500:138.8683,lwr_k=600:142.9698,lwr_k=700:145.8216,lwr_k=800:148.4651,lwr_k=900:149.8646,lwr_k=1000:151.8916'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:214.983,lwr_k=200:223.0652,lwr_k=300:234.7146,lwr_k=400:238.7015,lwr_k=500:239.3878,lwr_k=600:242.4368,lwr_k=700:246.4178,lwr_k=800:247.1239,lwr_k=900:248.5141,lwr_k=1000:250.2236'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:66.8986,lwr_k=200:74.2952,lwr_k=300:77.2567,lwr_k=400:79.1825,lwr_k=500:80.8768,lwr_k=600:81.5344,lwr_k=700:81.884,lwr_k=800:82.2256,lwr_k=900:82.5023,lwr_k=1000:82.7342'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:129.7485,lwr_k=200:115.6872,lwr_k=300:115.5538,lwr_k=400:118.369,lwr_k=500:123.7013,lwr_k=600:123.0605,lwr_k=700:119.2746,lwr_k=800:118.8535,lwr_k=900:118.8021,lwr_k=1000:119.1601'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:111.7855,lwr_k=200:126.9174,lwr_k=300:133.4407,lwr_k=400:138.6321,lwr_k=500:140.9789,lwr_k=600:143.2786,lwr_k=700:144.6297,lwr_k=800:146.204,lwr_k=900:147.686,lwr_k=1000:148.8662'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:149.6196,lwr_k=200:139.1737,lwr_k=300:137.2517,lwr_k=400:134.9708,lwr_k=500:135.9346,lwr_k=600:136.3966,lwr_k=700:137.6753,lwr_k=800:138.8049,lwr_k=900:138.6503,lwr_k=1000:139.1053'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:88.7838,lwr_k=200:97.6262,lwr_k=300:99.1852,lwr_k=400:100.1388,lwr_k=500:100.586,lwr_k=600:100.7824,lwr_k=700:101.2244,lwr_k=800:101.4802,lwr_k=900:101.5197,lwr_k=1000:101.6306'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:102.1162,lwr_k=200:97.9262,lwr_k=300:96.7559,lwr_k=400:93.577,lwr_k=500:92.5563,lwr_k=600:92.1875,lwr_k=700:92.7131,lwr_k=800:92.5223,lwr_k=900:92.319,lwr_k=1000:92.3'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:90.7324,lwr_k=200:101.7931,lwr_k=300:108.9216,lwr_k=400:113.5079,lwr_k=500:117.4889,lwr_k=600:119.5856,lwr_k=700:121.9358,lwr_k=800:123.6007,lwr_k=900:124.8649,lwr_k=1000:126.1618'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:141.3069,lwr_k=200:131.2326,lwr_k=300:125.4629,lwr_k=400:124.0692,lwr_k=500:119.9809,lwr_k=600:121.6803,lwr_k=700:121.7032,lwr_k=800:122.136,lwr_k=900:123.0719,lwr_k=1000:122.2202'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_88'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:355.0121,lwr_k=200:341.8789,lwr_k=300:340.4468,lwr_k=400:338.8941,lwr_k=500:337.9369,lwr_k=600:338.2378,lwr_k=700:337.8668,lwr_k=800:337.872,lwr_k=900:338.0717,lwr_k=1000:338.3747'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:359.1625,lwr_k=200:346.8612,lwr_k=300:345.5834,lwr_k=400:344.261,lwr_k=500:343.5918,lwr_k=600:343.7613,lwr_k=700:343.5979,lwr_k=800:343.6768,lwr_k=900:344.0134,lwr_k=1000:344.4167'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:342.141,lwr_k=200:325.3458,lwr_k=300:325.0387,lwr_k=400:323.1007,lwr_k=500:322.7478,lwr_k=600:322.7162,lwr_k=700:322.6374,lwr_k=800:322.6665,lwr_k=900:322.7149,lwr_k=1000:322.9889'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:446.4701,lwr_k=200:428.6428,lwr_k=300:428.2999,lwr_k=400:426.0387,lwr_k=500:425.557,lwr_k=600:425.5066,lwr_k=700:425.3437,lwr_k=800:425.2822,lwr_k=900:425.2919,lwr_k=1000:425.4501'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:161.687,lwr_k=200:153.6805,lwr_k=300:150.4889,lwr_k=400:150.5401,lwr_k=500:149.9658,lwr_k=600:149.1204,lwr_k=700:149.5276,lwr_k=800:150.2291,lwr_k=900:152.5371,lwr_k=1000:153.006'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:183.3399,lwr_k=200:166.8214,lwr_k=300:157.8346,lwr_k=400:161.2122,lwr_k=500:159.2652,lwr_k=600:156.3216,lwr_k=700:154.1704,lwr_k=800:150.6887,lwr_k=900:155.1334,lwr_k=1000:161.1171'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:104.1615,lwr_k=200:110.2366,lwr_k=300:111.7808,lwr_k=400:114.1162,lwr_k=500:115.5484,lwr_k=600:116.2741,lwr_k=700:115.7506,lwr_k=800:118.0422,lwr_k=900:117.1907,lwr_k=1000:117.2175'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:125.5,lwr_k=200:114.57,lwr_k=300:112.8081,lwr_k=400:112.9828,lwr_k=500:113.6415,lwr_k=600:110.2615,lwr_k=700:109.6805,lwr_k=800:110.1391,lwr_k=900:110.3528,lwr_k=1000:110.3926'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:206.909,lwr_k=200:213.9862,lwr_k=300:215.1283,lwr_k=400:213.6023,lwr_k=500:215.3835,lwr_k=600:217.4555,lwr_k=700:217.3168,lwr_k=800:219.5698,lwr_k=900:218.9121,lwr_k=1000:221.3927'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:225.9918,lwr_k=200:231.3777,lwr_k=300:230.3662,lwr_k=400:233.0789,lwr_k=500:234.743,lwr_k=600:236.6229,lwr_k=700:233.2225,lwr_k=800:234.7397,lwr_k=900:236.2713,lwr_k=1000:239.5089'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:359.9774,lwr_k=200:361.1123,lwr_k=300:360.7826,lwr_k=400:359.4066,lwr_k=500:359.0701,lwr_k=600:359.1555,lwr_k=700:359.0456,lwr_k=800:358.968,lwr_k=900:358.9739,lwr_k=1000:358.9884'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:329.3252,lwr_k=200:330.1032,lwr_k=300:329.8646,lwr_k=400:329.0191,lwr_k=500:328.9427,lwr_k=600:328.9432,lwr_k=700:329.4148,lwr_k=800:329.1892,lwr_k=900:329.0057,lwr_k=1000:328.9841'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_89'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:114.0802,lwr_k=200:116.5233,lwr_k=300:118.5038,lwr_k=400:115.291,lwr_k=500:120.2848,lwr_k=600:121.0081,lwr_k=700:121.2286,lwr_k=800:122.7315,lwr_k=900:122.7435,lwr_k=1000:120.9857'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:125.6766,lwr_k=200:125.3272,lwr_k=300:120.6847,lwr_k=400:120.9792,lwr_k=500:118.4255,lwr_k=600:120.2661,lwr_k=700:116.344,lwr_k=800:116.7841,lwr_k=900:126.4479,lwr_k=1000:119.8679'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:63.4737,lwr_k=200:77.2624,lwr_k=300:83.0665,lwr_k=400:85.8348,lwr_k=500:87.6388,lwr_k=600:88.8274,lwr_k=700:90.1514,lwr_k=800:90.5212,lwr_k=900:91.1379,lwr_k=1000:91.9937'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:205.1292,lwr_k=200:190.0222,lwr_k=300:176.1701,lwr_k=400:175.6241,lwr_k=500:169.5618,lwr_k=600:166.2055,lwr_k=700:163.7431,lwr_k=800:164.1003,lwr_k=900:160.4786,lwr_k=1000:160.1358'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:57.657,lwr_k=200:74.6961,lwr_k=300:80.1357,lwr_k=400:86.803,lwr_k=500:90.6418,lwr_k=600:92.3591,lwr_k=700:93.4556,lwr_k=800:94.9439,lwr_k=900:95.9517,lwr_k=1000:96.9205'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:249.76,lwr_k=200:112.7952,lwr_k=300:114.0641,lwr_k=400:105.5399,lwr_k=500:103.3427,lwr_k=600:103.848,lwr_k=700:104.2819,lwr_k=800:105.1796,lwr_k=900:106.0016,lwr_k=1000:107.0379'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:111.0663,lwr_k=200:115.628,lwr_k=300:117.1218,lwr_k=400:118.6267,lwr_k=500:118.675,lwr_k=600:119.2072,lwr_k=700:118.966,lwr_k=800:119.3273,lwr_k=900:120.5242,lwr_k=1000:119.7299'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:125.1811,lwr_k=200:116.1414,lwr_k=300:115.5566,lwr_k=400:116.0084,lwr_k=500:115.972,lwr_k=600:115.6472,lwr_k=700:116.7835,lwr_k=800:115.9345,lwr_k=900:115.6647,lwr_k=1000:115.6813'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:352.26,lwr_k=200:352.0487,lwr_k=300:351.9734,lwr_k=400:352.3741,lwr_k=500:352.2368,lwr_k=600:352.4188,lwr_k=700:352.2593,lwr_k=800:352.3445,lwr_k=900:352.0386,lwr_k=1000:352.1666'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:379.7377,lwr_k=200:379.7178,lwr_k=300:379.7723,lwr_k=400:379.7775,lwr_k=500:379.7312,lwr_k=600:379.7959,lwr_k=700:379.7375,lwr_k=800:379.7661,lwr_k=900:379.7208,lwr_k=1000:379.7162'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:101.1332,lwr_k=200:108.4286,lwr_k=300:110.8509,lwr_k=400:112.5705,lwr_k=500:114.5904,lwr_k=600:115.6455,lwr_k=700:116.3026,lwr_k=800:116.7436,lwr_k=900:117.4292,lwr_k=1000:117.9556'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:109.1527,lwr_k=200:118.1065,lwr_k=300:115.4095,lwr_k=400:116.0522,lwr_k=500:117.8644,lwr_k=600:119.0058,lwr_k=700:119.6171,lwr_k=800:119.9757,lwr_k=900:120.5696,lwr_k=1000:120.9534'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_90'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:149.4025,lwr_k=200:176.5434,lwr_k=300:220.0531,lwr_k=400:227.3895,lwr_k=500:234.0592,lwr_k=600:238.9829,lwr_k=700:243.5637,lwr_k=800:246.7642,lwr_k=900:249.2809,lwr_k=1000:250.0918'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:166.6038,lwr_k=200:170.9805,lwr_k=300:223.4133,lwr_k=400:227.7749,lwr_k=500:233.9256,lwr_k=600:241.6683,lwr_k=700:243.6171,lwr_k=800:245.8478,lwr_k=900:251.6956,lwr_k=1000:253.9047'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:57.3022,lwr_k=200:58.2086,lwr_k=300:58.8348,lwr_k=400:59.0938,lwr_k=500:59.0848,lwr_k=600:59.1805,lwr_k=700:59.2944,lwr_k=800:59.4033,lwr_k=900:59.5376,lwr_k=1000:59.5669'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:151.5056,lwr_k=200:148.7159,lwr_k=300:147.767,lwr_k=400:148.1316,lwr_k=500:148.0179,lwr_k=600:147.6191,lwr_k=700:147.5028,lwr_k=800:147.6349,lwr_k=900:147.1304,lwr_k=1000:146.9768'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:155.7054,lwr_k=200:173.6165,lwr_k=300:188.08,lwr_k=400:203.3101,lwr_k=500:213.1371,lwr_k=600:221.7753,lwr_k=700:229.0538,lwr_k=800:233.9244,lwr_k=900:238.139,lwr_k=1000:241.5768'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:182.5462,lwr_k=200:184.5087,lwr_k=300:195.6826,lwr_k=400:219.7588,lwr_k=500:218.1989,lwr_k=600:219.214,lwr_k=700:223.7711,lwr_k=800:226.3636,lwr_k=900:230.2946,lwr_k=1000:231.5377'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:151.092,lwr_k=200:178.0739,lwr_k=300:187.7705,lwr_k=400:197.7201,lwr_k=500:206.8673,lwr_k=600:216.5363,lwr_k=700:227.5085,lwr_k=800:232.8342,lwr_k=900:230.0269,lwr_k=1000:233.1271'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:143.6536,lwr_k=200:164.4762,lwr_k=300:165.7575,lwr_k=400:164.7958,lwr_k=500:166.5211,lwr_k=600:172.0621,lwr_k=700:174.9409,lwr_k=800:178.8646,lwr_k=900:181.513,lwr_k=1000:184.675'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:98.4504,lwr_k=200:102.4217,lwr_k=300:104.7092,lwr_k=400:106.0328,lwr_k=500:107.2278,lwr_k=600:108.236,lwr_k=700:108.995,lwr_k=800:109.9775,lwr_k=900:110.4835,lwr_k=1000:111.0752'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:109.173,lwr_k=200:106.7029,lwr_k=300:106.7216,lwr_k=400:107.0812,lwr_k=500:107.4373,lwr_k=600:107.9723,lwr_k=700:108.39,lwr_k=800:108.6507,lwr_k=900:109.3397,lwr_k=1000:109.3542'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:88.8982,lwr_k=200:92.8976,lwr_k=300:94.6078,lwr_k=400:95.6058,lwr_k=500:96.2442,lwr_k=600:96.8577,lwr_k=700:97.1513,lwr_k=800:97.6738,lwr_k=900:98.03,lwr_k=1000:98.2303'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:105.9576,lwr_k=200:105.4952,lwr_k=300:103.844,lwr_k=400:103.4854,lwr_k=500:102.6444,lwr_k=600:102.3938,lwr_k=700:102.1515,lwr_k=800:102.2942,lwr_k=900:101.791,lwr_k=1000:101.7914'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_91'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:124.261,lwr_k=200:125.0053,lwr_k=300:125.361,lwr_k=400:125.5799,lwr_k=500:125.5023,lwr_k=600:125.729,lwr_k=700:125.8227,lwr_k=800:125.8169,lwr_k=900:125.7357,lwr_k=1000:125.9001'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:125.0485,lwr_k=200:124.8201,lwr_k=300:124.5764,lwr_k=400:124.4109,lwr_k=500:124.386,lwr_k=600:124.1215,lwr_k=700:124.1734,lwr_k=800:127.9843,lwr_k=900:127.9011,lwr_k=1000:128.0858'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:91.6379,lwr_k=200:93.9274,lwr_k=300:94.8489,lwr_k=400:95.4549,lwr_k=500:95.9103,lwr_k=600:96.5479,lwr_k=700:96.9696,lwr_k=800:97.4797,lwr_k=900:97.9556,lwr_k=1000:98.3025'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:154.9552,lwr_k=200:150.813,lwr_k=300:151.0847,lwr_k=400:152.8877,lwr_k=500:153.6823,lwr_k=600:153.7902,lwr_k=700:154.575,lwr_k=800:154.8534,lwr_k=900:154.9751,lwr_k=1000:154.7634'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:84.0474,lwr_k=200:88.8636,lwr_k=300:89.881,lwr_k=400:90.503,lwr_k=500:90.8874,lwr_k=600:91.2237,lwr_k=700:91.7387,lwr_k=800:91.8449,lwr_k=900:91.901,lwr_k=1000:92.1034'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:111.3908,lwr_k=200:108.4654,lwr_k=300:107.1872,lwr_k=400:106.5679,lwr_k=500:106.6021,lwr_k=600:106.2078,lwr_k=700:106.0339,lwr_k=800:106.5173,lwr_k=900:106.3484,lwr_k=1000:106.3324'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:297.0881,lwr_k=200:299.0193,lwr_k=300:299.9866,lwr_k=400:302.4338,lwr_k=500:305.1705,lwr_k=600:308.2381,lwr_k=700:310.5609,lwr_k=800:312.8951,lwr_k=900:314.2206,lwr_k=1000:315.6675'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:239.5992,lwr_k=200:239.4357,lwr_k=300:235.6393,lwr_k=400:234.5994,lwr_k=500:233.5161,lwr_k=600:233.8843,lwr_k=700:234.8658,lwr_k=800:236.5739,lwr_k=900:237.6189,lwr_k=1000:238.7437'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:133.3762,lwr_k=200:136.2109,lwr_k=300:137.2899,lwr_k=400:138.6045,lwr_k=500:139.7678,lwr_k=600:140.7181,lwr_k=700:142.9125,lwr_k=800:143.1603,lwr_k=900:143.6957,lwr_k=1000:144.1156'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:112.5095,lwr_k=200:113.3335,lwr_k=300:113.0193,lwr_k=400:112.4885,lwr_k=500:112.6257,lwr_k=600:112.9757,lwr_k=700:126.1675,lwr_k=800:126.6958,lwr_k=900:126.9201,lwr_k=1000:127.0441'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:124.8621,lwr_k=200:126.9437,lwr_k=300:128.5353,lwr_k=400:129.4699,lwr_k=500:129.8801,lwr_k=600:130.3499,lwr_k=700:130.6583,lwr_k=800:130.9832,lwr_k=900:131.1855,lwr_k=1000:131.3992'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:77.5793,lwr_k=200:76.4698,lwr_k=300:75.6892,lwr_k=400:75.4368,lwr_k=500:75.1539,lwr_k=600:74.842,lwr_k=700:74.6531,lwr_k=800:74.8823,lwr_k=900:74.7533,lwr_k=1000:74.4978'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_92'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:112.7701,lwr_k=200:94.283,lwr_k=300:92.4701,lwr_k=400:92.5591,lwr_k=500:92.8197,lwr_k=600:87.5538,lwr_k=700:90.6817,lwr_k=800:87.6674,lwr_k=900:88.2225,lwr_k=1000:89.7778'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:306.0691,lwr_k=200:177.9973,lwr_k=300:162.3482,lwr_k=400:154.2142,lwr_k=500:144.4886,lwr_k=600:145.8487,lwr_k=700:140.8477,lwr_k=800:133.6409,lwr_k=900:135.9613,lwr_k=1000:137.7853'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:113.265,lwr_k=200:94.6674,lwr_k=300:86.0807,lwr_k=400:92.4122,lwr_k=500:83.6937,lwr_k=600:76.9589,lwr_k=700:77.0887,lwr_k=800:78.8937,lwr_k=900:75.7337,lwr_k=1000:74.6571'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:232.973,lwr_k=200:183.5632,lwr_k=300:177.5901,lwr_k=400:162.607,lwr_k=500:154.671,lwr_k=600:156.1576,lwr_k=700:155.1055,lwr_k=800:156.3143,lwr_k=900:150.4117,lwr_k=1000:144.391'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:101.5893,lwr_k=200:83.6262,lwr_k=300:85.806,lwr_k=400:80.7282,lwr_k=500:84.7727,lwr_k=600:84.8243,lwr_k=700:78.1226,lwr_k=800:82.2796,lwr_k=900:84.2706,lwr_k=1000:72.8059'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:214.072,lwr_k=200:183.7221,lwr_k=300:183.0796,lwr_k=400:142.1144,lwr_k=500:169.702,lwr_k=600:163.6278,lwr_k=700:160.7434,lwr_k=800:130.3091,lwr_k=900:118.9496,lwr_k=1000:135.7218'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:101.5741,lwr_k=200:86.8802,lwr_k=300:96.182,lwr_k=400:84.7162,lwr_k=500:90.4179,lwr_k=600:78.8985,lwr_k=700:80.8316,lwr_k=800:79.3732,lwr_k=900:80.9166,lwr_k=1000:78.7102'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:221.2353,lwr_k=200:169.6533,lwr_k=300:171.819,lwr_k=400:154.2837,lwr_k=500:136.2043,lwr_k=600:142.5977,lwr_k=700:142.7302,lwr_k=800:141.5726,lwr_k=900:142.1957,lwr_k=1000:142.9113'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:109.8989,lwr_k=200:117.4211,lwr_k=300:120.8323,lwr_k=400:107.804,lwr_k=500:112.1598,lwr_k=600:101.5546,lwr_k=700:106.2331,lwr_k=800:102.0237,lwr_k=900:102.5402,lwr_k=1000:98.523'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:286.6079,lwr_k=200:259.5405,lwr_k=300:223.2157,lwr_k=400:219.9532,lwr_k=500:213.6494,lwr_k=600:214.7835,lwr_k=700:122.6292,lwr_k=800:113.4039,lwr_k=900:121.8633,lwr_k=1000:113.5066'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:134.3113,lwr_k=200:108.3781,lwr_k=300:103.4052,lwr_k=400:121.0999,lwr_k=500:110.8453,lwr_k=600:118.1046,lwr_k=700:100.9771,lwr_k=800:97.9613,lwr_k=900:94.8219,lwr_k=1000:94.6746'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:290.4251,lwr_k=200:257.6278,lwr_k=300:199.829,lwr_k=400:161.4536,lwr_k=500:262.9185,lwr_k=600:242.7251,lwr_k=700:236.0372,lwr_k=800:216.5082,lwr_k=900:220.8607,lwr_k=1000:229.9349'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_93'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:117.4272,lwr_k=200:91.2133,lwr_k=300:87.589,lwr_k=400:86.2939,lwr_k=500:88.354,lwr_k=600:88.5145,lwr_k=700:88.9134,lwr_k=800:87.8776,lwr_k=900:89.0307,lwr_k=1000:90.1247'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:220.9064,lwr_k=200:121.4128,lwr_k=300:191.1127,lwr_k=400:105.4237,lwr_k=500:183.7676,lwr_k=600:110.4261,lwr_k=700:99.9839,lwr_k=800:95.6913,lwr_k=900:95.285,lwr_k=1000:93.6738'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:56.8208,lwr_k=200:55.2244,lwr_k=300:56.6688,lwr_k=400:54.6293,lwr_k=500:54.0767,lwr_k=600:52.8711,lwr_k=700:52.8323,lwr_k=800:51.8389,lwr_k=900:52.2722,lwr_k=1000:52.4595'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:218.9921,lwr_k=200:166.4775,lwr_k=300:157.772,lwr_k=400:153.4879,lwr_k=500:151.7813,lwr_k=600:148.8931,lwr_k=700:147.6192,lwr_k=800:143.7792,lwr_k=900:142.8165,lwr_k=1000:143.3981'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:77.564,lwr_k=200:95.5614,lwr_k=300:86.6642,lwr_k=400:97.3373,lwr_k=500:104.3414,lwr_k=600:95.6385,lwr_k=700:97.3467,lwr_k=800:107.6381,lwr_k=900:101.0168,lwr_k=1000:99.561'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:222.719,lwr_k=200:182.638,lwr_k=300:140.5246,lwr_k=400:141.6733,lwr_k=500:161.0319,lwr_k=600:139.0045,lwr_k=700:130.1691,lwr_k=800:137.8532,lwr_k=900:131.6084,lwr_k=1000:137.2976'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:37.4991,lwr_k=200:36.8727,lwr_k=300:36.6946,lwr_k=400:36.4454,lwr_k=500:37.085,lwr_k=600:37.3608,lwr_k=700:37.5659,lwr_k=800:37.7388,lwr_k=900:37.9242,lwr_k=1000:38.2004'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:144.8975,lwr_k=200:106.5497,lwr_k=300:117.7778,lwr_k=400:114.7808,lwr_k=500:111.6244,lwr_k=600:109.2986,lwr_k=700:105.463,lwr_k=800:104.9932,lwr_k=900:104.2464,lwr_k=1000:103.3455'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:146.2792,lwr_k=200:132.619,lwr_k=300:152.5222,lwr_k=400:118.6724,lwr_k=500:118.3583,lwr_k=600:117.3876,lwr_k=700:124.8092,lwr_k=800:129.201,lwr_k=900:137.3625,lwr_k=1000:136.7137'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:407.65,lwr_k=200:330.8593,lwr_k=300:271.023,lwr_k=400:166.1538,lwr_k=500:173.505,lwr_k=600:132.487,lwr_k=700:133.2503,lwr_k=800:157.764,lwr_k=900:140.8575,lwr_k=1000:156.9764'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:38.2303,lwr_k=200:57.2943,lwr_k=300:64.2462,lwr_k=400:68.749,lwr_k=500:72.1752,lwr_k=600:74.0645,lwr_k=700:78.2101,lwr_k=800:80.0997,lwr_k=900:79.5594,lwr_k=1000:80.5425'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:190.4001,lwr_k=200:111.6597,lwr_k=300:146.0748,lwr_k=400:133.945,lwr_k=500:145.8282,lwr_k=600:128.1114,lwr_k=700:176.7288,lwr_k=800:171.0948,lwr_k=900:157.0415,lwr_k=1000:155.1754'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_94'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:76.1344,lwr_k=200:83.6308,lwr_k=300:88.9868,lwr_k=400:90.7854,lwr_k=500:91.7809,lwr_k=600:94.0412,lwr_k=700:92.9339,lwr_k=800:89.7586,lwr_k=900:90.6543,lwr_k=1000:90.2319'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:270.3957,lwr_k=200:250.8026,lwr_k=300:139.6755,lwr_k=400:144.9903,lwr_k=500:155.59,lwr_k=600:177.8956,lwr_k=700:163.3828,lwr_k=800:144.3889,lwr_k=900:135.6474,lwr_k=1000:135.7794'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:71.9687,lwr_k=200:78.1197,lwr_k=300:76.0292,lwr_k=400:77.1429,lwr_k=500:75.0841,lwr_k=600:74.6402,lwr_k=700:74.7701,lwr_k=800:75.8754,lwr_k=900:76.065,lwr_k=1000:76.6827'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:247.224,lwr_k=200:186.0522,lwr_k=300:169.8756,lwr_k=400:173.3075,lwr_k=500:170.6596,lwr_k=600:172.065,lwr_k=700:172.1647,lwr_k=800:173.2643,lwr_k=900:158.466,lwr_k=1000:158.1725'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:46.5414,lwr_k=200:55.0957,lwr_k=300:59.2985,lwr_k=400:61.4911,lwr_k=500:62.4391,lwr_k=600:63.8041,lwr_k=700:64.4311,lwr_k=800:66.034,lwr_k=900:66.5087,lwr_k=1000:67.3043'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:145.2641,lwr_k=200:139.539,lwr_k=300:115.5772,lwr_k=400:112.4743,lwr_k=500:108.1173,lwr_k=600:110.3739,lwr_k=700:102.6674,lwr_k=800:100.4338,lwr_k=900:99.0516,lwr_k=1000:103.7552'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:136.3097,lwr_k=200:116.0846,lwr_k=300:145.4101,lwr_k=400:150.2317,lwr_k=500:139.4972,lwr_k=600:110.5875,lwr_k=700:105.45,lwr_k=800:99.1271,lwr_k=900:97.8,lwr_k=1000:94.8083'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:204.2362,lwr_k=200:141.787,lwr_k=300:141.2068,lwr_k=400:147.6516,lwr_k=500:126.668,lwr_k=600:101.4236,lwr_k=700:110.8355,lwr_k=800:101.6294,lwr_k=900:99.3393,lwr_k=1000:98.705'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:68.5116,lwr_k=200:79.5263,lwr_k=300:83.9719,lwr_k=400:87.8175,lwr_k=500:89.9321,lwr_k=600:91.7876,lwr_k=700:93.7636,lwr_k=800:94.0802,lwr_k=900:94.052,lwr_k=1000:93.3428'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:171.5201,lwr_k=200:112.8743,lwr_k=300:112.7349,lwr_k=400:112.1041,lwr_k=500:109.1715,lwr_k=600:110.7562,lwr_k=700:107.2934,lwr_k=800:109.5378,lwr_k=900:107.6689,lwr_k=1000:108.3921'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:75.3249,lwr_k=200:75.214,lwr_k=300:79.4458,lwr_k=400:81.1008,lwr_k=500:79.7512,lwr_k=600:79.3021,lwr_k=700:83.5322,lwr_k=800:82.3285,lwr_k=900:82.3768,lwr_k=1000:85.5697'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:170.2521,lwr_k=200:123.3728,lwr_k=300:108.343,lwr_k=400:115.3432,lwr_k=500:101.529,lwr_k=600:107.7727,lwr_k=700:137.8985,lwr_k=800:111.5895,lwr_k=900:120.813,lwr_k=1000:133.6504'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_95'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:74.1316,lwr_k=200:88.1178,lwr_k=300:94.5089,lwr_k=400:97.7857,lwr_k=500:99.485,lwr_k=600:100.9642,lwr_k=700:101.916,lwr_k=800:102.618,lwr_k=900:103.3414,lwr_k=1000:103.8032'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:142.4993,lwr_k=200:122.7155,lwr_k=300:118.1408,lwr_k=400:117.5022,lwr_k=500:117.9266,lwr_k=600:118.0127,lwr_k=700:117.5911,lwr_k=800:117.1933,lwr_k=900:117.1154,lwr_k=1000:116.646'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:54.4454,lwr_k=200:66.4143,lwr_k=300:71.6036,lwr_k=400:73.8412,lwr_k=500:76.7123,lwr_k=600:78.6502,lwr_k=700:80.2168,lwr_k=800:81.2691,lwr_k=900:82.3757,lwr_k=1000:82.9914'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:178.0881,lwr_k=200:164.3345,lwr_k=300:155.1716,lwr_k=400:155.4447,lwr_k=500:153.1203,lwr_k=600:152.5451,lwr_k=700:152.6494,lwr_k=800:152.5715,lwr_k=900:153.374,lwr_k=1000:153.7991'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:55.4535,lwr_k=200:66.4193,lwr_k=300:73.7763,lwr_k=400:75.9109,lwr_k=500:77.9569,lwr_k=600:80.0429,lwr_k=700:80.8874,lwr_k=800:81.9496,lwr_k=900:82.967,lwr_k=1000:83.5237'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:133.9591,lwr_k=200:108.8214,lwr_k=300:104.5567,lwr_k=400:102.1097,lwr_k=500:101.2509,lwr_k=600:100.2573,lwr_k=700:99.9389,lwr_k=800:100.4583,lwr_k=900:100.8171,lwr_k=1000:100.3921'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:56.0002,lwr_k=200:70.8438,lwr_k=300:77.0901,lwr_k=400:79.9233,lwr_k=500:81.9904,lwr_k=600:83.1723,lwr_k=700:85.8068,lwr_k=800:87.9717,lwr_k=900:88.5631,lwr_k=1000:89.4553'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:143.7305,lwr_k=200:115.7372,lwr_k=300:111.1046,lwr_k=400:109.1643,lwr_k=500:106.9091,lwr_k=600:107.3807,lwr_k=700:105.4951,lwr_k=800:106.2224,lwr_k=900:105.9153,lwr_k=1000:104.9947'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:80.018,lwr_k=200:96.8706,lwr_k=300:106.2604,lwr_k=400:110.6325,lwr_k=500:114.7213,lwr_k=600:117.2938,lwr_k=700:118.6056,lwr_k=800:119.4827,lwr_k=900:120.3425,lwr_k=1000:121.3669'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:139.9017,lwr_k=200:103.5929,lwr_k=300:106.4408,lwr_k=400:109.4168,lwr_k=500:106.3593,lwr_k=600:107.7612,lwr_k=700:110.6025,lwr_k=800:111.2665,lwr_k=900:112.4246,lwr_k=1000:112.8175'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:66.1691,lwr_k=200:77.5851,lwr_k=300:83.5035,lwr_k=400:86.6679,lwr_k=500:88.6666,lwr_k=600:90.2963,lwr_k=700:92.0639,lwr_k=800:93.0722,lwr_k=900:94.1173,lwr_k=1000:95.8841'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:114.4306,lwr_k=200:93.9591,lwr_k=300:91.4257,lwr_k=400:90.3075,lwr_k=500:88.9458,lwr_k=600:88.0471,lwr_k=700:86.3009,lwr_k=800:85.0197,lwr_k=900:84.1516,lwr_k=1000:83.9066'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_96'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:56.8298,lwr_k=200:67.2219,lwr_k=300:69.9004,lwr_k=400:77.4937,lwr_k=500:82.7934,lwr_k=600:78.3981,lwr_k=700:77.8727,lwr_k=800:78.9633,lwr_k=900:79.7903,lwr_k=1000:79.9346'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:224.3395,lwr_k=200:277.0563,lwr_k=300:175.7762,lwr_k=400:169.871,lwr_k=500:163.5877,lwr_k=600:164.0854,lwr_k=700:164.5529,lwr_k=800:272.3214,lwr_k=900:271.6802,lwr_k=1000:188.5864'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:46.5923,lwr_k=200:56.4388,lwr_k=300:57.6931,lwr_k=400:61.1632,lwr_k=500:62.9959,lwr_k=600:64.3561,lwr_k=700:65.2036,lwr_k=800:66.3212,lwr_k=900:66.9677,lwr_k=1000:66.9238'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:199.4112,lwr_k=200:180.4642,lwr_k=300:158.3012,lwr_k=400:154.0099,lwr_k=500:156.452,lwr_k=600:153.5274,lwr_k=700:151.4732,lwr_k=800:153.4192,lwr_k=900:154.6593,lwr_k=1000:154.9353'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:81.0378,lwr_k=200:81.8445,lwr_k=300:81.6929,lwr_k=400:89.1341,lwr_k=500:90.7435,lwr_k=600:89.9103,lwr_k=700:86.4743,lwr_k=800:87.2736,lwr_k=900:87.4341,lwr_k=1000:91.3585'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:187.6236,lwr_k=200:147.2425,lwr_k=300:161.9437,lwr_k=400:162.6155,lwr_k=500:163.5611,lwr_k=600:147.1989,lwr_k=700:148.1195,lwr_k=800:138.2377,lwr_k=900:146.258,lwr_k=1000:138.1336'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:120.905,lwr_k=200:100.8827,lwr_k=300:98.2524,lwr_k=400:101.4618,lwr_k=500:99.9844,lwr_k=600:100.5074,lwr_k=700:103.3014,lwr_k=800:102.985,lwr_k=900:105.4303,lwr_k=1000:105.8589'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:236.0548,lwr_k=200:171.7614,lwr_k=300:146.6775,lwr_k=400:136.4864,lwr_k=500:140.7358,lwr_k=600:126.8198,lwr_k=700:133.7271,lwr_k=800:123.3641,lwr_k=900:136.5908,lwr_k=1000:125.4363'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:100.1067,lwr_k=200:92.4495,lwr_k=300:92.315,lwr_k=400:92.2459,lwr_k=500:93.2902,lwr_k=600:94.236,lwr_k=700:96.1765,lwr_k=800:95.5558,lwr_k=900:96.1729,lwr_k=1000:98.0495'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:133.1334,lwr_k=200:115.5033,lwr_k=300:95.288,lwr_k=400:98.2567,lwr_k=500:97.151,lwr_k=600:100.7623,lwr_k=700:112.5652,lwr_k=800:98.8652,lwr_k=900:110.0139,lwr_k=1000:104.5801'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:51.0711,lwr_k=200:58.7524,lwr_k=300:64.1036,lwr_k=400:66.8697,lwr_k=500:68.5652,lwr_k=600:70.1359,lwr_k=700:71.4814,lwr_k=800:72.414,lwr_k=900:73.1935,lwr_k=1000:73.7264'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:154.1975,lwr_k=200:106.4176,lwr_k=300:90.3288,lwr_k=400:88.3896,lwr_k=500:85.3983,lwr_k=600:83.4291,lwr_k=700:83.1281,lwr_k=800:82.7502,lwr_k=900:93.4319,lwr_k=1000:93.1867'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_97'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:78.4642,lwr_k=200:94.3958,lwr_k=300:99.153,lwr_k=400:101.465,lwr_k=500:103.1098,lwr_k=600:104.2084,lwr_k=700:104.719,lwr_k=800:105.837,lwr_k=900:106.471,lwr_k=1000:106.8946'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:137.6525,lwr_k=200:114.4503,lwr_k=300:106.7292,lwr_k=400:105.1457,lwr_k=500:104.3318,lwr_k=600:104.2633,lwr_k=700:103.2614,lwr_k=800:103.2859,lwr_k=900:103.178,lwr_k=1000:103.4006'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:73.8935,lwr_k=200:79.9083,lwr_k=300:82.545,lwr_k=400:83.7353,lwr_k=500:83.9364,lwr_k=600:84.9876,lwr_k=700:85.3979,lwr_k=800:85.6591,lwr_k=900:85.7942,lwr_k=1000:85.8861'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:289.9429,lwr_k=200:232.9721,lwr_k=300:183.7104,lwr_k=400:174.3137,lwr_k=500:177.2994,lwr_k=600:158.1642,lwr_k=700:162.3569,lwr_k=800:152.7514,lwr_k=900:156.6669,lwr_k=1000:152.9761'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:47.3303,lwr_k=200:73.2123,lwr_k=300:79.9868,lwr_k=400:82.0128,lwr_k=500:83.4828,lwr_k=600:85.277,lwr_k=700:85.7254,lwr_k=800:86.403,lwr_k=900:87.4968,lwr_k=1000:88.4399'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:148.6539,lwr_k=200:130.966,lwr_k=300:128.4345,lwr_k=400:125.6625,lwr_k=500:122.834,lwr_k=600:122.3307,lwr_k=700:122.9827,lwr_k=800:122.7173,lwr_k=900:122.8491,lwr_k=1000:121.7361'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:88.1608,lwr_k=200:93.875,lwr_k=300:99.8419,lwr_k=400:103.5223,lwr_k=500:106.9932,lwr_k=600:109.2669,lwr_k=700:111.1989,lwr_k=800:112.1722,lwr_k=900:113.2455,lwr_k=1000:114.1681'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:220.7031,lwr_k=200:189.839,lwr_k=300:183.8919,lwr_k=400:182.0071,lwr_k=500:178.6038,lwr_k=600:169.2099,lwr_k=700:164.0564,lwr_k=800:156.9523,lwr_k=900:147.7731,lwr_k=1000:146.0334'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:95.8961,lwr_k=200:98.6878,lwr_k=300:104.0523,lwr_k=400:108.0807,lwr_k=500:103.7145,lwr_k=600:107.6113,lwr_k=700:108.8304,lwr_k=800:108.6303,lwr_k=900:109.5809,lwr_k=1000:110.1175'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:122.217,lwr_k=200:122.0034,lwr_k=300:106.8199,lwr_k=400:115.1125,lwr_k=500:114.5279,lwr_k=600:113.9998,lwr_k=700:118.369,lwr_k=800:114.2494,lwr_k=900:114.014,lwr_k=1000:113.6175'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:77.5365,lwr_k=200:89.9548,lwr_k=300:93.7612,lwr_k=400:96.6551,lwr_k=500:98.3147,lwr_k=600:99.4707,lwr_k=700:100.0415,lwr_k=800:100.9184,lwr_k=900:101.4628,lwr_k=1000:102.1681'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:118.4023,lwr_k=200:104.2072,lwr_k=300:117.793,lwr_k=400:119.3596,lwr_k=500:117.8725,lwr_k=600:117.0928,lwr_k=700:118.8057,lwr_k=800:118.3407,lwr_k=900:118.1756,lwr_k=1000:119.9571'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_98'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:36.0154,lwr_k=200:44.5881,lwr_k=300:48.1205,lwr_k=400:49.7494,lwr_k=500:51.3515,lwr_k=600:51.8977,lwr_k=700:52.3407,lwr_k=800:52.7905,lwr_k=900:53.0591,lwr_k=1000:53.391'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:148.5349,lwr_k=200:133.0496,lwr_k=300:133.8548,lwr_k=400:132.6026,lwr_k=500:131.6845,lwr_k=600:133.9921,lwr_k=700:134.0916,lwr_k=800:134.6464,lwr_k=900:134.7281,lwr_k=1000:134.537'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:38.5399,lwr_k=200:49.623,lwr_k=300:54.318,lwr_k=400:55.9793,lwr_k=500:57.7142,lwr_k=600:58.5891,lwr_k=700:59.0862,lwr_k=800:59.6263,lwr_k=900:60.1211,lwr_k=1000:60.6751'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:197.8686,lwr_k=200:168.1743,lwr_k=300:164.08,lwr_k=400:164.7936,lwr_k=500:164.1824,lwr_k=600:162.8671,lwr_k=700:163.7885,lwr_k=800:162.8214,lwr_k=900:161.9196,lwr_k=1000:161.5747'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:37.9837,lwr_k=200:47.6504,lwr_k=300:51.3524,lwr_k=400:53.5608,lwr_k=500:54.6831,lwr_k=600:55.5484,lwr_k=700:56.3349,lwr_k=800:57.0009,lwr_k=900:57.4724,lwr_k=1000:57.737'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:102.0939,lwr_k=200:93.6208,lwr_k=300:94.4867,lwr_k=400:95.3669,lwr_k=500:95.536,lwr_k=600:97.0799,lwr_k=700:98.2433,lwr_k=800:99.9829,lwr_k=900:101.1476,lwr_k=1000:102.1277'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:39.3471,lwr_k=200:49.184,lwr_k=300:52.4486,lwr_k=400:54.4358,lwr_k=500:55.8843,lwr_k=600:56.8467,lwr_k=700:57.2441,lwr_k=800:57.7627,lwr_k=900:58.2598,lwr_k=1000:58.4847'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:110.5705,lwr_k=200:97.3742,lwr_k=300:94.9697,lwr_k=400:93.2238,lwr_k=500:93.3488,lwr_k=600:92.7957,lwr_k=700:93.1542,lwr_k=800:93.1636,lwr_k=900:95.1958,lwr_k=1000:95.5193'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:45.8968,lwr_k=200:62.234,lwr_k=300:67.8803,lwr_k=400:70.8786,lwr_k=500:72.9859,lwr_k=600:73.6435,lwr_k=700:74.4968,lwr_k=800:74.8964,lwr_k=900:75.5083,lwr_k=1000:75.728'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:123.9983,lwr_k=200:92.4099,lwr_k=300:89.0253,lwr_k=400:86.9597,lwr_k=500:86.1133,lwr_k=600:87.6199,lwr_k=700:87.9707,lwr_k=800:88.0662,lwr_k=900:88.1161,lwr_k=1000:87.941'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:46.1525,lwr_k=200:60.0595,lwr_k=300:64.6564,lwr_k=400:66.6648,lwr_k=500:67.9076,lwr_k=600:68.9313,lwr_k=700:69.5454,lwr_k=800:70.3602,lwr_k=900:70.7168,lwr_k=1000:71.1137'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:187.1701,lwr_k=200:110.8614,lwr_k=300:104.3716,lwr_k=400:114.7214,lwr_k=500:107.7508,lwr_k=600:104.9869,lwr_k=700:106.4409,lwr_k=800:105.8215,lwr_k=900:103.5792,lwr_k=1000:103.0295'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_99'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:110.6597,lwr_k=200:121.2318,lwr_k=300:127.141,lwr_k=400:130.2119,lwr_k=500:132.3098,lwr_k=600:134.8386,lwr_k=700:136.4651,lwr_k=800:137.5208,lwr_k=900:138.4892,lwr_k=1000:139.1505'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:145.6896,lwr_k=200:149.0779,lwr_k=300:147.3895,lwr_k=400:149.0618,lwr_k=500:151.0698,lwr_k=600:152.2881,lwr_k=700:152.0944,lwr_k=800:151.378,lwr_k=900:152.9139,lwr_k=1000:152.495'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:91.9948,lwr_k=200:99.0455,lwr_k=300:102.1966,lwr_k=400:103.9611,lwr_k=500:107.0064,lwr_k=600:107.4873,lwr_k=700:108.2868,lwr_k=800:108.6755,lwr_k=900:108.9803,lwr_k=1000:109.2685'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:166.1628,lwr_k=200:162.9747,lwr_k=300:164.2476,lwr_k=400:167.3063,lwr_k=500:172.5212,lwr_k=600:172.7084,lwr_k=700:173.1201,lwr_k=800:173.3053,lwr_k=900:173.7271,lwr_k=1000:174.3803'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:125.0632,lwr_k=200:141.9305,lwr_k=300:153.0819,lwr_k=400:160.3848,lwr_k=500:166.849,lwr_k=600:172.6564,lwr_k=700:178.839,lwr_k=800:182.5981,lwr_k=900:185.238,lwr_k=1000:187.982'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:154.0614,lwr_k=200:153.7049,lwr_k=300:154.1031,lwr_k=400:158.3422,lwr_k=500:162.692,lwr_k=600:166.4404,lwr_k=700:170.5245,lwr_k=800:173.1524,lwr_k=900:174.9148,lwr_k=1000:177.0495'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:145.7586,lwr_k=200:165.1849,lwr_k=300:172.1125,lwr_k=400:178.4707,lwr_k=500:194.0397,lwr_k=600:196.6928,lwr_k=700:199.5422,lwr_k=800:202.2284,lwr_k=900:205.3333,lwr_k=1000:211.0564'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:152.9565,lwr_k=200:141.1108,lwr_k=300:140.3777,lwr_k=400:140.7941,lwr_k=500:145.8341,lwr_k=600:146.5293,lwr_k=700:147.7199,lwr_k=800:149.8371,lwr_k=900:153.1027,lwr_k=1000:152.4473'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:112.4031,lwr_k=200:122.6112,lwr_k=300:128.5152,lwr_k=400:131.0831,lwr_k=500:133.1819,lwr_k=600:134.7428,lwr_k=700:136.4643,lwr_k=800:138.1878,lwr_k=900:139.4315,lwr_k=1000:141.5511'\n",
      "Tested (test) on 2319 instances with mean losses of: lwr_k=100:116.9567,lwr_k=200:122.7387,lwr_k=300:124.826,lwr_k=400:126.1011,lwr_k=500:128.2427,lwr_k=600:127.7368,lwr_k=700:129.3418,lwr_k=800:131.2035,lwr_k=900:132.8563,lwr_k=1000:135.2575'\n",
      "Building final model - Train 9276 - Test 2320'\n",
      "Finished training DeepLWR with a train loss of lwr_k=100:105.5376,lwr_k=200:121.4462,lwr_k=300:127.4366,lwr_k=400:131.7964,lwr_k=500:134.561,lwr_k=600:136.7963,lwr_k=700:139.1802,lwr_k=800:141.1567,lwr_k=900:142.5431,lwr_k=1000:143.9754'\n",
      "Tested (test) on 2320 instances with mean losses of: lwr_k=100:124.6301,lwr_k=200:119.4143,lwr_k=300:122.6534,lwr_k=400:125.0638,lwr_k=500:124.5962,lwr_k=600:126.0454,lwr_k=700:129.4653,lwr_k=800:130.069,lwr_k=900:130.7018,lwr_k=1000:130.6772'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    }
   ],
   "source": [
    "for deep_name,deep_model in tqdm(deep_models.items()):\n",
    "        logging.getLogger().info(f\"Running model {deep_name}\")\n",
    "        temp_dict = {deep_name:deep_model}\n",
    "\n",
    "        lwr_scheme = DeepLWRScheme_1_to_n(lwr_models = setup_pls_models_slim(nrow,kernal=True),n_neighbours=500,loss_fun_sk = mean_squared_error)\n",
    "        lwr_scores, lwr_preds, _ , _, _,_= eval.evaluate(temp_dict,dataset,lwr_scheme,logger_name=\"log\",load_fun=load_fun_cv,load_fun_pp = load_fun_pp_cv)\n",
    "        lwr_scores_final, lwr_preds_final, _ , _, _,_= eval.build(temp_dict,dataset,lwr_scheme,logger_name=\"test_log\",load_fun=load_fun_build,load_fun_pp = load_fun_pp_build)\n",
    "\n",
    "        #scores\n",
    "        for k,v in ut.flip_dicts(lwr_scores).items():\n",
    "            dict1 = {'model_num':deep_name,\"predictor\":k}\n",
    "            all_scores.append({**dict1,**v})\n",
    "\n",
    "        for k,v in ut.flip_dicts(lwr_scores_final).items():\n",
    "            dict1 = {'model_num':deep_name,\"predictor\":k}\n",
    "            all_scores_final.append({**dict1,**v})\n",
    "\n",
    "        lwr_preds['deep'] = deep_preds[deep_name]\n",
    "        lwr_preds_final['deep'] = deep_preds_final[deep_name]\n",
    "\n",
    "        lwr_preds.to_csv(log_dir/deep_name/ f\"predictions.csv\",index=False)\n",
    "        lwr_preds_final.to_csv(log_dir/deep_name/ f\"predictions_test.csv\",index=False)\n",
    "\n",
    "        #preds\n",
    "        # todo save predictions - appending solns\n",
    "        plot_preds_and_res(lwr_preds,name_lambda=lambda x:f\"{deep_name} with {x} predictor\",save_lambda= lambda x:f\"deep_lwr{x}\",save_loc=log_dir/deep_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% save scores\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank - model_num - predictor - fold_0 - fold_1 - fold_2 - fold_3 - fold_4 - MSE - R2'\n",
      "0 - random_8 - lwr_k=800 - 96.50274565222863 - 140.04712136210648 - 101.63555161028458 - 106.83318457114252 - 83.76620429022118 - 105.75616344477817 - 0.6955615557388015'\n",
      "1 - random_8 - lwr_k=900 - 96.1411391779313 - 140.0827687764173 - 101.64263800210831 - 111.1609331621634 - 81.38623034706586 - 106.08188456278927 - 0.694623907972465'\n",
      "2 - random_60 - deep - 115.49591637973128 - 147.70492251035517 - 91.23585914259792 - 92.35466049806267 - 85.18894709940355 - 106.39684576148638 - 0.6937172342672797'\n",
      "3 - random_8 - lwr_k=1000 - 96.86717450805472 - 140.71985956733718 - 101.04622211173826 - 113.32143624212156 - 81.6731081381832 - 106.72470995950236 - 0.6927734175865394'\n",
      "4 - random_60 - lwr_k=600 - 121.8887074298133 - 147.0980301744677 - 89.48701820255773 - 97.69810132546326 - 79.74675078575416 - 107.18498969208366 - 0.6914484182564988'\n",
      "5 - random_60 - lwr_k=700 - 121.22297496192576 - 147.58820362692325 - 89.48084273647864 - 97.86457675028174 - 79.93962954052539 - 107.2204531577077 - 0.6913463301894678'\n",
      "6 - random_60 - lwr_k=1000 - 121.59885828076015 - 147.31700011404072 - 90.2083538332072 - 97.96540697480009 - 79.43470339848575 - 107.3060971861574 - 0.6910997882014436'\n",
      "7 - random_60 - lwr_k=800 - 120.96705023450812 - 147.67819266877817 - 90.05217962474475 - 98.22280194638255 - 80.20465690729631 - 107.42614409920832 - 0.6907542112226922'\n",
      "8 - random_60 - lwr_k=900 - 121.6404110114389 - 147.59065979691425 - 90.21672268840986 - 98.14853531073472 - 79.65092716676556 - 107.45067497545084 - 0.6906835946122141'\n",
      "9 - random_60 - lwr_k=500 - 122.3664265288857 - 147.99211895996368 - 89.7373446296189 - 97.72211463774634 - 80.00852954068759 - 107.56658325811013 - 0.6903499314374129'\n",
      "10 - random_62 - lwr_k=300 - 110.0277059180056 - 141.4032004181959 - 94.71183191315595 - 83.47616723029587 - 108.78576524606805 - 107.68113652284114 - 0.6900201689293586'\n",
      "11 - random_60 - lwr_k=400 - 120.54898855947201 - 148.8243264327391 - 91.10972303812201 - 98.52273245331071 - 80.3952787322329 - 107.88130235600013 - 0.6894439549967022'\n",
      "12 - random_40 - lwr_k=500 - 99.39242427551322 - 152.9049164754921 - 104.44388881516495 - 92.46424622924428 - 90.24295391673415 - 107.88895316719112 - 0.6894219307384419'\n",
      "13 - random_40 - lwr_k=300 - 99.96252118849307 - 152.57093721035324 - 104.33363556549669 - 92.20143424910265 - 91.46853078930951 - 108.10670941260585 - 0.6887950795893187'\n",
      "14 - random_40 - lwr_k=400 - 99.44973305916868 - 153.2218255847104 - 104.86998680859676 - 92.08948544153563 - 91.31715529692798 - 108.18888353827593 - 0.6885585263505982'\n",
      "15 - random_54 - deep - 122.67828775603196 - 141.09928568337898 - 95.2822361691546 - 84.50372972443166 - 97.66279256277015 - 108.24650952239685 - 0.6883926390848507'\n",
      "16 - random_2 - lwr_k=1000 - 114.91587813552692 - 141.1702804839602 - 89.53975429592961 - 97.53637652315031 - 98.46776095715242 - 108.32657836717911 - 0.6881621466207681'\n",
      "17 - random_40 - lwr_k=600 - 99.98655058478408 - 152.6680508734499 - 105.31907961137922 - 92.71483541724409 - 91.40029068886403 - 108.4170343559057 - 0.6879017525256627'\n",
      "18 - random_62 - lwr_k=400 - 109.06694347652736 - 141.29542771780743 - 94.42962938353588 - 84.90932588558167 - 112.67744209092807 - 108.47580469309112 - 0.687732571369325'\n",
      "19 - random_8 - lwr_k=500 - 100.63721520623533 - 137.02762301422874 - 103.0409750476096 - 112.17063620242558 - 89.68037306837952 - 108.51068546764972 - 0.6876321606850172'\n",
      "20 - random_40 - lwr_k=700 - 100.61628741993253 - 152.84167952038325 - 106.12189299907057 - 92.3423488346911 - 90.91255925125071 - 108.56626796638116 - 0.6874721563042723'\n",
      "21 - random_40 - lwr_k=800 - 100.25108803658439 - 152.69362148302176 - 106.15391721189998 - 92.66571097569424 - 91.46000268283025 - 108.64414422667448 - 0.6872479752565079'\n",
      "22 - random_8 - lwr_k=700 - 98.4399484334385 - 138.34520059980045 - 102.31274524134871 - 118.38029301104343 - 85.84080570614339 - 108.66291692793708 - 0.6871939346050622'\n",
      "23 - random_40 - lwr_k=900 - 100.81409386809459 - 152.7443551660868 - 106.22036260102067 - 92.3030040933638 - 91.33698329207819 - 108.68308115063311 - 0.6871358882048585'\n",
      "24 - random_40 - lwr_k=1000 - 100.78082084270869 - 152.75440111584504 - 106.34337515899738 - 92.13946541905167 - 92.3415592721602 - 108.87122661222526 - 0.6865942771085763'\n",
      "25 - random_2 - lwr_k=900 - 116.78413995120886 - 142.30622553158213 - 89.2875372476828 - 97.85928561196566 - 99.16911368800353 - 109.08192467648745 - 0.6859877442238398'\n",
      "26 - random_62 - lwr_k=500 - 109.35759206077547 - 141.6361298822022 - 93.00822760760028 - 85.95704345449131 - 115.46526667186966 - 109.08487545557797 - 0.6859792498669492'\n",
      "27 - random_40 - lwr_k=200 - 103.38353493152283 - 151.74954720849092 - 104.71255039048285 - 93.0944485139957 - 92.63636996865577 - 109.114795915352 - 0.6858931183552862'\n",
      "28 - random_62 - lwr_k=200 - 113.09125570095755 - 143.83264732495087 - 95.02172490781945 - 84.31767822137508 - 109.62120497776816 - 109.17723978723947 - 0.6857133622579408'\n",
      "29 - random_60 - lwr_k=300 - 119.9272843633937 - 149.56400722496974 - 91.49258111105074 - 102.19129856979805 - 82.87522330984919 - 109.21100313152871 - 0.6856161683008828'\n",
      "30 - random_62 - lwr_k=600 - 109.62854898498699 - 141.17411447299972 - 92.15482400447006 - 86.32519478234116 - 118.01841386950622 - 109.4602337390528 - 0.6848987124482361'\n",
      "31 - random_62 - lwr_k=700 - 110.4747041350262 - 141.05143713770443 - 92.68180128681057 - 86.67979721221967 - 117.85869785569855 - 109.74935008297804 - 0.6840664382139212'\n",
      "32 - random_62 - lwr_k=800 - 110.50548301522664 - 141.31915745019558 - 91.13233207364352 - 87.17378309306679 - 118.90432221579263 - 109.80707580306604 - 0.6839002641788104'\n",
      "33 - random_62 - lwr_k=900 - 110.84491945452648 - 141.08340513322565 - 91.05359574528995 - 86.97110538295291 - 119.4556699325065 - 109.8818221911288 - 0.6836850930402711'\n",
      "34 - random_62 - lwr_k=1000 - 110.62643693779239 - 141.3153284425081 - 91.4154025950846 - 86.77220857712462 - 119.87679868416086 - 110.00128896264029 - 0.6833411861049665'\n",
      "35 - random_2 - lwr_k=800 - 119.19327134316198 - 141.63849644457065 - 89.21439602537134 - 97.6019937255612 - 102.64965919919133 - 110.0603510077986 - 0.6831711652139167'\n",
      "36 - random_8 - lwr_k=600 - 101.17462398932443 - 138.19961160249878 - 99.55637057664326 - 124.60989351139415 - 87.4574934042974 - 110.1988203334039 - 0.6827725559537843'\n",
      "37 - random_2 - lwr_k=700 - 118.65044430052279 - 143.16252890380855 - 90.7694756055249 - 98.7641733440216 - 102.4644317896304 - 110.76289104340255 - 0.6811487753266157'\n",
      "38 - random_4 - lwr_k=900 - 110.93385222186211 - 141.81776906696103 - 95.80320163982655 - 109.63251892167692 - 95.90784126815025 - 110.81904652500602 - 0.6809871215099563'\n",
      "39 - random_4 - lwr_k=1000 - 111.29104386534448 - 141.70326052680554 - 95.94252385285056 - 107.72436842270172 - 97.6277472330579 - 110.85782614261197 - 0.6808754872933922'\n",
      "40 - random_4 - lwr_k=700 - 112.05714160618696 - 143.44559821877 - 96.12794621196295 - 108.63236194569602 - 94.1156575984404 - 110.87584299621214 - 0.6808236224875799'\n",
      "41 - random_4 - lwr_k=800 - 111.32160778059898 - 142.15772413024598 - 95.96366208720382 - 110.49294905781151 - 95.15019438268888 - 111.01725373644155 - 0.6804165458278848'\n",
      "42 - random_2 - lwr_k=600 - 117.28627380326998 - 143.85668964995781 - 91.20372832754391 - 100.44926320918715 - 103.32149148776683 - 111.22401212966963 - 0.6798213540062263'\n",
      "43 - random_62 - lwr_k=100 - 120.52624415689169 - 149.55399429705002 - 94.84438109899102 - 87.66371913402848 - 105.3769097147401 - 111.59382004895673 - 0.6787567943252011'\n",
      "44 - random_60 - lwr_k=200 - 124.62014815832863 - 148.6857039758969 - 93.81701684598843 - 104.14655732744208 - 86.77329230520215 - 111.60966579953231 - 0.6787111794362315'\n",
      "45 - random_2 - lwr_k=500 - 117.24402685711395 - 145.4921261810302 - 92.36774539588585 - 105.34127465797229 - 102.57520949701794 - 112.60447665150008 - 0.675847434589196'\n",
      "46 - random_42 - lwr_k=600 - 104.65501293407686 - 158.1097051013984 - 99.83709764128298 - 100.3725215469025 - 101.20293009292155 - 112.83474800966613 - 0.675184556400781'\n",
      "47 - random_54 - lwr_k=1000 - 123.94354135989772 - 138.9594094177096 - 95.38014810644597 - 86.6143096270968 - 119.3861220121965 - 112.85766219601592 - 0.6751185937276223'\n",
      "48 - random_18 - lwr_k=800 - 118.31806973512374 - 154.44038921444144 - 105.60889595030142 - 95.16857372409441 - 91.60430680624494 - 113.02850327978464 - 0.6746267964454185'\n",
      "49 - random_18 - lwr_k=700 - 117.85243589863319 - 155.24560909246796 - 106.11417117115693 - 94.86610588113894 - 91.11436736581147 - 113.03895301619981 - 0.6745967149693743'\n",
      "50 - random_18 - lwr_k=600 - 117.57615848624252 - 154.31551041251586 - 106.63831222655921 - 95.22382338518861 - 91.54901448328677 - 113.06095320844189 - 0.6745333833952942'\n",
      "51 - random_54 - lwr_k=800 - 124.61952056578568 - 139.19530904478427 - 95.81804750574781 - 86.70068277498432 - 119.02358037398176 - 113.07242392167635 - 0.6745003628508843'\n",
      "52 - random_54 - lwr_k=900 - 124.59165575031734 - 139.31798135461818 - 96.17290474452591 - 86.43353615949268 - 119.58664774213582 - 113.22152575651326 - 0.6740711459697524'\n",
      "53 - random_54 - lwr_k=700 - 124.65853684042254 - 141.04713102648375 - 95.72210912380976 - 86.67363322628727 - 118.65444915566152 - 113.35214698361936 - 0.6736951288954527'\n",
      "54 - random_18 - lwr_k=1000 - 116.92219911348552 - 154.81574415046143 - 107.99184911645999 - 95.39339092969077 - 92.13469896426025 - 113.45187574968521 - 0.6734080414161019'\n",
      "55 - random_18 - lwr_k=900 - 117.69951539247316 - 155.65223876489796 - 107.92037589114335 - 95.46184568232128 - 92.08896046105355 - 113.76492657333422 - 0.6725068674076842'\n",
      "56 - random_54 - lwr_k=300 - 122.88286374746717 - 141.80662505096868 - 97.53418720502353 - 88.11065430375251 - 118.68540390882593 - 113.80472977843559 - 0.6723922866074457'\n",
      "57 - random_54 - lwr_k=600 - 124.59443699841941 - 140.8699371156921 - 97.56775458199688 - 87.32911900902674 - 118.72338574817849 - 113.81785610686725 - 0.6723545000721135'\n",
      "58 - random_4 - lwr_k=500 - 113.44916625848408 - 144.78159446302536 - 97.90342868767893 - 115.97572398932982 - 96.99634910522921 - 113.82122041328464 - 0.6723448153012371'\n",
      "59 - random_18 - lwr_k=500 - 119.5545775116202 - 154.29371616235647 - 109.4338867912401 - 95.00956009054978 - 91.69946696883841 - 113.9987206646318 - 0.6718338483881425'\n",
      "60 - random_54 - lwr_k=400 - 124.22761278300219 - 141.18333692862146 - 98.2112764096243 - 87.58610390298432 - 119.43310778395484 - 114.12915849344334 - 0.6714583592593513'\n",
      "61 - random_98 - lwr_k=500 - 131.68450867071928 - 164.18239998626663 - 95.53601367496644 - 93.34876662422725 - 86.11328366925419 - 114.17450465911105 - 0.6713278220341019'\n",
      "62 - random_54 - lwr_k=500 - 125.31695734214239 - 141.23011466214498 - 98.3453394295387 - 88.02840870529904 - 118.9930234249639 - 114.38371164043332 - 0.6707255815041153'\n",
      "63 - random_15 - lwr_k=1000 - 106.32423942080841 - 149.4467806607507 - 104.35081405956338 - 116.03060569127246 - 95.78086641791951 - 114.38596597394792 - 0.6707190919930961'\n",
      "64 - random_4 - lwr_k=600 - 112.66191521642962 - 143.8390651254098 - 95.53113038084042 - 126.05810131109932 - 94.20877142743144 - 114.45964164899655 - 0.6705070030976492'\n",
      "65 - random_98 - lwr_k=400 - 132.6026344989935 - 164.79359133287133 - 95.36688797530148 - 93.22382697020234 - 86.95973866046383 - 114.59088929379378 - 0.6701291827655427'\n",
      "66 - random_16 - lwr_k=900 - 114.2761456517606 - 153.84213556566922 - 105.8279884035129 - 107.50850323122708 - 91.71326774796489 - 114.63357729366714 - 0.6700062975563337'\n",
      "67 - random_65 - lwr_k=1000 - 107.32499205845703 - 144.61310438660348 - 104.02152082667006 - 88.53414063723737 - 128.68866871769757 - 114.63585480676923 - 0.6699997413186355'\n",
      "68 - random_15 - lwr_k=800 - 106.12532745229518 - 149.44460463539465 - 102.86751246386021 - 115.1055428471267 - 99.83351072492408 - 114.67456230390503 - 0.6698883147140251'\n",
      "69 - random_35 - lwr_k=100 - 121.35581721093989 - 143.0670555589277 - 109.33772710135884 - 101.41726490590185 - 98.3708051121493 - 114.71030711369832 - 0.669785416746385'\n",
      "70 - random_65 - lwr_k=800 - 107.67967901038485 - 143.6109383155413 - 104.33435567518187 - 88.37383234378358 - 129.621932331143 - 114.7235400439578 - 0.6697473233382092'\n",
      "71 - random_65 - lwr_k=700 - 107.83633114231532 - 143.37345021494852 - 104.07942140220557 - 88.53373810497992 - 129.82322543356784 - 114.72863883893105 - 0.6697326455250332'\n",
      "72 - random_83 - deep - 112.60841353843952 - 145.18868143427932 - 112.28975962728917 - 98.95855506692996 - 104.6625034616856 - 114.7413996438966 - 0.669695910856624'\n",
      "73 - random_16 - lwr_k=1000 - 113.85656548546736 - 155.41197782209076 - 106.4899861597593 - 108.36239147998661 - 89.70998093422993 - 114.76610193418098 - 0.6696248011586756'\n",
      "74 - random_65 - lwr_k=900 - 107.35704322263425 - 144.4796581367079 - 104.09779595506193 - 88.57577981582925 - 129.3982817948039 - 114.7810715065871 - 0.6695817085086955'\n",
      "75 - random_35 - lwr_k=200 - 120.78399966082446 - 145.08896166859228 - 109.29034276127666 - 101.63416163803781 - 97.4569631817234 - 114.85139743385695 - 0.669379262474423'\n",
      "76 - random_65 - lwr_k=600 - 107.95016769707503 - 143.38683932498955 - 104.17797269219446 - 88.4042802590191 - 130.42241485786994 - 114.86773836677555 - 0.6693322221125806'\n",
      "77 - random_98 - lwr_k=600 - 133.99209698448325 - 162.86705720866547 - 97.07991727150849 - 92.7957495255647 - 87.6198523911314 - 114.87258362093334 - 0.6693182741629784'\n",
      "78 - random_42 - lwr_k=700 - 113.59222015578491 - 159.1620378094504 - 99.91627491545184 - 101.2262096455576 - 100.51822841591402 - 114.8828828764248 - 0.6692886258737749'\n",
      "79 - random_18 - lwr_k=400 - 122.12394789841274 - 155.3467518441132 - 110.39971684326675 - 94.41437287158774 - 92.1349925239876 - 114.88458074876603 - 0.6692837382379409'\n",
      "80 - random_65 - lwr_k=500 - 108.06980948574461 - 143.40316611839415 - 104.15200673327098 - 88.45813623700607 - 131.42547934131198 - 115.10111317489294 - 0.6686604100763004'\n",
      "81 - random_35 - lwr_k=300 - 120.12882518395706 - 147.3261007684837 - 108.55393008275622 - 102.11567813702348 - 97.55707115016001 - 115.1367516012232 - 0.6685578183529011'\n",
      "82 - random_98 - lwr_k=300 - 133.85484173957906 - 164.08003571204915 - 94.48666306095942 - 94.9696703789868 - 89.02532200159237 - 115.28490812530127 - 0.6681313227215516'\n",
      "83 - random_78 - lwr_k=1000 - 116.82933020979414 - 141.8207618524502 - 103.7091203543177 - 95.20009134268874 - 119.03539826234066 - 115.31907065525012 - 0.6680329796356181'\n",
      "84 - random_35 - lwr_k=400 - 120.23420331608403 - 148.4950174774858 - 108.39559417771399 - 102.66821146605038 - 97.44330556030847 - 115.44767920885214 - 0.6676627564098321'\n",
      "85 - random_98 - lwr_k=700 - 134.0916037153633 - 163.78847652228617 - 98.24330775976802 - 93.1542270685875 - 87.97072233925647 - 115.45127509887175 - 0.6676524049832329'\n",
      "86 - random_65 - lwr_k=400 - 108.76337395124963 - 143.28614431439073 - 104.2943254754402 - 88.76563358026287 - 132.27333987660583 - 115.47598451672955 - 0.6675812744080853'\n",
      "87 - random_16 - lwr_k=800 - 114.16531257100743 - 154.86925962937894 - 105.7121906814088 - 111.1995877727304 - 91.54943714798708 - 115.49904253420125 - 0.6675148976907558'\n",
      "88 - random_65 - lwr_k=300 - 109.41172921837969 - 142.9336977117066 - 103.2066189443435 - 89.406760632387 - 132.55999953652838 - 115.5032358523401 - 0.6675028264581306'\n",
      "89 - random_35 - lwr_k=500 - 120.57376560869645 - 148.77646580742936 - 108.27668367541612 - 102.5962900463198 - 97.54804365786781 - 115.55468262529368 - 0.6673547275198741'\n",
      "90 - random_35 - lwr_k=900 - 120.7349230007227 - 148.92401410426655 - 108.1029525353454 - 103.04094626550143 - 97.35467166188282 - 115.63194161543127 - 0.6671323233970643'\n",
      "91 - random_35 - lwr_k=700 - 120.73866925184441 - 148.89613356573668 - 108.02913345303527 - 103.10551223247597 - 97.50643364756372 - 115.6556148134377 - 0.6670641757701363'\n",
      "92 - random_35 - lwr_k=600 - 120.73164790661434 - 148.7958303772283 - 108.26366250913574 - 103.03939502784964 - 97.50579756361746 - 115.66770341207621 - 0.6670293765296672'\n",
      "93 - random_35 - lwr_k=1000 - 120.77816905889117 - 148.93089117427454 - 108.19694723746166 - 102.99356212921613 - 97.44851070233224 - 115.67005660484689 - 0.667022602434212'\n",
      "94 - random_35 - lwr_k=800 - 120.75121914847801 - 148.9628571152849 - 108.1016960918386 - 103.13117552030626 - 97.57975669416857 - 115.70577605356634 - 0.6669197774730579'\n",
      "95 - random_98 - lwr_k=800 - 134.64639216564427 - 162.82143825268972 - 99.98290484923068 - 93.16361779175611 - 88.06616006939053 - 115.73773338544731 - 0.6668277824527803'\n",
      "96 - random_65 - lwr_k=200 - 107.86691603816558 - 142.55118700484016 - 103.78491476680172 - 90.56598870170752 - 133.98652672098314 - 115.75042674044508 - 0.666791242310607'\n",
      "97 - random_2 - lwr_k=400 - 122.31667768107366 - 148.05576617628284 - 93.99340446948487 - 108.2792164538675 - 106.7296863304348 - 115.87550573511759 - 0.6664311795652547'\n",
      "98 - random_78 - lwr_k=900 - 116.23808026456167 - 143.430001500757 - 103.72769604940834 - 95.16456139914855 - 120.99825716609263 - 115.91174742026654 - 0.6663268512510954'\n",
      "99 - random_15 - lwr_k=900 - 106.12165250256342 - 149.37927641304074 - 107.54480210808983 - 116.34620278667899 - 100.99616174419403 - 116.07676054187162 - 0.6658518308230863'\n",
      "100 - random_16 - lwr_k=700 - 114.62150281695352 - 156.00520487847734 - 106.92177347556596 - 109.6302215796433 - 93.63334870311125 - 116.16227740807749 - 0.6656056548948209'\n",
      "101 - random_78 - lwr_k=800 - 116.36846675064687 - 143.39868944783024 - 103.80203955515377 - 95.23930013616615 - 122.02864018818374 - 116.16744455256882 - 0.6655907803250656'\n",
      "102 - random_98 - lwr_k=900 - 134.72805196855708 - 161.91958294428187 - 101.14755140595871 - 95.19583945855067 - 88.11612673099515 - 116.22302645039818 - 0.6654307776741271'\n",
      "103 - random_54 - lwr_k=200 - 123.53150004060271 - 146.40404674252306 - 99.31677932707217 - 90.96833669785822 - 121.09797874241329 - 116.26435505825968 - 0.6653118057232639'\n",
      "104 - random_5 - lwr_k=1000 - 134.78765824759856 - 149.61883164030007 - 100.24745303107629 - 98.81583123955545 - 97.86799799011013 - 116.26915154113011 - 0.6652979981707257'\n",
      "105 - random_15 - lwr_k=700 - 106.46032610602728 - 149.80754371301495 - 110.34552936676066 - 114.85598823849789 - 100.01810248419237 - 116.29664965711466 - 0.6652188398183634'\n",
      "106 - random_98 - lwr_k=1000 - 134.53702967675986 - 161.57474307606572 - 102.12773219134826 - 95.51933890015775 - 87.94096810255425 - 116.34153164319635 - 0.6650896388188849'\n",
      "107 - random_78 - lwr_k=600 - 116.34346668215254 - 143.5582224079035 - 105.37287032282654 - 96.75975914977712 - 121.49520521859378 - 116.70587350081139 - 0.6640408141953491'\n",
      "108 - random_15 - lwr_k=400 - 107.88051449145908 - 155.97032094278345 - 93.72118902788068 - 121.29894003120936 - 104.93122644914085 - 116.7596724137709 - 0.6638859441922131'\n",
      "109 - random_78 - lwr_k=700 - 116.88243064517705 - 143.8382177830149 - 104.97352555461968 - 96.24805032475163 - 121.85778536033672 - 116.76001249141991 - 0.6638849652165486'\n",
      "110 - random_78 - lwr_k=500 - 117.15768624176978 - 144.205741888684 - 103.33958568482328 - 96.26483718724545 - 123.20772622302574 - 116.83514326252917 - 0.6636686875611608'\n",
      "111 - random_5 - lwr_k=800 - 138.63265895788373 - 142.83423554528184 - 100.97688121115206 - 103.00654915591592 - 98.74656147893955 - 116.84125664907643 - 0.6636510890607141'\n",
      "112 - random_98 - lwr_k=200 - 133.0495639014126 - 168.17430292402358 - 93.62076986299986 - 97.37419640848545 - 92.40986763411924 - 116.92713061048508 - 0.6634038851686456'\n",
      "113 - random_79 - lwr_k=400 - 106.3301451150361 - 152.31413067881272 - 107.06579484425637 - 109.96137451901444 - 109.57226463389632 - 117.0478176224975 - 0.6630564655480503'\n",
      "114 - random_95 - lwr_k=500 - 117.9266044797005 - 153.12031538089886 - 101.2508764265901 - 106.90909874904243 - 106.35934176244861 - 117.11331750091581 - 0.6628679121773962'\n",
      "115 - random_95 - lwr_k=600 - 118.01265776079808 - 152.54514039866103 - 100.25725656616243 - 107.38067275524506 - 107.76116981883438 - 117.19145028421588 - 0.6626429926812754'\n",
      "116 - random_16 - lwr_k=500 - 116.35263236352814 - 160.40106695691787 - 104.45622963554605 - 115.15338495178706 - 89.71122102073002 - 117.21483262604143 - 0.6625756823370204'\n",
      "117 - random_79 - lwr_k=300 - 106.70283536286114 - 153.00738425638372 - 106.15070178031266 - 110.96168487510387 - 109.3794450132701 - 117.23950153260398 - 0.6625046683810398'\n",
      "118 - random_95 - lwr_k=700 - 117.59107927104714 - 152.64940909412442 - 99.9388501535378 - 105.49507398400343 - 110.60248994433039 - 117.25540943894137 - 0.6624588745653506'\n",
      "119 - random_42 - lwr_k=1000 - 132.6746405991334 - 156.44595931969891 - 99.13339294221947 - 98.84455311176315 - 99.27235624563886 - 117.27550852752618 - 0.6624010156655897'\n",
      "120 - random_79 - lwr_k=600 - 106.16001537519477 - 150.6554418786005 - 108.18761833360517 - 112.26323493485438 - 109.55159677728184 - 117.36261530208698 - 0.6621502628955562'\n",
      "121 - random_8 - lwr_k=400 - 101.46512726815827 - 135.8006223485489 - 115.58454164710226 - 140.93120229623088 - 93.2175345276208 - 117.39843146451831 - 0.6620471595263073'\n",
      "122 - random_79 - lwr_k=500 - 106.27577430227188 - 151.1100672681128 - 108.29765806360274 - 112.27779354754819 - 109.27345984122438 - 117.44598723991744 - 0.6619102615356223'\n",
      "123 - random_79 - lwr_k=1000 - 106.83460719954944 - 150.36831596187406 - 108.69156852200254 - 112.0330389775946 - 109.55527161568689 - 117.49564100438809 - 0.6617673240999706'\n",
      "124 - random_5 - lwr_k=900 - 137.1333570495537 - 150.90987857531505 - 101.17100199965815 - 102.26936299978635 - 96.0083053642253 - 117.50007445192054 - 0.6617545616110003'\n",
      "125 - random_79 - lwr_k=700 - 106.22814799053535 - 150.54870466054365 - 108.71502022248897 - 111.97681025640254 - 110.07169653783727 - 117.50710319055165 - 0.6617343281022925'\n",
      "126 - random_95 - lwr_k=800 - 117.19332200817017 - 152.57152646764058 - 100.45828782076683 - 106.2223818544738 - 111.26647836936488 - 117.54236920083248 - 0.6616328084465535'\n",
      "127 - random_79 - lwr_k=800 - 106.80991631633947 - 150.27406620486684 - 108.71449310849054 - 112.02430311645192 - 109.94855443563738 - 117.55334007975839 - 0.6616012267665492'\n",
      "128 - random_18 - lwr_k=300 - 123.25200115293049 - 154.70208352030227 - 120.16376898375731 - 96.10426250238213 - 94.30199868417122 - 117.70530133867973 - 0.6611637785106077'\n",
      "129 - random_67 - lwr_k=1000 - 100.67814181382936 - 151.02328234081986 - 110.91460054628297 - 117.15152961658784 - 108.80980655524968 - 117.71400293254284 - 0.6611387293823869'\n",
      "130 - random_79 - lwr_k=900 - 106.75832171687874 - 150.6729029579363 - 108.90339273708939 - 112.2342247065064 - 110.00764407299496 - 117.71435234560083 - 0.6611377235330413'\n",
      "131 - random_95 - lwr_k=1000 - 116.6460318994089 - 153.79912363282767 - 100.39207823607879 - 104.99469716586205 - 112.81753218188905 - 117.72979915471348 - 0.6610932570699826'\n",
      "132 - random_78 - lwr_k=400 - 117.30066261203474 - 144.32112355779222 - 104.83235779646525 - 97.08605020187107 - 125.13297662062237 - 117.73459673351206 - 0.661079446362615'\n",
      "133 - random_67 - lwr_k=800 - 100.84311566387782 - 151.37479639648944 - 110.98632076184788 - 116.76906124541492 - 108.75634346007682 - 117.74446986395448 - 0.6610510247522429'\n",
      "134 - random_67 - lwr_k=900 - 100.54196460776055 - 151.19262476180003 - 111.04633722893071 - 117.14101034036293 - 108.85872646896101 - 117.75464818966293 - 0.6610217245815206'\n",
      "135 - random_67 - lwr_k=700 - 100.92697042511523 - 151.93657194023186 - 111.0321496850007 - 116.4497301456738 - 108.72911332439406 - 117.81345074528018 - 0.6608524506785386'\n",
      "136 - random_84 - lwr_k=600 - 113.47902987087733 - 161.80890720107575 - 98.81925178429823 - 106.78686319719657 - 108.38936167482414 - 117.85630523160864 - 0.6607290862076417'\n",
      "137 - random_95 - lwr_k=900 - 117.11537776992931 - 153.3740102251802 - 100.81712404550836 - 105.91533220834626 - 112.42457429710701 - 117.9292135207062 - 0.660519206372901'\n",
      "138 - random_67 - lwr_k=600 - 101.10936401602198 - 152.45395607443209 - 110.9957074662598 - 116.33022282873871 - 109.01916291974251 - 117.98022764908274 - 0.6603723528811287'\n",
      "139 - random_16 - lwr_k=600 - 114.96694991978238 - 158.9586189461173 - 106.43027250459828 - 115.8644922960855 - 93.94974857747053 - 118.03375195531915 - 0.6602182733836319'\n",
      "140 - random_67 - lwr_k=500 - 101.56322868617696 - 153.0122997647156 - 110.74188835959009 - 116.27392965429584 - 108.87941046019411 - 118.09272581387526 - 0.6600485063538334'\n",
      "141 - random_57 - lwr_k=400 - 120.12115573738014 - 154.1411135325532 - 100.42819681709932 - 111.6247945478131 - 104.39447251089818 - 118.14211730947892 - 0.6599063239070471'\n",
      "142 - random_57 - lwr_k=700 - 119.80098104578747 - 152.4794441668584 - 101.71887060583295 - 109.91370221750657 - 106.81525621522349 - 118.14579360034489 - 0.6598957410319177'\n",
      "143 - random_11 - deep - 128.6319887884732 - 154.07143070143633 - 101.93082692490512 - 90.12416169765747 - 116.12413431540189 - 118.17740749700282 - 0.6598047342827631'\n",
      "144 - random_39 - lwr_k=1000 - 124.52728759569426 - 143.8443336431664 - 108.5267101090735 - 118.24540630555752 - 95.76603845390207 - 118.18250242158001 - 0.6597900679811616'\n",
      "145 - random_42 - lwr_k=500 - 118.06900808870961 - 160.13601567294353 - 102.32651589400356 - 105.37155744018655 - 105.01910426479725 - 118.18443031764522 - 0.6597845181799995'\n",
      "146 - random_48 - lwr_k=100 - 113.08675638729368 - 150.4421221097323 - 113.0131687439 - 120.17100302736925 - 94.44141068547052 - 118.23044857780012 - 0.6596520462079382'\n",
      "147 - random_84 - lwr_k=500 - 114.8889398552806 - 164.04321819531444 - 98.05819782470243 - 106.74159021649385 - 107.44153665137556 - 118.23440802184024 - 0.6596406482246486'\n",
      "148 - random_57 - lwr_k=500 - 120.23776981787175 - 153.07170060255905 - 101.21375567030195 - 110.93859273832818 - 105.85085472859325 - 118.26270504915642 - 0.6595591900600106'\n",
      "149 - random_84 - lwr_k=1000 - 112.73436111905441 - 160.71726088937896 - 101.81419050951102 - 106.30510726633084 - 109.82665840012245 - 118.27903744142267 - 0.6595121742839969'\n",
      "150 - random_69 - deep - 105.78770186325599 - 155.21988306543136 - 112.0024343822063 - 101.79959000837911 - 116.74917532463122 - 118.31067885912228 - 0.6594210883100767'\n",
      "151 - random_57 - lwr_k=600 - 120.07813645097245 - 153.41541769627543 - 100.78596846989589 - 110.88528884221597 - 106.42852857413882 - 118.31881973733479 - 0.6593976536746808'\n",
      "152 - random_57 - lwr_k=800 - 119.12184836035581 - 152.35297706131698 - 101.94065798885865 - 110.01047027616072 - 108.36830970113593 - 118.35891847574455 - 0.659282222127803'\n",
      "153 - random_15 - lwr_k=600 - 106.47235173940958 - 151.3935270917665 - 101.28345549727854 - 115.17399040520043 - 117.57401690796738 - 118.37844149867728 - 0.6592260215383023'\n",
      "154 - random_39 - lwr_k=900 - 125.40187044405621 - 144.1196369438859 - 107.95607129468442 - 118.42202105854727 - 97.15795577468747 - 118.61209668090085 - 0.6585534023939169'\n",
      "155 - random_48 - lwr_k=700 - 114.5674659139166 - 149.41914834362672 - 117.74365921425876 - 114.13563871469266 - 97.22479904832562 - 118.61779293027443 - 0.65853700469906'\n",
      "156 - random_40 - lwr_k=100 - 118.67168272838846 - 163.34447892573965 - 108.84723512695821 - 97.71094880278207 - 104.74841011329332 - 118.66455175443656 - 0.6584024008781598'\n",
      "157 - random_18 - lwr_k=200 - 131.38223034355372 - 155.5376114113776 - 108.44725955961903 - 97.04852558046606 - 101.10743120948233 - 118.70570489605693 - 0.6582839340389395'\n",
      "158 - random_84 - lwr_k=900 - 113.17275153063983 - 162.8503203765511 - 101.18047670649369 - 106.10551749170529 - 110.25713851528947 - 118.71276313098363 - 0.6582636156197541'\n",
      "159 - random_67 - lwr_k=400 - 102.15979756030622 - 152.13577624153248 - 111.29129836887601 - 116.48651641664028 - 111.52682434798662 - 118.71861448729024 - 0.6582467714212433'\n",
      "160 - random_95 - lwr_k=400 - 117.50215288778307 - 155.44467760314018 - 102.10968864329887 - 109.16430397623627 - 109.41682737020002 - 118.7274244237267 - 0.6582214104091246'\n",
      "161 - random_57 - lwr_k=900 - 119.28504546351654 - 152.7254730176488 - 102.92059792443102 - 109.41957342076627 - 109.38290439973215 - 118.74676526869415 - 0.6581657342521916'\n",
      "162 - random_48 - lwr_k=400 - 119.98240962929819 - 153.135799063586 - 112.19984961768377 - 112.37153608908052 - 96.17383281564261 - 118.7727897655993 - 0.6580908179816649'\n",
      "163 - random_57 - lwr_k=300 - 119.4703861942232 - 154.75696748873736 - 99.50757512256354 - 114.01415830529939 - 106.21055657435738 - 118.79198724492315 - 0.6580355545289382'\n",
      "164 - random_79 - lwr_k=200 - 107.9853968441162 - 153.0739798405946 - 105.55620559272666 - 116.8807413284879 - 110.61321778191001 - 118.8209737733041 - 0.6579521115094735'\n",
      "165 - random_48 - lwr_k=500 - 119.87496300318509 - 150.6949066226016 - 113.66005365359007 - 115.60911471107917 - 95.13218678027411 - 118.99432090430547 - 0.6574531001123104'\n",
      "166 - random_84 - lwr_k=800 - 114.73817917662824 - 163.01483351773481 - 101.10981082143437 - 106.15470793194552 - 110.02243097361693 - 119.00762426994741 - 0.6574148039430134'\n",
      "167 - random_58 - lwr_k=300 - 108.79192043827366 - 158.2870489946067 - 111.87889976311668 - 107.34342919504239 - 108.78681380415105 - 119.01674060892421 - 0.6573885608951848'\n",
      "168 - random_67 - lwr_k=300 - 102.57011687691914 - 151.82784170646153 - 111.32279214273167 - 116.37895427865271 - 113.2002693927043 - 119.05857284793105 - 0.6572681391500271'\n",
      "169 - random_57 - lwr_k=1000 - 119.24651728225699 - 152.66231209743822 - 104.23640861759232 - 109.16935932243794 - 109.97895788428507 - 119.05872723658007 - 0.6572676947140852'\n",
      "170 - random_58 - lwr_k=200 - 109.83134480781378 - 159.85543782457486 - 112.23968095972023 - 108.92871331861168 - 104.54701408627177 - 119.07964058872308 - 0.657207491807859'\n",
      "171 - random_95 - lwr_k=300 - 118.1407938897453 - 155.17162891370586 - 104.55669924688432 - 111.10461162645169 - 106.44078432545207 - 119.08282235607818 - 0.6571983325090338'\n",
      "172 - random_5 - lwr_k=700 - 144.25542077435227 - 147.15482452896015 - 103.53868535711084 - 101.53447992933657 - 99.27548295805829 - 119.1539435631394 - 0.6569935971165383'\n",
      "173 - random_4 - lwr_k=300 - 118.70069474596305 - 148.73178402102 - 97.11471142729123 - 128.71442166329777 - 103.04888698798885 - 119.26205135543304 - 0.6566823890788567'\n",
      "174 - random_48 - lwr_k=800 - 115.21932918182961 - 151.83611773769263 - 118.52524553138001 - 114.17498377171466 - 96.65201953821717 - 119.28118884085512 - 0.6566272983295405'\n",
      "175 - random_78 - lwr_k=300 - 121.6187177121645 - 145.86624144507354 - 105.0739654297677 - 98.08436037806806 - 125.88827308309673 - 119.30651102375126 - 0.6565544037982424'\n",
      "176 - random_5 - deep - 126.03696396926354 - 143.111247448841 - 111.72653537929547 - 102.51857789645373 - 113.2364223393041 - 119.32652774906697 - 0.656496781622675'\n",
      "177 - random_84 - lwr_k=700 - 115.06361436741278 - 163.46286329633895 - 99.92698969912877 - 107.47490474020076 - 111.19563647423058 - 119.42442562134818 - 0.6562149650792231'\n",
      "178 - random_98 - deep - 132.19047654250573 - 154.04513381359237 - 122.51563661187926 - 93.70780312002296 - 95.11717935901821 - 119.51633850293945 - 0.6559503766776242'\n",
      "179 - random_48 - lwr_k=1000 - 118.0670375269931 - 150.0465635975717 - 118.8672793153674 - 114.7074017931905 - 96.3519214174371 - 119.60790783918377 - 0.6556867779823393'\n",
      "180 - random_65 - lwr_k=100 - 107.74555108636123 - 148.8248991021766 - 103.65532226194267 - 94.65712086695214 - 143.2211907524726 - 119.61979273104487 - 0.655652565149058'\n",
      "181 - random_42 - lwr_k=400 - 118.50692768083356 - 163.80296878694722 - 104.84482318646893 - 109.72971784593423 - 101.30654463447657 - 119.63809887012405 - 0.6555998675821251'\n",
      "182 - random_58 - lwr_k=400 - 109.25153609937527 - 158.72829532126883 - 112.93486751636469 - 105.34291599792121 - 112.12581626474389 - 119.67578721008478 - 0.6554913748077025'\n",
      "183 - random_94 - deep - 129.63758139774717 - 154.59333669013532 - 107.69154795473534 - 88.8937071709347 - 117.88189181072437 - 119.74046618949896 - 0.6553051841700324'\n",
      "184 - random_57 - lwr_k=200 - 122.304251461361 - 155.16058332276833 - 100.33037134280455 - 116.71566604347998 - 104.94686789497558 - 119.89175607650047 - 0.6548696689556857'\n",
      "185 - random_58 - lwr_k=500 - 111.80351848661496 - 158.67100253648755 - 110.66201735023019 - 105.18713078130195 - 113.21053565010469 - 119.90614215769897 - 0.6548282559917844'\n",
      "186 - random_84 - lwr_k=400 - 113.06371476286084 - 167.9102492931854 - 99.87237813402844 - 107.24523519181308 - 111.92409046466523 - 120.00253513719552 - 0.654550771183715'\n",
      "187 - random_39 - lwr_k=800 - 128.20571279687502 - 145.03718754899145 - 108.48038650972818 - 119.32845320620524 - 99.11658038865352 - 120.03436882005849 - 0.6544591321097335'\n",
      "188 - random_94 - lwr_k=900 - 135.64735635616475 - 158.46599372174157 - 99.05164855223559 - 99.33930829334976 - 107.66886918735818 - 120.03598161067757 - 0.6544544893971862'\n",
      "189 - random_48 - lwr_k=900 - 119.40427171562384 - 150.51681525105874 - 118.102085447391 - 117.18794384719392 - 95.36454095888573 - 120.11507014188089 - 0.6542268186060551'\n",
      "190 - random_15 - lwr_k=300 - 113.63043321905236 - 163.38615859689014 - 96.22989479725169 - 127.75193144968728 - 100.63775507494518 - 120.32665711795784 - 0.6536177268262042'\n",
      "191 - random_48 - lwr_k=600 - 120.29501692129345 - 149.76430812416856 - 122.2132234343437 - 115.634536568118 - 93.8053101106094 - 120.34247493873401 - 0.6535721923382666'\n",
      "192 - random_58 - lwr_k=600 - 113.46872944261531 - 158.64518186536964 - 111.02391852808577 - 104.80270228620815 - 113.89411079934474 - 120.36633370685475 - 0.6535035105138502'\n",
      "193 - random_48 - lwr_k=200 - 123.04944533378891 - 150.34125121294156 - 116.5247428666521 - 114.51710885538182 - 97.78306080662648 - 120.44334657564377 - 0.6532818148963228'\n",
      "194 - random_58 - lwr_k=700 - 114.45922872397624 - 158.8471575467524 - 110.67227028103528 - 104.38278818179532 - 114.33654053647804 - 120.5390727035133 - 0.6530062497425342'\n",
      "195 - random_58 - lwr_k=800 - 115.52102362206341 - 158.74023241369613 - 110.72382378199624 - 103.42580157521613 - 115.02721076110265 - 120.68717288107287 - 0.6525799163149361'\n",
      "196 - random_94 - lwr_k=1000 - 135.77944300884133 - 158.17246552315876 - 103.75518203741328 - 98.70497574159211 - 108.39211203993284 - 120.96211357699507 - 0.6517884492741111'\n",
      "197 - random_67 - lwr_k=200 - 103.18444272709894 - 149.79434020649413 - 111.96344533346847 - 116.358604515047 - 123.62429889494767 - 120.9834912730097 - 0.6517269096691878'\n",
      "198 - random_58 - lwr_k=900 - 116.6614924942592 - 158.9647882665482 - 110.71772645151347 - 103.23555032797486 - 115.65929757479066 - 121.04739276512416 - 0.6515429575456272'\n",
      "199 - random_16 - lwr_k=400 - 117.46470374053709 - 165.80976154328167 - 105.66109261520636 - 126.36330813792736 - 89.95079539894431 - 121.04962310913979 - 0.651536537092765'\n",
      "200 - random_15 - lwr_k=500 - 107.0909279515895 - 153.2977517389234 - 105.09324144428332 - 118.89961854962606 - 120.95734765226183 - 121.06657215089014 - 0.6514877461785136'\n",
      "201 - random_59 - lwr_k=300 - 109.87006888745947 - 135.72769359578498 - 182.398021819459 - 93.90887638414895 - 83.68851082044672 - 121.11766426304882 - 0.6513406681961723'\n",
      "202 - random_78 - deep - 113.42071264858903 - 144.13250533050893 - 135.1070058509264 - 93.71504827139962 - 120.29090617164654 - 121.33255158960507 - 0.6507220753744922'\n",
      "203 - random_58 - lwr_k=1000 - 117.32270165228203 - 159.18367627251652 - 110.96447912759365 - 103.39355433344282 - 115.87898518747954 - 121.34833212790349 - 0.6506766485907894'\n",
      "204 - random_48 - lwr_k=300 - 122.35886733282884 - 149.27514495629737 - 119.08487407752358 - 111.45858829066836 - 104.88312563445675 - 121.41220170265254 - 0.6504927883471983'\n",
      "205 - random_39 - lwr_k=700 - 131.30115661799937 - 145.29147165088906 - 109.11656995384406 - 123.68161244344526 - 98.71299138622817 - 121.62159521525933 - 0.6498900108528191'\n",
      "206 - random_65 - deep - 111.6635530406031 - 151.77320405829306 - 113.65190112667898 - 103.10806514251432 - 128.39023047904095 - 121.71652502480975 - 0.6496167376658348'\n",
      "207 - random_43 - deep - 118.10001007606243 - 134.58910893962116 - 119.04225647578006 - 115.32494832771305 - 121.95063146448896 - 121.80107345459761 - 0.6493733495585234'\n",
      "208 - random_26 - lwr_k=1000 - 103.6198730157531 - 155.9190390407508 - 99.22443150561364 - 138.37298154896575 - 112.34397753225227 - 121.89448445178603 - 0.6491044493127847'\n",
      "209 - random_25 - deep - 104.98873572513975 - 161.6788170252344 - 112.46336744735343 - 110.22852998727762 - 120.13657486402768 - 121.89774761500303 - 0.6490950553195172'\n",
      "210 - random_3 - lwr_k=200 - 110.40898606135654 - 158.49440726379524 - 104.7900953569175 - 129.0765989871504 - 106.87399301043249 - 121.92782270456836 - 0.6490084790593181'\n",
      "211 - random_3 - lwr_k=100 - 110.26670044159584 - 156.92230843766075 - 105.58407132087939 - 128.16486669069988 - 109.35040369928416 - 122.05665330449551 - 0.6486376166326044'\n",
      "212 - random_3 - lwr_k=300 - 110.41555317469931 - 159.34641479494059 - 105.01734653355788 - 129.57637850090336 - 106.03004652451648 - 122.07614224905474 - 0.6485815141439302'\n",
      "213 - random_5 - lwr_k=600 - 150.58077139115906 - 150.44220240238295 - 106.91938978398325 - 105.1597500679287 - 97.8416013493112 - 122.19119143189475 - 0.6482503240449661'\n",
      "214 - random_3 - lwr_k=400 - 110.52622189381825 - 159.6368402686968 - 105.17034537553646 - 129.43617446504078 - 106.35847706698944 - 122.22460289801771 - 0.6481541430335183'\n",
      "215 - random_78 - lwr_k=200 - 125.40406894735848 - 147.81577021518638 - 105.2122215115039 - 100.28203750664018 - 132.6529395126536 - 122.27367751636847 - 0.6480128727758163'\n",
      "216 - random_59 - lwr_k=400 - 107.56659528526725 - 134.38070872785235 - 191.9955014995824 - 94.02248831288045 - 83.41398871969454 - 122.27458803188881 - 0.6480102516904869'\n",
      "217 - random_58 - lwr_k=100 - 111.34259365523133 - 158.12554835853746 - 117.33054235522609 - 123.76639154968352 - 100.97941457465235 - 122.30795240149098 - 0.6479142062549814'\n",
      "218 - random_38 - lwr_k=700 - 112.38065818490549 - 159.63663858854878 - 103.9037709370122 - 96.64615516726614 - 138.9832774202398 - 122.30924377795644 - 0.6479104887918162'\n",
      "219 - random_38 - lwr_k=600 - 111.65632633405747 - 160.3856110980169 - 103.54990427751403 - 96.32852948239524 - 139.65892090155788 - 122.31493917654852 - 0.6478940935460893'\n",
      "220 - random_3 - lwr_k=500 - 110.9481213875474 - 160.08980905048463 - 104.99492310049885 - 129.65973669688265 - 106.27546138929954 - 122.39262330450993 - 0.6476704655863561'\n",
      "221 - random_3 - lwr_k=600 - 110.97630080030294 - 160.25668993772956 - 105.10059724031062 - 129.41733819913864 - 106.36499889306617 - 122.42219787335296 - 0.6475853297849523'\n",
      "222 - random_39 - deep - 103.30295160227809 - 159.53693733692376 - 119.09634432149068 - 120.00433273035785 - 110.6135246043269 - 122.50916155338831 - 0.6473349885551699'\n",
      "223 - random_26 - lwr_k=900 - 104.30794082305609 - 155.91997188464893 - 99.50240253213397 - 138.76407388259025 - 114.2139747782645 - 122.5401003644819 - 0.6472459259166137'\n",
      "224 - random_3 - lwr_k=700 - 111.22064514405984 - 160.3764845269134 - 105.41763440479353 - 129.39857355634547 - 106.29890845855857 - 122.54147286386768 - 0.6472419749262963'\n",
      "225 - random_83 - lwr_k=1000 - 116.42735985336948 - 173.52451030769754 - 120.22886537903196 - 96.52698852457985 - 106.4679811235879 - 122.63460569950362 - 0.6469738750381471'\n",
      "226 - random_59 - lwr_k=500 - 107.23517647797136 - 133.0041839619474 - 196.27976480835454 - 94.25042466610688 - 82.45700432004287 - 122.64398192877759 - 0.646946883848601'\n",
      "227 - random_84 - lwr_k=300 - 119.89306651638506 - 172.45562798230236 - 100.65338780986353 - 108.45142954636601 - 111.90552873305512 - 122.67156848827386 - 0.6468674708954265'\n",
      "228 - random_38 - lwr_k=800 - 111.26706340945312 - 160.93054655156175 - 103.2010411118715 - 97.91844563276366 - 140.3046237335534 - 122.72335605052794 - 0.6467183909328866'\n",
      "229 - random_38 - lwr_k=400 - 115.00499447202671 - 159.31764881896237 - 105.44480058368315 - 96.53621699211334 - 137.48744830636633 - 122.75755322240528 - 0.6466199481238308'\n",
      "230 - random_3 - lwr_k=800 - 111.17311430760887 - 160.5404466498855 - 106.50102633632027 - 129.55090670208713 - 106.35600643144235 - 122.82329532643314 - 0.6464306974624908'\n",
      "231 - random_35 - deep - 119.64489496165308 - 155.1154432440688 - 109.10057468258034 - 103.44470150607472 - 126.97173451452021 - 122.85519191067029 - 0.6463388769308329'\n",
      "232 - random_74 - lwr_k=500 - 114.11373647439669 - 166.35721338085202 - 103.47763463637831 - 113.68620483294582 - 116.86243878428324 - 122.89868797179298 - 0.646213665954213'\n",
      "233 - random_93 - lwr_k=900 - 95.28500854788153 - 142.8164524220248 - 131.60842379410053 - 104.24641115427873 - 140.85753441303112 - 122.96037922963707 - 0.6460360763939377'\n",
      "234 - random_3 - lwr_k=900 - 111.34113754436568 - 160.70886181432286 - 107.14225855508926 - 129.50829548652325 - 106.39308479957396 - 123.01772060391986 - 0.6458710087684696'\n",
      "235 - random_39 - lwr_k=600 - 133.48700519539648 - 145.52575126231272 - 110.41784852534491 - 127.88850381211361 - 97.81036352026548 - 123.02679659405702 - 0.6458448818721498'\n",
      "236 - random_92 - deep - 113.81454588791419 - 144.9818302629939 - 127.72709860067627 - 103.26520935427068 - 125.35857681202035 - 123.02865738954621 - 0.6458395248471408'\n",
      "237 - random_95 - lwr_k=200 - 122.71554340459517 - 164.33453999122335 - 108.82142176791444 - 115.73722774969987 - 103.59290259643474 - 123.04029909372089 - 0.6458060124591776'\n",
      "238 - random_74 - lwr_k=600 - 115.36314015955288 - 166.10753461013113 - 103.35744783063093 - 113.78804342113314 - 117.25020750102298 - 123.17260118478525 - 0.6454251567107826'\n",
      "239 - random_3 - lwr_k=1000 - 111.38605623413602 - 160.9020894303795 - 107.63657320458722 - 129.54834867276773 - 106.43129306391094 - 123.17985497594353 - 0.6454042753472451'\n",
      "240 - random_54 - lwr_k=100 - 126.99536070024767 - 151.4959377665832 - 103.13421453751475 - 105.14219513321096 - 129.43481370889495 - 123.2408281754589 - 0.6452287528490999'\n",
      "241 - random_93 - lwr_k=700 - 99.98394628727155 - 147.61918724224375 - 130.16910645775192 - 105.46296115625974 - 133.25031610178826 - 123.29509300087732 - 0.6450725416318017'\n",
      "242 - random_13 - lwr_k=900 - 114.33872067509756 - 154.4211243785888 - 116.21581341056248 - 125.81819759310622 - 105.7310019778472 - 123.30419838774655 - 0.6450463301116163'\n",
      "243 - random_13 - lwr_k=800 - 114.41615374134818 - 154.62498991916797 - 116.59704752412551 - 125.93121840316621 - 105.40783341784024 - 123.3946742569713 - 0.6447858787868636'\n",
      "244 - random_13 - lwr_k=500 - 114.56442546331813 - 155.41068546242545 - 117.43970305435323 - 125.63326045482263 - 104.2814785770849 - 123.46514296829089 - 0.6445830217226091'\n",
      "245 - random_13 - lwr_k=700 - 114.18291256360214 - 155.0302586428999 - 116.86051103320531 - 126.16564353772358 - 105.10644504304015 - 123.46835334988238 - 0.6445737800524749'\n",
      "246 - random_13 - lwr_k=600 - 114.37512054432185 - 155.18541934389282 - 117.06602886431635 - 126.32572817224276 - 104.46159721398736 - 123.48199341396443 - 0.644534514635184'\n",
      "247 - random_13 - lwr_k=400 - 115.75769302719496 - 155.15402544109673 - 117.47107767395423 - 124.68326207870297 - 104.4033267720729 - 123.4932098561439 - 0.6445022260564757'\n",
      "248 - random_13 - lwr_k=1000 - 114.3713113335208 - 154.93069769670765 - 116.50809237073048 - 125.60484410205069 - 106.15298661670444 - 123.51279802491813 - 0.6444458379327644'\n",
      "249 - random_38 - lwr_k=900 - 112.69303580537921 - 160.7631098691135 - 103.75288654654717 - 98.6394712987341 - 142.35081031430119 - 123.63891874931231 - 0.6440827763780239'\n",
      "250 - random_13 - lwr_k=200 - 114.4601901012362 - 156.4021474771823 - 116.21584621617481 - 123.43697931489812 - 107.8188265659036 - 123.66600398821511 - 0.6440048065677979'\n",
      "251 - random_59 - lwr_k=200 - 115.43805529225315 - 136.7440674527457 - 180.28111629048072 - 99.02960572166887 - 87.68549769392469 - 123.83494430832451 - 0.6435184809811673'\n",
      "252 - random_38 - lwr_k=500 - 116.31647982338686 - 161.06557563161942 - 105.18227683777963 - 95.9278642104418 - 140.77869495903755 - 123.85352826671408 - 0.6434649836605757'\n",
      "253 - random_39 - lwr_k=500 - 130.72805390464384 - 146.15065187914166 - 110.64303688548542 - 132.01100002047463 - 99.90328972756176 - 123.88779641511213 - 0.6433663365326335'\n",
      "254 - random_8 - lwr_k=300 - 108.40182330931559 - 151.23447352643805 - 106.83486173382984 - 150.05142184546457 - 103.02965416933338 - 123.90910950538893 - 0.6433049829071339'\n",
      "255 - random_13 - lwr_k=300 - 117.34084687902325 - 156.5807988894947 - 117.74654157483363 - 123.88819292913989 - 104.14005272010685 - 123.93871757129304 - 0.6432197506783178'\n",
      "256 - random_43 - lwr_k=300 - 114.05050348810022 - 146.02377250500703 - 131.2422326113561 - 105.47375639217886 - 123.29654737366354 - 124.01650296569741 - 0.6429958312046213'\n",
      "257 - random_43 - lwr_k=1000 - 144.85199282507088 - 136.66889228038187 - 128.60333074412918 - 103.9039729627298 - 106.05395454881862 - 124.01822546113196 - 0.6429908726867133'\n",
      "258 - random_59 - lwr_k=700 - 108.59494513451821 - 132.69673556596936 - 195.11672308818322 - 101.0367350731317 - 82.79796156267166 - 124.04728741199465 - 0.6429072125498508'\n",
      "259 - random_43 - lwr_k=400 - 143.4805407651895 - 135.02870663952947 - 128.95368514703276 - 113.09544975556497 - 99.9935332724358 - 124.11205353313306 - 0.6427207714336269'\n",
      "260 - random_43 - lwr_k=600 - 142.03126831409165 - 140.91744263557177 - 120.05395701926841 - 110.44428603605144 - 107.38184139234866 - 124.16729974083539 - 0.642561735128069'\n",
      "261 - random_74 - lwr_k=400 - 113.54320678062628 - 164.3580894944648 - 102.77215463999596 - 125.60341181233814 - 114.94601137502116 - 124.24365197053758 - 0.6423419412808902'\n",
      "262 - random_42 - lwr_k=800 - 164.15437403864468 - 157.96316577528165 - 99.52719595372156 - 99.75935766635953 - 99.878807257838 - 124.26002078979204 - 0.6422948206431334'\n",
      "263 - random_26 - lwr_k=700 - 109.36275200496743 - 157.12701513065304 - 100.3078076029783 - 138.41612149692088 - 116.6329355764046 - 124.36803224593456 - 0.641983889926669'\n",
      "264 - random_26 - lwr_k=800 - 108.62729474559026 - 155.6446211624239 - 100.64913862716202 - 140.9166992524664 - 116.05657675805395 - 124.37750774665538 - 0.6419566129661205'\n",
      "265 - random_59 - lwr_k=900 - 110.29583607317632 - 132.40987182858393 - 197.4402626770748 - 98.94220584280315 - 83.31372055647196 - 124.47915616836067 - 0.6416639994071791'\n",
      "266 - random_11 - lwr_k=900 - 131.03897101873255 - 155.00824510060036 - 107.30970115286986 - 102.42075373321637 - 126.64044503074902 - 124.48418851835916 - 0.6416495128680062'\n",
      "267 - random_66 - lwr_k=1000 - 106.57522761146876 - 156.65461177156192 - 111.53197970708365 - 126.21497005979775 - 121.45628631157665 - 124.48507047454328 - 0.6416469739959456'\n",
      "268 - random_85 - lwr_k=1000 - 99.02344633767503 - 161.50839162413544 - 116.96237447420467 - 122.15877419214955 - 122.99143954034845 - 124.52668573052084 - 0.6415271768760931'\n",
      "269 - random_9 - lwr_k=700 - 137.72239582378367 - 161.9406152512754 - 104.82041121564082 - 110.02838320193486 - 108.30234904776272 - 124.5639657446538 - 0.6414198595421904'\n",
      "270 - random_9 - lwr_k=1000 - 137.18023760915966 - 164.24255319941634 - 104.47918888032547 - 107.41368150978147 - 109.50140885537219 - 124.56450204320136 - 0.6414183157088084'\n",
      "271 - random_11 - lwr_k=1000 - 130.34980108794443 - 156.27793382114186 - 108.45509321863368 - 102.8961786412193 - 125.16039868461147 - 124.62837452982684 - 0.6412344470828487'\n",
      "272 - random_43 - lwr_k=500 - 145.4229306233831 - 142.2909387527112 - 126.09696015427954 - 104.08789737120193 - 105.51273631399617 - 124.68408124590675 - 0.6410740851197645'\n",
      "273 - random_9 - lwr_k=600 - 139.70857961992778 - 160.46110282395819 - 105.03506427139645 - 110.39358427706291 - 108.02243941094021 - 124.72544628706788 - 0.6409550082889897'\n",
      "274 - random_9 - lwr_k=900 - 138.14631671973183 - 163.79020831748022 - 104.67111350657719 - 108.05454571200406 - 109.12212772823204 - 124.7580170582679 - 0.6408612473715214'\n",
      "275 - random_5 - lwr_k=500 - 160.6722948599149 - 148.92030340720606 - 107.70721382051205 - 105.53130748757108 - 101.17130903388014 - 124.80357918586807 - 0.6407300884603807'\n",
      "276 - random_59 - lwr_k=800 - 109.3718523996795 - 132.72593778461987 - 198.25811472946978 - 100.39680106401704 - 83.3935221350562 - 124.82791262901702 - 0.640660040197145'\n",
      "277 - random_67 - lwr_k=100 - 106.02216936307322 - 159.009500082706 - 114.98404497240696 - 117.87540323525714 - 126.27457877307883 - 124.83151709058882 - 0.6406496640957664'\n",
      "278 - random_9 - lwr_k=800 - 138.39944729428996 - 162.8998812399665 - 104.77394291179215 - 109.81921934744265 - 108.6980340610652 - 124.9192675564859 - 0.6403970583425815'\n",
      "279 - random_2 - lwr_k=300 - 129.40831023529523 - 157.74531153146904 - 102.65694061630457 - 115.22482688296476 - 120.15619464848655 - 125.038693636427 - 0.640053268545399'\n",
      "280 - random_7 - lwr_k=900 - 129.12379904416258 - 149.5006650841773 - 100.51367367663204 - 137.45270776029315 - 108.87124763177323 - 125.09276629208142 - 0.639897610523887'\n",
      "281 - random_59 - lwr_k=600 - 109.32443026487849 - 132.67841989211524 - 199.90787758187133 - 101.60997277578541 - 82.36232257394434 - 125.17523757957196 - 0.6396602018505577'\n",
      "282 - random_7 - lwr_k=500 - 130.6806710767796 - 149.93210498572265 - 100.52835049994731 - 137.93270584577925 - 106.96977213870399 - 125.20919279194669 - 0.6395624555661501'\n",
      "283 - random_7 - lwr_k=600 - 130.76874907822017 - 149.7241682856029 - 100.63851462291 - 136.78632233383846 - 108.15107798102946 - 125.21424550331933 - 0.6395479104130406'\n",
      "284 - random_8 - deep - 126.88925126174401 - 183.6903170790843 - 123.49864082616071 - 98.86146795693118 - 93.2369432338305 - 125.2354695153966 - 0.6394868128233917'\n",
      "285 - random_38 - lwr_k=1000 - 116.01978093903729 - 161.08095681766184 - 105.12628669526242 - 99.2183251879271 - 144.8132179693871 - 125.25091739107015 - 0.6394423436820688'\n",
      "286 - random_26 - lwr_k=600 - 112.03595575780984 - 158.75356664086212 - 101.18566831671058 - 137.8547809797401 - 116.44647224038691 - 125.2541487963267 - 0.6394330414914726'\n",
      "287 - random_81 - lwr_k=1000 - 140.95353498751558 - 158.80954484165963 - 112.5832885757582 - 100.7539504011375 - 113.44996473181557 - 125.3114057484776 - 0.6392682168905399'\n",
      "288 - random_7 - lwr_k=400 - 132.48456377083536 - 151.59960974227147 - 98.4854488021751 - 137.27094238712726 - 106.83296932926088 - 125.33532338592735 - 0.6391993656000011'\n",
      "289 - random_7 - lwr_k=800 - 129.01042446048888 - 149.94978769378346 - 99.57142746506152 - 139.39451734339553 - 108.82551615338491 - 125.35065025704807 - 0.6391552444003717'\n",
      "290 - random_7 - lwr_k=1000 - 129.8867427049613 - 149.11024023292634 - 100.09521358554044 - 138.45371587252163 - 109.25119307045716 - 125.35981151425513 - 0.6391288720472081'\n",
      "291 - random_66 - lwr_k=900 - 107.0815745182741 - 157.0890967894377 - 110.95517238951032 - 126.80192169296805 - 125.00292126788956 - 125.3845588077445 - 0.639057632440133'\n",
      "292 - random_59 - lwr_k=1000 - 109.58235043297027 - 132.2279991541668 - 203.92334669880734 - 99.85426336896873 - 82.81395046958566 - 125.67899378485218 - 0.6382100475481797'\n",
      "293 - random_80 - lwr_k=800 - 116.87692195249278 - 155.96925272951916 - 116.03449058224005 - 123.54953696558148 - 116.41593750929457 - 125.7684611052941 - 0.6379524995154187'\n",
      "294 - random_66 - lwr_k=800 - 107.47053736828998 - 157.29404347836814 - 111.74379150150597 - 126.57990802527505 - 125.78401217879947 - 125.77288004191185 - 0.6379397787987684'\n",
      "295 - random_7 - lwr_k=700 - 130.3954903004351 - 149.35472359181776 - 101.27927031834238 - 139.25942090317383 - 108.69669003374243 - 125.79751557755945 - 0.6378688609070654'\n",
      "296 - random_25 - lwr_k=700 - 115.8600316885098 - 172.59839963689393 - 102.92331387657612 - 138.16346797959505 - 99.54643458106418 - 125.8174707824464 - 0.6378114162028249'\n",
      "297 - random_94 - lwr_k=800 - 144.38893886968197 - 173.26429603250233 - 100.43381770525244 - 101.62935253411621 - 109.53776461145964 - 125.8524326143589 - 0.6377107721801102'\n",
      "298 - random_80 - lwr_k=1000 - 116.0490018921165 - 155.82560736002776 - 116.72043889869424 - 123.58712750764101 - 117.21237750151937 - 125.8780629337642 - 0.637636990621556'\n",
      "299 - random_80 - lwr_k=700 - 117.14753878590705 - 156.19742954148572 - 115.803307404165 - 123.51033612548703 - 116.74486442143048 - 125.87994213768279 - 0.6376315809896367'\n",
      "300 - random_80 - lwr_k=900 - 116.22635770518285 - 156.16353355150844 - 116.47302848277333 - 123.39899420507408 - 117.26051636974739 - 125.90365145365081 - 0.6375633293905556'\n",
      "301 - random_9 - lwr_k=500 - 148.25833176262665 - 156.87286098049663 - 108.07975624353011 - 109.30932989973444 - 108.72451120085786 - 126.25085603174244 - 0.6365638375581029'\n",
      "302 - random_0 - lwr_k=1000 - 117.0095631326636 - 175.85910615776544 - 121.47614331458148 - 100.38348279795494 - 116.7919711434737 - 126.30325178374653 - 0.6364130068102161'\n",
      "303 - random_16 - lwr_k=300 - 126.06700895800178 - 170.4957493270832 - 106.43909048237005 - 132.01155829249356 - 96.60711327426021 - 126.32408189582512 - 0.6363530435257281'\n",
      "304 - random_74 - lwr_k=300 - 109.96486303320067 - 172.3319331199212 - 104.48367621482286 - 129.94919936958496 - 115.04356384142359 - 126.3532337159905 - 0.6362691246836536'\n",
      "305 - random_74 - lwr_k=800 - 118.67492342237634 - 175.92175677425158 - 103.8833688294357 - 114.69356331583275 - 119.47690797959385 - 126.52942665996537 - 0.6357619210937511'\n",
      "306 - random_79 - lwr_k=100 - 113.35737541604652 - 159.83537744531208 - 108.87772118160203 - 133.88999583450624 - 116.79560834332399 - 126.5500778517963 - 0.6357024728638951'\n",
      "307 - random_80 - lwr_k=600 - 117.00318439277501 - 158.28534213507606 - 115.37943648444349 - 123.84469947981029 - 118.68286822526063 - 126.63827517402238 - 0.6354485807531007'\n",
      "308 - random_74 - lwr_k=700 - 117.52050574437007 - 179.5148485570483 - 103.38401201619017 - 114.69617693556421 - 118.12589601634791 - 126.64750070470532 - 0.6354220233769903'\n",
      "309 - random_81 - lwr_k=800 - 149.53907843625615 - 157.34298811647966 - 111.55042299850184 - 101.53652347257842 - 113.53150762443141 - 126.7020736859022 - 0.6352649250769659'\n",
      "310 - random_43 - lwr_k=700 - 167.8589665532813 - 137.24761651919331 - 125.01273114358953 - 101.83826521985286 - 101.64220103214952 - 126.72350378337353 - 0.6352032346248728'\n",
      "311 - random_26 - lwr_k=500 - 113.36592708827801 - 161.22208265782982 - 100.75849361473661 - 140.9587782133173 - 117.48169609243006 - 126.7562406981644 - 0.635108995432714'\n",
      "312 - random_66 - lwr_k=700 - 107.83996210302652 - 158.7545762981303 - 111.54590940195436 - 127.4802413617006 - 129.02576618002615 - 126.9276448695052 - 0.6346155772000253'\n",
      "313 - random_39 - lwr_k=400 - 135.11363371990495 - 149.05541098355542 - 112.31374003477099 - 136.623233817787 - 101.56077924296073 - 126.93406499909904 - 0.6345970956836593'\n",
      "314 - random_93 - lwr_k=1000 - 93.67380534956628 - 143.39813020637646 - 137.29755284459893 - 103.34552610734849 - 156.97644014300155 - 126.93542231293269 - 0.6345931884077292'\n",
      "315 - random_43 - lwr_k=200 - 113.98535181280717 - 139.74365095178675 - 143.3263550986829 - 122.2793803929211 - 115.63926177326505 - 126.99367811487905 - 0.6344254884351125'\n",
      "316 - random_3 - deep - 106.07641762042867 - 171.13481908669087 - 127.56898812396406 - 98.34691881835332 - 132.12519304769827 - 127.0486583358101 - 0.634267217423385'\n",
      "317 - random_66 - lwr_k=600 - 109.2589776559096 - 158.085273804136 - 111.67000110349909 - 126.76875675583513 - 129.55371331967459 - 127.06580879420687 - 0.6342178470922083'\n",
      "318 - random_11 - lwr_k=800 - 138.68576790516923 - 158.02221840138432 - 113.21727605322987 - 104.94097480645311 - 120.69522110874125 - 127.1132897126232 - 0.6340811645910289'\n",
      "319 - random_81 - lwr_k=900 - 151.57123689992306 - 158.53449264405813 - 112.75268204869182 - 101.10112786680747 - 111.85623896205499 - 127.16526055505696 - 0.633931557022996'\n",
      "320 - random_74 - lwr_k=900 - 120.11865771244891 - 176.3569727743637 - 104.81873857413231 - 114.23533636163569 - 120.7132792685427 - 127.24798207627003 - 0.6336934280061639'\n",
      "321 - random_25 - lwr_k=800 - 116.42341368852965 - 179.5699354046286 - 102.2310937771015 - 137.3501321361582 - 100.93829545736828 - 127.30163591059062 - 0.6335389756384044'\n",
      "322 - random_18 - lwr_k=100 - 135.7384958279298 - 163.71000211066655 - 137.94639234508733 - 97.66884196804814 - 101.46138293355185 - 127.30575031135746 - 0.6335271315838575'\n",
      "323 - random_80 - lwr_k=500 - 118.31556899859002 - 159.18992514678555 - 115.45790240500533 - 124.20164032438055 - 119.71630711952139 - 127.37548743461025 - 0.6333263805295568'\n",
      "324 - random_97 - lwr_k=1000 - 103.40063835859883 - 152.97613808796564 - 121.7361276811406 - 146.03338045116573 - 113.61746219618249 - 127.5506665582707 - 0.6328220954067132'\n",
      "325 - random_43 - lwr_k=900 - 166.41185235737763 - 134.72759194566987 - 126.7817006238867 - 106.52209699023936 - 103.61405344894624 - 127.61480508851233 - 0.6326374609255827'\n",
      "326 - random_40 - deep - 109.13796973392881 - 155.0450106049571 - 128.78865159015402 - 101.64310977437364 - 143.52883310058908 - 127.62712073180771 - 0.6326020076960619'\n",
      "327 - random_74 - lwr_k=1000 - 121.70444758882343 - 175.44967485677236 - 104.80590908157157 - 115.31674274858392 - 120.95504728169828 - 127.64585190059626 - 0.6325480870029457'\n",
      "328 - random_78 - lwr_k=100 - 134.9727200725054 - 142.11602907563133 - 104.19433110923823 - 109.18727987553656 - 147.7644375886174 - 127.64759129323024 - 0.6325430798433609'\n",
      "329 - random_15 - lwr_k=200 - 122.6331171893982 - 165.39577320985222 - 98.03923419244737 - 131.80036459266037 - 121.00799185677816 - 127.77485276402074 - 0.632176734441638'\n",
      "330 - random_25 - lwr_k=600 - 119.52179733481186 - 167.12029252699918 - 102.29789243267204 - 148.87896488493794 - 101.24470389073961 - 127.81201523189351 - 0.6320697554861278'\n",
      "331 - random_64 - lwr_k=1000 - 120.97363162060749 - 168.7909381028551 - 132.29965721589315 - 107.10603971434674 - 110.14355485352692 - 127.86217020583697 - 0.6319253752274085'\n",
      "332 - random_7 - lwr_k=300 - 135.71836219243065 - 159.0275178297732 - 101.51225946821198 - 136.8400119588309 - 106.55832952764057 - 127.9319677257325 - 0.6317244503103339'\n",
      "333 - random_25 - lwr_k=1000 - 113.66657128420673 - 171.33104797883635 - 99.79440393126019 - 146.13366550677387 - 108.75153086744021 - 127.93421341416662 - 0.6317179856857587'\n",
      "334 - random_13 - lwr_k=100 - 117.24665785108814 - 158.8086551726998 - 118.44761005114084 - 127.60025064074418 - 117.66638993088951 - 127.95298937169966 - 0.6316639356605432'\n",
      "335 - random_64 - lwr_k=800 - 124.89865157739536 - 168.10201349104955 - 127.65543732099255 - 106.02263513125146 - 113.37445632553275 - 128.01037040194598 - 0.6314987538781367'\n",
      "336 - random_93 - lwr_k=800 - 95.69125363667843 - 143.779175364257 - 137.85317820621535 - 104.9932000965954 - 157.76398512903745 - 128.01337089559001 - 0.631490116408818'\n",
      "337 - random_93 - lwr_k=600 - 110.42612296454035 - 148.89314359493005 - 139.00448178388723 - 109.29859336044203 - 132.4869541925138 - 128.02034178221072 - 0.631470049437695'\n",
      "338 - random_80 - lwr_k=400 - 118.97484448689231 - 159.67759759934387 - 115.95462362719778 - 124.25521792405945 - 121.49240577081625 - 128.0701534651912 - 0.631326657404752'\n",
      "339 - random_66 - lwr_k=500 - 109.41197484216633 - 159.3240542388563 - 112.85143651900722 - 129.15394225869608 - 129.79099854037835 - 128.10486912852406 - 0.631226722023283'\n",
      "340 - random_70 - lwr_k=400 - 120.45835505437185 - 149.04547833177935 - 144.550538898956 - 119.25132155663205 - 107.246978357704 - 128.10987454170086 - 0.6312123130266742'\n",
      "341 - random_64 - lwr_k=900 - 122.02947446390337 - 169.32419156399112 - 129.95914243120924 - 106.9293104922523 - 112.54266031095874 - 128.15642743909726 - 0.6310783020036042'\n",
      "342 - random_27 - lwr_k=300 - 122.88984029031351 - 160.94334029423118 - 109.83515537096474 - 113.48516818821592 - 133.97174824423186 - 128.22459038702684 - 0.6308820825006645'\n",
      "343 - random_70 - lwr_k=500 - 120.81871990313078 - 148.94460413563309 - 145.28225835833845 - 119.41683523573731 - 107.18844900697705 - 128.32952556550862 - 0.6305800074116608'\n",
      "344 - random_83 - lwr_k=900 - 122.48115779013129 - 176.87438840336628 - 122.9949249575308 - 102.52966503568862 - 117.42309722257389 - 128.4601310308672 - 0.6302040357104319'\n",
      "345 - random_0 - lwr_k=900 - 116.19933181597125 - 180.04882656747645 - 129.14925442622143 - 100.4157797810908 - 116.70734477082173 - 128.50304634988996 - 0.6300804961215067'\n",
      "346 - random_4 - lwr_k=400 - 112.73856576951493 - 146.71598281080136 - 98.05415762536174 - 185.99798356356774 - 99.78932503632754 - 128.65783001896293 - 0.6296349230421256'\n",
      "347 - random_83 - lwr_k=800 - 123.53193042579359 - 174.9376260395069 - 122.73548088529412 - 102.63616140030827 - 120.02953480054721 - 128.7736946392065 - 0.6293013855575678'\n",
      "348 - random_26 - lwr_k=400 - 114.5227298036923 - 165.04677945441125 - 102.442892281029 - 141.77074093667875 - 120.13555180858276 - 128.78250903547027 - 0.6292760117070211'\n",
      "349 - random_97 - lwr_k=900 - 103.17795366361491 - 156.66686108395723 - 122.84908248162765 - 147.7731184213573 - 114.01397291429241 - 128.89397985818815 - 0.6289551225716425'\n",
      "350 - random_27 - lwr_k=600 - 122.94146720600244 - 167.31358838966432 - 110.89780424476692 - 110.78606706831424 - 133.73288771224725 - 129.13382886972178 - 0.6282646733576163'\n",
      "351 - random_70 - lwr_k=100 - 119.670495766125 - 149.49407571965364 - 153.45497174048583 - 121.99835118646294 - 101.13777991123251 - 129.15031728639437 - 0.628217208436834'\n",
      "352 - random_21 - lwr_k=600 - 123.39615873040673 - 184.5940492784103 - 116.44727409596122 - 125.33702498269544 - 96.18533599743168 - 129.19146880584907 - 0.628098746267314'\n",
      "353 - random_33 - lwr_k=800 - 119.32157273825864 - 160.75687185188144 - 110.93691895596184 - 142.3381243857325 - 113.4168170600763 - 129.35319583037074 - 0.6276331854703201'\n",
      "354 - random_21 - lwr_k=500 - 122.70407399474783 - 182.75936760993454 - 118.84471143966104 - 126.0330524787943 - 96.47377409552789 - 129.3624216807244 - 0.6276066271739792'\n",
      "355 - random_42 - lwr_k=300 - 140.04353178072208 - 174.32722560416687 - 104.59607166460147 - 116.66656204420572 - 111.29173935375071 - 129.38594524313643 - 0.6275389103004568'\n",
      "356 - random_66 - lwr_k=400 - 112.92566741735223 - 158.2604516975453 - 111.5241792509059 - 130.9537982895708 - 133.45270216246786 - 129.4219370581229 - 0.6274353012832259'\n",
      "357 - random_27 - lwr_k=700 - 123.5530066131169 - 167.60850919137985 - 110.25738071328942 - 109.82800104824004 - 135.96891163653854 - 129.44265389335638 - 0.6273756640868389'\n",
      "358 - random_27 - lwr_k=800 - 123.69932195461625 - 167.24215945725598 - 111.19402102638294 - 110.16821048807313 - 135.10405748514273 - 129.48105544206243 - 0.6272651182106981'\n",
      "359 - random_27 - lwr_k=500 - 123.18135595908413 - 166.68623740450764 - 110.75125863096382 - 111.31590935962197 - 135.529869196204 - 129.49238182151552 - 0.6272325131574559'\n",
      "360 - random_27 - lwr_k=900 - 123.21804805986513 - 167.62550786451445 - 111.16779473299103 - 110.0546607172503 - 135.54991963578058 - 129.5226424681943 - 0.6271454023556157'\n",
      "361 - random_57 - deep - 125.52663224318931 - 159.43750461909008 - 112.60013957679554 - 117.86615945635916 - 132.21083750854334 - 129.52790872013682 - 0.6271302420814865'\n",
      "362 - random_80 - lwr_k=300 - 123.39245702479899 - 159.92947753046306 - 117.55144523250782 - 124.54192343519483 - 122.26837819235737 - 129.536206421107 - 0.6271063560382355'\n",
      "363 - random_33 - lwr_k=1000 - 120.04495330298255 - 163.7487306141618 - 107.52036328426038 - 142.88989331164458 - 113.7121944205289 - 129.58240443810544 - 0.626973366603248'\n",
      "364 - random_21 - lwr_k=700 - 123.31838433096614 - 187.30360956986237 - 117.70245054367552 - 125.42918744616118 - 94.17275166080151 - 129.58473627459315 - 0.6269666539857564'\n",
      "365 - random_72 - lwr_k=800 - 157.83762417696937 - 154.58957176212064 - 96.66432505517027 - 118.71479021207188 - 120.44008126217128 - 129.65170936173135 - 0.6267738597145613'\n",
      "366 - random_27 - lwr_k=1000 - 122.91662158191531 - 168.3034553395448 - 111.39443265313467 - 109.56515547823223 - 136.18906322730516 - 129.67316294439536 - 0.6267121016560491'\n",
      "367 - random_27 - lwr_k=400 - 123.17439033484557 - 166.2478661402259 - 109.95294793883821 - 113.31039864133724 - 135.6910557201272 - 129.6747711357733 - 0.6267074721832577'\n",
      "368 - random_70 - lwr_k=200 - 120.3694265904671 - 148.42198206477167 - 152.6899949735185 - 119.41557355931096 - 107.50572770717679 - 129.67973801989163 - 0.6266931740995854'\n",
      "369 - random_70 - lwr_k=300 - 120.51536313212453 - 149.1642988581497 - 152.61868007775752 - 119.02684031338993 - 107.3697286465458 - 129.73818679173755 - 0.6265249186278459'\n",
      "370 - random_25 - lwr_k=500 - 122.20016792036085 - 165.84334544883026 - 104.7704432372341 - 151.97324022953745 - 104.08497714227545 - 129.77378161637245 - 0.6264224524198952'\n",
      "371 - random_27 - lwr_k=200 - 123.86518104563007 - 161.80894878212067 - 111.72573130437367 - 119.80461967393654 - 131.86218475220787 - 129.81282016304513 - 0.626310072828691'\n",
      "372 - random_9 - lwr_k=400 - 155.77203616853637 - 158.77214126275143 - 112.19082642434077 - 115.49216644155238 - 107.17825869238509 - 129.88331854630667 - 0.6261071303483288'\n",
      "373 - random_97 - lwr_k=800 - 103.28588340512562 - 152.7514010747359 - 122.71732115926739 - 156.95226248665597 - 114.24940391761518 - 129.98895142739306 - 0.6258030467948671'\n",
      "374 - random_60 - lwr_k=100 - 168.88892459093722 - 158.23530697998495 - 101.483909009824 - 118.88215783166486 - 102.46128316918883 - 129.9936708013385 - 0.6257894612144618'\n",
      "375 - random_7 - lwr_k=200 - 139.71133679291563 - 158.89438310674205 - 104.57206237003463 - 137.20822067755722 - 109.58886035972988 - 129.99581056792672 - 0.6257833015052781'\n",
      "376 - random_67 - deep - 122.0244542089002 - 149.93178301404498 - 132.60622681374693 - 121.75761722911787 - 124.03339718271299 - 130.0700032775867 - 0.6255697238240505'\n",
      "377 - random_69 - lwr_k=800 - 110.0855280149649 - 150.8731689303074 - 139.53405268824278 - 109.18149822584994 - 140.9744410697601 - 130.1280092406568 - 0.625402743465459'\n",
      "378 - random_70 - lwr_k=600 - 121.12055348818355 - 149.06254093173186 - 153.67304449963058 - 119.53847860314745 - 107.3548359490771 - 130.14911203471235 - 0.6253419952160655'\n",
      "379 - random_83 - lwr_k=600 - 127.3934707853052 - 174.8419482754491 - 120.23979939003854 - 107.84747137244821 - 120.47157093896324 - 130.15861367526188 - 0.6253146430072428'\n",
      "380 - random_64 - deep - 111.43445726591965 - 147.4580487526404 - 136.4568878969996 - 130.2746971002063 - 125.39094962925682 - 130.20139052955597 - 0.6251915016067737'\n",
      "381 - random_0 - lwr_k=800 - 116.04286662299961 - 190.8495710959231 - 126.97840157557674 - 100.22253447478397 - 117.15733887473218 - 130.24891734116034 - 0.6250546874012364'\n",
      "382 - random_83 - lwr_k=700 - 123.81996001885024 - 171.60702041404184 - 126.3966760644999 - 104.11966689484929 - 125.79768787214981 - 130.34763927924644 - 0.6247704982602311'\n",
      "383 - random_81 - lwr_k=500 - 155.54247362561543 - 156.0010449767352 - 119.55324048057604 - 103.28864334417177 - 117.35174823938677 - 130.3496028690242 - 0.624764845708194'\n",
      "384 - random_72 - lwr_k=700 - 156.8126380457464 - 154.6774476722749 - 95.52030252337516 - 120.52128386555486 - 124.2103625852825 - 130.35068912464092 - 0.6247617187228793'\n",
      "385 - random_72 - lwr_k=900 - 160.83169976518522 - 154.3364136531079 - 97.03275805252788 - 120.55849972743661 - 119.32542666768342 - 130.41958243798575 - 0.6245633967296118'\n",
      "386 - random_96 - deep - 131.4955086017477 - 156.02835883123703 - 123.7364508368528 - 137.0092915760746 - 104.22535886184673 - 130.49907995979936 - 0.62433454817576'\n",
      "387 - random_85 - lwr_k=900 - 100.12582007175222 - 167.71722291622038 - 117.07789809555499 - 129.99299399016286 - 137.6201763978637 - 130.50420233896222 - 0.6243198028796868'\n",
      "388 - random_72 - lwr_k=600 - 154.7431674998473 - 155.63872734382284 - 95.81940352004142 - 121.7767242017216 - 124.64512298697159 - 130.52671763569563 - 0.6242549884831889'\n",
      "389 - random_72 - lwr_k=1000 - 163.12275261902326 - 153.65911274223112 - 97.26521392364629 - 121.14837430487783 - 118.03853773636813 - 130.6495988821967 - 0.6239012523576042'\n",
      "390 - random_85 - lwr_k=700 - 101.03957282557101 - 167.8744225462798 - 122.17173624230497 - 132.28889644978608 - 130.6453555126609 - 130.80142993161152 - 0.623464178933524'\n",
      "391 - random_33 - lwr_k=900 - 123.59368210823858 - 164.1615742120305 - 110.04862181183114 - 142.38587060595634 - 114.07339087690146 - 130.85200193594306 - 0.6233185981766172'\n",
      "392 - random_38 - lwr_k=300 - 116.23741748129805 - 161.5560498682001 - 115.74458759255278 - 95.22148780775716 - 165.55828539313518 - 130.86230431881395 - 0.6232889408846802'\n",
      "393 - random_93 - deep - 128.16660482472386 - 140.63603488121015 - 142.0107156606315 - 102.4210575148174 - 141.2921973389251 - 130.90508540388217 - 0.6231657873029518'\n",
      "394 - random_25 - lwr_k=900 - 116.21898757191217 - 175.8840530541597 - 101.26995446690161 - 140.2732005958269 - 121.20740248644839 - 130.96944749534094 - 0.6229805096697743'\n",
      "395 - random_5 - lwr_k=400 - 176.1099440703388 - 157.64676633142926 - 114.73524917392037 - 106.24344979757208 - 100.61338015222333 - 131.07364201911585 - 0.6226805666907049'\n",
      "396 - random_85 - lwr_k=600 - 101.86588107175291 - 168.05522459059515 - 125.27794578545904 - 140.94949806631274 - 119.4432745698308 - 131.11584218107566 - 0.6225590858120862'\n",
      "397 - random_21 - lwr_k=800 - 125.64878884672422 - 195.59420557036177 - 116.25719348738522 - 124.31955521934118 - 94.15148914552154 - 131.1937682322725 - 0.6223347614321064'\n",
      "398 - random_69 - lwr_k=600 - 110.28629801231347 - 155.43167150261658 - 145.9074600146694 - 123.50317356261313 - 121.1337179001802 - 131.25065614689296 - 0.6221709991731497'\n",
      "399 - random_81 - lwr_k=600 - 165.65548873941404 - 155.96473167124162 - 118.14507339559096 - 103.13639055454455 - 113.38785398279894 - 131.26087400030397 - 0.6221415852147116'\n",
      "400 - random_94 - lwr_k=700 - 163.382803266443 - 172.16465723968938 - 102.66736626187681 - 110.83551713929523 - 107.29335302625479 - 131.2715087954631 - 0.6221109710132522'\n",
      "401 - random_43 - lwr_k=800 - 186.5204581475633 - 138.74602351563763 - 125.45857563240067 - 101.36610220281918 - 104.73977636213134 - 131.37094349075264 - 0.6218247300703746'\n",
      "402 - random_74 - lwr_k=200 - 116.73402280405105 - 170.59814688495098 - 112.37507127743208 - 138.9203534334644 - 118.34933406056528 - 131.39412134551736 - 0.6217580083794959'\n",
      "403 - random_24 - lwr_k=900 - 111.76256397836708 - 152.3901957337578 - 102.0863380465213 - 168.06973963568936 - 122.90012742723691 - 131.4400958938604 - 0.6216256622398739'\n",
      "404 - random_72 - lwr_k=500 - 153.46295979313558 - 154.71749062343858 - 95.94885808332542 - 129.6473856876328 - 124.32840392115031 - 131.62290319712213 - 0.621099418007937'\n",
      "405 - random_81 - lwr_k=700 - 167.88357195601193 - 156.93245780646035 - 117.81261137629446 - 102.463372564058 - 113.54763327638274 - 131.73104733673148 - 0.6207881053378643'\n",
      "406 - random_72 - lwr_k=400 - 148.30960106207672 - 156.35791437997645 - 98.6972653620717 - 130.33557269591148 - 125.42792684286428 - 131.8270775885002 - 0.6205116647078508'\n",
      "407 - random_42 - deep - 128.50389485852472 - 187.04285322010853 - 109.34258421272386 - 104.11064792121469 - 131.03169601854964 - 132.0060341593891 - 0.619996505099581'\n",
      "408 - random_53 - lwr_k=1000 - 150.78373980335496 - 159.33005850111613 - 115.70677110415576 - 124.1432973942116 - 110.39562466287688 - 132.07351193935816 - 0.6198022583910558'\n",
      "409 - random_12 - lwr_k=100 - 162.53220448150742 - 148.5328504982114 - 117.36554261316533 - 117.77106530352674 - 115.35947513730952 - 132.3148336758088 - 0.6191075696692037'\n",
      "410 - random_24 - lwr_k=1000 - 110.90976550201667 - 153.66223392300745 - 101.4101285143962 - 170.36121019191742 - 125.77596704952599 - 132.42200573300494 - 0.6187990553160134'\n",
      "411 - random_31 - lwr_k=700 - 115.25402284420505 - 154.99013614626972 - 120.11312435878094 - 150.95010333614835 - 121.36745285125387 - 132.53347765680874 - 0.6184781630109577'\n",
      "412 - random_85 - lwr_k=400 - 103.42118058462717 - 169.96173058583616 - 132.3295930602773 - 134.03116027907487 - 123.18246827930821 - 132.58271154868595 - 0.6183364342553013'\n",
      "413 - random_25 - lwr_k=400 - 129.22810260220322 - 165.98226272670044 - 108.25866890895134 - 150.24339162394406 - 109.25190662127498 - 132.59257633070465 - 0.6183080366773928'\n",
      "414 - random_85 - lwr_k=800 - 100.83276232431281 - 167.65066586219828 - 119.74189913857555 - 131.62759360651842 - 143.6761228881589 - 132.7030601397341 - 0.6179899888413009'\n",
      "415 - random_52 - lwr_k=900 - 123.01592965490894 - 167.89759052978866 - 112.54176962383707 - 105.61410483949282 - 155.04435918515819 - 132.82190505940085 - 0.6176478720201797'\n",
      "416 - random_52 - lwr_k=700 - 121.5200228019582 - 166.53872762510787 - 113.51503496802081 - 105.85072400624041 - 156.74718221060402 - 132.83336261390744 - 0.6176148893555727'\n",
      "417 - random_21 - lwr_k=900 - 129.23176310646105 - 199.91389135873536 - 116.02465328794656 - 125.01148333982961 - 94.18752300243112 - 132.8735487366633 - 0.6174992062568845'\n",
      "418 - random_63 - lwr_k=500 - 106.20430214433537 - 136.59998167288845 - 161.35817552071663 - 106.7976829549049 - 153.89213776306838 - 132.9681477881863 - 0.6172268855983416'\n",
      "419 - random_63 - lwr_k=300 - 106.44169096857327 - 140.5481455169059 - 163.59671416700638 - 108.61634668962486 - 145.78567752066326 - 132.99542487045022 - 0.6171483635319397'\n",
      "420 - random_24 - lwr_k=800 - 114.27148310944749 - 155.34161363405497 - 104.9568928884741 - 163.4827290555207 - 127.627425392033 - 133.13440200099515 - 0.6167482924624831'\n",
      "421 - random_33 - lwr_k=700 - 119.59851117033212 - 160.64393501179723 - 129.1928145770657 - 142.78071619542064 - 113.89657913312816 - 133.2213363296523 - 0.6164980361094989'\n",
      "422 - random_12 - lwr_k=200 - 161.81215322686126 - 150.6176918722723 - 116.00045128099693 - 115.6041520613048 - 122.3037062830002 - 133.27009252838846 - 0.6163576824808514'\n",
      "423 - random_18 - deep - 113.13415971953293 - 169.77488611771557 - 116.03291010229482 - 142.6260836997285 - 124.93593408239973 - 133.29905802642386 - 0.6162742995838121'\n",
      "424 - random_33 - lwr_k=600 - 118.78370468556605 - 159.09157133686134 - 128.88183524381816 - 142.31116759418742 - 117.86941560530805 - 133.38627950765255 - 0.6160232170269393'\n",
      "425 - random_64 - lwr_k=600 - 132.69799726663587 - 168.7858983394955 - 140.2890839120397 - 106.16071400920417 - 119.24304755011104 - 133.43528462883384 - 0.6158821468295408'\n",
      "426 - random_66 - lwr_k=300 - 120.9458992076835 - 163.3419770055657 - 113.58766539224933 - 129.6521127068164 - 140.09800311556256 - 133.52404669493407 - 0.6156266290002932'\n",
      "427 - random_97 - lwr_k=600 - 104.26331618395334 - 158.16416687513856 - 122.33067589504321 - 169.2099094177447 - 113.99981235877324 - 133.59104680325703 - 0.6154337569436794'\n",
      "428 - random_69 - lwr_k=1000 - 113.94678755259379 - 150.13226942226882 - 130.7604579251846 - 122.58041453580323 - 150.80990166413594 - 133.6442674291498 - 0.6152805516455517'\n",
      "429 - random_80 - lwr_k=200 - 132.60389382282125 - 163.02319036792565 - 122.29256189825225 - 125.6881906380405 - 124.83593379172908 - 133.68866054905547 - 0.6151527579367051'\n",
      "430 - random_85 - lwr_k=500 - 102.28195497491656 - 166.69922663338787 - 127.75614492358726 - 134.9692598004348 - 137.13520444241996 - 133.76564287354364 - 0.61493115024653'\n",
      "431 - random_92 - lwr_k=900 - 135.9613187108588 - 150.41167992552707 - 118.94958088431274 - 142.19569146617135 - 121.86331204374757 - 133.8764964096855 - 0.6146120380833746'\n",
      "432 - random_64 - lwr_k=700 - 128.38725961122876 - 171.1903120493822 - 144.35880204438945 - 108.13667045560373 - 117.73080679256223 - 133.96028954984513 - 0.6143708242155579'\n",
      "433 - random_11 - lwr_k=700 - 143.34077383080265 - 159.19020810623223 - 122.81828542122098 - 105.21149570275702 - 139.37990532565914 - 133.9889402175339 - 0.614288348032477'\n",
      "434 - random_53 - lwr_k=400 - 138.07680039504598 - 160.91813380610165 - 116.58400064710025 - 135.77885675498902 - 118.7065171045778 - 134.01321220195044 - 0.6142184766894345'\n",
      "435 - random_94 - lwr_k=500 - 155.5900294809658 - 170.65960032734904 - 108.11731837280279 - 126.66799120463934 - 109.17152858554054 - 134.0431518846947 - 0.6141322898409385'\n",
      "436 - random_97 - lwr_k=700 - 103.2613796412282 - 162.35693653857425 - 122.98271705591124 - 164.05636534386718 - 118.36903003355036 - 134.20261722434395 - 0.6136732397171909'\n",
      "437 - random_12 - lwr_k=300 - 161.559960604387 - 150.02560158251683 - 115.46278697341089 - 115.50911578262972 - 128.99304391781388 - 134.31245170823584 - 0.613357061082177'\n",
      "438 - random_83 - lwr_k=500 - 130.20941699875613 - 191.25455371710288 - 117.9771024345552 - 109.21865987806241 - 123.01119811422238 - 134.3338305223282 - 0.6132955182586627'\n",
      "439 - random_52 - lwr_k=600 - 122.11992406268162 - 167.7423714859536 - 112.10136318800335 - 106.01289979901485 - 163.8403692907135 - 134.36232973037318 - 0.6132134780798468'\n",
      "440 - random_9 - lwr_k=300 - 164.126988908086 - 155.55970820782537 - 125.60067590816637 - 122.85052974851726 - 103.73362456412991 - 134.3768712385972 - 0.613171617690847'\n",
      "441 - random_72 - lwr_k=300 - 145.515169417546 - 158.96443728464186 - 108.68246064141238 - 127.19798189501785 - 131.9012887305649 - 134.4532216212446 - 0.6129518291607527'\n",
      "442 - random_94 - lwr_k=600 - 177.8956230318454 - 172.06503845334967 - 110.37393014393945 - 101.42363712402681 - 110.75622495170664 - 134.506632784117 - 0.6127980753671072'\n",
      "443 - random_52 - lwr_k=800 - 122.30099397273823 - 167.1219384694985 - 112.65810311435565 - 105.38493086702927 - 165.08507800785048 - 134.50915600470475 - 0.6127908118155065'\n",
      "444 - random_12 - lwr_k=400 - 160.36341271115077 - 150.01127463137064 - 115.28332015587095 - 115.64381504196633 - 131.5607342659959 - 134.57473548177367 - 0.6126020293055892'\n",
      "445 - random_21 - lwr_k=1000 - 130.37222308519753 - 205.03913980978697 - 115.45606463821208 - 126.83238760647797 - 95.20321995035525 - 134.58024410114388 - 0.6125861717379878'\n",
      "446 - random_39 - lwr_k=300 - 145.29468268264174 - 152.20352986131238 - 116.37373577748683 - 147.69711033316207 - 112.42913863692404 - 134.80054451549964 - 0.6119519967336007'\n",
      "447 - random_92 - lwr_k=1000 - 137.78527468894046 - 144.3910106508262 - 135.7217803519324 - 142.9113438079692 - 113.50660411417111 - 134.86345471241657 - 0.6117708982344332'\n",
      "448 - random_12 - lwr_k=500 - 160.77238911623462 - 149.9156380508221 - 115.12047717676317 - 115.45692636695786 - 133.4235687744239 - 134.94002778503784 - 0.6115504686505508'\n",
      "449 - random_27 - lwr_k=100 - 119.50886011035533 - 173.73351185818558 - 120.37916853287179 - 131.92950447025893 - 129.6160006469793 - 135.03207033888944 - 0.6112855073377756'\n",
      "450 - random_92 - lwr_k=800 - 133.6408863638534 - 156.31425192008328 - 130.30908720710156 - 141.5725914277152 - 113.40388571638084 - 135.0480191701656 - 0.6112395957121706'\n",
      "451 - random_52 - lwr_k=500 - 126.73255592816795 - 166.34436989712754 - 111.55583026112785 - 108.67673642394932 - 162.81481457316028 - 135.2241290688721 - 0.6107326311832502'\n",
      "452 - random_12 - lwr_k=700 - 160.81934700390136 - 149.8530361137396 - 115.14328102060678 - 116.38886520266428 - 134.13228679882266 - 135.26956674500246 - 0.61060183052841'\n",
      "453 - random_12 - lwr_k=600 - 160.64855724472196 - 149.82288138825342 - 115.0877486760351 - 115.2825194978579 - 135.65150506447313 - 135.3008284655817 - 0.6105118379523929'\n",
      "454 - random_57 - lwr_k=100 - 141.49821468231667 - 169.48620932847248 - 114.91440393845095 - 137.81196784102139 - 112.79165664051565 - 135.30102495702434 - 0.6105112723158649'\n",
      "455 - random_31 - lwr_k=800 - 115.97118169043897 - 154.99522378352435 - 121.66921695933539 - 160.15762792559195 - 123.91853095902573 - 135.3406857587046 - 0.6103971014498992'\n",
      "456 - random_63 - lwr_k=800 - 109.86518987059438 - 133.98902154208554 - 162.9697932894187 - 105.9292022520837 - 164.40530212425642 - 135.42949704577183 - 0.6101414419291027'\n",
      "457 - random_63 - lwr_k=700 - 108.45658206034524 - 134.71719871190157 - 165.53096377416747 - 106.57129095401793 - 161.95743307942445 - 135.44436617960915 - 0.6100986384098988'\n",
      "458 - random_12 - lwr_k=800 - 160.5506883935883 - 149.83077514485095 - 114.9937326943919 - 117.36755931662536 - 134.6633838241979 - 135.48338978060522 - 0.6099863018425561'\n",
      "459 - random_63 - lwr_k=400 - 107.23649599470643 - 140.35901945591607 - 167.53838673605824 - 108.40572722253314 - 153.95775206965627 - 135.49703899150524 - 0.6099470100944778'\n",
      "460 - random_48 - deep - 123.49070792362608 - 140.29725822655996 - 112.66368087235583 - 188.49079118426783 - 113.58350256508206 - 135.70413576926856 - 0.6093508437462959'\n",
      "461 - random_12 - lwr_k=900 - 160.6743743631539 - 149.8663071210147 - 115.00117702182806 - 118.01802883985928 - 135.31653374768214 - 135.77743143241443 - 0.6091398492093577'\n",
      "462 - random_94 - lwr_k=300 - 139.67553521983302 - 169.87557890022728 - 115.57721093079547 - 141.2067911603909 - 112.73488924365034 - 135.81433409668213 - 0.6090336181460059'\n",
      "463 - random_80 - deep - 137.36338040582064 - 167.81139590587014 - 122.2772018840571 - 127.04902627047612 - 125.14419715555643 - 135.92916416449458 - 0.6087030583841928'\n",
      "464 - random_26 - lwr_k=300 - 117.72637790069706 - 174.09728705337912 - 106.82564227784155 - 144.5903511384871 - 136.54273985361567 - 135.95490754220455 - 0.6086289517921472'\n",
      "465 - random_12 - lwr_k=1000 - 160.42890621357984 - 149.84598872386036 - 115.11380936985343 - 118.61627032678979 - 135.99529084068257 - 136.00215975700206 - 0.608492927656132'\n",
      "466 - random_31 - lwr_k=1000 - 116.74934979412599 - 156.99378671033918 - 118.64560220853197 - 154.85160641102928 - 133.2956914722414 - 136.10553796279225 - 0.6081953345975704'\n",
      "467 - random_69 - lwr_k=500 - 112.20252458997484 - 159.83213385572003 - 147.65013983154537 - 127.3147313628536 - 133.6022752990863 - 136.11829839414875 - 0.6081586013638398'\n",
      "468 - random_63 - lwr_k=600 - 107.02551960362558 - 135.39057115830653 - 173.1912811694554 - 106.88851808324684 - 158.39899656237728 - 136.17646321935956 - 0.6079911632843831'\n",
      "469 - random_53 - lwr_k=700 - 165.33084063554682 - 159.63767496830008 - 116.24847027244687 - 127.70255271408921 - 111.957474604302 - 136.17791690575282 - 0.6079869785824272'\n",
      "470 - random_64 - lwr_k=500 - 137.16355708394616 - 170.77159005879415 - 141.2376430950682 - 113.94675705038698 - 118.11971824026197 - 136.2479320729197 - 0.6077854271279051'\n",
      "471 - random_52 - lwr_k=1000 - 122.60277846017381 - 168.433374416164 - 112.0814765308289 - 104.72756049122025 - 173.4118864591982 - 136.250238259029 - 0.6077787883497155'\n",
      "472 - random_93 - lwr_k=400 - 105.42373280833363 - 153.48792189431947 - 141.67325897865697 - 114.78079382629944 - 166.15383581310533 - 136.30124566174078 - 0.6076319542189985'\n",
      "473 - random_31 - lwr_k=900 - 117.16984435522232 - 155.79567950771852 - 120.58320984222918 - 159.03686543625426 - 129.03961817765835 - 136.3233915839348 - 0.6075682031346185'\n",
      "474 - random_69 - lwr_k=900 - 109.79518520313934 - 152.8869211147576 - 130.53018806231015 - 130.72425954947548 - 157.7948178008315 - 136.3439846695641 - 0.6075089221740833'\n",
      "475 - random_37 - lwr_k=900 - 100.42008118520974 - 151.74727375903805 - 155.99910670842178 - 165.51356668856147 - 108.2036673933728 - 136.37363836579266 - 0.6074235585900392'\n",
      "476 - random_58 - deep - 143.48919285741346 - 167.66582944013786 - 115.52159550951801 - 109.54921443753327 - 145.8706377201319 - 136.41990259977865 - 0.6072903781103143'\n",
      "477 - random_63 - lwr_k=900 - 109.360098343179 - 133.5559010087864 - 167.07103304744098 - 106.96093566055914 - 165.35281958304012 - 136.45782051073417 - 0.6071812249008641'\n",
      "478 - random_98 - lwr_k=100 - 148.53487028409324 - 197.86860438331158 - 102.09388632980655 - 110.57049392583095 - 123.99834932643353 - 136.6142689310813 - 0.6067308595306937'\n",
      "479 - random_61 - lwr_k=200 - 140.19925625727814 - 186.14164808735327 - 118.772599195408 - 116.628292715531 - 121.40893928835526 - 136.63045489674207 - 0.6066842652704245'\n",
      "480 - random_33 - lwr_k=500 - 122.34373480281083 - 158.6876066355036 - 139.22344701136856 - 144.01272819320388 - 119.61666505142097 - 136.7755916767767 - 0.6062664625241864'\n",
      "481 - random_63 - lwr_k=1000 - 107.9932547787475 - 133.29341514785878 - 166.2335824406962 - 106.0209598644175 - 170.54140195959937 - 136.8140372166651 - 0.6061557900113875'\n",
      "482 - random_75 - lwr_k=200 - 114.27251869373434 - 166.3187586265499 - 117.55838715333115 - 148.79897836636894 - 137.43036998705693 - 136.87385333430515 - 0.6059835983848864'\n",
      "483 - random_31 - lwr_k=500 - 115.18704658922489 - 154.17678068850515 - 124.81822491931337 - 159.7650413160157 - 131.03655794678676 - 136.99484949827283 - 0.6056352888138878'\n",
      "484 - random_37 - lwr_k=1000 - 100.09421717068098 - 150.68693483937687 - 157.81660527418182 - 168.06504710824365 - 108.60797555768603 - 137.0509686893423 - 0.605473739466163'\n",
      "485 - random_69 - lwr_k=700 - 109.95146332173759 - 155.0573770310138 - 136.84354247539392 - 131.9826106365537 - 152.17869006509923 - 137.20038664788925 - 0.6050436125651577'\n",
      "486 - random_75 - lwr_k=400 - 120.54607622492988 - 161.72288211975467 - 112.52051269606886 - 151.0062446259511 - 140.24631678870907 - 137.20696958781724 - 0.6050246623694899'\n",
      "487 - random_42 - lwr_k=900 - 231.05058603214727 - 157.29346877890225 - 98.7346403778116 - 98.86987177574628 - 100.05650311567766 - 137.20910728718638 - 0.6050185086111742'\n",
      "488 - random_61 - lwr_k=300 - 142.6982376599925 - 190.00623657688303 - 117.00182325503968 - 115.70683314129796 - 121.93247581653226 - 137.4695722313401 - 0.6042687126673743'\n",
      "489 - random_75 - lwr_k=300 - 125.02134382591117 - 164.56639407530622 - 112.35548108394237 - 146.99667954277425 - 138.74520652796082 - 137.53594170131478 - 0.6040776560185003'\n",
      "490 - random_64 - lwr_k=400 - 142.53632882956464 - 178.83427105175775 - 121.95342134948176 - 125.27956594020193 - 119.44439049842737 - 137.6100203987794 - 0.6038644069493735'\n",
      "491 - random_75 - lwr_k=500 - 120.03944277804037 - 161.79399292360372 - 112.8090389140409 - 152.52213240896884 - 141.23749063288025 - 137.6788982330631 - 0.6036661295154122'\n",
      "492 - random_5 - lwr_k=300 - 183.96593124447423 - 161.7384406935625 - 120.1953289317923 - 112.3665596241364 - 110.38911503668469 - 137.73506224446552 - 0.6035044511441677'\n",
      "493 - random_24 - lwr_k=600 - 115.68302884693458 - 157.4409609251087 - 108.63872655034581 - 191.7858559188869 - 115.20355770219268 - 137.74852297065806 - 0.603465701983767'\n",
      "494 - random_81 - deep - 129.22057884479392 - 196.70493244600482 - 123.64199983071232 - 111.07034724680912 - 128.72026506812986 - 137.87087761490125 - 0.6031134813474435'\n",
      "495 - random_75 - lwr_k=600 - 120.43544804892643 - 162.51184051183293 - 114.2085562913009 - 151.97778643305077 - 141.21875015675445 - 138.0689555029092 - 0.602543278378072'\n",
      "496 - random_94 - lwr_k=400 - 144.9903259542015 - 173.30749487124785 - 112.47434090272813 - 147.6516388686251 - 112.10407131442525 - 138.1061681000424 - 0.602436155116324'\n",
      "497 - random_70 - deep - 142.49576055592505 - 159.53031665428594 - 122.56052303931075 - 138.5042744431325 - 127.64246337482672 - 138.14704203603563 - 0.602318491589085'\n",
      "498 - random_61 - lwr_k=400 - 144.72473311359323 - 192.18228186571545 - 115.92589385065659 - 115.35779680089688 - 122.7606672539925 - 138.1908380866757 - 0.6021924170856823'\n",
      "499 - random_24 - lwr_k=700 - 116.07585768260361 - 157.3150368031012 - 104.58532173235855 - 179.41264016533202 - 133.79903822220427 - 138.23566776889172 - 0.6020633666524402'\n",
      "500 - random_75 - lwr_k=700 - 120.63297925509858 - 162.20385174787017 - 114.49614700047984 - 152.1064773544982 - 141.76045998945278 - 138.23846470074807 - 0.60205531517293'\n",
      "501 - random_70 - lwr_k=1000 - 121.6862162748709 - 149.14443357141565 - 193.27271800144337 - 120.18765828721808 - 107.36257185586395 - 138.32928423223245 - 0.6017938745535558'\n",
      "502 - random_70 - lwr_k=700 - 120.75947980767378 - 149.12195589859635 - 194.83012219930987 - 119.59434515321634 - 107.40128180141906 - 138.33992076325003 - 0.6017632553551107'\n",
      "503 - random_87 - lwr_k=200 - 115.96344774049312 - 223.06519225982802 - 115.6872432076633 - 139.17369634297998 - 97.92618720812021 - 138.3612216766176 - 0.6017019367830696'\n",
      "504 - random_70 - lwr_k=900 - 121.5750256148711 - 149.1004782658339 - 194.1143070791707 - 120.10260369301766 - 107.44903363545724 - 138.46683283945325 - 0.6013979157495618'\n",
      "505 - random_96 - lwr_k=600 - 164.08539493570078 - 153.5274227734927 - 147.19886454071812 - 126.81975076119957 - 100.7623315082342 - 138.4809611344686 - 0.6013572448703011'\n",
      "506 - random_75 - lwr_k=900 - 121.06198326130487 - 162.44926198868163 - 114.70386999954232 - 151.98203338176307 - 142.26041882032453 - 138.49001042631596 - 0.6013311948298881'\n",
      "507 - random_70 - lwr_k=800 - 121.54545864308095 - 149.13345716415373 - 194.4833864345684 - 119.92223674368381 - 107.41623774884023 - 138.49869323090272 - 0.6013061997900329'\n",
      "508 - random_61 - lwr_k=500 - 145.75806514522827 - 193.09956590838163 - 115.60483021079804 - 114.92697318605315 - 123.2231310595424 - 138.52313707164916 - 0.6012358337271368'\n",
      "509 - random_0 - lwr_k=700 - 118.47969834461483 - 220.1270879529016 - 136.4368684893692 - 101.71939846435508 - 115.92495343142045 - 138.53587161050672 - 0.6011991750296963'\n",
      "510 - random_44 - lwr_k=1000 - 143.1303935740671 - 175.75921170243208 - 138.9587750135261 - 112.11411722824312 - 122.81285789678928 - 138.55546564341952 - 0.6011427700249905'\n",
      "511 - random_44 - lwr_k=900 - 143.84542251672931 - 175.57751131223995 - 138.16646859624075 - 112.72539100967498 - 123.14013028608753 - 138.69142924555467 - 0.6007513739478079'\n",
      "512 - random_75 - lwr_k=1000 - 120.84511699841048 - 163.11506136133997 - 114.14523555879701 - 152.90022278271658 - 142.46327832053979 - 138.69224379549513 - 0.6007490291169386'\n",
      "513 - random_75 - lwr_k=800 - 120.95571158467436 - 162.24648480802702 - 115.80475968191574 - 152.7399716654819 - 141.99253594493518 - 138.7463583992048 - 0.6005932503402834'\n",
      "514 - random_31 - lwr_k=600 - 114.51022144421192 - 154.31456308071674 - 123.14679819506094 - 146.63786045847303 - 155.5661070194879 - 138.8330123430917 - 0.6003438011261031'\n",
      "515 - random_44 - lwr_k=800 - 145.27114396750125 - 175.13574942273482 - 138.29112452609164 - 113.34189156040281 - 122.29495400725116 - 138.86752497096595 - 0.6002444502193081'\n",
      "516 - random_84 - deep - 135.91391396358097 - 173.14800572693477 - 105.97390447026854 - 128.28531792802306 - 151.119014228403 - 138.8877751387994 - 0.600186155975557'\n",
      "517 - random_8 - lwr_k=200 - 116.67669210072611 - 160.23037504829003 - 121.85992042327202 - 181.91110985911325 - 114.45116692633763 - 139.02392555516536 - 0.5997942225542314'\n",
      "518 - random_81 - lwr_k=400 - 161.8823184800491 - 154.707858361573 - 155.8489114577248 - 107.00321958002195 - 115.77579040289261 - 139.04558918894827 - 0.5997318598251848'\n",
      "519 - random_11 - lwr_k=500 - 134.89595852250633 - 161.6711122620293 - 121.07646110465427 - 117.40369974303539 - 160.44591779905133 - 139.09826746202594 - 0.5995802157887546'\n",
      "520 - random_33 - lwr_k=400 - 138.22577783874956 - 157.68743784423398 - 132.5481341354391 - 144.09874411353314 - 123.39091083573305 - 139.19011778493527 - 0.5993158078478784'\n",
      "521 - random_2 - lwr_k=200 - 135.9092777773122 - 167.52219559947906 - 117.23592591086434 - 138.6853277884502 - 137.37548491776394 - 139.34534605825814 - 0.598868954893909'\n",
      "522 - random_25 - lwr_k=300 - 139.23833430672195 - 170.345653600615 - 114.25725157023103 - 160.65237536011352 - 112.81180998488502 - 139.4610657552465 - 0.5985358345974272'\n",
      "523 - random_61 - lwr_k=600 - 147.04593368702336 - 194.9889104461777 - 116.27240118153381 - 114.88861792998004 - 124.13193139167353 - 139.4662126332763 - 0.5985210183683851'\n",
      "524 - random_97 - lwr_k=500 - 104.33177212496578 - 177.2993848133057 - 122.83404671704841 - 178.60380133220693 - 114.52790908433325 - 139.51634835329153 - 0.5983766935356172'\n",
      "525 - random_13 - deep - 133.31610541508115 - 177.11568882240206 - 151.5741532174818 - 118.70020584198564 - 117.24367835080639 - 139.58942377366466 - 0.5981663321459966'\n",
      "526 - random_44 - lwr_k=700 - 145.0182748217216 - 173.95271281441399 - 138.45668493755423 - 118.56879101959751 - 121.97936819270737 - 139.59563402781507 - 0.5981484552314096'\n",
      "527 - random_61 - lwr_k=100 - 135.38538478219374 - 192.62900590815096 - 126.07346505333211 - 128.51301349863763 - 115.60695959314853 - 139.6411987283736 - 0.598017288913538'\n",
      "528 - random_20 - lwr_k=100 - 108.05397435011245 - 164.4506849236855 - 179.44931496535352 - 123.8025938177303 - 122.51375699477398 - 139.65133992489993 - 0.5979880956259522'\n",
      "529 - random_61 - lwr_k=700 - 145.8434915639602 - 197.2562010333558 - 116.11927626115306 - 114.99234868554653 - 124.3021286840862 - 139.70321880773818 - 0.5978387527804212'\n",
      "530 - random_44 - lwr_k=600 - 148.69937752851888 - 173.97930656505469 - 140.76112076635576 - 115.5694931431929 - 119.77869865366945 - 139.758370440206 - 0.5976799887269603'\n",
      "531 - random_87 - lwr_k=400 - 113.2522112050378 - 238.7015421475961 - 118.36896085264631 - 134.97083187071215 - 93.57703803332842 - 139.77182966201448 - 0.5976412438972041'\n",
      "532 - random_52 - lwr_k=400 - 125.27307527154967 - 166.1792588071453 - 113.54900545964057 - 111.18884094776183 - 183.05690157295405 - 139.8481593972242 - 0.5974215148045328'\n",
      "533 - random_87 - lwr_k=300 - 115.16368498285783 - 234.71455542249484 - 115.55378187748458 - 137.25172601459678 - 96.75592130477534 - 139.88580178445207 - 0.5973131543134498'\n",
      "534 - random_11 - lwr_k=600 - 141.2944845845433 - 156.6783209704864 - 128.6036285539795 - 110.30204334066671 - 163.00647499228347 - 139.97710410464714 - 0.5970503238985383'\n",
      "535 - random_20 - lwr_k=700 - 108.67972372395394 - 169.40083896784998 - 163.94752076326762 - 123.6292743399675 - 134.27331207568798 - 139.98343421472404 - 0.5970321015198037'\n",
      "536 - random_61 - lwr_k=800 - 145.0135306792192 - 198.86143368920577 - 116.23740104289504 - 115.61989483264699 - 124.60216486597183 - 140.06731160405548 - 0.5967906451253489'\n",
      "537 - random_72 - lwr_k=200 - 139.29330454277923 - 169.28947353164705 - 114.59631205125375 - 137.80791120017216 - 139.5569143580184 - 140.10871281264562 - 0.5966714641800199'\n",
      "538 - random_2 - deep - 117.45002544008452 - 178.69123460671779 - 122.4333360843897 - 150.64754567674748 - 131.39804097217134 - 140.1220841832548 - 0.5966329718073704'\n",
      "539 - random_7 - deep - 121.41075689381566 - 153.31234467487081 - 141.59264125388057 - 152.18629711195538 - 133.0030527694722 - 140.29938825964882 - 0.5961225696191759'\n",
      "540 - random_86 - lwr_k=700 - 102.20809366160434 - 192.17907171899492 - 102.96461603203397 - 119.83597057269077 - 184.928270188262 - 140.4199088922221 - 0.5957756293913844'\n",
      "541 - random_97 - lwr_k=400 - 105.14571427276907 - 174.31372271912255 - 125.66249116108578 - 182.0071131991222 - 115.11247198447853 - 140.44525829094493 - 0.5957026565143553'\n",
      "542 - random_61 - lwr_k=900 - 145.56986663205475 - 199.70861451014383 - 116.71754047955187 - 115.95136882296738 - 124.71725016744024 - 140.53336249105095 - 0.5954490324726023'\n",
      "543 - random_0 - lwr_k=600 - 122.53270411615856 - 193.59071468319277 - 169.441312359401 - 101.90736278654785 - 115.53654291305416 - 140.60016915993788 - 0.5952567172667578'\n",
      "544 - random_15 - deep - 124.42538407424401 - 167.37744901261487 - 112.34945013154011 - 138.14923672431397 - 160.94782490666543 - 140.64846742844173 - 0.5951176814337005'\n",
      "545 - random_31 - lwr_k=400 - 120.6606708954389 - 157.2244074090376 - 123.26754903041194 - 143.22290140359306 - 158.97049322550095 - 140.66747892422998 - 0.5950629538088448'\n",
      "546 - random_83 - lwr_k=400 - 143.29410497668738 - 180.73058750987497 - 115.55415636381336 - 114.59391021992127 - 149.63225208489055 - 140.76122067729014 - 0.5947931010406053'\n",
      "547 - random_87 - lwr_k=500 - 112.91032401219084 - 239.38784974640802 - 123.70131620149952 - 135.9346274051186 - 92.5562785886892 - 140.8956656210004 - 0.5944060766992457'\n",
      "548 - random_61 - lwr_k=1000 - 146.1913739380801 - 200.49311434089907 - 117.41996617426344 - 115.65150946843445 - 124.82009788407204 - 140.91566735956107 - 0.594348498039321'\n",
      "549 - random_74 - lwr_k=100 - 119.77936789327289 - 178.04639246815492 - 142.90215347553817 - 141.1039850918863 - 122.8912022597144 - 140.94279501760775 - 0.594270406117837'\n",
      "550 - random_10 - lwr_k=700 - 119.30660451178981 - 214.24112352189908 - 135.02954144942981 - 108.60698268882881 - 128.64984076822338 - 141.16493343676854 - 0.5936309400807084'\n",
      "551 - random_10 - lwr_k=800 - 120.05962045157132 - 214.2106024472379 - 135.2878449120177 - 108.7419067170048 - 128.51966853451887 - 141.3620914006591 - 0.5930633848494533'\n",
      "552 - random_7 - lwr_k=100 - 153.27303152983944 - 173.87886177792595 - 110.45618214409379 - 154.12682202477694 - 115.24726854133246 - 141.3974574014487 - 0.5929615773527608'\n",
      "553 - random_10 - lwr_k=900 - 119.89251266817888 - 214.55814922076834 - 135.23292781648905 - 108.94829451290882 - 128.50929364353692 - 141.426378404137 - 0.5928783229609778'\n",
      "554 - random_87 - lwr_k=600 - 113.32332521576143 - 242.4367879727049 - 123.06050116964819 - 136.39658592764476 - 92.18754081795801 - 141.47852000213317 - 0.5927282238418514'\n",
      "555 - random_86 - lwr_k=1000 - 100.18621855924235 - 198.2653560331434 - 105.21493979586656 - 117.37795716391705 - 186.39404798382412 - 141.4841422061511 - 0.5927120393000829'\n",
      "556 - random_86 - lwr_k=800 - 101.35250127474022 - 195.37668308173596 - 103.8780322075227 - 122.14547814617275 - 184.75599660133443 - 141.498275927273 - 0.592671352800783'\n",
      "557 - random_10 - lwr_k=1000 - 119.44696571031638 - 214.74799360645704 - 135.64947254470636 - 109.3130975111961 - 128.50573988069212 - 141.53074925528378 - 0.5925778723913433'\n",
      "558 - random_86 - lwr_k=900 - 102.16112790877418 - 194.66253288763744 - 105.41267819360196 - 120.80560147110398 - 184.80244852693025 - 141.56547940593236 - 0.5924778953054882'\n",
      "559 - random_10 - lwr_k=600 - 120.27959024730545 - 214.6573160763651 - 135.40530189528195 - 109.1642950843299 - 128.6447154406553 - 141.62840254031033 - 0.5922967595634748'\n",
      "560 - random_9 - lwr_k=200 - 162.7959312175072 - 168.2563620453627 - 117.54401989756578 - 151.14320727260787 - 108.45871474380432 - 141.64147148209102 - 0.5922591382261053'\n",
      "561 - random_10 - lwr_k=300 - 124.86586995255185 - 216.01447761477976 - 133.79363860924366 - 109.02878414337063 - 125.28177720635807 - 141.79544942941115 - 0.591815884564042'\n",
      "562 - random_38 - lwr_k=200 - 117.771244441349 - 168.85056892152122 - 113.26627192153498 - 100.58940480065745 - 208.99865563855602 - 141.8931487735006 - 0.5915346391467433'\n",
      "563 - random_97 - lwr_k=300 - 106.7292315982745 - 183.71044532336586 - 128.43453289010986 - 183.89186551835428 - 106.81989664481102 - 141.91415990353806 - 0.5914741547689782'\n",
      "564 - random_10 - lwr_k=200 - 121.40135442307415 - 218.04943113025666 - 135.8545169256202 - 111.22141363566645 - 123.61080614790764 - 142.02572572276807 - 0.5911529921687781'\n",
      "565 - random_76 - lwr_k=500 - 140.47261865292265 - 188.9148600231425 - 130.33548901905144 - 124.91586904886063 - 125.53902280080433 - 142.03543712512948 - 0.5911250361222444'\n",
      "566 - random_96 - lwr_k=700 - 164.55289987996198 - 151.4732090250428 - 148.11950037631962 - 133.72709381887054 - 112.56523078129995 - 142.08952410926753 - 0.5909693368535737'\n",
      "567 - random_10 - lwr_k=500 - 121.3875655396285 - 214.44637739000856 - 134.87774968156805 - 109.3188107139235 - 130.61368860716198 - 142.12704972891697 - 0.5908613124993054'\n",
      "568 - random_30 - lwr_k=300 - 168.80392282649163 - 180.83562434541076 - 117.41056957003185 - 122.776735130337 - 120.83956473064563 - 142.1355833067429 - 0.5908367470359538'\n",
      "569 - random_87 - lwr_k=700 - 115.01364188853702 - 246.4177747756678 - 119.27455553966477 - 137.6753434521537 - 92.71313122784068 - 142.21654328782074 - 0.5906036889343367'\n",
      "570 - random_87 - lwr_k=800 - 114.02848633936132 - 247.12389215547847 - 118.85346731641613 - 138.8049442946986 - 92.52227681698906 - 142.26417822358104 - 0.5904665630660848'\n",
      "571 - random_96 - lwr_k=1000 - 188.58641061312724 - 154.93530278726288 - 138.13361934498016 - 125.43627626319473 - 104.58014425887858 - 142.33833927542386 - 0.5902530769248293'\n",
      "572 - random_87 - lwr_k=900 - 113.80691087499882 - 248.5141186036085 - 118.80209283967525 - 138.65029259172928 - 92.31904354717351 - 142.41602432503345 - 0.5900294463119635'\n",
      "573 - random_59 - lwr_k=100 - 136.62256372658183 - 150.01500135530438 - 208.18213206757093 - 105.84591499603897 - 111.58194701482634 - 142.44900933567732 - 0.5899344929305503'\n",
      "574 - random_10 - lwr_k=400 - 125.92033531504555 - 213.59000708641824 - 132.56899605724016 - 109.0117759990201 - 131.43911673017612 - 142.50461594170883 - 0.5897744191525364'\n",
      "575 - random_20 - lwr_k=1000 - 108.43264656560694 - 169.2140484621208 - 179.18917257595854 - 122.75553794726287 - 133.54848616082882 - 142.62502945206785 - 0.589427786856435'\n",
      "576 - random_87 - lwr_k=1000 - 112.93233262457841 - 250.22363849651603 - 119.16013706719802 - 139.10534187973022 - 92.30000847400544 - 142.7417208254215 - 0.589091869411954'\n",
      "577 - random_86 - lwr_k=600 - 103.344695340881 - 188.6703166124797 - 102.50762444620001 - 122.3856687332157 - 197.18898122633252 - 142.8160531012517 - 0.5888778903711585'\n",
      "578 - random_87 - lwr_k=100 - 117.74070918142634 - 214.9829750821136 - 129.74849063848018 - 149.6195945447511 - 102.1162004976722 - 142.83942937308947 - 0.5888105975004725'\n",
      "579 - random_1 - lwr_k=700 - 137.17218229071872 - 185.3257684611662 - 107.90096600226214 - 158.5263319269644 - 125.84510961379026 - 142.95357304830702 - 0.5884820140706886'\n",
      "580 - random_97 - deep - 108.9950623084759 - 166.09283913122573 - 154.75843123764557 - 148.2564866023621 - 137.8246273190464 - 143.18253905036906 - 0.5878228931401771'\n",
      "581 - random_42 - lwr_k=200 - 150.33998614472176 - 183.73315931424432 - 110.34959556142584 - 141.02338650129926 - 130.63846964266764 - 143.2175337021639 - 0.587722155087639'\n",
      "582 - random_85 - deep - 110.71670815040325 - 155.88570347136596 - 162.7768893058879 - 148.73040027297637 - 138.15683285744862 - 143.2505027105856 - 0.5876272473231281'\n",
      "583 - random_16 - lwr_k=200 - 142.23132258996495 - 179.11067785790382 - 133.60779358356925 - 145.68525621573266 - 116.2408512684085 - 143.37508166067767 - 0.5872686244959067'\n",
      "584 - random_1 - lwr_k=800 - 136.84041819079098 - 186.32635578393885 - 108.25926129097112 - 159.3673365640431 - 126.14860659969575 - 143.38783101035372 - 0.5872319231628185'\n",
      "585 - random_1 - lwr_k=900 - 136.29616511414443 - 184.804724240558 - 107.65596478551706 - 159.65605822942925 - 128.64648457772918 - 143.4112657542328 - 0.5871644619696755'\n",
      "586 - random_9 - deep - 140.31542716190734 - 203.05215714665619 - 123.12938931009901 - 104.5608378414863 - 146.01282507142614 - 143.41385808324125 - 0.5871569990279255'\n",
      "587 - random_1 - lwr_k=1000 - 136.91501811421656 - 184.42805409189992 - 107.60022872700284 - 159.36354530468714 - 128.98625879821003 - 143.45805670892 - 0.5870297656551869'\n",
      "588 - random_84 - lwr_k=200 - 109.34987921506514 - 176.2561258525945 - 107.76949414245816 - 109.39814342357612 - 214.62790260190638 - 143.47736575375754 - 0.5869741810407396'\n",
      "589 - random_69 - lwr_k=400 - 113.0663333102273 - 167.68043728309192 - 150.4298736503998 - 132.74569096953604 - 154.85118907095637 - 143.75205839499796 - 0.5861834280706499'\n",
      "590 - random_30 - lwr_k=400 - 175.0909618517844 - 179.28084980483524 - 114.75799137441477 - 133.29251940051685 - 117.2774458473499 - 143.94264001385162 - 0.5856348040504633'\n",
      "591 - random_20 - lwr_k=800 - 111.55431502046602 - 167.90244571128267 - 191.5385926920679 - 119.34087914287711 - 129.80219626176392 - 144.02488537152587 - 0.5853980457574006'\n",
      "592 - random_20 - lwr_k=200 - 112.21956595593144 - 167.69184316926845 - 187.0994676850711 - 120.98538142924941 - 132.26027531818926 - 144.0485616493864 - 0.5853298892645498'\n",
      "593 - random_20 - lwr_k=300 - 110.48266713931785 - 167.77651524795317 - 184.9857348267398 - 122.30323925758074 - 135.21941496757626 - 144.15061062733437 - 0.5850361226312589'\n",
      "594 - random_44 - lwr_k=500 - 154.61299051242256 - 180.34091237837794 - 142.6485225394637 - 121.72339642544809 - 121.47950843583024 - 144.1619673970851 - 0.5850034300939908'\n",
      "595 - random_96 - lwr_k=400 - 169.8710394444465 - 154.00985722819107 - 162.6154846539879 - 136.48644679308006 - 98.25672571712128 - 144.25012041971763 - 0.5847496655076726'\n",
      "596 - random_96 - lwr_k=500 - 163.5876676583378 - 156.45204126464665 - 163.5610767783282 - 140.73575272909585 - 97.1509658407975 - 144.29916437328262 - 0.5846084835241607'\n",
      "597 - random_20 - deep - 109.88826474156873 - 160.00942195680133 - 185.16602704441718 - 123.91914261328141 - 142.63371799628555 - 144.32034711806847 - 0.5845475046702107'\n",
      "598 - random_66 - lwr_k=200 - 134.45951793839492 - 171.73633218251868 - 120.52756608410186 - 145.49495937976062 - 149.73631483222923 - 144.39008163116367 - 0.5843467615814935'\n",
      "599 - random_92 - lwr_k=700 - 140.8477092819969 - 155.1054691314169 - 160.74341085035107 - 142.73018713872978 - 122.62919498801905 - 144.4108869751538 - 0.5842868695964747'\n",
      "600 - random_76 - lwr_k=600 - 146.68294524521468 - 192.57027406605087 - 129.7388491827407 - 126.51794531156335 - 127.39288538439571 - 144.5807611389078 - 0.5837978557702457'\n",
      "601 - random_24 - lwr_k=500 - 120.32774717763341 - 163.36420720760913 - 107.66804533640584 - 190.8526665750603 - 140.88662563801637 - 144.61776351705802 - 0.5836913376622594'\n",
      "602 - random_86 - lwr_k=500 - 105.7101012401043 - 190.97024047120755 - 105.3433720467528 - 122.40783810776523 - 198.80066758566113 - 144.64308615119418 - 0.5836184418319844'\n",
      "603 - random_20 - lwr_k=900 - 108.2434301826876 - 167.1748608458543 - 189.60001139776537 - 123.2665120040627 - 136.9944562487952 - 145.0526795563269 - 0.5824393523588365'\n",
      "604 - random_71 - lwr_k=100 - 138.59662898259663 - 192.92182005330972 - 115.43237557173855 - 118.45138399863646 - 160.45236592563523 - 145.17034796209876 - 0.5821006223479791'\n",
      "605 - random_0 - lwr_k=500 - 118.65857405880298 - 206.4748946661221 - 177.14535465054888 - 105.24030151344822 - 118.4404280705895 - 145.18962244628892 - 0.5820451372226715'\n",
      "606 - random_69 - lwr_k=300 - 117.31399458530716 - 181.4773524938139 - 166.91775278179207 - 123.83601855835084 - 139.091216945148 - 145.72481680792143 - 0.5805044824416588'\n",
      "607 - random_62 - deep - 126.25941372575431 - 180.28556920455006 - 101.66947748168577 - 147.3776136384333 - 173.5302776848257 - 145.82278322524152 - 0.5802224677487849'\n",
      "608 - random_76 - lwr_k=700 - 146.77593680109334 - 192.22864487837325 - 130.72895587848984 - 127.26062466188588 - 132.5024668883833 - 145.89940141762472 - 0.5800019087358929'\n",
      "609 - random_99 - lwr_k=200 - 149.0778754465688 - 162.97474543911272 - 153.7048885645407 - 141.11081082237126 - 122.73865377533433 - 145.92166701384886 - 0.5799378131479458'\n",
      "610 - random_26 - deep - 127.55148089178677 - 176.5231567817086 - 111.09548164884198 - 115.41127540716687 - 199.8058858314636 - 146.0758597117585 - 0.5794939408297999'\n",
      "611 - random_85 - lwr_k=300 - 107.5004669798126 - 164.27589015605383 - 136.05512427830269 - 173.2449371185914 - 149.605890204206 - 146.1331299092795 - 0.5793290785569118'\n",
      "612 - random_91 - lwr_k=500 - 124.38597862889588 - 153.68229466438254 - 106.6021184017655 - 233.51609326236874 - 112.62574398139964 - 146.16056785854906 - 0.5792500934054345'\n",
      "613 - random_99 - lwr_k=300 - 147.38946318573028 - 164.2476177831335 - 154.1031465582719 - 140.3777241789811 - 124.82595057625316 - 146.18888399931026 - 0.5791685802192521'\n",
      "614 - random_91 - lwr_k=400 - 124.41086548807787 - 152.88773803637542 - 106.56788556158102 - 234.59936306753616 - 112.4885369275738 - 146.18899958129197 - 0.5791682474953942'\n",
      "615 - random_91 - lwr_k=600 - 124.12151380476162 - 153.79023215510836 - 106.20776485956006 - 233.88432620521218 - 112.97574592092158 - 146.19401296693405 - 0.5791538155485871'\n",
      "616 - random_31 - lwr_k=300 - 120.66114109393062 - 164.60908571675515 - 135.99157197265154 - 138.05611837516855 - 171.7498111979902 - 146.2113421179552 - 0.5791039304203316'\n",
      "617 - random_20 - lwr_k=600 - 113.91425649464666 - 167.66597039947752 - 198.96614884145052 - 124.03997818583997 - 126.9150764938892 - 146.29749322090237 - 0.5788559287256945'\n",
      "618 - random_91 - lwr_k=300 - 124.5764391602853 - 151.08468644932935 - 107.18715518001659 - 235.63931885264404 - 113.01932503510538 - 146.29951144929336 - 0.5788501188864237'\n",
      "619 - random_76 - lwr_k=800 - 145.47378493103943 - 193.79635609303398 - 126.69203393047296 - 127.52453353675996 - 138.92160565779238 - 146.48157591382295 - 0.5783260130515371'\n",
      "620 - random_66 - deep - 122.89265562912513 - 167.63739190013206 - 149.8330066445677 - 137.7001438149094 - 155.40982283986608 - 146.69255220626422 - 0.577718678776945'\n",
      "621 - random_20 - lwr_k=400 - 103.86872554148772 - 171.3993877578621 - 198.2164914240479 - 122.92887837399597 - 137.07445222035844 - 146.69389364672375 - 0.5777148176546169'\n",
      "622 - random_27 - deep - 130.86780474432584 - 196.8147749347695 - 128.90168632582993 - 112.20077126105318 - 165.25500759044152 - 146.80663487674798 - 0.5773902709600792'\n",
      "623 - random_5 - lwr_k=200 - 203.3905423858759 - 160.95123413047074 - 125.9312342851347 - 125.88725785661636 - 118.44311530137215 - 146.92554656298694 - 0.5770479623987286'\n",
      "624 - random_53 - deep - 150.13307881848564 - 177.74225764173843 - 125.19118338548299 - 141.48835254157603 - 140.73660352344191 - 147.05855864260533 - 0.5766650623581855'\n",
      "625 - random_99 - lwr_k=100 - 145.68957530142475 - 166.1627941644575 - 154.06143862608786 - 152.95647460741583 - 116.9567155733381 - 147.16527238442123 - 0.5763578678099172'\n",
      "626 - random_91 - lwr_k=200 - 124.82006169614675 - 150.81302491767804 - 108.46539642287482 - 239.4356708945461 - 113.33347429847834 - 147.37158071114405 - 0.5757639715869474'\n",
      "627 - random_96 - lwr_k=300 - 175.77622168488634 - 158.30116701259251 - 161.94365690035474 - 146.6775190150689 - 95.2880159395309 - 147.5997461644341 - 0.575107155630589'\n",
      "628 - random_95 - lwr_k=100 - 142.49930092382434 - 178.0880656957618 - 133.9590998353588 - 143.73049944488298 - 139.90174268456022 - 147.6352987675164 - 0.5750048109650938'\n",
      "629 - random_30 - lwr_k=900 - 188.36650744716928 - 176.07848214175638 - 112.59491938885267 - 143.7906088478 - 117.6727762149179 - 147.70416569397705 - 0.5748065649312948'\n",
      "630 - random_76 - lwr_k=900 - 146.34182630684577 - 194.83293821577183 - 122.94415973093191 - 127.64270395164485 - 147.04748921850427 - 147.76170102896378 - 0.5746409389544924'\n",
      "631 - random_64 - lwr_k=300 - 149.02116268344048 - 180.5034478216031 - 138.47425261386616 - 142.24483288697107 - 128.85601970143736 - 147.8200467305928 - 0.5744729801892093'\n",
      "632 - random_71 - lwr_k=200 - 139.34396717654957 - 194.32914231624767 - 124.37499317920857 - 116.08628423484507 - 165.04161249368042 - 147.83446762478513 - 0.5744314670097428'\n",
      "633 - random_99 - lwr_k=400 - 149.0617980746224 - 167.30628280074328 - 158.3422024121202 - 140.79406266666737 - 126.10106622377953 - 148.3211463124095 - 0.5730304734629903'\n",
      "634 - random_71 - lwr_k=300 - 137.40648726288327 - 195.6675660311492 - 125.548062786366 - 116.57565153714157 - 166.45251707560067 - 148.32911492675544 - 0.5730075343502674'\n",
      "635 - random_30 - lwr_k=200 - 170.40062775043506 - 185.73718294531685 - 120.53280125622287 - 127.18902813985319 - 138.0347586360824 - 148.38077882698988 - 0.5728588103714513'\n",
      "636 - random_76 - lwr_k=400 - 143.67077374450804 - 187.30590944151405 - 172.11301255542028 - 119.92904346756718 - 119.69408346351481 - 148.54214440766887 - 0.5723942900566152'\n",
      "637 - random_76 - lwr_k=1000 - 146.9069696580654 - 195.06007872885846 - 121.36393283491803 - 127.82724237567253 - 151.60098711372177 - 148.55170029398198 - 0.5723667816914483'\n",
      "638 - random_79 - deep - 125.28059942311253 - 149.93058366927673 - 176.31156317680856 - 160.46449549396576 - 130.90179721360167 - 148.5757991521192 - 0.5722974082589523'\n",
      "639 - random_91 - lwr_k=100 - 125.0485224288026 - 154.95520522924346 - 111.39079455374926 - 239.59921105131008 - 112.50945830872912 - 148.698598635522 - 0.5719439079684776'\n",
      "640 - random_75 - deep - 151.28967648210195 - 195.87792877947373 - 120.2017876516904 - 119.58846613352235 - 157.77709674711832 - 148.94719411663385 - 0.5712282799770316'\n",
      "641 - random_44 - lwr_k=400 - 163.75294254625678 - 193.75518628055832 - 144.7782210162577 - 120.52704292378552 - 121.95764240255399 - 148.95548322701046 - 0.5712044187241627'\n",
      "642 - random_85 - lwr_k=200 - 120.35056761945879 - 178.16935463525337 - 142.54383355191476 - 157.99404754558535 - 145.74507980192476 - 148.95810939997097 - 0.5711968588053418'\n",
      "643 - random_71 - lwr_k=400 - 136.96702566598782 - 198.45939979663657 - 125.45934118534444 - 117.2543289963261 - 166.8167547217059 - 148.9903331342206 - 0.5711040968971588'\n",
      "644 - random_91 - lwr_k=700 - 124.17338442897551 - 154.5749777743392 - 106.03394691545338 - 234.8658085603581 - 126.16745952301098 - 149.1609604101574 - 0.5706129150998742'\n",
      "645 - random_1 - deep - 165.13981959902006 - 165.56575204998114 - 113.40091965557328 - 154.46922518231324 - 149.1386085368786 - 149.5442116674082 - 0.5695096558455226'\n",
      "646 - random_71 - lwr_k=500 - 137.6063793221089 - 199.08584208753157 - 125.83690825975866 - 117.59037536973085 - 167.9353486589808 - 149.6099355040739 - 0.5693204582382434'\n",
      "647 - random_1 - lwr_k=500 - 137.35354582013142 - 184.93872308256596 - 140.2083924233968 - 162.55834022280007 - 123.55627978568737 - 149.72198956197954 - 0.568997890153786'\n",
      "648 - random_43 - lwr_k=100 - 144.77481636668722 - 165.62975900666328 - 170.75057815278586 - 121.79113802172199 - 146.1822335503944 - 149.82526944801785 - 0.5687005801265987'\n",
      "649 - random_71 - lwr_k=600 - 137.5137894352234 - 199.99321039479824 - 125.37857721536948 - 118.20695565581745 - 168.304413346083 - 149.87832284176477 - 0.5685478562367658'\n",
      "650 - random_1 - lwr_k=400 - 139.00215579044558 - 185.2614451498222 - 141.55971060352763 - 161.56690204401147 - 123.09638361640309 - 150.09636263128263 - 0.5679201888541219'\n",
      "651 - random_0 - lwr_k=400 - 117.11898704030173 - 228.90055177783813 - 176.4046736804099 - 107.23593968340221 - 120.95478639047563 - 150.12014156058123 - 0.5678517368603462'\n",
      "652 - random_6 - lwr_k=900 - 126.18112846795916 - 147.76922775369636 - 135.17317860255454 - 223.41296877076556 - 118.21969478469697 - 150.1491725742436 - 0.5677681657818688'\n",
      "653 - random_20 - lwr_k=500 - 115.70841447788247 - 169.81960199822225 - 206.6202835834531 - 124.81128317175708 - 133.8089524536942 - 150.15073669092914 - 0.5677636631861347'\n",
      "654 - random_1 - lwr_k=600 - 138.44131548367315 - 184.81522982018458 - 138.90407020206067 - 162.5172900445316 - 126.41268217879785 - 150.21710195408843 - 0.5675726186473475'\n",
      "655 - random_6 - lwr_k=1000 - 120.98390640919031 - 147.59690532780166 - 123.54699113384248 - 245.56649086409104 - 113.75556636024487 - 150.28744476259996 - 0.5673701240173359'\n",
      "656 - random_30 - lwr_k=500 - 181.73512231547065 - 178.36131812298888 - 114.1313690526788 - 159.76898836950593 - 118.05731296161251 - 150.41352346663754 - 0.5670071834258712'\n",
      "657 - random_71 - lwr_k=700 - 138.45790920742905 - 201.78137631920418 - 125.53041509004332 - 118.18180630304226 - 168.50921505633036 - 150.49110660328265 - 0.5667838461880992'\n",
      "658 - random_91 - lwr_k=800 - 127.98430462479027 - 154.8534408690174 - 106.51728433634248 - 236.57385722284556 - 126.6957890973908 - 150.52299140198096 - 0.5666920599678438'\n",
      "659 - random_90 - lwr_k=100 - 166.6038290433038 - 151.5056208172999 - 182.54624372397757 - 143.65363558032104 - 109.17304823118157 - 150.6978472758332 - 0.5661887053786496'\n",
      "660 - random_33 - lwr_k=300 - 146.55915997121713 - 156.8363596427876 - 179.35164558079092 - 147.58194597375785 - 123.24954545088751 - 150.71537287516858 - 0.5661382547380192'\n",
      "661 - random_91 - lwr_k=900 - 127.90111881185409 - 154.97505236457337 - 106.34838307323118 - 237.61886961376726 - 126.92014426372816 - 150.75074298082805 - 0.5660364354247084'\n",
      "662 - random_71 - lwr_k=800 - 139.44233011197377 - 202.9463831653438 - 124.66081151297986 - 118.44161177408753 - 168.27994512463155 - 150.75324083881867 - 0.5660292448839788'\n",
      "663 - random_53 - lwr_k=200 - 194.22236831507723 - 168.89027861017405 - 121.91681506791535 - 139.85432367190595 - 129.06017886386027 - 150.792538471103 - 0.5659161194674915'\n",
      "664 - random_10 - lwr_k=100 - 154.26990098105694 - 212.9420412889132 - 134.72585544091953 - 129.59205256837114 - 123.09628006549127 - 150.9255145024544 - 0.5655333236588125'\n",
      "665 - random_91 - lwr_k=1000 - 128.08575234778522 - 154.76344274804418 - 106.33243990399542 - 238.7437187337958 - 127.04410737479876 - 150.9919167008255 - 0.5653421727952679'\n",
      "666 - random_37 - lwr_k=800 - 100.48973588470926 - 151.21222327161053 - 232.08429517780675 - 163.8831702760934 - 107.80294550347661 - 151.09011004049214 - 0.5650595053214187'\n",
      "667 - random_71 - lwr_k=900 - 140.09054597379344 - 204.6660009086476 - 124.23939235714278 - 118.48780575502988 - 168.12665718166667 - 151.12112911286374 - 0.5649702112526132'\n",
      "668 - random_25 - lwr_k=200 - 153.67077426297615 - 174.60605344661553 - 132.30954404872583 - 161.11489050940466 - 134.65339889925076 - 151.27113918769172 - 0.564538379836679'\n",
      "669 - random_30 - lwr_k=800 - 188.32060084689877 - 174.27192099675776 - 117.12937188876562 - 159.93135561924194 - 117.54059935898229 - 151.44195030707448 - 0.5640466688124364'\n",
      "670 - random_83 - lwr_k=300 - 160.66349477578274 - 181.64407609214695 - 126.47007340545608 - 114.36096275086864 - 174.30622525236163 - 151.48975763575785 - 0.5639090466796493'\n",
      "671 - random_71 - lwr_k=1000 - 140.89426090180868 - 206.73589333932367 - 123.73143253233377 - 118.60693895361064 - 168.23560458464192 - 151.6398993147445 - 0.5634768364170926'\n",
      "672 - random_24 - lwr_k=400 - 122.84482885489244 - 164.061660919206 - 113.77902005040967 - 225.07412360920168 - 134.01503025797675 - 151.95242238098274 - 0.5625771816548337'\n",
      "673 - random_21 - lwr_k=400 - 126.90649532174322 - 185.34411122739928 - 217.0121577988881 - 132.95031014563315 - 97.7829883464607 - 151.99704865656867 - 0.5624487167647506'\n",
      "674 - random_99 - lwr_k=500 - 151.0697747695796 - 172.52122447961463 - 162.69203219277514 - 145.83411378062934 - 128.24273104777265 - 152.07188882767855 - 0.5622332756545705'\n",
      "675 - random_80 - lwr_k=100 - 170.15026008146347 - 189.83626127908033 - 142.41812765531103 - 130.3496457142059 - 128.64372757634476 - 152.28114556645704 - 0.5616308918886526'\n",
      "676 - random_30 - lwr_k=600 - 181.41522521510547 - 176.19172417122198 - 113.71261440884855 - 173.53550204393233 - 117.1730025834557 - 152.40811537574518 - 0.5612653860878677'\n",
      "677 - random_30 - lwr_k=1000 - 188.16742155744066 - 176.1039042725921 - 116.54787975741466 - 163.0186673154588 - 118.68772258125902 - 152.50819449373373 - 0.5609772900564778'\n",
      "678 - random_29 - lwr_k=100 - 128.80901639754998 - 237.0703552706071 - 140.99432454651932 - 131.4815849903241 - 125.70515867436107 - 152.81001803179 - 0.5601084358415152'\n",
      "679 - random_37 - lwr_k=300 - 104.54149835520691 - 155.37544944532524 - 241.14353324711502 - 155.74942250230245 - 107.6834335438941 - 152.89449725931175 - 0.5598652469458388'\n",
      "680 - random_29 - lwr_k=500 - 133.3635279501295 - 237.9750945274158 - 124.65991311032533 - 132.37480309458564 - 136.3532989345854 - 152.94363885493877 - 0.5597237838818065'\n",
      "681 - random_44 - lwr_k=300 - 168.56204097897825 - 186.06696954404114 - 168.22366061368592 - 122.09119391010493 - 119.8641123655587 - 152.96294081237173 - 0.5596682196697484'\n",
      "682 - random_99 - lwr_k=600 - 152.28805691626098 - 172.7084243676615 - 166.44040039852527 - 146.52925276284216 - 127.73680977561715 - 153.14051532469816 - 0.5591570389828533'\n",
      "683 - random_6 - lwr_k=700 - 126.40949106502553 - 150.7048872083892 - 128.0191341658421 - 237.0086052219821 - 124.67212934423739 - 153.36052503421564 - 0.5585237008254718'\n",
      "684 - random_76 - lwr_k=300 - 142.6832019444013 - 182.08891481134407 - 207.83663608491705 - 119.55525372359834 - 114.7987085335892 - 153.3916194820506 - 0.5584341897746196'\n",
      "685 - random_29 - lwr_k=900 - 133.62854078329502 - 228.61415519277438 - 124.94975106661119 - 143.44156359552068 - 138.2222933402247 - 153.76952375532537 - 0.5573463232589673'\n",
      "686 - random_30 - deep - 217.14801433168608 - 179.66234828460418 - 129.3720605545077 - 113.16058784376295 - 130.15322198575814 - 153.90470524256912 - 0.5569571781606673'\n",
      "687 - random_30 - lwr_k=700 - 184.39213866830238 - 175.82322503082642 - 124.12541641299454 - 167.14458212615386 - 118.28698581180603 - 153.95709445212236 - 0.5568063667281908'\n",
      "688 - random_37 - lwr_k=600 - 100.01938196306521 - 150.66971991913232 - 245.78230941854025 - 163.55637856374983 - 110.09187824688446 - 154.01927645155527 - 0.5566273644785651'\n",
      "689 - random_86 - lwr_k=400 - 118.6650089148054 - 196.12882245136922 - 108.50672175134652 - 131.26983595117972 - 215.59490748433066 - 154.030009289013 - 0.556596468044431'\n",
      "690 - random_37 - lwr_k=500 - 100.25025336171474 - 151.51898393766106 - 245.3996390543152 - 161.86483233133694 - 111.6516123853192 - 154.13241719692107 - 0.5563016681656527'\n",
      "691 - random_63 - lwr_k=200 - 107.91345151944205 - 142.30074118467036 - 256.09407416066205 - 114.70904975935446 - 149.69660577171587 - 154.13879781713382 - 0.5562833003842609'\n",
      "692 - random_37 - lwr_k=400 - 100.64610213249047 - 156.20111252154322 - 243.9362180710387 - 162.66302416529246 - 108.37868519188223 - 154.36039587710104 - 0.555645389869826'\n",
      "693 - random_99 - lwr_k=700 - 152.09435073618863 - 173.12009566357435 - 170.52454126036903 - 147.71994527809957 - 129.34178483156106 - 154.55993091228748 - 0.555070991804417'\n",
      "694 - random_37 - lwr_k=700 - 100.81404432250473 - 151.19159932179264 - 246.9001875247818 - 164.88635040718873 - 109.23492455929261 - 154.60078243986612 - 0.5549533932163382'\n",
      "695 - random_29 - lwr_k=1000 - 138.5754752241895 - 235.4412084083704 - 125.80534258218985 - 134.20330697923572 - 139.60923885873387 - 154.72552156480512 - 0.5545943088481278'\n",
      "696 - random_6 - lwr_k=400 - 114.27155514150293 - 153.35027162149615 - 129.60701690153442 - 222.87711244698852 - 153.74486352076056 - 154.7666714627807 - 0.5544758513463317'\n",
      "697 - random_90 - lwr_k=200 - 170.9804679647784 - 148.71589946224867 - 184.50869530787733 - 164.4761892230734 - 106.70293611707588 - 155.0782090905497 - 0.5535790333488035'\n",
      "698 - random_29 - lwr_k=600 - 134.42906750611067 - 226.65898066600968 - 124.8152090488799 - 153.15125870366327 - 136.80520787096688 - 155.1701559632264 - 0.5533143474717767'\n",
      "699 - random_81 - lwr_k=300 - 180.2373792068736 - 184.93496688576212 - 169.67851138119988 - 112.16244457306689 - 129.44954600256847 - 155.2947207662927 - 0.5529557649209353'\n",
      "700 - random_99 - lwr_k=800 - 151.37799246001438 - 173.3052927228744 - 173.15237943286346 - 149.83710361142218 - 131.20352315591023 - 155.7748790712171 - 0.5515735414874368'\n",
      "701 - random_39 - lwr_k=200 - 165.73103518214072 - 159.18577266499895 - 129.95589876235985 - 189.2306919686849 - 136.27604594474695 - 156.0767215318955 - 0.5507046328002483'\n",
      "702 - random_21 - lwr_k=300 - 134.00075678364897 - 184.7264519481608 - 222.8080273870018 - 133.77123394434597 - 105.50917788766486 - 156.16121838174723 - 0.5504613931754744'\n",
      "703 - random_93 - lwr_k=500 - 183.76758402589059 - 151.7813111063439 - 161.03187337834822 - 111.62437105194212 - 173.50495943864814 - 156.34438488856037 - 0.5499341149104675'\n",
      "704 - random_6 - lwr_k=500 - 126.35069382853737 - 155.53927670604165 - 138.3148073966282 - 226.8348087359599 - 134.75513724879877 - 156.35635697265897 - 0.5498996510783953'\n",
      "705 - random_21 - lwr_k=200 - 162.07962016603827 - 197.60002927335447 - 146.85257317793778 - 149.74013283201174 - 125.7391845783568 - 156.40279759782686 - 0.5497659632514537'\n",
      "706 - random_29 - lwr_k=700 - 132.366889132298 - 233.53273555596064 - 126.25795485076557 - 154.33757233796263 - 137.42204467895633 - 156.78133371010387 - 0.5486762778717598'\n",
      "707 - random_45 - lwr_k=100 - 176.77425777446106 - 194.522557915356 - 121.19875958947739 - 128.3194252706346 - 163.33191183548186 - 156.83110245597987 - 0.5485330094411189'\n",
      "708 - random_1 - lwr_k=200 - 168.42968096702063 - 182.53073140735276 - 141.01899786278196 - 168.84254705111408 - 123.85540958934999 - 156.93646459728927 - 0.5482297052615852'\n",
      "709 - random_96 - lwr_k=800 - 272.32136068982936 - 153.41915018181686 - 138.2376533052668 - 123.3640965505296 - 98.86521509139185 - 157.25141926393297 - 0.5473230506931648'\n",
      "710 - random_26 - lwr_k=200 - 142.53047938364836 - 193.44593624762132 - 130.76650954328957 - 152.3829755828849 - 167.82013518880274 - 157.38792582258796 - 0.5469300916164102'\n",
      "711 - random_69 - lwr_k=200 - 135.63548273444502 - 194.28600606807126 - 164.28609390221794 - 140.89910704624393 - 152.04840204867244 - 157.42913878631632 - 0.5468114525683191'\n",
      "712 - random_99 - lwr_k=900 - 152.91389557679193 - 173.7271268968969 - 174.91475702342734 - 153.10273584285014 - 132.8562720483301 - 157.50256173241087 - 0.5466000911993549'\n",
      "713 - random_36 - lwr_k=300 - 147.84084852991018 - 179.0057609215679 - 216.6797705492498 - 92.77456460725963 - 151.58474620572454 - 157.5762985379034 - 0.5463878263287532'\n",
      "714 - random_6 - lwr_k=800 - 127.6889707242205 - 148.33597285988068 - 151.28145445099486 - 234.70711034300106 - 125.96617897359926 - 157.59335839421425 - 0.5463387164145371'\n",
      "715 - random_37 - lwr_k=200 - 104.06723085585483 - 176.3324332943561 - 238.9537530254872 - 154.53473282283358 - 114.55181172581965 - 157.6833682709233 - 0.5460796065344047'\n",
      "716 - random_97 - lwr_k=200 - 114.45032982712871 - 232.97213433793831 - 130.96597936602592 - 189.83904591075856 - 122.00337028892362 - 158.04241238750384 - 0.5450460324266412'\n",
      "717 - random_36 - lwr_k=200 - 147.98573493641277 - 180.90984351307293 - 214.7834345000844 - 94.04588270605636 - 153.25917681541736 - 158.19593392508511 - 0.5446040926234237'\n",
      "718 - random_99 - lwr_k=1000 - 152.49502082844742 - 174.38029477267895 - 177.04947195070716 - 152.44726413815107 - 135.25749538846338 - 158.32540657948854 - 0.5442313819255594'\n",
      "719 - random_1 - lwr_k=300 - 154.43342539999648 - 188.74109790674154 - 145.89637098311562 - 181.08931381003197 - 122.00809501919954 - 158.43331565699881 - 0.5439207459246984'\n",
      "720 - random_36 - lwr_k=400 - 148.45560507097733 - 177.76788561827732 - 217.72509753028407 - 92.22989898790073 - 156.08816498268394 - 158.45246826784813 - 0.5438656116340757'\n",
      "721 - random_53 - lwr_k=300 - 255.34817431861413 - 163.86477864891594 - 116.58618297051373 - 133.8168193907187 - 123.11021187970462 - 158.5535814014103 - 0.5435745389998825'\n",
      "722 - random_29 - lwr_k=300 - 129.39935942171815 - 238.52168988254763 - 148.7994406292683 - 142.98823313310996 - 134.0776845761899 - 158.75474980020297 - 0.5429954389988226'\n",
      "723 - random_33 - lwr_k=200 - 152.02377985380852 - 161.0562616156618 - 208.42918949472974 - 153.11612324593526 - 119.36272679290177 - 158.7970320477662 - 0.5428737218217927'\n",
      "724 - random_36 - lwr_k=500 - 148.56566978874542 - 178.01014555993592 - 219.69702535015605 - 91.15180174262717 - 157.3014614742331 - 158.9443256856064 - 0.5424497101661784'\n",
      "725 - random_45 - lwr_k=200 - 177.77842871455644 - 212.7276055626386 - 118.91314342407 - 120.728394273254 - 165.29001904645327 - 159.0891300453906 - 0.5420328643523836'\n",
      "726 - random_36 - lwr_k=100 - 147.95838583353495 - 186.07931101367322 - 213.8889354351198 - 97.73748717625017 - 150.18672583502328 - 159.1692021922814 - 0.5418023620437327'\n",
      "727 - random_29 - lwr_k=200 - 128.56278695569526 - 245.662313704483 - 151.1072646903602 - 139.89838636605526 - 130.8578326293413 - 159.21507329123654 - 0.5416703137020799'\n",
      "728 - random_11 - lwr_k=400 - 159.79594367897056 - 180.0252959484352 - 142.35396230182917 - 118.65838978920249 - 195.7852567637715 - 159.32381041513636 - 0.5413572940183544'\n",
      "729 - random_95 - deep - 133.55443096818595 - 179.84540387770903 - 119.06374822355849 - 169.9610668575522 - 196.2481991009961 - 159.7323121908573 - 0.5401813464999601'\n",
      "730 - random_36 - lwr_k=600 - 150.21703146258537 - 176.84038936970302 - 221.19444275752636 - 90.79334243945463 - 159.64158071390673 - 159.73653634778267 - 0.540169186992264'\n",
      "731 - random_55 - lwr_k=800 - 103.25047418280589 - 256.1696318271087 - 203.15404754424404 - 134.0157895490256 - 102.13454512745726 - 159.74002575724737 - 0.5401591420894024'\n",
      "732 - random_64 - lwr_k=200 - 159.18058577492673 - 185.72081131311208 - 121.49837244363927 - 155.8228060034798 - 177.2501233971406 - 159.89447821746938 - 0.5397145224552299'\n",
      "733 - random_55 - lwr_k=700 - 102.86942955055044 - 253.53276783504577 - 202.20561930974122 - 138.62452026507157 - 102.42437467750696 - 159.9264215003344 - 0.5396225678776123'\n",
      "734 - random_55 - lwr_k=900 - 103.29686238503344 - 259.7728518097615 - 203.3518319330057 - 131.52071870638304 - 101.9334795753887 - 159.97026113730453 - 0.5394963674705493'\n",
      "735 - random_36 - lwr_k=700 - 149.77566660896682 - 178.76611294216278 - 221.79934864308558 - 90.6618002354773 - 159.659622536109 - 160.13161705383774 - 0.539031874975723'\n",
      "736 - random_29 - lwr_k=800 - 123.94177872714482 - 237.6753307399747 - 146.9839648280306 - 154.42114657289778 - 137.67979899856292 - 160.13728232575005 - 0.5390155664551497'\n",
      "737 - random_36 - lwr_k=800 - 150.61979787860307 - 176.0143555924825 - 221.65491490534342 - 91.92262585240744 - 160.66695950568715 - 160.1749066754259 - 0.5389072578884568'\n",
      "738 - random_45 - lwr_k=300 - 174.66764058887566 - 220.22759751321328 - 119.02412859613243 - 120.6917711596579 - 166.5624487485257 - 160.23596196799258 - 0.53873149907046'\n",
      "739 - random_28 - lwr_k=800 - 133.92228694764324 - 182.8772380837168 - 189.56184230775912 - 118.92716774718889 - 177.23316575972026 - 160.50204782242906 - 0.5379655223091449'\n",
      "740 - random_55 - lwr_k=1000 - 103.02145112284882 - 262.5838671413369 - 204.7148634420043 - 130.4967396650212 - 101.74782294637605 - 160.5079909903077 - 0.5379484138142447'\n",
      "741 - random_11 - lwr_k=300 - 136.5830664865222 - 170.9989096547791 - 134.6751298866849 - 131.01646936657633 - 229.74583955062482 - 160.60181151469266 - 0.5376783342883508'\n",
      "742 - random_37 - lwr_k=100 - 125.60106813484855 - 228.0692293177765 - 157.49110759408603 - 163.71626427450454 - 128.19890115126586 - 160.61229458380643 - 0.537648156858042'\n",
      "743 - random_28 - lwr_k=900 - 133.25263290587924 - 183.36058553863708 - 189.15488816613907 - 119.72295308050992 - 178.05808618123132 - 160.70746135831268 - 0.537374201905136'\n",
      "744 - random_19 - lwr_k=100 - 123.95087231364481 - 156.76112156086177 - 121.98214701948626 - 301.23809646387525 - 99.67760882488493 - 160.71879822094843 - 0.5373415666741366'\n",
      "745 - random_12 - deep - 164.4251945232523 - 162.24008735370512 - 122.78074503870245 - 175.88987395042693 - 178.96537085924234 - 160.8605617810045 - 0.5369334738545459'\n",
      "746 - random_28 - lwr_k=1000 - 131.9572092107965 - 183.62336903553233 - 190.90167083400956 - 119.54395113996833 - 178.3137115488329 - 160.86548918608534 - 0.5369192899220419'\n",
      "747 - random_36 - lwr_k=1000 - 151.95143651573377 - 177.12925048709323 - 221.6824096541665 - 91.7002986576744 - 162.80009713856634 - 161.05191362845514 - 0.5363826330941213'\n",
      "748 - random_28 - lwr_k=400 - 124.5713311128522 - 183.05949869267633 - 197.32852453150028 - 123.6906670620811 - 177.7609160848456 - 161.27902167613018 - 0.535728861073075'\n",
      "749 - random_19 - lwr_k=200 - 121.24545616659583 - 157.74316476779654 - 116.61922343810673 - 301.23299586783935 - 110.06033183637172 - 161.37677332718675 - 0.535447465080612'\n",
      "750 - random_36 - lwr_k=900 - 152.14956846692877 - 179.25086644499154 - 221.92753140919226 - 92.08574139653987 - 162.6876931672435 - 161.61946345468616 - 0.5347488371950415'\n",
      "751 - random_16 - deep - 196.1024364602977 - 184.1957801861617 - 119.7660444653615 - 184.64077214302304 - 124.68631976984246 - 161.88122236530958 - 0.5339953158255369'\n",
      "752 - random_44 - lwr_k=200 - 179.88165423602803 - 194.1150765263842 - 184.15193650799972 - 129.64025236127776 - 122.840343290506 - 162.12738378499552 - 0.53328669569774'\n",
      "753 - random_14 - lwr_k=800 - 125.04786169046385 - 271.994854574837 - 129.26056829718254 - 112.75866706322162 - 171.72550568973622 - 162.15429125355286 - 0.5332092376319425'\n",
      "754 - random_55 - lwr_k=600 - 102.62713651338343 - 248.38930818980944 - 201.00504510596895 - 156.5824533988149 - 102.44062655985782 - 162.20377582167313 - 0.5330667872589072'\n",
      "755 - random_53 - lwr_k=900 - 301.63946826579956 - 159.3101861988576 - 115.30705898301981 - 124.31028997851551 - 111.35537179364718 - 162.39648392575666 - 0.5325120417623543'\n",
      "756 - random_28 - lwr_k=300 - 133.80978030633466 - 184.71373260911926 - 194.53365582506675 - 123.15397208749408 - 175.92354778397424 - 162.424469875087 - 0.5324314790925506'\n",
      "757 - random_76 - lwr_k=200 - 148.2285089316411 - 187.62202228498006 - 211.57376239879554 - 118.24869083198068 - 146.84175840110214 - 162.50171759008296 - 0.5322091074272965'\n",
      "758 - random_19 - lwr_k=300 - 120.7829545475537 - 160.2312923950153 - 116.8046721190138 - 301.21697334805066 - 115.3818560596759 - 162.87991908027564 - 0.5311203853184272'\n",
      "759 - random_14 - lwr_k=900 - 128.27412112679474 - 264.93145419058726 - 130.6818924223088 - 111.99394491863846 - 178.91057228824297 - 162.95540593413486 - 0.5309030826137666'\n",
      "760 - random_45 - lwr_k=400 - 175.3037534963182 - 221.30032002177597 - 120.45440209837899 - 123.14562706892191 - 174.7884633179797 - 162.99957436317027 - 0.5307759357186543'\n",
      "761 - random_52 - lwr_k=200 - 137.26612277053536 - 189.7763743145895 - 138.44090055842972 - 128.99186965431775 - 221.1570245997908 - 163.12422827125312 - 0.5304170966627383'\n",
      "762 - random_28 - lwr_k=600 - 138.73487832878396 - 182.3058745467714 - 192.20469855691022 - 125.33602533114869 - 177.06912655098193 - 163.12801689935125 - 0.5304061904043558'\n",
      "763 - random_29 - lwr_k=400 - 135.7054059487523 - 255.41785031232592 - 148.2865090184789 - 140.8831632879417 - 135.4638988980728 - 163.14899864596492 - 0.5303457906121462'\n",
      "764 - random_10 - deep - 120.27718469027815 - 227.762444565655 - 153.8904681285942 - 131.92766621545658 - 182.65744318927142 - 163.29932771346478 - 0.5299130404063294'\n",
      "765 - random_19 - lwr_k=400 - 120.71675954619518 - 161.08100718051665 - 117.08421789295758 - 301.27056637340564 - 117.12108175757052 - 163.4510409717397 - 0.5294763065767507'\n",
      "766 - random_28 - lwr_k=200 - 131.69605157030307 - 186.53710251425383 - 200.52284456570902 - 124.00351141883675 - 174.62760863480085 - 163.47468302224237 - 0.5294082486136047'\n",
      "767 - random_38 - lwr_k=100 - 143.72776950354933 - 195.11463859585103 - 134.8591723568167 - 127.05613639840371 - 216.91685516908316 - 163.5332063032488 - 0.5292397786536297'\n",
      "768 - random_92 - lwr_k=500 - 144.48855844531346 - 154.67103679420606 - 169.70201080740307 - 136.20433936777826 - 213.6493839255764 - 163.74140542415904 - 0.5286404394340336'\n",
      "769 - random_96 - lwr_k=900 - 271.68024858222867 - 154.65934398743258 - 146.2579598140685 - 136.59084077266888 - 110.01388807993649 - 163.8497559878961 - 0.5283285325343837'\n",
      "770 - random_6 - lwr_k=300 - 127.49099377319615 - 169.32819043959356 - 153.90084492362962 - 214.42100462791262 - 154.82593632724968 - 163.99024617421102 - 0.52792410585737'\n",
      "771 - random_19 - lwr_k=500 - 122.14860969797385 - 162.8258183151839 - 117.57133904007948 - 301.2635766165539 - 118.87249308577903 - 164.5327119736 - 0.5263625189138619'\n",
      "772 - random_92 - lwr_k=600 - 145.84874448732583 - 156.15756688547822 - 163.6277973737466 - 142.59774713720313 - 214.78347603671097 - 164.60144907451235 - 0.5261646466066303'\n",
      "773 - random_28 - lwr_k=700 - 149.59118337453154 - 187.04246543708572 - 190.8464877788139 - 121.76612413435281 - 175.46158712419702 - 164.94024580417056 - 0.5251893583024949'\n",
      "774 - random_28 - lwr_k=500 - 142.83526164747806 - 183.82504306015758 - 195.46385667276937 - 127.22052792096427 - 176.0655010175471 - 165.08011957668288 - 0.5247867060852666'\n",
      "775 - random_19 - lwr_k=600 - 121.33909558490599 - 165.22533245841714 - 118.20106019484498 - 301.26257518574295 - 120.70224708552645 - 165.34226708925235 - 0.5240320665607789'\n",
      "776 - random_74 - deep - 160.71764410610857 - 230.15619393924845 - 120.06812791055314 - 158.2393201077072 - 158.18377441432568 - 165.47260122569256 - 0.5236568754114481'\n",
      "777 - random_24 - lwr_k=300 - 130.46920670817198 - 188.5669449809225 - 115.53078827188311 - 222.78094242616584 - 172.3409325675209 - 165.93470430205025 - 0.5223266277710974'\n",
      "778 - random_19 - lwr_k=700 - 121.52839514767366 - 167.25820959765687 - 117.7680795639713 - 301.2271372070848 - 121.94233749843904 - 165.94100147900383 - 0.522308500196377'\n",
      "779 - random_94 - lwr_k=200 - 250.80259373840804 - 186.05222025866294 - 139.5390469524125 - 141.78698980082348 - 112.87425448736063 - 166.21831593996976 - 0.521510199839049'\n",
      "780 - random_19 - lwr_k=800 - 121.40190292736696 - 168.63229871006558 - 117.30453982742831 - 301.30750712538105 - 123.3095523975817 - 166.3872804754821 - 0.5210238045439555'\n",
      "781 - random_86 - lwr_k=300 - 124.33869771304173 - 199.3991716499941 - 126.28781782204341 - 155.5234589725644 - 226.61997516446382 - 166.43019412242845 - 0.520900269768408'\n",
      "782 - random_45 - lwr_k=500 - 176.63541554897785 - 227.08876896195756 - 124.95126752590357 - 123.79118505462601 - 180.03946334945542 - 166.50209402708205 - 0.5206932927525794'\n",
      "783 - random_92 - lwr_k=400 - 154.21420310309372 - 162.60696958000986 - 142.11439359143145 - 154.2837147091918 - 219.9531879019671 - 166.6334226930862 - 0.5203152391861215'\n",
      "784 - random_55 - lwr_k=400 - 102.90238742553599 - 245.58407893252283 - 208.4696345167452 - 175.41721953357865 - 102.14976791197726 - 166.89909832721114 - 0.5195504433189442'\n",
      "785 - random_46 - lwr_k=300 - 175.09587420964553 - 218.07228570082836 - 124.58661832334936 - 147.15949706052328 - 169.5880790170405 - 166.9011776062708 - 0.519544457734166'\n",
      "786 - random_4 - lwr_k=200 - 123.26776124053009 - 151.9709080520394 - 106.9403123284986 - 306.8407191473297 - 146.1937877017686 - 167.03892269088965 - 0.5191479333340738'\n",
      "787 - random_19 - lwr_k=900 - 121.3906876982244 - 170.04154323859404 - 118.07029981258965 - 301.37540314783797 - 124.3425040998979 - 167.04015060392157 - 0.5191443985620005'\n",
      "788 - random_75 - lwr_k=100 - 225.73200362251853 - 186.3906867713713 - 126.8830925202955 - 157.70033699622084 - 139.11644055205878 - 167.16956275578463 - 0.5187718620317561'\n",
      "789 - random_55 - lwr_k=500 - 102.40692472459607 - 247.60382782553287 - 204.3337619125811 - 179.36065085846414 - 102.37016889023204 - 167.20947800629315 - 0.5186569586883356'\n",
      "790 - random_77 - lwr_k=300 - 112.51619365974743 - 360.05297646533523 - 139.01821103092885 - 112.65023005923858 - 112.41820447437334 - 167.32643607950126 - 0.5183202734995821'\n",
      "791 - random_32 - lwr_k=1000 - 163.62721978626283 - 150.36868965896525 - 145.19435757972383 - 124.77027331692699 - 253.00441778530546 - 167.3926668779516 - 0.5181296160420342'\n",
      "792 - random_46 - lwr_k=200 - 182.99712098267307 - 215.2660360589678 - 123.98670044739767 - 144.66556991313047 - 170.1803433345933 - 167.42049753876634 - 0.518049500398603'\n",
      "793 - random_19 - lwr_k=1000 - 121.4625133654132 - 170.99500855860256 - 118.22431370322504 - 302.14458552665553 - 124.89861905136627 - 167.54103404185662 - 0.5177025140454469'\n",
      "794 - random_77 - lwr_k=200 - 110.3828315322339 - 359.72855480306083 - 139.08822566221025 - 119.05800763755296 - 110.29007935487058 - 167.70459613566544 - 0.5172316706661362'\n",
      "795 - random_32 - lwr_k=900 - 163.43498013838894 - 150.47162955974105 - 145.76795248320653 - 125.99746727629355 - 253.21140563577634 - 167.77631260449706 - 0.5170252217038457'\n",
      "796 - random_90 - lwr_k=300 - 223.41325699280424 - 147.7670130317309 - 195.6826081362533 - 165.75754373284198 - 106.72158522115079 - 167.87319142429894 - 0.5167463383156661'\n",
      "797 - random_0 - lwr_k=300 - 123.43459982925309 - 241.53904845823703 - 234.21548552589405 - 113.81388911265203 - 126.89785147232487 - 167.97633341925052 - 0.5164494252331728'\n",
      "798 - random_46 - lwr_k=100 - 174.0038426526763 - 211.6751579572987 - 130.78574985900354 - 151.75356329070166 - 172.96406917372607 - 168.2369739449139 - 0.5156991238459084'\n",
      "799 - random_46 - lwr_k=400 - 179.55790893974412 - 226.64812543334475 - 118.34568907193542 - 146.34391833655656 - 170.91695425870896 - 168.36348466077717 - 0.51533493962945'\n",
      "800 - random_77 - lwr_k=400 - 115.55517799265343 - 362.39824231875406 - 138.536131469559 - 111.70059615996577 - 113.73697041884705 - 168.38086776943092 - 0.515284899174288'\n",
      "801 - random_32 - lwr_k=800 - 164.19680824620139 - 152.44135288173362 - 145.44188057666867 - 126.35359215041933 - 253.97342449666334 - 168.48104218056284 - 0.5149965288241691'\n",
      "802 - random_77 - lwr_k=500 - 117.45433401156642 - 365.31568036125884 - 137.55536808508907 - 109.64137041046885 - 113.88681217686651 - 168.76628765729095 - 0.5141753975885133'\n",
      "803 - random_32 - lwr_k=400 - 167.76144785950893 - 156.99720670804552 - 142.08482794708175 - 124.60460539946132 - 252.5960352719041 - 168.8087343149532 - 0.514053207126966'\n",
      "804 - random_52 - lwr_k=300 - 126.42855039107677 - 167.80412637844438 - 119.29119971886824 - 119.4532549005381 - 311.4903215823289 - 168.88982856077394 - 0.5138197625197485'\n",
      "805 - random_77 - lwr_k=600 - 118.75694792973452 - 366.20104757549876 - 137.04434890619328 - 109.22198577492671 - 113.7038387318123 - 168.9813022308689 - 0.5135564388428346'\n",
      "806 - random_32 - lwr_k=700 - 163.80696871691504 - 151.78805493580214 - 149.72210913347686 - 124.83328157060505 - 255.6196476851392 - 169.1535512973415 - 0.5130605884250976'\n",
      "807 - random_14 - deep - 114.39822187752559 - 206.69627578426096 - 209.12676565047738 - 137.3474888014043 - 178.66349682596132 - 169.24171897226236 - 0.51280678112971'\n",
      "808 - random_77 - lwr_k=700 - 119.5557265896373 - 366.96680052845164 - 137.33668826017205 - 108.81348469646372 - 113.85469405614555 - 169.30118857503257 - 0.5126355875393696'\n",
      "809 - random_77 - lwr_k=800 - 120.21066581931544 - 367.98777662397595 - 137.05258024372822 - 108.89114784970926 - 113.43433837899055 - 169.5110499173312 - 0.5120314633117228'\n",
      "810 - random_29 - deep - 153.69930041082975 - 238.88355704205156 - 167.96837913378295 - 112.54150763211861 - 175.41405130265446 - 169.69997972367014 - 0.5114875938045277'\n",
      "811 - random_77 - lwr_k=900 - 120.96805063033216 - 368.1088131913502 - 136.72438419153562 - 109.40271453337945 - 113.46311350417389 - 169.72920984929019 - 0.511403450077105'\n",
      "812 - random_61 - deep - 165.47713788788894 - 234.12488623352183 - 139.70916953339767 - 157.17886245615682 - 153.24453567235108 - 169.94653132752322 - 0.5107778499527857'\n",
      "813 - random_32 - lwr_k=500 - 164.45303697092163 - 153.4579359920361 - 152.57560524347807 - 125.38795895646888 - 254.52381111423094 - 170.07918443356746 - 0.5103959842756062'\n",
      "814 - random_77 - lwr_k=1000 - 121.90015738693533 - 368.57635479718675 - 137.03635159957247 - 109.71817191355339 - 113.213948617749 - 170.08484121971924 - 0.5103797001828454'\n",
      "815 - random_14 - lwr_k=1000 - 118.4245582478136 - 321.22391799350754 - 130.29542678461192 - 109.50021713529107 - 172.01092574627808 - 170.2865363933896 - 0.5097990837640278'\n",
      "816 - random_83 - lwr_k=200 - 188.89198599660654 - 213.88224350520522 - 148.83688400290288 - 140.1260660990439 - 159.99664889385375 - 170.34836497688508 - 0.5096210988867675'\n",
      "817 - random_73 - lwr_k=200 - 172.9321940904797 - 193.31816190116203 - 115.41801507479349 - 156.86156425304836 - 214.00547901578557 - 170.5072920004812 - 0.5091635984041603'\n",
      "818 - random_32 - lwr_k=600 - 163.6074354548713 - 153.04583767614264 - 154.77043806370446 - 126.15463273756491 - 256.22811325139395 - 170.7606745122803 - 0.5084341905362644'\n",
      "819 - random_46 - lwr_k=500 - 181.45837207033293 - 234.4917306759072 - 114.11013359816036 - 148.3660108164903 - 175.45561999906553 - 170.77729461159097 - 0.5083863465429425'\n",
      "820 - random_45 - lwr_k=600 - 176.74190429185785 - 240.436256987587 - 126.79403295432097 - 124.16450491173848 - 186.2335126179975 - 170.8745483773588 - 0.5081063838042756'\n",
      "821 - random_5 - lwr_k=100 - 212.75048128055903 - 183.65895572414723 - 154.7050344691417 - 144.7885636152822 - 158.67846930111 - 170.91990851692373 - 0.5079758063526094'\n",
      "822 - random_55 - lwr_k=300 - 104.11119407456248 - 242.49866973322216 - 236.97959990949855 - 169.11137357780822 - 103.31644871032994 - 171.1976713901903 - 0.5071762151585231'\n",
      "823 - random_73 - lwr_k=100 - 172.44434136127836 - 192.4809091804045 - 116.55561144449204 - 156.2514761204854 - 219.14508669705387 - 171.37557713532033 - 0.5066640809574673'\n",
      "824 - random_73 - lwr_k=400 - 178.30503106252576 - 193.67711361627488 - 114.64626337586985 - 157.84260657950654 - 213.70207928827244 - 171.6351940183876 - 0.5059167263124946'\n",
      "825 - random_63 - lwr_k=100 - 122.94298721745426 - 150.595619738626 - 229.34548256263082 - 143.14313597758738 - 212.7712543231152 - 171.75548617527056 - 0.5055704433545525'\n",
      "826 - random_14 - lwr_k=300 - 132.73865811150603 - 263.0811241749114 - 129.48944096795563 - 149.94640319991552 - 183.92546191346236 - 171.83284603164242 - 0.505347748869853'\n",
      "827 - random_73 - lwr_k=300 - 178.2111246797334 - 193.57379134860747 - 115.4185395633009 - 158.33221265879234 - 213.64544063557358 - 171.8367715273656 - 0.5053364486129981'\n",
      "828 - random_24 - deep - 106.17067202864023 - 164.6707920931495 - 99.59146685186981 - 269.3765573637297 - 221.94080679146265 - 172.3443520156352 - 0.5038752848736252'\n",
      "829 - random_73 - lwr_k=500 - 178.28491570980646 - 195.0683711789071 - 114.47194544926072 - 157.90987476319452 - 216.019343314456 - 172.35140181351707 - 0.5038549912815069'\n",
      "830 - random_73 - lwr_k=600 - 176.3954673851232 - 194.67896424746158 - 114.38755668332506 - 159.01642095798985 - 217.3881831873896 - 172.3736653488371 - 0.503790901626249'\n",
      "831 - random_47 - lwr_k=600 - 135.1785669487528 - 166.49467739856215 - 140.96476867724007 - 257.52714828286827 - 163.12863018494087 - 172.65552613295455 - 0.5029795138468524'\n",
      "832 - random_45 - lwr_k=700 - 177.24064437630867 - 243.30238687325942 - 125.90163726215253 - 124.41173330991484 - 192.49935852549487 - 172.6715461270586 - 0.5029333973660417'\n",
      "833 - random_77 - lwr_k=100 - 115.86618413701106 - 361.34197850602646 - 143.4387992694575 - 135.7844648739724 - 109.6063332855145 - 173.2026070878806 - 0.501404641334674'\n",
      "834 - random_90 - lwr_k=400 - 227.77489281078522 - 148.13162462580775 - 219.75875369513048 - 164.79583165740453 - 107.0812133634936 - 173.5131429847137 - 0.500510707002719'\n",
      "835 - random_59 - deep - 135.3920064991918 - 131.54895198175547 - 416.56997455631466 - 93.59493808820362 - 91.37970112828974 - 173.69381277451902 - 0.4999906154501983'\n",
      "836 - random_46 - lwr_k=600 - 184.30593010062438 - 239.0213985658118 - 113.52554772748097 - 150.19109368354978 - 182.53099268955322 - 173.91588863287515 - 0.4993513300494953'\n",
      "837 - random_55 - lwr_k=200 - 105.21127762192522 - 237.01197342683295 - 241.31251726069544 - 180.7019906327309 - 105.66712776580107 - 173.97504687421875 - 0.49918103224014565'\n",
      "838 - random_11 - lwr_k=200 - 138.29594713886868 - 191.69679656785527 - 144.4778835790855 - 141.7413147066485 - 255.4079076545946 - 174.32086299401982 - 0.49818553733863424'\n",
      "839 - random_73 - lwr_k=700 - 179.0213643045672 - 199.47610883894615 - 114.44081723215692 - 160.39627141055456 - 218.58217524864006 - 174.3837473739355 - 0.4980045131584232'\n",
      "840 - random_91 - deep - 136.05235290527344 - 154.96760158366095 - 114.08383511818396 - 252.2651417326341 - 215.2113026417037 - 174.51272758021884 - 0.497633219516192'\n",
      "841 - random_73 - lwr_k=800 - 178.42996236437006 - 199.4361009272088 - 114.40680567827309 - 161.38459219887815 - 219.5236928220203 - 174.6365579567881 - 0.4972767516925244'\n",
      "842 - random_86 - deep - 125.9085180611446 - 425.243785678852 - 98.24538721674318 - 119.38342576645839 - 105.09383507482242 - 174.77077649247045 - 0.49689037856096374'\n",
      "843 - random_90 - lwr_k=500 - 233.92558373115608 - 148.01791552349428 - 218.19894156975195 - 166.52108052959977 - 107.4373246899718 - 174.82526626075423 - 0.4967335203516047'\n",
      "844 - random_6 - lwr_k=600 - 126.85046219633188 - 154.31839231281646 - 232.28818079060633 - 230.09077782405166 - 131.22334593493596 - 174.95008350460677 - 0.49637421110382485'\n",
      "845 - random_45 - lwr_k=800 - 181.23338393193703 - 244.17963866375482 - 125.25871508321197 - 124.17532850993241 - 200.4620242202718 - 175.0623502968828 - 0.4960510306246032'\n",
      "846 - random_93 - lwr_k=300 - 191.11270049776368 - 157.77204143648052 - 140.52460117839988 - 117.7777621463154 - 271.02304789619933 - 175.6433647695163 - 0.4943784742800821'\n",
      "847 - random_72 - deep - 244.82405421816068 - 178.42599792612066 - 116.04557136216519 - 153.7132869389819 - 187.54282474229953 - 176.11627256473528 - 0.4930171222125602'\n",
      "848 - random_89 - lwr_k=700 - 116.34404728940642 - 163.74314865054083 - 104.28194915673717 - 116.78352799326802 - 379.7374582709159 - 176.17286639126797 - 0.4928542070912101'\n",
      "849 - random_46 - lwr_k=700 - 185.57184657657223 - 244.85266197355944 - 114.03207292116879 - 153.1423929736145 - 183.79032865472848 - 176.27866210198772 - 0.49254965480289026'\n",
      "850 - random_89 - lwr_k=800 - 116.78411269677893 - 164.1003228901544 - 105.17961093074683 - 115.93450011250512 - 379.7660891001014 - 176.34779013204826 - 0.4923506571346602'\n",
      "851 - random_45 - lwr_k=900 - 185.5667930790699 - 244.62108124906706 - 124.9341173437916 - 124.5068153964401 - 202.63724221316298 - 176.45399578104096 - 0.4920449247641022'\n",
      "852 - random_89 - lwr_k=1000 - 119.8678990101148 - 160.13583050474838 - 107.03790582035094 - 115.68126512058596 - 379.71619335375055 - 176.4829360506518 - 0.4919616149089615'\n",
      "853 - random_73 - lwr_k=900 - 182.68843423109723 - 202.99479481978994 - 114.33592213081475 - 162.2271166532041 - 220.18187392330321 - 176.48616326073807 - 0.4919523247949297'\n",
      "854 - random_73 - lwr_k=1000 - 183.65332255100444 - 203.07354130329432 - 114.31942746627696 - 163.15764541416573 - 220.33499804083925 - 176.90836866739588 - 0.49073692937044755'\n",
      "855 - random_89 - lwr_k=600 - 120.26608137634707 - 166.2055168567278 - 103.84804559853634 - 115.64719174475304 - 379.795937311819 - 177.14764887970622 - 0.49004811754893807'\n",
      "856 - random_53 - lwr_k=100 - 184.24333291314724 - 206.32178928190896 - 157.35230343839504 - 195.89201254111111 - 142.7430581689543 - 177.3110971329362 - 0.489577601880539'\n",
      "857 - random_89 - lwr_k=500 - 118.42546637418245 - 169.56184958495473 - 103.34271171996069 - 115.97199807635356 - 379.7311687606999 - 177.40155256548198 - 0.4893172093865016'\n",
      "858 - random_45 - lwr_k=1000 - 187.23021096475398 - 244.52109347207434 - 124.46487953010732 - 124.50487521897278 - 206.3179346936137 - 177.4086458276627 - 0.48929679013496397'\n",
      "859 - random_46 - lwr_k=800 - 186.4179983339633 - 247.1946180969511 - 113.59400201103584 - 152.8115924914925 - 187.46018080283554 - 177.49644777809274 - 0.4890440361741418'\n",
      "860 - random_89 - lwr_k=900 - 126.44785188641166 - 160.4785793406449 - 106.00157395209645 - 115.6646944637308 - 379.72078973896964 - 177.65828128047642 - 0.48857816886116245'\n",
      "861 - random_90 - lwr_k=600 - 241.66827333811446 - 147.61912343728275 - 219.21402426037312 - 172.06213609149694 - 107.97232933421142 - 177.7126930818823 - 0.4884215345466233'\n",
      "862 - random_47 - lwr_k=1000 - 139.45754038637813 - 164.74479093607505 - 142.24553959534396 - 275.9209314880038 - 169.4271108984342 - 178.3558279141866 - 0.48657015339379683'\n",
      "863 - random_96 - lwr_k=200 - 277.05628042424337 - 180.464151051768 - 147.24252125303158 - 171.76136784343305 - 115.50333109274139 - 178.41403764160603 - 0.4864025860551263'\n",
      "864 - random_31 - lwr_k=200 - 142.637746670917 - 176.41628982003564 - 149.53058743703025 - 228.1042999047201 - 195.4741695969999 - 178.4295318566878 - 0.48635798312532164'\n",
      "865 - random_81 - lwr_k=200 - 220.1875855828055 - 207.02013046499093 - 175.73747077931895 - 148.11060117175762 - 141.8379323745246 - 178.58233228108762 - 0.48591811917818606'\n",
      "866 - random_46 - lwr_k=900 - 186.5928637329039 - 249.69378079171548 - 114.38706029522179 - 154.87429643433512 - 191.94992165052778 - 179.5001962814541 - 0.48327587990613796'\n",
      "867 - random_36 - deep - 161.4445924430058 - 219.51129758373423 - 234.7987843308278 - 100.05135039987108 - 182.03454765856085 - 179.5665499750281 - 0.48308486811041706'\n",
      "868 - random_89 - lwr_k=400 - 120.97916796330668 - 175.62411378826386 - 105.53993635138461 - 116.00837556857911 - 379.7775267381369 - 179.5807700412203 - 0.48304393361315134'\n",
      "869 - random_90 - lwr_k=700 - 243.61705175537122 - 147.50283243037242 - 223.77111104761568 - 174.94094082473507 - 108.39003358673949 - 179.64991071560237 - 0.4828448995459309'\n",
      "870 - random_24 - lwr_k=200 - 155.72372429699743 - 220.91831478665978 - 126.80540853281866 - 247.05368882440436 - 152.79537021205604 - 180.65715097037395 - 0.47994537439135176'\n",
      "871 - random_89 - lwr_k=300 - 120.68469137627488 - 176.1700738168834 - 114.06411198043402 - 115.5566357385518 - 379.7723055944069 - 181.24434079062328 - 0.4782550411801242'\n",
      "872 - random_90 - lwr_k=800 - 245.8478473806944 - 147.63488718092282 - 226.36357784879377 - 178.86459261877064 - 108.65065327777424 - 181.4778631908599 - 0.4775828042729553'\n",
      "873 - random_93 - lwr_k=200 - 121.4127555006876 - 166.47753987098463 - 182.63797097155302 - 106.54971659460499 - 330.85927302629665 - 181.5822619296577 - 0.4772822734236206'\n",
      "874 - random_28 - lwr_k=100 - 133.0730605838649 - 188.13846264987131 - 293.5349122147103 - 123.40546618686979 - 170.8147242329528 - 181.78912370206103 - 0.47668678400611286'\n",
      "875 - random_53 - lwr_k=800 - 400.29240945701804 - 159.27647720826812 - 114.5986452925777 - 127.27143743759633 - 112.0800816390692 - 182.72257431515442 - 0.4739996758207522'\n",
      "876 - random_19 - deep - 135.01129034634295 - 208.69751230546657 - 118.012085852514 - 301.2543086767094 - 153.30997129492124 - 183.2528772137004 - 0.4724731014365491'\n",
      "877 - random_46 - lwr_k=1000 - 187.04250970636087 - 250.18888371752837 - 114.56721652458164 - 156.54729327970082 - 208.09957478801394 - 183.28941928503284 - 0.4723679089795014'\n",
      "878 - random_92 - lwr_k=300 - 162.34815810872072 - 177.59011598957372 - 183.07962441273307 - 171.81904479189555 - 223.21574914391954 - 183.60870489326808 - 0.4714487870041957'\n",
      "879 - random_33 - deep - 152.23486570160964 - 252.27530315180388 - 134.1424307286508 - 111.7785384329088 - 267.6836321437189 - 183.6202500322285 - 0.4714155516369427'\n",
      "880 - random_97 - lwr_k=100 - 137.65245642937663 - 289.9428961031083 - 148.65391534563497 - 220.70310779670504 - 122.21698745349778 - 183.82989009580976 - 0.47081206497531325'\n",
      "881 - random_90 - lwr_k=900 - 251.69562982497726 - 147.13041340409018 - 230.29460352612142 - 181.51300718950097 - 109.33969512041372 - 184.0005081158848 - 0.47032090982271824'\n",
      "882 - random_25 - lwr_k=100 - 190.05728210197617 - 202.3385924818348 - 165.35129420344822 - 186.0529295928793 - 178.88983222651245 - 184.53846208683387 - 0.46877231100194705'\n",
      "883 - random_89 - deep - 132.88629676555766 - 167.00844580225515 - 125.65583798966566 - 118.16372270078277 - 379.8450143903738 - 184.70738849322066 - 0.4682860248903178'\n",
      "884 - random_89 - lwr_k=200 - 125.32720917524092 - 190.0221553017269 - 112.79517885464738 - 116.14144521309584 - 379.7177845529962 - 184.79562582121062 - 0.4680320181938127'\n",
      "885 - random_90 - lwr_k=1000 - 253.9047253046518 - 146.9767935577779 - 231.53772299290895 - 184.6750394987013 - 109.35417167743402 - 185.29560773588247 - 0.46659272887673264'\n",
      "886 - random_47 - lwr_k=400 - 172.39369458644742 - 170.04404002723106 - 148.22395073225894 - 279.8910198751258 - 157.19064901079895 - 185.54753640550834 - 0.46586750615923367'\n",
      "887 - random_14 - lwr_k=200 - 181.98614636547342 - 232.819402076165 - 129.3793194449413 - 202.3080005969767 - 181.95695635928655 - 185.68964556372183 - 0.4654584189757299'\n",
      "888 - random_53 - lwr_k=500 - 412.0347973537764 - 159.7360338517962 - 115.67052585875432 - 128.28725772110752 - 114.89134913687576 - 186.14347457167915 - 0.4641519892354409'\n",
      "889 - random_14 - lwr_k=400 - 134.4893991670644 - 259.705625497368 - 222.84104128142786 - 129.29827341112843 - 185.4963597341457 - 186.3616661427654 - 0.46352388492181296'\n",
      "890 - random_4 - deep - 124.98053483634159 - 160.6519472386211 - 100.69921731229176 - 238.1518584560248 - 308.2364429640019 - 186.5386923837313 - 0.4630142819503056'\n",
      "891 - random_41 - lwr_k=1000 - 177.11529197182168 - 262.05021029156086 - 147.33669445827036 - 150.87356052161687 - 198.05611986649222 - 187.0855155492851 - 0.46144015211587286'\n",
      "892 - random_55 - lwr_k=100 - 108.57846882513883 - 227.4703863822286 - 223.5713026993922 - 264.0612289704302 - 112.13387464736996 - 187.15627543500162 - 0.46123645685290227'\n",
      "893 - random_14 - lwr_k=700 - 124.42612854856215 - 293.66562516967014 - 127.7323107253723 - 115.00190630824048 - 275.5787407546073 - 187.27552191376438 - 0.46089318407053526'\n",
      "894 - random_47 - lwr_k=500 - 148.11142016192545 - 180.58935208179926 - 137.01943642302803 - 231.38217357348873 - 241.07185837213186 - 187.63143975511002 - 0.4598686095173873'\n",
      "895 - random_53 - lwr_k=600 - 423.05424938196563 - 159.35490646045488 - 115.86048385666133 - 127.08754028479817 - 112.90301283592339 - 187.67233885809813 - 0.4597508739747975'\n",
      "896 - random_84 - lwr_k=100 - 154.9113430827613 - 272.4051004246346 - 149.76096940220793 - 133.92283920895335 - 236.31965131402282 - 189.4610009833767 - 0.4546018831548623'\n",
      "897 - random_1 - lwr_k=100 - 209.8303821362726 - 191.65852040055591 - 180.5576222927743 - 211.67516568246435 - 156.61653186218322 - 190.06934874681122 - 0.45285064293739263'\n",
      "898 - random_44 - deep - 230.59168801143252 - 284.52646963654945 - 112.1612001084315 - 151.16683218077165 - 172.58480174338936 - 190.20968048443197 - 0.4524466717884872'\n",
      "899 - random_47 - lwr_k=900 - 150.50474197917728 - 165.39350542602844 - 148.4841957307772 - 303.29312811661146 - 188.57377664318284 - 191.24635585652698 - 0.4494624128646052'\n",
      "900 - random_0 - lwr_k=200 - 174.87633873089197 - 243.78001739413054 - 250.0672682657444 - 131.30766334893508 - 160.37254863459492 - 192.07928362286356 - 0.44706467806498873'\n",
      "901 - random_30 - lwr_k=100 - 234.97694346815507 - 222.43726289935285 - 135.05445732056987 - 142.10673618077857 - 227.98457214314664 - 192.5156564366422 - 0.4458084991699022'\n",
      "902 - random_33 - lwr_k=100 - 227.48880925687183 - 191.15692374084136 - 235.44406054488923 - 168.84576181325167 - 139.76515223191743 - 192.54315537299925 - 0.44572933845594764'\n",
      "903 - random_47 - lwr_k=700 - 130.70985273879148 - 164.7551598355437 - 154.37774764453673 - 350.44978402411573 - 163.70940729900667 - 192.79503582947774 - 0.44500425452880477'\n",
      "904 - random_86 - lwr_k=200 - 150.91436084497505 - 241.74695707358111 - 137.29457701505757 - 194.40514404317622 - 248.13808772676782 - 194.49606667699166 - 0.4401075263572325'\n",
      "905 - random_31 - deep - 187.31683349609375 - 208.46300873421245 - 139.8563715880058 - 199.78356264582376 - 238.49235473474894 - 194.78178557749973 - 0.4392850316749759'\n",
      "906 - random_17 - lwr_k=1000 - 127.18785532330999 - 316.2083734478689 - 150.6415948094289 - 102.06846057336232 - 277.95905764405006 - 194.80723658890412 - 0.43921176689700026'\n",
      "907 - random_92 - lwr_k=200 - 177.99728089082984 - 183.56321304648802 - 183.72213301946366 - 169.65328968503928 - 259.5405473646749 - 194.89383557364215 - 0.43896247589277426'\n",
      "908 - random_51 - lwr_k=200 - 113.65298544658688 - 263.4095873160725 - 265.117246158365 - 137.15687006117528 - 196.8740787972089 - 195.23511758070848 - 0.43798003326341517'\n",
      "909 - random_17 - lwr_k=200 - 128.2242741764891 - 308.31973138868176 - 146.9923266649002 - 106.6749338040781 - 287.1000936490332 - 195.45647355807836 - 0.43734281962797916'\n",
      "910 - random_96 - lwr_k=100 - 224.3395341707534 - 199.41120675876584 - 187.62357791793158 - 236.0548354956399 - 133.13341412810206 - 196.11494789745316 - 0.43544728090063256'\n",
      "911 - random_17 - lwr_k=500 - 149.0712336585367 - 304.9801590449978 - 147.2239174166152 - 101.27328180362282 - 278.5216091018659 - 196.20997476820577 - 0.4351737286862498'\n",
      "912 - random_6 - lwr_k=200 - 124.30923994563794 - 198.91309019327323 - 200.31021087621386 - 261.7713514302595 - 196.50337513700092 - 196.3552399761553 - 0.4347555562367875'\n",
      "913 - random_51 - lwr_k=100 - 116.1077593157367 - 255.05026435856223 - 260.5109139539873 - 143.88155766907266 - 206.8662301525703 - 196.47641377006735 - 0.434406735325531'\n",
      "914 - random_17 - lwr_k=900 - 138.1249226393774 - 314.81451150144227 - 150.18606441687788 - 101.22236626768229 - 278.16526694763496 - 196.4975920579735 - 0.43434576975329153'\n",
      "915 - random_41 - lwr_k=800 - 171.55834549535714 - 249.2864264170833 - 145.4307781950474 - 154.50649217614472 - 263.4273353016877 - 196.83969515055645 - 0.43336096347921294'\n",
      "916 - random_28 - deep - 205.38525124911604 - 195.75110245573464 - 295.89078914877564 - 138.1199227465078 - 151.06189736666067 - 197.2424956104556 - 0.43220142812295415'\n",
      "917 - random_15 - lwr_k=100 - 144.18386402030822 - 174.3545267043995 - 196.56806593231371 - 183.11698234971598 - 288.5252221710136 - 197.34514739006772 - 0.431905926832337'\n",
      "918 - random_76 - deep - 195.00365316456762 - 277.95266376214073 - 145.2838759823265 - 160.8728712336058 - 207.78310575563367 - 197.3790269503834 - 0.43180839770834933'\n",
      "919 - random_69 - lwr_k=100 - 222.22285362508157 - 210.63012305962707 - 209.5066806044662 - 193.7667251036745 - 153.60453585665866 - 197.94827718818857 - 0.43016970748164773'\n",
      "920 - random_47 - lwr_k=800 - 134.77927181223615 - 165.79954277033173 - 153.96018448379544 - 365.2390220188685 - 173.5519660817964 - 198.6604880572741 - 0.42811947833274777'\n",
      "921 - random_42 - lwr_k=100 - 224.4487044607688 - 225.3342151752944 - 160.55679942192765 - 232.75963857822396 - 156.68571746778858 - 199.95912710164234 - 0.4243811085068533'\n",
      "922 - random_17 - lwr_k=600 - 144.4222147077758 - 336.2788445738648 - 147.61087676003092 - 101.12400890484918 - 271.1863706028403 - 200.11965953550154 - 0.42391898655740234'\n",
      "923 - random_56 - lwr_k=1000 - 135.25656636861353 - 188.1845741419019 - 433.2348935358321 - 113.3927150679992 - 130.67964075749376 - 200.14408181089746 - 0.42384868257430885'\n",
      "924 - random_82 - lwr_k=400 - 121.4784205335716 - 298.555782157827 - 143.82654692117708 - 194.95950682946847 - 242.32829165860474 - 200.22291837408926 - 0.4236217371191483'\n",
      "925 - random_23 - lwr_k=100 - 169.67812555447193 - 177.86593200561776 - 162.4981635428972 - 155.11664582419502 - 338.0340039980733 - 200.63590426019505 - 0.4224328817695163'\n",
      "926 - random_82 - lwr_k=500 - 130.01762815948345 - 298.91352992556875 - 144.564658547259 - 195.15513180497896 - 234.82521923245287 - 200.68913853520985 - 0.42227963717989625'\n",
      "927 - random_82 - lwr_k=600 - 130.34263405557263 - 299.8589260735689 - 145.29975252895414 - 195.271146704424 - 233.4409935522513 - 200.83661089543028 - 0.42185511103924755'\n",
      "928 - random_51 - lwr_k=300 - 114.54270502588423 - 265.1533983986119 - 294.62189864757386 - 137.43422976726603 - 194.25709757975648 - 201.19439268953963 - 0.420825170757417'\n",
      "929 - random_87 - deep - 133.2781880872003 - 425.212820851736 - 132.46663454556682 - 176.31623100257733 - 139.08093381472938 - 201.2650998341275 - 0.4206216266894014'\n",
      "930 - random_44 - lwr_k=100 - 230.07678732201268 - 238.16330472882697 - 231.1719942832013 - 172.1164052202392 - 139.60786856570604 - 202.22967367243618 - 0.4178449252422626'\n",
      "931 - random_82 - lwr_k=100 - 123.063345959509 - 306.6356300796759 - 144.81254352691002 - 193.67206940971633 - 243.97418489542642 - 202.42471032712655 - 0.41728347658722653'\n",
      "932 - random_17 - lwr_k=100 - 144.66426688653593 - 310.1185014652954 - 150.5927545281696 - 126.51224976252169 - 280.67092744559324 - 202.50675144629417 - 0.41704730623203135'\n",
      "933 - random_56 - lwr_k=500 - 127.37959984857524 - 194.4809553262372 - 457.4480490825417 - 118.36473165546104 - 119.47962855321698 - 203.4240345116055 - 0.41440673928771'\n",
      "934 - random_56 - lwr_k=700 - 132.45495326080643 - 187.34664350153497 - 456.1328031177837 - 117.40654272540782 - 126.83138196618621 - 204.028292138244 - 0.412667273276513'\n",
      "935 - random_56 - lwr_k=600 - 132.11797224042124 - 188.70227571222105 - 457.8709960205322 - 118.12250944041207 - 123.36780551002005 - 204.03010978924493 - 0.4126620408359165'\n",
      "936 - random_56 - lwr_k=800 - 131.02174780193909 - 185.10696524659417 - 463.42288607817426 - 114.07425924049782 - 127.10457555650989 - 204.13978078793528 - 0.41234633282293354'\n",
      "937 - random_32 - lwr_k=300 - 169.97117783556925 - 160.21569718024634 - 141.85949766927286 - 123.34552763523114 - 426.790649854648 - 204.43353786077833 - 0.4115006994022983'\n",
      "938 - random_51 - lwr_k=600 - 113.06079227489461 - 289.6207970691011 - 278.97562843706345 - 138.1799073595795 - 205.5975951898058 - 205.07900804058082 - 0.4096425955248948'\n",
      "939 - random_56 - lwr_k=900 - 134.34158512029066 - 184.39491755726314 - 464.3000291679696 - 113.77862581648581 - 129.0929853743249 - 205.17551960041214 - 0.4093647693616602'\n",
      "940 - random_41 - lwr_k=900 - 173.3744939406685 - 275.4050040486654 - 142.82246145216453 - 161.56804319070037 - 273.6570790033566 - 205.36265753766762 - 0.40882605860831434'\n",
      "941 - random_51 - lwr_k=700 - 112.2626233460079 - 293.74111701778713 - 276.8127429052708 - 137.7636393239478 - 207.50976438788862 - 205.60992674474457 - 0.40811424900531645'\n",
      "942 - random_51 - lwr_k=400 - 113.48753123242254 - 277.76508077555036 - 301.69529942480006 - 138.16893780918934 - 198.89060895495012 - 205.99351354690174 - 0.4070100242917485'\n",
      "943 - random_51 - lwr_k=800 - 112.35444215198606 - 297.20879924949276 - 277.05544311063375 - 137.76691466512702 - 207.45870558407006 - 206.36075346530086 - 0.4059528570706019'\n",
      "944 - random_82 - lwr_k=800 - 136.55333807403744 - 301.29748035757575 - 146.52537557545747 - 210.44547143756384 - 237.11864071141673 - 206.3820389197962 - 0.4058915829997498'\n",
      "945 - random_51 - lwr_k=500 - 113.33533524323529 - 284.2082421358989 - 296.56677063932324 - 138.32386813008648 - 201.6023307489158 - 206.7992486711325 - 0.4046905684821067'\n",
      "946 - random_56 - lwr_k=400 - 131.86033075608356 - 196.9338787638672 - 467.09475385780974 - 120.15877941112551 - 118.04375282534195 - 206.811835000013 - 0.4046543364341718'\n",
      "947 - random_64 - lwr_k=100 - 212.76686383601057 - 209.2343856806139 - 133.34941943138443 - 275.45974832009057 - 205.3149068619262 - 207.22554273209428 - 0.4034634031189762'\n",
      "948 - random_82 - lwr_k=900 - 136.56583470152262 - 302.0171388940555 - 147.1423926986226 - 208.65240334600267 - 242.0208498151904 - 207.27362576334596 - 0.40332498732595945'\n",
      "949 - random_51 - lwr_k=900 - 112.81702956751778 - 300.33568889204633 - 277.0853943634855 - 136.91960033837353 - 210.20794832132344 - 207.46496947292655 - 0.4027741694883371'\n",
      "950 - random_94 - lwr_k=100 - 270.39570150791593 - 247.2240339924027 - 145.26412816855353 - 204.2362136461741 - 171.520141590206 - 207.73344802887098 - 0.402001304898446'\n",
      "951 - random_51 - lwr_k=1000 - 112.35616509156937 - 304.2915960665148 - 275.1696891524497 - 137.13143669261325 - 212.3378408850856 - 208.24907538270983 - 0.4005169773253945'\n",
      "952 - random_82 - lwr_k=700 - 130.2115762451176 - 300.6404108229726 - 145.95417094635215 - 210.067919223636 - 260.4975440777161 - 209.467488910622 - 0.3970095513104742'\n",
      "953 - random_72 - lwr_k=100 - 205.50968519696139 - 199.83175824694374 - 241.03117332272103 - 220.6208926856784 - 182.4077131763629 - 209.87986762341123 - 0.3958224438201997'\n",
      "954 - random_82 - lwr_k=300 - 121.45253351173095 - 298.04119204359864 - 147.2223466318856 - 194.54147809796146 - 288.624820567601 - 209.96884016394472 - 0.3955663201018301'\n",
      "955 - random_14 - lwr_k=500 - 139.8786439347319 - 296.89187794501714 - 221.08939781775462 - 118.8863362294548 - 275.9760166645643 - 210.53836053670886 - 0.39392685162442165'\n",
      "956 - random_32 - lwr_k=200 - 180.03412283581318 - 172.07908336295193 - 159.91891194073824 - 126.46467593805797 - 414.8431301750223 - 210.66534308939092 - 0.3935613091394483'\n",
      "957 - random_17 - lwr_k=700 - 136.88049960371666 - 391.14150129467174 - 148.44469751405597 - 100.33291588687418 - 278.10178510417387 - 210.97388976511752 - 0.39267310114399456'\n",
      "958 - random_17 - lwr_k=800 - 134.13540395026521 - 393.3731527145521 - 149.47045790756303 - 100.93831572891582 - 278.0431779265176 - 211.18545653537865 - 0.3920640675302771'\n",
      "959 - random_71 - deep - 269.863853059966 - 329.2487964177759 - 133.1363894078895 - 136.17379219298633 - 199.1735166950234 - 213.5241315721377 - 0.3853317633569857'\n",
      "960 - random_73 - deep - 151.45242688409212 - 301.6401112282158 - 177.60751333232582 - 181.61841851218398 - 256.4443052521344 - 213.74718423820102 - 0.38468966549236405'\n",
      "961 - random_82 - lwr_k=1000 - 147.14856610400852 - 302.25944760194966 - 147.73894663329892 - 210.00087052256754 - 261.64690066444103 - 213.7532020503203 - 0.38467234278947093'\n",
      "962 - random_82 - lwr_k=200 - 119.2882354864261 - 299.03558483400906 - 146.87727430339038 - 194.135566862839 - 312.1863350609931 - 214.29640541820496 - 0.3831086326202514'\n",
      "963 - random_32 - deep - 221.49887692681673 - 212.30369267169644 - 220.3934501657819 - 133.92565447752617 - 284.52272228785455 - 214.5294813059959 - 0.38243768040921655'\n",
      "964 - random_56 - lwr_k=300 - 131.59889793894789 - 203.8462667110134 - 467.3011930528681 - 134.69964715337892 - 137.24284920083576 - 214.93058394758745 - 0.3812830338222213'\n",
      "965 - random_38 - deep - 165.66735621485216 - 206.45037193676683 - 121.56405814344794 - 121.29849775807001 - 460.9773030677917 - 215.18724582856646 - 0.38054418501720655'\n",
      "966 - random_85 - lwr_k=100 - 219.34050483121214 - 204.10035510987126 - 177.0565162803676 - 209.60024602262158 - 268.8845775203983 - 215.79674558111756 - 0.3787896292621331'\n",
      "967 - random_14 - lwr_k=600 - 126.92550111611989 - 349.3099859496103 - 220.43993199137412 - 115.52052256920841 - 267.5661133959226 - 215.94473362346326 - 0.3783636186362417'\n",
      "968 - random_89 - lwr_k=100 - 125.67656763385162 - 205.12916846053102 - 249.7599937570063 - 125.18111670368552 - 379.7376591447138 - 217.0890173581789 - 0.37506958877885066'\n",
      "969 - random_63 - deep - 162.3539414241396 - 131.34509967575303 - 280.72828952646194 - 141.01198715565275 - 372.94907388033255 - 217.67290385125438 - 0.3733887641172182'\n",
      "970 - random_41 - lwr_k=700 - 174.77913259803407 - 254.86233384089317 - 242.57467409912402 - 147.4109939049548 - 270.4224743980166 - 218.00619369049068 - 0.37242932908497484'\n",
      "971 - random_76 - lwr_k=100 - 254.47430975394442 - 234.23112822901143 - 259.831858811051 - 147.2838284523123 - 197.96090943943807 - 218.75948712883968 - 0.37026083625228157'\n",
      "972 - random_22 - lwr_k=800 - 121.49963394124846 - 425.3933738957336 - 121.88383389486513 - 301.30750712538105 - 132.74059194419235 - 220.55644509765742 - 0.365087964330587'\n",
      "973 - random_22 - lwr_k=500 - 122.22337690875167 - 425.3166335608153 - 120.68851401567906 - 301.2635766165539 - 133.6401459795037 - 220.61796346657917 - 0.3649108724626714'\n",
      "974 - random_22 - lwr_k=700 - 121.95796317864074 - 425.3004769115154 - 121.7435983584168 - 301.2271372070848 - 132.94127209219943 - 220.62558005264395 - 0.3648889467276918'\n",
      "975 - random_22 - lwr_k=900 - 121.7813377248616 - 425.467395290302 - 122.06363597946815 - 301.37540314783797 - 132.60515792483437 - 220.65005917245628 - 0.36481847910755694'\n",
      "976 - random_22 - lwr_k=600 - 122.42864393362417 - 425.3157786094391 - 120.63058509742443 - 301.26257518574295 - 133.823852860939 - 220.68381321166606 - 0.36472131193687074'\n",
      "977 - random_22 - lwr_k=400 - 123.11090526709823 - 425.3226414834978 - 120.04916342649295 - 301.27056637340564 - 133.8368618026363 - 220.7096103610019 - 0.364647050127872'\n",
      "978 - random_22 - lwr_k=1000 - 121.94285659412863 - 426.27226886022135 - 121.92418431405137 - 302.14458552665553 - 133.52240104664958 - 221.15270300733116 - 0.3633715269666854'\n",
      "979 - random_22 - lwr_k=300 - 124.27306684150851 - 425.2852865775276 - 119.55909042037256 - 301.21697334805066 - 136.1017473133042 - 221.27886672508728 - 0.363008341647695'\n",
      "980 - random_22 - lwr_k=200 - 126.7854410544649 - 425.2916379584126 - 132.8311497760864 - 301.23299586783935 - 130.4704758283999 - 223.3140150798759 - 0.35714979516878365'\n",
      "981 - random_77 - deep - 156.86845471612338 - 420.54748177209666 - 152.08674195862474 - 168.0054655515104 - 219.81237106685137 - 223.45835612030513 - 0.356734282404933'\n",
      "982 - random_83 - lwr_k=100 - 267.96280586237515 - 260.28243455359035 - 175.45850972936614 - 167.13170644542754 - 246.44128907629556 - 223.45918730663695 - 0.3567318903826654'\n",
      "983 - random_47 - lwr_k=300 - 270.7956170546345 - 190.14456034766053 - 157.59082945481225 - 293.40273437417886 - 207.1725672091574 - 223.82531259834798 - 0.35567793181814755'\n",
      "984 - random_41 - lwr_k=400 - 209.91107048412067 - 246.1925238337933 - 259.7459403227297 - 145.28132985303242 - 258.5514446542565 - 223.93525232705588 - 0.3553614502166964'\n",
      "985 - random_41 - lwr_k=500 - 174.05086267409388 - 263.94732379704965 - 268.654229369229 - 146.2244921801951 - 269.89640325311393 - 224.55030697709051 - 0.35359090300041107'\n",
      "986 - random_23 - deep - 241.66924393752527 - 207.23157748953955 - 165.85073224487567 - 180.56599973552747 - 327.54564692738654 - 224.57411319500324 - 0.3535223717483852'\n",
      "987 - random_41 - lwr_k=600 - 180.63925794129693 - 294.0879507472783 - 154.8547915936401 - 138.49215399852523 - 356.19940172796606 - 224.85089820991132 - 0.3527255962012825'\n",
      "988 - random_17 - deep - 161.4822677349222 - 362.5226207501213 - 166.78477221740224 - 134.5731945819205 - 300.14240592571616 - 225.09556787667265 - 0.35202126914555043'\n",
      "989 - random_99 - deep - 183.26589316006365 - 199.6665765615104 - 382.49085117486493 - 187.18516304637913 - 176.3185925074516 - 225.78174585865295 - 0.35004598041775337'\n",
      "990 - random_66 - lwr_k=100 - 186.60408511606994 - 214.16987484020518 - 153.26277034496752 - 232.14398911708292 - 346.30062547407454 - 226.49282881084787 - 0.3479989988905263'\n",
      "991 - random_22 - lwr_k=100 - 130.83922677515895 - 425.3141960096375 - 144.36137750868716 - 301.23809646387525 - 133.20594961159662 - 226.98347740224068 - 0.34658657725024467'\n",
      "992 - random_17 - lwr_k=300 - 124.31991037113708 - 306.68855449596754 - 145.20195824088574 - 104.31949557746823 - 459.9366451764311 - 228.0843637035265 - 0.34341747483620966'\n",
      "993 - random_17 - lwr_k=400 - 138.8108598587311 - 304.21552970203356 - 146.18845891584348 - 102.31024596441296 - 464.3361693607995 - 231.16428782479136 - 0.33455135037236206'\n",
      "994 - random_82 - deep - 226.98311020423625 - 311.878838990258 - 173.85766759973603 - 182.2648378432234 - 261.13926689868447 - 231.22437915436313 - 0.33437836577421365'\n",
      "995 - random_22 - deep - 134.3421373301539 - 425.3072456863226 - 127.09661263831477 - 301.25317775632556 - 172.72920147433987 - 232.13723906102805 - 0.3317505316975661'\n",
      "996 - random_90 - deep - 324.4493117233803 - 150.12187324448257 - 259.60278391375425 - 301.15412375782006 - 128.29626402663692 - 232.73277761950038 - 0.33003616511570066'\n",
      "997 - random_4 - lwr_k=100 - 179.68348765229584 - 219.67313497259633 - 142.0783589049818 - 387.329981924112 - 244.88835540284654 - 234.72591668822415 - 0.3242985507727867'\n",
      "998 - random_2 - lwr_k=100 - 229.6507567026027 - 259.4604454339278 - 224.1391932261912 - 227.58548120341047 - 233.55283278381515 - 234.87729111238622 - 0.3238627918279484'\n",
      "999 - random_52 - deep - 195.80513800259294 - 235.44244986168522 - 173.9695132585783 - 118.51421182985294 - 462.1618255707353 - 237.1750556136133 - 0.31724825603387885'\n",
      "1000 - random_56 - lwr_k=200 - 225.90428584259547 - 229.14554332620523 - 394.6396958452585 - 182.73909713304667 - 154.62156961499403 - 237.4090461350597 - 0.3165746722966868'\n",
      "1001 - random_9 - lwr_k=100 - 354.07158087448005 - 215.74856231549228 - 220.53998880333393 - 238.76673412548033 - 159.1878820386303 - 237.67298831993142 - 0.31581486647998647'\n",
      "1002 - random_26 - lwr_k=100 - 188.41136035846912 - 276.9645567604764 - 204.7237188274803 - 183.51696200791542 - 338.3318130102961 - 238.3853722307134 - 0.31376413919893853'\n",
      "1003 - random_93 - lwr_k=100 - 220.9064099799772 - 218.99206678473584 - 222.7189692023864 - 144.8975376260602 - 407.65002097861145 - 243.03109279164516 - 0.3003905835216768'\n",
      "1004 - random_41 - lwr_k=300 - 278.46597001970247 - 266.9114524393301 - 281.8884548864695 - 149.88203003186715 - 239.4384289720265 - 243.32029837566995 - 0.29955805239342603'\n",
      "1005 - random_0 - deep - 173.71247653303476 - 417.41226105486436 - 371.9671009321982 - 132.7262662087293 - 131.1809832501792 - 245.39363574541528 - 0.29358957108745465'\n",
      "1006 - random_39 - lwr_k=100 - 293.1744907456665 - 230.94560074870788 - 173.21410495287756 - 340.79924970510655 - 200.69288095003174 - 247.76918135919183 - 0.2867511296563815'\n",
      "1007 - random_8 - lwr_k=100 - 221.35304709976276 - 258.38724861677633 - 198.39600457300313 - 339.29319397679564 - 226.43457090301334 - 248.7704484455696 - 0.28386879935866827'\n",
      "1008 - random_23 - lwr_k=800 - 169.10019493930963 - 176.96493179900241 - 163.64440608845496 - 134.7101067286148 - 600.8718243343717 - 249.0513974607863 - 0.2830600362726857'\n",
      "1009 - random_23 - lwr_k=900 - 169.09733107878318 - 176.9442587814259 - 163.512084096231 - 136.3152884137972 - 600.5937742630173 - 249.28563156119316 - 0.2823857506065077'\n",
      "1010 - random_21 - lwr_k=100 - 230.56254149192046 - 315.48804982393636 - 232.6838151629176 - 272.58011769888907 - 195.34712789610407 - 249.33071177134863 - 0.28225597898280597'\n",
      "1011 - random_47 - lwr_k=200 - 275.86717605597653 - 225.88664557949616 - 191.96128076665232 - 253.87196552745652 - 304.9747260012672 - 250.51454530024955 - 0.2788480978148111'\n",
      "1012 - random_92 - lwr_k=100 - 306.069070808569 - 232.97299468919823 - 214.07200645703563 - 221.23530813539716 - 286.6078574104218 - 252.19609372496987 - 0.27400745335781895'\n",
      "1013 - random_88 - lwr_k=800 - 343.6768355106818 - 425.28220365605665 - 150.6887370143562 - 110.13912425421437 - 234.73972792873923 - 252.91315350221961 - 0.27194326574059147'\n",
      "1014 - random_88 - lwr_k=700 - 343.59787586517774 - 425.3436603612248 - 154.17037804149547 - 109.68054671295597 - 233.22249430603094 - 253.2107864079123 - 0.2710864750267592'\n",
      "1015 - random_11 - lwr_k=100 - 234.10769691343728 - 269.3279623215662 - 199.13271923170635 - 233.13938967524055 - 332.93788517708776 - 253.72743857742006 - 0.26959919733451154'\n",
      "1016 - random_88 - lwr_k=900 - 344.0134163617908 - 425.29194953454595 - 155.1334341262687 - 110.35279770262642 - 236.27126592627056 - 254.22031685272475 - 0.2681803571415968'\n",
      "1017 - random_88 - lwr_k=600 - 343.7612677602393 - 425.50661652801347 - 156.32160153317577 - 110.26147773294412 - 236.62292550916806 - 254.50247585426968 - 0.2673681108888336'\n",
      "1018 - random_81 - lwr_k=100 - 318.64001635368624 - 247.90898078986945 - 216.40442394135778 - 246.71490660380047 - 243.32252624674348 - 254.60369354024488 - 0.2670767372814118'\n",
      "1019 - random_88 - lwr_k=300 - 345.5834060563992 - 428.2998543502577 - 157.83457190979803 - 112.80807915864804 - 230.36618310769995 - 254.98623238561365 - 0.26597552930298163'\n",
      "1020 - random_88 - lwr_k=500 - 343.59178221165894 - 425.5569683029479 - 159.26521411581547 - 113.64145592626583 - 234.7429848404488 - 255.36728991877993 - 0.26487858555244814'\n",
      "1021 - random_88 - lwr_k=400 - 344.26097125091155 - 426.03867111111583 - 161.21216842152143 - 112.98279611292128 - 233.07891904226562 - 255.5223583669523 - 0.2644321926843839'\n",
      "1022 - random_88 - lwr_k=1000 - 344.4166572407776 - 425.450079012188 - 161.1171371493084 - 110.39261070972368 - 239.50885553184455 - 256.18467741387644 - 0.26252558626370526'\n",
      "1023 - random_52 - lwr_k=100 - 228.94686996410138 - 266.78745735763016 - 213.9294676600733 - 189.6530151867714 - 386.87288789016105 - 257.23549988514793 - 0.2595006017339322'\n",
      "1024 - random_88 - lwr_k=200 - 346.86115934340205 - 428.6428006102799 - 166.82136144619253 - 114.57003634640886 - 231.37771780235698 - 257.66230798177304 - 0.25827195662520075'\n",
      "1025 - random_16 - lwr_k=100 - 275.8844775977172 - 265.2937167315128 - 181.90613210175164 - 378.32327878775527 - 210.73050009390857 - 262.4287815365318 - 0.24455079138663194'\n",
      "1026 - random_41 - lwr_k=200 - 236.2963871128696 - 266.84354847119005 - 313.83710647307254 - 166.16255066308784 - 334.6038575111565 - 263.54633989942005 - 0.24133369539632166'\n",
      "1027 - random_23 - lwr_k=600 - 168.30628970972404 - 177.02192677511727 - 165.70366185099232 - 167.08124411841487 - 648.0408765134927 - 265.2224413501122 - 0.23650873104954762'\n",
      "1028 - random_23 - lwr_k=1000 - 169.3947892873836 - 177.2072253003649 - 162.96436251084674 - 137.1738721952945 - 680.1087024407437 - 265.3615137859523 - 0.23610838562638115'\n",
      "1029 - random_6 - deep - 118.52910473264497 - 160.92647351474275 - 286.3153218889915 - 652.5157805751654 - 114.90207127303387 - 266.6249776320098 - 0.23247127320275351'\n",
      "1030 - random_88 - deep - 343.5871689368939 - 425.80236600585516 - 178.41506261196358 - 120.96233309523306 - 267.9228046708397 - 267.34451988097356 - 0.23039993933475333'\n",
      "1031 - random_0 - lwr_k=100 - 290.9611307383776 - 359.1974271331902 - 247.61291241935893 - 176.53668873842275 - 264.28831445697034 - 267.7212989949564 - 0.22931531263262672'\n",
      "1032 - random_88 - lwr_k=100 - 359.16247772630504 - 446.47005181612093 - 183.33986831343324 - 125.50001201197736 - 225.9918489907414 - 268.10070530965584 - 0.22822312221622343'\n",
      "1033 - random_32 - lwr_k=100 - 232.21945774995743 - 221.56081021842243 - 205.42138311194333 - 153.8596526678883 - 534.8782524325222 - 269.58468870652564 - 0.22395120479848918'\n",
      "1034 - random_47 - deep - 165.98947293511753 - 204.92318539374756 - 132.85353211423043 - 222.70919343488595 - 679.2869813696174 - 281.14253696621256 - 0.19067982541363082'\n",
      "1035 - random_21 - deep - 317.37327517805426 - 424.6596329899994 - 272.83320959830604 - 265.11021623039824 - 142.0375128248839 - 284.4056102943801 - 0.18128647247551588'\n",
      "1036 - random_6 - lwr_k=100 - 206.93404062032135 - 290.4134721621687 - 256.5459845272484 - 362.86602779665924 - 310.52300890324284 - 285.44973528880445 - 0.17828076874714338'\n",
      "1037 - random_45 - deep - 211.80033553550984 - 283.727095182418 - 129.14418567034025 - 125.55511441051883 - 680.5763326482868 - 286.15419837039724 - 0.17625284212475922'\n",
      "1038 - random_24 - lwr_k=100 - 269.45425314305135 - 368.0297567494888 - 178.49338759127696 - 354.33451922646765 - 268.2639595764039 - 287.7136004968933 - 0.17176381900617166'\n",
      "1039 - random_14 - lwr_k=100 - 252.7369388141835 - 333.26051115217604 - 223.25950868654525 - 398.1785052922581 - 264.5350776445971 - 294.39051594390025 - 0.15254309763221818'\n",
      "1040 - random_51 - deep - 115.55259533586174 - 425.26405885210323 - 364.75147534527105 - 221.32528613486542 - 379.6607437791779 - 301.2948128979279 - 0.13266781609136813'\n",
      "1041 - random_41 - lwr_k=100 - 209.59562439307575 - 374.8457082906148 - 320.48353698865714 - 245.97568999545047 - 356.01609246174166 - 301.3754149631593 - 0.13243578925907384'\n",
      "1042 - random_50 - lwr_k=1000 - 115.96207075514698 - 425.450079012188 - 286.95040113638436 - 301.3072493748316 - 379.71619335375055 - 301.8611660317405 - 0.13103746603326705'\n",
      "1043 - random_50 - lwr_k=900 - 116.83408213514105 - 425.29194953454595 - 286.9500694270784 - 301.3007823548826 - 379.72078973896964 - 302.00356486815946 - 0.1306275449581077'\n",
      "1044 - random_50 - lwr_k=800 - 117.0983725292693 - 425.28220365605665 - 286.95721351998156 - 301.4897290288949 - 379.7660891001014 - 302.1027660348637 - 0.13034197626994226'\n",
      "1045 - random_50 - lwr_k=700 - 117.81358523506549 - 425.3436603612248 - 287.0699831713659 - 302.3825643853428 - 379.7374582709159 - 302.453526184658 - 0.12933224907453433'\n",
      "1046 - random_50 - lwr_k=600 - 118.38805294674812 - 425.50661652801347 - 287.3746368067483 - 301.4830433339586 - 379.795937311819 - 302.49377935816887 - 0.1292163729579936'\n",
      "1047 - random_50 - lwr_k=500 - 118.89205769577896 - 425.5569683029479 - 287.5068171455987 - 301.3258237443412 - 379.7311687606999 - 302.5867245540339 - 0.12894881322521778'\n",
      "1048 - random_50 - lwr_k=200 - 116.05916475080606 - 428.6428006102799 - 287.5630570865691 - 301.6192722384436 - 379.7177845529962 - 302.70431880995267 - 0.12861029666826385'\n",
      "1049 - random_50 - lwr_k=400 - 119.27546787652174 - 426.03867111111583 - 287.59830063151054 - 301.5901315636435 - 379.7775267381369 - 302.8401882154631 - 0.12821917175328068'\n",
      "1050 - random_50 - deep - 121.82150527691019 - 425.3134516378372 - 286.9505926068871 - 301.21643222678887 - 379.9104133596087 - 303.02684695729516 - 0.12768183950296907'\n",
      "1051 - random_50 - lwr_k=300 - 115.46344615652455 - 428.2998543502577 - 289.2267592770185 - 302.78468063058006 - 379.7723055944069 - 303.09322724564805 - 0.1274907526599851'\n",
      "1052 - random_56 - lwr_k=100 - 351.7315087910707 - 356.02721962942087 - 246.89398883612506 - 266.30468647116345 - 310.86979993093706 - 306.36935294871995 - 0.11805982608549637'\n",
      "1053 - random_50 - lwr_k=100 - 123.24696572117777 - 446.47005181612093 - 296.5969670531411 - 307.87120724859193 - 379.7376591447138 - 310.76839758511784 - 0.1053963721390051'\n",
      "1054 - random_46 - deep - 343.55128973599136 - 425.205128869192 - 119.83991698429162 - 301.21374105080906 - 379.91472014130676 - 313.94751212117444 - 0.09624470895558335'\n",
      "1055 - random_31 - lwr_k=100 - 332.89867771328414 - 271.5049085474552 - 278.2279088233429 - 352.9723514806768 - 345.61533499301134 - 316.2452725690051 - 0.0896301865551421'\n",
      "1056 - random_68 - deep - 303.4639804182381 - 384.2863908498747 - 254.60753367737792 - 301.2164983416282 - 379.91632844991545 - 324.696313285115 - 0.06530232034208061'\n",
      "1057 - random_68 - lwr_k=200 - 304.45335449522946 - 383.36414877466046 - 254.45573445586584 - 301.6192698171455 - 379.7177845529962 - 324.7203105143913 - 0.06523324094832428'\n",
      "1058 - random_86 - lwr_k=100 - 345.71446842509727 - 378.92296332900935 - 201.68732936486052 - 347.44183422175354 - 357.88741206100707 - 326.3324730625243 - 0.06059233641791317'\n",
      "1059 - random_68 - lwr_k=100 - 309.3398011360732 - 382.8287217559198 - 254.68715647940732 - 307.87121708934404 - 379.7376591447138 - 326.89139739998217 - 0.05898337056464398'\n",
      "1060 - random_37 - deep - 105.30901952283136 - 148.89699438308116 - 618.7841123356394 - 632.6660194150227 - 141.2800672629004 - 329.36791645634486 - 0.051854256826198264'\n",
      "1061 - random_34 - deep - 343.5969034129176 - 425.33133464910287 - 246.65008929517055 - 301.2183632400335 - 332.26264000514703 - 329.81305335929585 - 0.05057284889732816'\n",
      "1062 - random_34 - lwr_k=100 - 345.97725211522504 - 425.3141960096375 - 249.15702758349494 - 301.23809646387525 - 332.52761650165246 - 330.84414287572054 - 0.047604670316855224'\n",
      "1063 - random_34 - lwr_k=400 - 343.64510512050555 - 425.3226414834978 - 280.10449889978037 - 301.27056637340564 - 331.3666495522597 - 336.34252209037703 - 0.03177658087465274'\n",
      "1064 - random_68 - lwr_k=800 - 303.5106477964869 - 382.90632563222005 - 330.9990827077203 - 301.4897290288949 - 379.7660891001014 - 339.731251040817 - 0.022021505273470243'\n",
      "1065 - random_68 - lwr_k=900 - 303.6252986891337 - 383.08544833570863 - 330.9944121598623 - 301.30078124670575 - 379.72078973896964 - 339.7422311627975 - 0.02198989698564413'\n",
      "1066 - random_68 - lwr_k=300 - 303.4787403324183 - 383.2773248795964 - 331.0674989170634 - 302.78468540778226 - 379.7723055944069 - 340.07295499221635 - 0.021037848000234893'\n",
      "1067 - random_34 - lwr_k=300 - 343.6622656395826 - 425.2852865775276 - 316.7247168017579 - 301.21697334805066 - 331.49492766938494 - 343.6768327509337 - 0.010663427234258949'\n",
      "1068 - random_34 - lwr_k=600 - 343.8962823498964 - 425.3157786094391 - 316.85440604519266 - 301.26257518574295 - 331.525774817753 - 343.770974208689 - 0.010392423843005427'\n",
      "1069 - random_34 - lwr_k=700 - 344.42039067459126 - 425.3004769115154 - 318.50314874320264 - 301.2271372070848 - 331.60223757421596 - 344.21069630701794 - 0.009126603420232304'\n",
      "1070 - random_34 - lwr_k=900 - 344.10137197439116 - 425.467395290302 - 321.6669252088291 - 301.37540314783797 - 331.92321332940605 - 344.9067923274234 - 0.007122763808375621'\n",
      "1071 - random_34 - lwr_k=800 - 344.1508292219724 - 425.3933738957336 - 321.9689178701478 - 301.30750712538105 - 331.74736100694093 - 344.91353204543884 - 0.007103362298330529'\n",
      "1072 - random_34 - lwr_k=1000 - 344.08191569806945 - 426.27226886022135 - 321.3094940384524 - 302.14458552665553 - 332.24395636874135 - 345.2103467779382 - 0.0062489268455629965'\n",
      "1073 - random_56 - deep - 343.35131583378234 - 424.81380903170816 - 286.58872546960487 - 300.96975005603446 - 379.39886416377254 - 347.02417345510145 - 0.0010274941777101976'\n",
      "1074 - random_49 - deep - 343.65586084826236 - 425.3303865380922 - 286.98748146310453 - 301.21640717052804 - 379.9160946825035 - 347.42092611385885 - -0.0001146308614894398'\n",
      "1075 - random_49 - lwr_k=800 - 343.6768355106818 - 425.28220365605665 - 286.95721351998156 - 301.4897290288949 - 379.7660891001014 - 347.4340901222108 - -0.00015252477782223828'\n",
      "1076 - random_49 - lwr_k=900 - 344.0134163617908 - 425.29194953454595 - 286.9500694270784 - 301.30078124670575 - 379.72078973896964 - 347.4551044366284 - -0.00021301832240716578'\n",
      "1077 - random_49 - lwr_k=500 - 343.59178221165894 - 425.5569683029479 - 287.5068171455987 - 301.3258237443412 - 379.7311687606999 - 347.5421713354104 - -0.00046365630272382496'\n",
      "1078 - random_49 - lwr_k=600 - 343.61210876102876 - 425.50661652801347 - 287.3746368067483 - 301.4830433339586 - 379.795937311819 - 347.55412857247825 - -0.0004980773949043726'\n",
      "1079 - random_49 - lwr_k=1000 - 344.4166572407776 - 425.450079012188 - 286.95040113638436 - 301.30724707513184 - 379.71619335375055 - 347.56784379249063 - -0.0005375591622764908'\n",
      "1080 - random_49 - lwr_k=700 - 343.58691884856 - 425.3436603612248 - 287.0699831713659 - 302.3825643853428 - 379.7374582709159 - 347.6237688531046 - -0.0006985496700062654'\n",
      "1081 - random_49 - lwr_k=400 - 344.0010774437867 - 426.03867111111583 - 287.59830063151054 - 301.59012923155603 - 379.7775267381369 - 347.800813326531 - -0.0012082045430588106'\n",
      "1082 - random_49 - lwr_k=200 - 346.86115934340205 - 428.6428006102799 - 287.5630570865691 - 301.6192698171455 - 379.7177845529962 - 348.8806401138362 - -0.0043166832973058344'\n",
      "1083 - random_49 - lwr_k=300 - 345.5834060563992 - 428.2998543502577 - 289.2267592770185 - 302.78468540778226 - 379.7723055944069 - 349.1330959974626 - -0.005043423696657889'\n",
      "1084 - random_49 - lwr_k=100 - 359.1624927822659 - 446.47005181612093 - 296.5969670531411 - 307.87121708934404 - 379.7376591447138 - 357.96778061395787 - -0.03047567797457118'\n",
      "1085 - random_55 - deep - 102.4699027225889 - 751.1347900495904 - 614.3749610996288 - 218.2957524031902 - 131.616879399453 - 363.5559390903447 - -0.046562214567587024'\n",
      "1086 - random_68 - lwr_k=600 - 555.3652804269589 - 382.8814035447184 - 255.8591122599175 - 301.4830433339586 - 379.795937311819 - 375.09250283365424 - -0.07977232056402928'\n",
      "1087 - random_47 - lwr_k=100 - 418.6211771201438 - 353.0827360592881 - 468.7174245173137 - 448.8657794426488 - 230.27411453360835 - 383.9152395159377 - -0.10517018052973337'\n",
      "1088 - random_68 - lwr_k=700 - 555.359632009211 - 382.9121288221083 - 331.63275309686003 - 302.3825643853428 - 379.7374582709159 - 390.41913245699567 - -0.12389282499906473'\n",
      "1089 - random_68 - lwr_k=500 - 555.3636882628902 - 382.96954219418734 - 332.84203297287655 - 301.3258237443412 - 379.7311687606999 - 390.46067309430117 - -0.12401240731543273'\n",
      "1090 - random_41 - deep - 670.9764976764548 - 424.10509601180127 - 313.6904936901913 - 193.51943184086454 - 379.4247473008985 - 396.36693608971865 - -0.14101466600533397'\n",
      "1091 - random_34 - lwr_k=500 - 343.6657877319237 - 425.3166335608153 - 1667.7833867834959 - 301.2635766165539 - 331.57581062026304 - 613.8977331595981 - -0.7672168196246394'\n",
      "1092 - random_34 - lwr_k=200 - 347.7480712171849 - 425.2916379584126 - 1669.0656778209893 - 301.23299586783935 - 332.46125846764255 - 615.1368675681514 - -0.7707838944813921'\n",
      "1093 - random_23 - lwr_k=300 - 164.42343260421075 - 177.89376018847398 - 163.6580930206666 - 168.15583951343635 - 4874.568987010576 - 1109.6585015473397 - -2.194354795677539'\n",
      "1094 - random_23 - lwr_k=200 - 153.07448658330753 - 177.14665094297905 - 157.52368117328868 - 152.31873544849105 - 5631.877032143834 - 1254.2931436786391 - -2.610712046191076'\n",
      "1095 - random_23 - lwr_k=700 - 168.7426067549721 - 176.63691350748982 - 163.3269094511534 - 167.71384777774557 - 14280.113654936 - 2991.0633780532776 - -7.610322574500943'\n",
      "1096 - random_23 - lwr_k=500 - 167.0985972561305 - 176.72565730924242 - 165.61746660891285 - 166.27465212816273 - 14299.37860467461 - 2994.775125260956 - -7.621007517189758'\n",
      "1097 - random_68 - lwr_k=400 - 14350.035718980522 - 383.48590938474354 - 1226.6282378719234 - 301.59012923155603 - 379.7775267381369 - 3329.253981520933 - -8.583866033622451'\n",
      "1098 - random_23 - lwr_k=400 - 165.02801816391639 - 176.26860173279385 - 167.11801186137836 - 166.62569516525357 - 31332.769053467648 - 6401.024058394694 - -17.42651759047339'\n",
      "1099 - random_68 - lwr_k=1000 - 50840.86444674101 - 383.2955671778886 - 330.8172551770957 - 301.30724707513184 - 379.71619335375055 - 10450.683555522159 - -29.08420256064377'\n"
     ]
    }
   ],
   "source": [
    "scores_df = pd.DataFrame(all_scores)\n",
    "scores_df.to_csv(log_dir/f\"scores.csv\",index=False)\n",
    "\n",
    "scores_df_final = pd.DataFrame(all_scores_final)\n",
    "scores_df_final.to_csv(log_dir/f\"test_scores.csv\",index=False)\n",
    "\n",
    "scores_df_sorted = pd.DataFrame(scores_df).sort_values(by='MSE')\n",
    "\n",
    "best_5 = []\n",
    "summary_logger.info(f\"Rank - \" +\" - \".join(list(scores_df_sorted.columns)))\n",
    "for i,(index,row) in enumerate(scores_df_sorted.iterrows()):\n",
    "    if i < 5:\n",
    "        best_5.append((row[\"model_num\"],row[\"predictor\"],row[\"MSE\"],row[\"R2\"]))\n",
    "    s = f\"{i} - \" + \" - \".join([f\"{i}\" for i in row.tolist()])\n",
    "    summary_logger.info(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% save scores\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      " Best 5 on Test Sest \n",
      " ---------------------'\n",
      "Rank -  Deep Model - Predictor - Val Set - Test Set'\n",
      "0 - random_8 - lwr_k=800 - 105.75616344477817 - 0.6955615557388015 - 113.8248299208801 - 0.6539647625223532'\n",
      "1 - random_8 - lwr_k=900 - 106.08188456278927 - 0.694623907972465 - 114.46174080250027 - 0.6520285100515459'\n",
      "2 - random_60 - deep - 106.39684576148638 - 0.6937172342672797 - 112.66962403605382 - 0.6574766710846802'\n",
      "3 - random_8 - lwr_k=1000 - 106.72470995950236 - 0.6927734175865394 - 114.9739585938095 - 0.6504713330702215'\n",
      "4 - random_60 - lwr_k=600 - 107.18498969208366 - 0.6914484182564988 - 137.81167999934814 - 0.5810431041371579'\n"
     ]
    }
   ],
   "source": [
    "summary_logger.info(\"-----------------------\\n Best 5 on Test Sest \\n ---------------------\")\n",
    "summary_logger.info(f\"Rank -  Deep Model - Predictor - Val Set - Test Set\")\n",
    "for i, (j,k,v,x) in enumerate(best_5):\n",
    "\n",
    "    row = scores_df_final.loc[(scores_df_final['model_num']==j) & (scores_df_final['predictor'] == k)].iloc[0]\n",
    "    #print(row)\n",
    "    s = f\"{i} - {j} - {k} - {v} - {x} - {row['MSE']} - {row['R2']}\"\n",
    "    summary_logger.info(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote Summary Graph'\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#take 1 is a scatter plot - lets, for each dataset\n",
    "#graph our deep models by rank - plot - then overlay our knn moels\n",
    "#plot points\n",
    "\n",
    "deep_set = scores_df[scores_df[\"predictor\"]==\"deep\"].sort_values(\"R2\")\n",
    "deep_set[\"order\"] = [i for i in range(0,100)]\n",
    "deep_ordering = {row[\"model_num\"]:row[\"order\"] for index, row in deep_set.iterrows()}\n",
    "\n",
    "def order_models(x):\n",
    "    x = [deep_ordering[i] for i in x]\n",
    "    return x\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "set_deep = False\n",
    "knn_models = scores_df[\"predictor\"].unique()\n",
    "for knn_model in knn_models:\n",
    "    subset = scores_df[scores_df[\"predictor\"]==knn_model]\n",
    "    s=3\n",
    "    if knn_model == \"deep\":\n",
    "        s=10\n",
    "    ax.scatter(x=order_models(subset[\"model_num\"].tolist()), y=subset[\"R2\"], s=s, label=knn_model)\n",
    "\n",
    "#ax.set_ylim(0,scores_db[\"deep_mean\"].max())\n",
    "ax.set_ylim(0,1)\n",
    "# plot residuals\n",
    "ax.legend(loc='upper right',bbox_to_anchor=(1.4, 1))\n",
    "ax.set_ylabel(\"R2 Score\")\n",
    "ax.set_xlabel(\"Deep Model Rank\")\n",
    "#ax.set_yscale(\"symlog\")\n",
    "ax.set_title(\"Summary of LWR improvements over Deep Models\")\n",
    "plt.savefig(log_dir/f\"summary_plot.png\", bbox_inches='tight')\n",
    "logging.getLogger().info(\"Wrote Summary Graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df[\"n_features\"] = [deep_models[i].n_features for i in scores_df[\"model_num\"]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import Colormap\n",
    "import seaborn as sns #heatmap of features - pls model - score\n",
    "class nlcmap(Colormap):\n",
    "    def __init__(self, cmap, levels):\n",
    "        self.cmap = cmap\n",
    "        self.N = cmap.N\n",
    "        self.monochrome = self.cmap.monochrome\n",
    "        self.levels = np.asarray(levels, dtype='float64')\n",
    "        self._x = self.levels\n",
    "        self.levmax = self.levels.max()\n",
    "        self.levmin = self.levels.min()\n",
    "        self.transformed_levels = np.linspace(self.levmin, self.levmax, #uniform spacing along levels (colour segments)\n",
    "             len(self.levels))\n",
    "\n",
    "    def __call__(self, xi, alpha=1.0, **kw):\n",
    "        yi = np.interp(xi, self._x, self.transformed_levels)\n",
    "        return self.cmap((yi-self.levmin) / (self.levmax-self.levmin), alpha)\n",
    "    \n",
    "levels = np.concatenate((\n",
    "    [0, 1],\n",
    "    [0.6,0.8,0.9,0.95,0.98]\n",
    "    ))\n",
    "\n",
    "levels = levels[levels <= 1]\n",
    "levels.sort()\n",
    "cmap_nonlin = nlcmap(plt.cm.YlGnBu, levels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = scores_df[[\"predictor\",\"n_features\",\"R2\"]]\n",
    "subset = subset[np.logical_not(subset[\"predictor\"]==\"deep\")]\n",
    "subset = subset[np.logical_not(subset[\"predictor\"]==\"lr\")]\n",
    "trans = subset[\"predictor\"].transform(lambda x: int(x.replace(\"lwr_k=\",\"\"))).tolist()\n",
    "subset.loc[:,\"predictor\"]=trans\n",
    "subset=subset.sort_values(\"predictor\",ascending=False)\n",
    "\n",
    "def rand_jitter(arr):\n",
    "    stdev = .01 * (max(arr) - min(arr))\n",
    "    return arr + np.random.randn(len(arr)) * stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\AppData\\Local\\Temp\\ipykernel_100516\\3702862414.py:6: MatplotlibDeprecationWarning: Auto-removal of grids by pcolor() and pcolormesh() is deprecated since 3.5 and will be removed two minor releases later; please call grid(False) first.\n",
      "  cbar = fig.colorbar(sc,label=\"R2 Score\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sc = ax.scatter(x=rand_jitter(subset[\"n_features\"]), y=rand_jitter(subset[\"predictor\"]), s=20,c=subset[\"R2\"],cmap=cmap_nonlin,vmin=0)\n",
    "ax.set_xlabel(\"Number of Features\")\n",
    "ax.set_ylabel(\"Number of Neighbours\")\n",
    "\n",
    "cbar = fig.colorbar(sc,label=\"R2 Score\")\n",
    "\n",
    "ax.set_title(\"LWR performance as a function of the number of components\")\n",
    "plt.savefig(log_dir/f\"heat_scatter.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (lazydeep)",
   "language": "python",
   "name": "pycharm-12fcba0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
