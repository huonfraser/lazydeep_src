{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "# set seed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import utils as ut\n",
    "import experiment as exp\n",
    "from evaluation import *\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import random\n",
    "#define fixed_hyperparams and create a config gen\n",
    "from configurations import RandomConfigGen, Configuration\n",
    "from torch import nn\n",
    "from deep_net import RandomNet\n",
    "from experiment import run_experiment\n",
    "import regex as re\n",
    "from pathlib import *\n",
    "from plot import *\n",
    "from sk_models import setup_pls_models_slim\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed + 1)\n",
    "np.random.seed(seed + 2)\n",
    "random_state = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\workspace\\lazydeep\\experiments\\1.02\\A_AL_RT\n"
     ]
    }
   ],
   "source": [
    "#setup input and output formats, load data\n",
    "\n",
    "file_name = \"A_AL_RT.csv\"\n",
    "\n",
    "id_cols =[]#[\"db_id\", \"sample_id\"]#[\"sample_id\"]\n",
    "\n",
    "data_path = Path('D:/workspace/lazydeep/data/soil_data/')\n",
    "model_path = Path('D:/workspace/lazydeep/experiments/1.01/')\n",
    "log_path = Path(\"D:/workspace/lazydeep/experiments/1.02\")\n",
    "n_components = 22\n",
    "\n",
    "data_file = data_path / file_name\n",
    "log_dir = log_path / re.sub(r'\\.(?=csv$)[^.]+$', '',file_name)\n",
    "model_dir = model_path / re.sub(r'\\.(?=csv$)[^.]+$', '',file_name)\n",
    "\n",
    "if not log_dir.exists():\n",
    "    log_dir.mkdir()\n",
    "print(log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% load data\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1438, 1702)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(data_file)\n",
    "nrow, ncol = data.shape\n",
    "data = ut.sample_data(data,random_state)\n",
    "\n",
    "n_features = ncol - 1-len(id_cols)\n",
    "dataset = TabularDataset(data,id_cols = id_cols, cat_cols=None, output_cols=None, ignore_cols= None)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% load models\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 models\n"
     ]
    }
   ],
   "source": [
    "n_models = 100\n",
    "model_names = [f\"random_{i}\" for i in range(0,n_models)]\n",
    "deep_models = {name:torch.load(model_dir/\"models\"/name/\"_model\") for name in model_names}\n",
    "#for each model, load state\n",
    "print(f\"Loaded {len(deep_models)} models\")\n",
    "#print(deep_models)\n",
    "for name in model_names:\n",
    "    sub_path = log_dir / name\n",
    "    if not sub_path.exists():\n",
    "        sub_path.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% setup logging and tensorboard outputs\n"
    }
   },
   "outputs": [],
   "source": [
    "# set logging, in this case the root logger\n",
    "ut.setup_logger(logger_name=\"\",file_name=log_dir/\"log.txt\")\n",
    "ut.setup_logger(logger_name=\"summary\",file_name=log_dir/\"summary.txt\")\n",
    "summary_logger = logging.getLogger(\"summary\")\n",
    "tb = SummaryWriter(log_dir/\"tb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% setup experiment\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\sklearn\\model_selection\\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "fixed_hyperparams = {'bs': 32,'loss': nn.MSELoss(),'epochs': 100}\n",
    "preprocessing = Preprocess_PLS(n_components=n_components)\n",
    "eval = CrossValEvaluation(preprocessing=preprocessing,tensorboard=None,time=True,random_state=random_state)\n",
    "scores={} #->model->knn:{fold_0:number,...,fold_n:number,mean:number,median:number\n",
    "preds={} #model-> foldsxknn_models\n",
    "deep_scores_dict={}\n",
    "deep_preds_dict={}\n",
    "actual_y = None\n",
    "\n",
    "device = \"cpu\" #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "load_fun_cv = lambda name,model, fold : model.load_state(model_dir/'models'/name/f\"_fold_{fold}\")\n",
    "load_fun_build = lambda name,model : model.load_state(model_dir/'models'/name/f\"_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the best model on the test split - best model is random_57  - MSE:0.4028,R2:0.9337'\n",
    "model = torch.load(model_dir/\"models\"/\"random_5\"/f\"_model\")\n",
    "tt_splitter= train_test_split\n",
    "train_split1,test_split = tt_splitter([i for i in range(0,len(data))],train_size=5/6,random_state=random_state,shuffle=False) #take our training and our test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01695968  0.02120247  0.07917055 ...  0.11714174  0.08865453\n",
      "   0.05088219]\n",
      " [ 0.01810302  0.01935943  0.07575022 ...  0.09206153  0.05771873\n",
      "   0.08069276]\n",
      " [ 0.01921557  0.01744857  0.07227662 ...  0.0805834   0.04197767\n",
      "   0.09448845]\n",
      " ...\n",
      " [ 0.02311599  0.01646588  0.01949793 ... -0.01682693 -0.00362798\n",
      "   0.01046275]\n",
      " [ 0.02308568  0.01651355  0.01956011 ... -0.01889519 -0.00592329\n",
      "   0.01299535]\n",
      " [ 0.0230551   0.01656241  0.01962823 ... -0.02135868 -0.00859426\n",
      "   0.01603646]]\n",
      "[[ 0.02477695 -0.02356564 -0.03739578 -0.03558914  0.02909668 -0.03307994\n",
      "   0.02678511 -0.03401774 -0.01054564 -0.02391075  0.03878953 -0.02226663\n",
      "   0.02977767 -0.03633407  0.02407958 -0.06044229  0.03856325  0.04666698\n",
      "   0.03257466 -0.03104083 -0.06529193  0.05352792]]\n",
      "-2.163925624453733 2541.6070273990995\n",
      "0.8663137513298369 107.39124411161661\n"
     ]
    }
   ],
   "source": [
    "#run  the best model on the test split\n",
    "\n",
    "train_split, val_data, test_split = dataset.split(train_split1, None, test_split, preprocessing = preprocessing)\n",
    "test_X,test_y = zip(*[ (X,y) for X,y in test_split])\n",
    "test_y = np.asarray(test_y)\n",
    "model = torch.load(model_dir/\"models\"/\"random_5\"/f\"_model\")\n",
    "for code in ['init_state','final']:\n",
    "    model.load_state_dict(torch.load(model_dir/\"models\"/\"random_5\"/f\"_{code}\"))\n",
    "    preds = model.forward(torch.tensor(test_X).float()).detach().numpy()\n",
    "\n",
    "    val_score = r2_score(test_y,preds)\n",
    "    val_mse =  mean_squared_error(test_y,preds)\n",
    "\n",
    "    print(val_score,val_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\sklearn\\model_selection\\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01694714  0.02137554  0.08178384 ...  0.05415289  0.00442955\n",
      "   0.06188886]\n",
      " [ 0.01809617  0.01955103  0.07852984 ...  0.03513465  0.02700274\n",
      "   0.08497646]\n",
      " [ 0.01919185  0.01766278  0.07534896 ...  0.03013528  0.03091026\n",
      "   0.08608117]\n",
      " ...\n",
      " [ 0.02322585  0.01677321  0.02119172 ... -0.00607184  0.0093803\n",
      "  -0.00449615]\n",
      " [ 0.02319835  0.01681831  0.02124571 ... -0.00836434  0.01181686\n",
      "  -0.00196554]\n",
      " [ 0.02317117  0.01686441  0.02131657 ... -0.01139777  0.0150539\n",
      "   0.00126707]]\n",
      "[[ 0.02498952 -0.02320319 -0.04105547 -0.03646868  0.028643   -0.03229133\n",
      "   0.02282377 -0.03420892 -0.00947398 -0.02095455  0.03703694 -0.04147501\n",
      "  -0.04219318  0.01547907  0.01398815  0.08056134 -0.02697616  0.03705634\n",
      "  -0.03712943 -0.02817297  0.06312697  0.03979943]]\n",
      "0.85559276381646 116.61514177257519\n",
      "[[ 0.01728657  0.02055388 -0.0749399  ...  0.04875266 -0.03983927\n",
      "  -0.06500631]\n",
      " [ 0.01836846  0.01895595 -0.07216779 ...  0.03292234 -0.03312869\n",
      "  -0.07433996]\n",
      " [ 0.01940502  0.01727566 -0.06954137 ...  0.02342462 -0.03251151\n",
      "  -0.0732303 ]\n",
      " ...\n",
      " [ 0.02315672  0.01676073 -0.0204607  ... -0.02344349  0.00252031\n",
      "   0.00217963]\n",
      " [ 0.0231238   0.01681188 -0.02054244 ... -0.02534943  0.0035518\n",
      "   0.00079328]\n",
      " [ 0.02309123  0.01686336 -0.02062801 ... -0.02717439  0.00445752\n",
      "  -0.00056688]]\n",
      "[[ 0.02471347 -0.02373631  0.03797512 -0.03492257  0.03110571 -0.03430055\n",
      "   0.0300516  -0.039494    0.02986     0.00997918 -0.03460145  0.02546201\n",
      "   0.03289167  0.03191124 -0.05818768 -0.04318588 -0.02901368  0.03725223\n",
      "  -0.06648768 -0.04816169  0.04739061 -0.05806244]]\n",
      "0.6938288070045548 259.4104047590708\n",
      "[[ 0.01677113  0.02221472  0.08103181 ... -0.07110815  0.13427197\n",
      "   0.04300174]\n",
      " [ 0.01791319  0.02035159  0.07752598 ... -0.04514168  0.10915624\n",
      "   0.06231667]\n",
      " [ 0.01903255  0.01841296  0.07397035 ... -0.02623977  0.0923818\n",
      "   0.0707743 ]\n",
      " ...\n",
      " [ 0.02328935  0.01599593  0.02069867 ...  0.0042658  -0.00137943\n",
      "   0.01207447]\n",
      " [ 0.02325649  0.01605     0.02077724 ...  0.00584911 -0.00330202\n",
      "   0.01451131]\n",
      " [ 0.02322268  0.01610565  0.02085704 ...  0.00741699 -0.00526439\n",
      "   0.01724495]]\n",
      "[[ 0.02458042 -0.02331744 -0.03779651 -0.03290112  0.0303641  -0.03573404\n",
      "   0.0328757  -0.03985031 -0.01290031 -0.02654475  0.04433436  0.02217174\n",
      "   0.03976876 -0.02738778  0.04257772  0.03577498 -0.03872974  0.05458664\n",
      "  -0.04291622  0.04219328 -0.07909594  0.08286036]]\n",
      "0.8285031603182588 96.10623459678807\n",
      "[[ 0.01698937  0.0208443   0.07657943 ...  0.06417226  0.15139306\n",
      "  -0.0382185 ]\n",
      " [ 0.01817123  0.01882225  0.07247274 ...  0.08908254  0.11724907\n",
      "  -0.00429218]\n",
      " [ 0.01933227  0.0167655   0.06821395 ...  0.105126    0.09743344\n",
      "   0.01636847]\n",
      " ...\n",
      " [ 0.02294441  0.01636872  0.01725783 ...  0.00169189 -0.01413579\n",
      "   0.00501648]\n",
      " [ 0.02291114  0.01642289  0.0173224  ...  0.0038116  -0.01740698\n",
      "   0.00859444]\n",
      " [ 0.02287649  0.01648033  0.01739062 ...  0.0062441  -0.02090117\n",
      "   0.0126872 ]]\n",
      "[[ 0.02483356 -0.02350645 -0.03453638 -0.03560837  0.0278118  -0.0342651\n",
      "  -0.03379011 -0.03369947  0.01461281 -0.01627285  0.03365196 -0.02423358\n",
      "   0.03025476 -0.03565971  0.04848835  0.03760671  0.03358826 -0.04422182\n",
      "  -0.04425389  0.03723447 -0.09854492  0.09297692]]\n",
      "0.8795841393312376 93.69857029768767\n",
      "[[ 0.01695942  0.02058161  0.07777509 ... -0.02326955  0.07509951\n",
      "   0.07181791]\n",
      " [ 0.01812808  0.01866106  0.07437098 ...  0.01491048  0.0328818\n",
      "   0.11548677]\n",
      " [ 0.01927965  0.01667091  0.07070818 ...  0.02886591  0.01827814\n",
      "   0.13319213]\n",
      " ...\n",
      " [ 0.02296707  0.01643393  0.01745926 ...  0.01511345 -0.00749182\n",
      "   0.0065149 ]\n",
      " [ 0.02294142  0.01647089  0.01750676 ...  0.01774078 -0.01045433\n",
      "   0.00990699]\n",
      " [ 0.02291622  0.01650764  0.01755842 ...  0.02148504 -0.01433876\n",
      "   0.01428774]]\n",
      "[[ 0.02480699 -0.02415266 -0.03624284 -0.0369374   0.02842283 -0.02923536\n",
      "  -0.01926835  0.0302173   0.01399017  0.0258552   0.0303265   0.0220174\n",
      "  -0.02454888 -0.04818601 -0.03403735  0.03647715  0.0408109   0.02909241\n",
      "   0.04964479  0.04252983 -0.07191266  0.05113637]]\n",
      "0.8443800109003252 104.69125997547383\n"
     ]
    }
   ],
   "source": [
    "cv_splitter = K2Fold(n_splits=5,random_state=random_state)\n",
    "for fold, (train_ind, val_ind, test_ind) in enumerate(cv_splitter.split(train_split)):\n",
    "        train_data, val_data, test_data = dataset.split(train_ind, val_ind, test_ind, preprocessing = preprocessing)\n",
    "        test_X,test_y = zip(*[ (X,y) for X,y in test_data])\n",
    "        test_y = np.asarray(test_y)\n",
    "        \n",
    "        model.load_state_dict(torch.load(model_dir/\"models\"/\"random_5\"/f\"_fold_{fold}\"))\n",
    "        preds = model.forward(torch.tensor(test_X).float()).detach().numpy()\n",
    "\n",
    "        val_score = r2_score(test_y,preds)\n",
    "        val_mse =  mean_squared_error(test_y,preds)\n",
    "\n",
    "        print(val_score,val_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% deep experiment\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deep_scheme = DeepScheme(None, fixed_hyperparams=fixed_hyperparams,loss_eval=loss_target,device=device,tensorboard=tb,adaptive_lr=True,update=False)\n",
    "deep_scores, deep_preds, _ , _, _ = eval.evaluate(deep_models,dataset,deep_scheme,logger_name=\"log\",load_fun=load_fun_cv)\n",
    "deep_scores_final, deep_preds_final, _ ,_, _ = eval.build(deep_models,dataset,deep_scheme,logger_name=\"test_log\",load_fun=load_fun_build)\n",
    "\n",
    "all_scores = []\n",
    "for k,v in ut.flip_dicts(deep_scores).items():\n",
    "    dict1 = {'model_num':k,\"predictor\":\"deep\"}\n",
    "    all_scores.append({**dict1,**v})\n",
    "\n",
    "all_scores_final = []\n",
    "for k,v in ut.flip_dicts(deep_scores_final).items():\n",
    "    dict1 = {'model_num':k,\"predictor\":\"deep\"}\n",
    "    all_scores_final.append({**dict1,**v})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% lwr part\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for deep_name,deep_model in deep_models.items():\n",
    "    logging.getLogger().info(f\"Running model {deep_name}\")\n",
    "    temp_dict = {deep_name:deep_model}\n",
    "\n",
    "    lwr_scheme = DeepLWRScheme_1_to_n(lwr_models = setup_pls_models_slim(nrow),n_neighbours=500,loss_fun_sk = mean_squared_error)\n",
    "    lwr_scores, lwr_preds, _ , _, _= eval.evaluate(temp_dict,dataset,lwr_scheme,logger_name=\"log\",load_fun=load_fun_cv)\n",
    "    lwr_scores_final, lwr_preds_final, _ , _, _= eval.build(temp_dict,dataset,lwr_scheme,logger_name=\"test_log\",load_fun=load_fun_build)\n",
    "\n",
    "    #scores\n",
    "    for k,v in ut.flip_dicts(lwr_scores).items():\n",
    "        dict1 = {'model_num':deep_name,\"predictor\":k}\n",
    "        all_scores.append({**dict1,**v})\n",
    "\n",
    "    for k,v in ut.flip_dicts(lwr_scores_final).items():\n",
    "        dict1 = {'model_num':deep_name,\"predictor\":k}\n",
    "        all_scores_final.append({**dict1,**v})\n",
    "\n",
    "    lwr_preds['deep'] = deep_preds[deep_name]\n",
    "    lwr_preds_final['deep'] = deep_preds_final[deep_name]\n",
    "\n",
    "    lwr_preds.to_csv(log_dir/deep_name/ f\"predictions.csv\",index=False)\n",
    "    lwr_preds_final.to_csv(log_dir/deep_name/ f\"predictions_test.csv\",index=False)\n",
    "\n",
    "    #preds\n",
    "    # todo save predictions - appending solns\n",
    "    plot_preds_and_res(lwr_preds,name_lambda=lambda x:f\"{deep_name} with {x} predictor\",save_lambda= lambda x:f\"deep_lwr{x}\",save_loc=log_dir/deep_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% save scores\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(all_scores)\n",
    "scores_df.to_csv(log_dir/f\"scores.csv\",index=False)\n",
    "scores_df_final = pd.DataFrame(all_scores_final)\n",
    "scores_df_final.to_csv(log_dir/f\"test_scores.csv\",index=False)\n",
    "\n",
    "scores_df_sorted = pd.DataFrame(scores_df).sort_values(by='MSE')\n",
    "\n",
    "best_5 = []\n",
    "summary_logger.info(f\"Rank - \" +\" - \".join(list(scores_df_sorted.columns)))\n",
    "for i,(index,row) in enumerate(scores_df_sorted.iterrows()):\n",
    "    if i < 5:\n",
    "        best_5.append((row[\"model_num\"],row[\"predictor\"],row[\"MSE\"],row[\"R2\"]))\n",
    "    s = f\"{i} - \" + \" - \".join([f\"{i}\" for i in row.tolist()])\n",
    "    summary_logger.info(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% save scores\n"
    }
   },
   "outputs": [],
   "source": [
    "summary_logger.info(\"-----------------------\\n Best 5 on Test Sest \\n ---------------------\")\n",
    "summary_logger.info(f\"Rank -  Deep Model - Predictor - Val Set - Test Set\")\n",
    "for i, (j,k,v,x) in enumerate(best_5):\n",
    "\n",
    "    row = scores_df_final.loc[(scores_df_final['model_num']==j) & (scores_df_final['predictor'] == k)].iloc[0]\n",
    "    #print(row)\n",
    "    s = f\"{i} - {j} - {k} - {v} - {x} - {row['MSE']} - {row['R2']}\"\n",
    "    summary_logger.info(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#take 1 is a scatter plot - lets, for each dataset\n",
    "#graph our deep models by rank - plot - then overlay our knn moels\n",
    "#plot points\n",
    "\n",
    "deep_set = scores_df[scores_df[\"predictor\"]==\"deep\"].sort_values(\"R2\")\n",
    "deep_set[\"order\"] = [i for i in range(0,100)]\n",
    "deep_ordering = {row[\"model_num\"]:row[\"order\"] for index, row in deep_set.iterrows()}\n",
    "\n",
    "def order_models(x):\n",
    "    x = [deep_ordering[i] for i in x]\n",
    "    return x\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "set_deep = False\n",
    "knn_models = scores_df[\"predictor\"].unique()\n",
    "for knn_model in knn_models:\n",
    "    subset = scores_df[scores_df[\"predictor\"]==knn_model]\n",
    "    s=3\n",
    "    if knn_model == \"deep\":\n",
    "        s=10\n",
    "    ax.scatter(x=order_models(subset[\"model_num\"].tolist()), y=subset[\"R2\"], s=s, label=knn_model)\n",
    "\n",
    "#ax.set_ylim(0,scores_db[\"deep_mean\"].max())\n",
    "ax.set_ylim(0,1)\n",
    "# plot residuals\n",
    "ax.legend(loc='upper right',bbox_to_anchor=(1.4, 1))\n",
    "ax.set_ylabel(\"R2 Score\")\n",
    "ax.set_xlabel(\"Deep Model Rank\")\n",
    "#ax.set_yscale(\"symlog\")\n",
    "ax.set_title(\"Summary of LWR improvements over Deep Models\")\n",
    "plt.savefig(log_dir/f\"summary_plot.png\", bbox_inches='tight')\n",
    "logging.getLogger().info(\"Wrote Summary Graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (lazydeep)",
   "language": "python",
   "name": "pycharm-12fcba0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
