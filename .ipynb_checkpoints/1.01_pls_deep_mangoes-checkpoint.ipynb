{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU detected is GeForce GTX 970\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "# set seed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils as ut\n",
    "import experiment as ex\n",
    "from evaluation import *\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import random\n",
    "#define fixed_hyperparams and create a config gen\n",
    "from configurations import RandomConfigGen, Configuration\n",
    "from torch import nn\n",
    "from deep_net import RandomNet\n",
    "from experiment import run_experiment\n",
    "import regex as re\n",
    "from pathlib import *\n",
    "from sk_models import PLSRegression\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed + 1)\n",
    "np.random.seed(seed + 2)\n",
    "random_state = np.random.RandomState(seed)\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"GPU detected is {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory is D:\\workspace\\lazydeep\\experiments\\1.01\\mango_684_990\n"
     ]
    }
   ],
   "source": [
    "#setup input and outpu t formats, load data\n",
    "\n",
    "#we need to set parametesr\n",
    "file_name = \"mango_684_990.csv\"#fitlered=513-1050 #\"mango_684_990.csv\" #\"mango_729_975.csv\" \n",
    "id_cols =['Set','Season','Region','Date','Type','Cultivar','Pop','Temp',\"FruitID\"]#\n",
    "output_cols = ['DM']\n",
    "\n",
    "data_path = Path('D:/workspace/lazydeep/data/soil_data/')\n",
    "log_path = Path(\"D:/workspace/lazydeep/experiments/1.01\") #1.01/\")\n",
    "if not log_path.exists():\n",
    "    log_path.mkdir()\n",
    "\n",
    "data_file = data_path / file_name\n",
    "log_dir = log_path / re.sub(r'\\.(?=csv$)[^.]+$', '',file_name)\n",
    "if not log_dir.exists():\n",
    "    log_dir.mkdir()\n",
    "print(f\"Output directory is {log_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape is (11691, 113)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\sklearn\\model_selection\\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(data_file)\n",
    "nrow, ncol = data.shape\n",
    "n_features = ncol - 1-len(id_cols)\n",
    "n_comps = [i for i in range(1,min(101,n_features))]\n",
    "data = ut.sample_data(data,random_state)\n",
    "dataset = ut.TabularDataset(data,id_cols = id_cols, cat_cols=None, output_cols=output_cols, ignore_cols= None)\n",
    "eval = MangoesSplitter(preprocessing=None,tensorboard=None,time=True,random_state=random_state)\n",
    "print(f\"Dataset shape is {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% setup logging and tensorboard outputs\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 5879 - Val 1929 - Test 1905-----------------------------------'\n",
      "Finished training SKLearn with a train loss of 1:5.2855,2:5.2288,3:5.0751,4:4.7223,5:3.7266,6:2.7844,7:2.5206,8:1.2334,9:0.9928,10:0.8635,11:0.8248,12:0.7986,13:0.7882,14:0.773,15:0.7589,16:0.7496,17:0.7459,18:0.7411,19:0.7336,20:0.7298,21:0.7271,22:0.7241,23:0.7216,24:0.7198,25:0.7168,26:0.7143,27:0.7122,28:0.7104,29:0.7081,30:0.7073,31:0.7059,32:0.7048,33:0.7043,34:0.7037,35:0.7028,36:0.7022,37:0.7017,38:0.7013,39:0.7011,40:0.7009,41:0.7008,42:0.7006,43:0.7005,44:0.7004,45:0.7004,46:0.7003,47:0.7002,48:0.7002,49:0.7002,50:0.7001,51:0.7001,52:0.7001,53:0.7,54:0.7,55:0.6999,56:0.6999,57:0.6998,58:0.6997,59:0.6997,60:0.6996,61:0.6996,62:0.6995,63:0.6995,64:0.6995,65:0.6995,66:0.6995,67:0.6995,68:0.6994,69:0.6994,70:0.6994,71:0.6994,72:0.6994,73:0.6994,74:0.6994,75:0.6994,76:0.6994,77:0.6993,78:0.6993,79:0.6993,80:0.6993,81:0.6993,82:0.6993,83:0.6993,84:0.6993,85:0.6993,86:0.6993,87:0.6993,88:0.6993,89:0.6993,90:0.6993,91:0.6993,92:0.6993,93:0.6993,94:0.6993,95:0.6993,96:0.6993,97:0.6993,98:0.6993,99:0.6993,100:0.6992'\n",
      "Tested (test) on 1905 instances with mean losses of: 1:5.3976,2:5.2991,3:5.0659,4:4.8002,5:3.7157,6:2.8257,7:2.5519,8:1.2381,9:1.0004,10:0.8613,11:0.8273,12:0.8097,13:0.7959,14:0.7859,15:0.766,16:0.7621,17:0.7598,18:0.7497,19:0.733,20:0.7278,21:0.7264,22:0.7232,23:0.722,24:0.7214,25:0.7176,26:0.714,27:0.7143,28:0.715,29:0.7151,30:0.7134,31:0.7144,32:0.7155,33:0.7157,34:0.716,35:0.7147,36:0.713,37:0.712,38:0.7119,39:0.7108,40:0.7105,41:0.7112,42:0.7112,43:0.7102,44:0.7095,45:0.7104,46:0.7104,47:0.7103,48:0.7105,49:0.7103,50:0.71,51:0.7099,52:0.7098,53:0.7097,54:0.7099,55:0.7096,56:0.7101,57:0.7096,58:0.7098,59:0.71,60:0.7101,61:0.7102,62:0.7096,63:0.7096,64:0.7095,65:0.7095,66:0.7093,67:0.7094,68:0.7094,69:0.7097,70:0.7098,71:0.7098,72:0.7098,73:0.7098,74:0.7099,75:0.7099,76:0.7096,77:0.7095,78:0.7094,79:0.7094,80:0.7094,81:0.7094,82:0.7094,83:0.7094,84:0.7094,85:0.7095,86:0.7094,87:0.7093,88:0.7093,89:0.7092,90:0.7092,91:0.7092,92:0.7092,93:0.7092,94:0.7092,95:0.7092,96:0.7092,97:0.7092,98:0.7092,99:0.7092,100:0.7091'\n",
      "-----------------------------------Fold 1 - Train 5855 - Val 1903 - Test 1955-----------------------------------'\n",
      "Finished training SKLearn with a train loss of 1:5.5725,2:5.2037,3:5.0666,4:4.6997,5:3.7583,6:2.8311,7:2.5371,8:1.2569,9:1.0087,10:0.8725,11:0.8316,12:0.8041,13:0.7916,14:0.7763,15:0.7635,16:0.7535,17:0.7501,18:0.7447,19:0.7375,20:0.7333,21:0.7303,22:0.7275,23:0.724,24:0.7223,25:0.7199,26:0.7169,27:0.7151,28:0.7122,29:0.7102,30:0.7094,31:0.7082,32:0.7071,33:0.7067,34:0.706,35:0.7054,36:0.7048,37:0.7043,38:0.704,39:0.7038,40:0.7037,41:0.7036,42:0.7035,43:0.7034,44:0.7033,45:0.7032,46:0.7031,47:0.7031,48:0.703,49:0.703,50:0.703,51:0.7029,52:0.7029,53:0.7029,54:0.7028,55:0.7028,56:0.7027,57:0.7027,58:0.7026,59:0.7026,60:0.7026,61:0.7025,62:0.7025,63:0.7024,64:0.7024,65:0.7024,66:0.7024,67:0.7024,68:0.7024,69:0.7024,70:0.7024,71:0.7024,72:0.7024,73:0.7023,74:0.7023,75:0.7023,76:0.7023,77:0.7023,78:0.7023,79:0.7023,80:0.7023,81:0.7023,82:0.7023,83:0.7023,84:0.7023,85:0.7023,86:0.7023,87:0.7023,88:0.7023,89:0.7023,90:0.7023,91:0.7023,92:0.7023,93:0.7023,94:0.7023,95:0.7023,96:0.7023,97:0.7023,98:0.7023,99:0.7023,100:0.7023'\n",
      "Tested (test) on 1955 instances with mean losses of: 1:5.4148,2:5.0001,3:4.8727,4:4.4427,5:3.6343,6:2.72,7:2.4848,8:1.2479,9:0.9765,10:0.8521,11:0.8384,12:0.8094,13:0.797,14:0.7818,15:0.781,16:0.7616,17:0.7542,18:0.7486,19:0.7392,20:0.7435,21:0.7451,22:0.7414,23:0.7345,24:0.7348,25:0.7351,26:0.7294,27:0.7298,28:0.7297,29:0.7321,30:0.7296,31:0.7271,32:0.726,33:0.7249,34:0.7251,35:0.7258,36:0.7258,37:0.7237,38:0.7239,39:0.7241,40:0.7235,41:0.7236,42:0.7239,43:0.7237,44:0.7233,45:0.723,46:0.7227,47:0.7223,48:0.7221,49:0.7224,50:0.7223,51:0.7225,52:0.7227,53:0.7224,54:0.7227,55:0.7231,56:0.7238,57:0.7233,58:0.7233,59:0.723,60:0.7234,61:0.7231,62:0.7229,63:0.7229,64:0.7229,65:0.723,66:0.723,67:0.7229,68:0.7229,69:0.723,70:0.7229,71:0.723,72:0.7227,73:0.7225,74:0.7228,75:0.7227,76:0.7227,77:0.7228,78:0.7227,79:0.7227,80:0.7227,81:0.7227,82:0.7227,83:0.7227,84:0.7227,85:0.7227,86:0.7227,87:0.7227,88:0.7226,89:0.7226,90:0.7227,91:0.7228,92:0.7228,93:0.7228,94:0.7228,95:0.7228,96:0.7229,97:0.7229,98:0.7229,99:0.7229,100:0.7228'\n",
      "-----------------------------------Fold 2 - Train 5821 - Val 1955 - Test 1937-----------------------------------'\n",
      "Finished training SKLearn with a train loss of 1:5.337,2:5.2096,3:5.0896,4:4.6821,5:3.7839,6:2.8575,7:2.5489,8:1.2522,9:1.0117,10:0.8801,11:0.8418,12:0.8165,13:0.8055,14:0.7882,15:0.7741,16:0.7635,17:0.7602,18:0.7542,19:0.7469,20:0.7416,21:0.7396,22:0.7376,23:0.7336,24:0.7324,25:0.7305,26:0.7279,27:0.7262,28:0.7222,29:0.7211,30:0.7199,31:0.7187,32:0.7176,33:0.7169,34:0.7162,35:0.7155,36:0.7151,37:0.7145,38:0.7142,39:0.7139,40:0.7138,41:0.7136,42:0.7135,43:0.7134,44:0.7133,45:0.7132,46:0.7132,47:0.7131,48:0.7131,49:0.713,50:0.713,51:0.713,52:0.7129,53:0.7129,54:0.7129,55:0.7128,56:0.7128,57:0.7127,58:0.7127,59:0.7126,60:0.7126,61:0.7124,62:0.7123,63:0.7122,64:0.7122,65:0.7122,66:0.7122,67:0.7121,68:0.7121,69:0.7121,70:0.7121,71:0.7121,72:0.7121,73:0.7121,74:0.7121,75:0.712,76:0.712,77:0.712,78:0.712,79:0.712,80:0.712,81:0.712,82:0.712,83:0.712,84:0.712,85:0.712,86:0.712,87:0.712,88:0.712,89:0.712,90:0.712,91:0.712,92:0.712,93:0.712,94:0.712,95:0.712,96:0.712,97:0.712,98:0.712,99:0.712,100:0.712'\n",
      "Tested (test) on 1937 instances with mean losses of: 1:5.3681,2:5.2533,3:5.1413,4:4.6538,5:3.875,6:2.7863,7:2.5406,8:1.2282,9:0.979,10:0.8488,11:0.8052,12:0.7633,13:0.7524,14:0.7326,15:0.7218,16:0.708,17:0.704,18:0.7082,19:0.6991,20:0.6987,21:0.6966,22:0.6938,23:0.6907,24:0.6902,25:0.6902,26:0.6867,27:0.6837,28:0.6818,29:0.6799,30:0.679,31:0.6772,32:0.676,33:0.676,34:0.679,35:0.679,36:0.6789,37:0.6789,38:0.6791,39:0.6793,40:0.6786,41:0.6782,42:0.678,43:0.6782,44:0.6786,45:0.6788,46:0.6784,47:0.6787,48:0.6791,49:0.679,50:0.6789,51:0.6787,52:0.6781,53:0.6784,54:0.6781,55:0.6779,56:0.6779,57:0.6779,58:0.6781,59:0.6779,60:0.6777,61:0.6782,62:0.678,63:0.6782,64:0.6785,65:0.6784,66:0.6782,67:0.6784,68:0.6784,69:0.6783,70:0.6783,71:0.6782,72:0.6781,73:0.6784,74:0.6785,75:0.6787,76:0.679,77:0.679,78:0.6793,79:0.6795,80:0.6795,81:0.6795,82:0.6795,83:0.6795,84:0.6795,85:0.6797,86:0.6797,87:0.6797,88:0.6798,89:0.6798,90:0.6798,91:0.6799,92:0.6799,93:0.6799,94:0.6798,95:0.6799,96:0.6799,97:0.6798,98:0.6799,99:0.6799,100:0.6799'\n",
      "-----------------------------------Fold 3 - Train 5787 - Val 1937 - Test 1989-----------------------------------'\n",
      "Finished training SKLearn with a train loss of 1:5.1331,2:5.052,3:4.9162,4:4.5676,5:3.6928,6:2.8173,7:2.5085,8:1.2256,9:0.9933,10:0.8661,11:0.8266,12:0.8008,13:0.7913,14:0.7736,15:0.7565,16:0.7465,17:0.7414,18:0.7352,19:0.7265,20:0.7202,21:0.7174,22:0.7161,23:0.7136,24:0.7114,25:0.7092,26:0.707,27:0.7049,28:0.7021,29:0.7004,30:0.6993,31:0.6981,32:0.697,33:0.6966,34:0.6958,35:0.6954,36:0.6949,37:0.6944,38:0.6941,39:0.6939,40:0.6937,41:0.6935,42:0.6934,43:0.6933,44:0.6932,45:0.6931,46:0.6931,47:0.693,48:0.693,49:0.6929,50:0.6929,51:0.6929,52:0.6928,53:0.6928,54:0.6927,55:0.6927,56:0.6926,57:0.6925,58:0.6925,59:0.6924,60:0.6923,61:0.692,62:0.6919,63:0.6918,64:0.6917,65:0.6917,66:0.6917,67:0.6917,68:0.6916,69:0.6916,70:0.6916,71:0.6916,72:0.6916,73:0.6916,74:0.6915,75:0.6915,76:0.6915,77:0.6915,78:0.6915,79:0.6915,80:0.6915,81:0.6915,82:0.6915,83:0.6915,84:0.6914,85:0.6914,86:0.6914,87:0.6914,88:0.6914,89:0.6914,90:0.6914,91:0.6914,92:0.6914,93:0.6914,94:0.6914,95:0.6914,96:0.6914,97:0.6914,98:0.6914,99:0.6913,100:0.6913'\n",
      "Tested (test) on 1989 instances with mean losses of: 1:5.5116,2:5.4271,3:5.3455,4:4.978,5:3.9609,6:2.9384,7:2.5749,8:1.2689,9:1.0403,10:0.914,11:0.8704,12:0.86,13:0.8536,14:0.8389,15:0.8253,16:0.8224,17:0.8214,18:0.8118,19:0.8079,20:0.7969,21:0.7963,22:0.7992,23:0.8021,24:0.8012,25:0.7967,26:0.797,27:0.7955,28:0.7912,29:0.7905,30:0.7896,31:0.7886,32:0.7857,33:0.786,34:0.7832,35:0.78,36:0.7798,37:0.7819,38:0.7812,39:0.7813,40:0.7802,41:0.7799,42:0.7793,43:0.7794,44:0.7792,45:0.7788,46:0.7788,47:0.7785,48:0.7788,49:0.7791,50:0.7792,51:0.7792,52:0.7791,53:0.7789,54:0.7794,55:0.7795,56:0.7797,57:0.7792,58:0.7789,59:0.7786,60:0.7788,61:0.7795,62:0.7789,63:0.7787,64:0.779,65:0.7789,66:0.7787,67:0.7788,68:0.7788,69:0.779,70:0.779,71:0.7791,72:0.779,73:0.7791,74:0.7793,75:0.7793,76:0.7793,77:0.7794,78:0.7794,79:0.7794,80:0.7794,81:0.7795,82:0.7796,83:0.7796,84:0.7795,85:0.7794,86:0.7795,87:0.7797,88:0.7795,89:0.7795,90:0.7795,91:0.7796,92:0.7796,93:0.7797,94:0.7797,95:0.7797,96:0.7799,97:0.78,98:0.78,99:0.7804,100:0.78'\n",
      "-----------------------------------Fold 4 - Train 5797 - Val 1989 - Test 1927-----------------------------------'\n",
      "Finished training SKLearn with a train loss of 1:5.1834,2:5.1613,3:4.9975,4:4.675,5:3.6928,6:2.7775,7:2.5151,8:1.2109,9:0.9767,10:0.8513,11:0.8105,12:0.7831,13:0.7727,14:0.7573,15:0.7393,16:0.7298,17:0.7238,18:0.7185,19:0.7112,20:0.7065,21:0.7029,22:0.7006,23:0.6985,24:0.6966,25:0.6941,26:0.6921,27:0.6903,28:0.6882,29:0.6861,30:0.6852,31:0.6842,32:0.6832,33:0.6828,34:0.6823,35:0.6817,36:0.6812,37:0.6807,38:0.6804,39:0.6801,40:0.68,41:0.6799,42:0.6797,43:0.6796,44:0.6795,45:0.6794,46:0.6794,47:0.6793,48:0.6793,49:0.6793,50:0.6792,51:0.6792,52:0.6791,53:0.6791,54:0.679,55:0.679,56:0.6789,57:0.6789,58:0.6788,59:0.6788,60:0.6787,61:0.6785,62:0.6785,63:0.6785,64:0.6784,65:0.6784,66:0.6784,67:0.6784,68:0.6784,69:0.6784,70:0.6784,71:0.6783,72:0.6783,73:0.6782,74:0.6782,75:0.6782,76:0.6782,77:0.6782,78:0.6782,79:0.6782,80:0.6782,81:0.6782,82:0.6782,83:0.6781,84:0.6781,85:0.6781,86:0.6781,87:0.6781,88:0.6781,89:0.6781,90:0.6781,91:0.6781,92:0.6781,93:0.6781,94:0.678,95:0.678,96:0.678,97:0.678,98:0.6779,99:0.6778,100:0.6777'\n",
      "Tested (test) on 1927 instances with mean losses of: 1:4.9632,2:4.9049,3:4.7683,4:4.5296,5:3.5569,6:2.8619,7:2.5215,8:1.2209,9:1.0154,10:0.8926,11:0.8415,12:0.8251,13:0.8083,14:0.8043,15:0.7863,16:0.7723,17:0.7681,18:0.7651,19:0.758,20:0.7544,21:0.7442,22:0.7415,23:0.7424,24:0.7403,25:0.735,26:0.7318,27:0.7306,28:0.7296,29:0.7241,30:0.7223,31:0.7233,32:0.7232,33:0.723,34:0.7222,35:0.7213,36:0.7202,37:0.7195,38:0.7202,39:0.72,40:0.7208,41:0.7203,42:0.7208,43:0.7209,44:0.721,45:0.7209,46:0.7208,47:0.7211,48:0.7209,49:0.7209,50:0.7205,51:0.7206,52:0.7206,53:0.7207,54:0.7209,55:0.7205,56:0.7202,57:0.72,58:0.7201,59:0.7199,60:0.7202,61:0.7204,62:0.7206,63:0.7211,64:0.7209,65:0.721,66:0.7207,67:0.7207,68:0.7207,69:0.7206,70:0.7204,71:0.7205,72:0.7203,73:0.7206,74:0.7208,75:0.7208,76:0.7207,77:0.7207,78:0.7207,79:0.7207,80:0.7208,81:0.7209,82:0.7207,83:0.7207,84:0.7209,85:0.7208,86:0.7207,87:0.7208,88:0.7209,89:0.7209,90:0.7209,91:0.7208,92:0.7209,93:0.7209,94:0.7208,95:0.7208,96:0.7209,97:0.7208,98:0.7203,99:0.7205,100:0.7212'\n",
      "Train times: {'fold_0': 28, 'fold_1': 28, 'fold_2': 28, 'fold_3': 27, 'fold_4': 27, 'mean': 27.6}'\n",
      "Test times: {'fold_0': 0, 'fold_1': 0, 'fold_2': 0, 'fold_3': 0, 'fold_4': 0, 'mean': 0.0}'\n",
      "Scores: {'fold_0': {1: 5.397604977935402, 2: 5.299072153140445, 3: 5.065854861944807, 4: 4.8002302669665635, 5: 3.715696823094543, 6: 2.8256721536359195, 7: 2.5519066661067207, 8: 1.238075338572452, 9: 1.0004213875299952, 10: 0.861332833481699, 11: 0.8273082334046411, 12: 0.8096803600403776, 13: 0.7959040347250804, 14: 0.7858549326745825, 15: 0.7660080484520673, 16: 0.7621030308136848, 17: 0.759752968867632, 18: 0.7497423093108908, 19: 0.7329961018489892, 20: 0.7277582428821165, 21: 0.7263898636935519, 22: 0.7231989631092334, 23: 0.7220427712185392, 24: 0.7214327776684041, 25: 0.717577394855007, 26: 0.7139848146901216, 27: 0.7143222624236197, 28: 0.7150022282075326, 29: 0.7151272397616933, 30: 0.7134088783136844, 31: 0.7144010200388925, 32: 0.715496632919228, 33: 0.7156820455516821, 34: 0.7159633452907412, 35: 0.7146624479430996, 36: 0.7129683100817252, 37: 0.7119653175424516, 38: 0.7119212181841938, 39: 0.7108497388725766, 40: 0.7105113619176454, 41: 0.7111863006810212, 42: 0.7112423248081001, 43: 0.7101714497773479, 44: 0.7094879059472369, 45: 0.7104352204798654, 46: 0.7104312855777075, 47: 0.7103372522704192, 48: 0.7105177995991266, 49: 0.7102569289347965, 50: 0.7099757030453538, 51: 0.7098883077451437, 52: 0.7097982345015884, 53: 0.7097299770763308, 54: 0.7098718407281466, 55: 0.7095817399100819, 56: 0.7101422315587961, 57: 0.7096241375210741, 58: 0.7098441765679, 59: 0.7100495337314221, 60: 0.7100912110042046, 61: 0.7101782824889614, 62: 0.7095730019600884, 63: 0.7096068255426246, 64: 0.7094961370253774, 65: 0.7094607949392564, 66: 0.7093284207378824, 67: 0.7094135079063381, 68: 0.7093775339633598, 69: 0.7097070637132271, 70: 0.7098342977908083, 71: 0.7098409528357863, 72: 0.7097561934123456, 73: 0.7097514628160398, 74: 0.7098592348677172, 75: 0.7099259261083624, 76: 0.7095765300059955, 77: 0.7095089741210587, 78: 0.7094044176612048, 79: 0.7093791267432935, 80: 0.709421215046626, 81: 0.7094183699293635, 82: 0.7094163843060898, 83: 0.7094095428909806, 84: 0.7094480717076858, 85: 0.7094605730395624, 86: 0.7094457375959111, 87: 0.7093470298853857, 88: 0.7093055101817832, 89: 0.709236326821753, 90: 0.7091646769486696, 91: 0.7091795764016068, 92: 0.7091939943957873, 93: 0.7091995451060946, 94: 0.7092126915969412, 95: 0.7091539087714552, 96: 0.7091507195729062, 97: 0.709166553862136, 98: 0.7091765049816587, 99: 0.7091717242872025, 100: 0.7090711669075844}, 'fold_1': {1: 5.414841974088104, 2: 5.000051977511262, 3: 4.872659348241044, 4: 4.44267903405236, 5: 3.634326814444159, 6: 2.7200341869113904, 7: 2.48480485497013, 8: 1.2478552794700257, 9: 0.9764574392476754, 10: 0.8520660630293129, 11: 0.8384343914580595, 12: 0.8094456104723539, 13: 0.7970259206241639, 14: 0.7818366329652755, 15: 0.7810367616617813, 16: 0.7616361594356764, 17: 0.7541911123061905, 18: 0.7486028366351876, 19: 0.7391937788146206, 20: 0.7434593789732655, 21: 0.7450629582359874, 22: 0.7413803683785616, 23: 0.7345278407040161, 24: 0.7348134361826781, 25: 0.7350596641916012, 26: 0.7294177814010651, 27: 0.7297575249177523, 28: 0.729667602381068, 29: 0.7321163981629701, 30: 0.7295713543461257, 31: 0.7270624741765249, 32: 0.725967117980724, 33: 0.7248750326685701, 34: 0.7250650723328005, 35: 0.7258318360753345, 36: 0.725804919333879, 37: 0.723664228062234, 38: 0.7239356812402492, 39: 0.7240835798317038, 40: 0.7234759175040051, 41: 0.7236352550737485, 42: 0.7238994676002842, 43: 0.723660663944263, 44: 0.7232964447402308, 45: 0.7229557185390828, 46: 0.722673976695227, 47: 0.722291580671801, 48: 0.7221250639193153, 49: 0.7224175769993796, 50: 0.7223469006473242, 51: 0.722545879359316, 52: 0.7227279617544952, 53: 0.7224075644052697, 54: 0.7226615886553653, 55: 0.7230664211753914, 56: 0.7237759960904468, 57: 0.7233338038833168, 58: 0.7233300397246964, 59: 0.722998145780305, 60: 0.7233642508626537, 61: 0.7230643356898583, 62: 0.7228567534044732, 63: 0.7228913103116156, 64: 0.7228669508557205, 65: 0.7230290670638141, 66: 0.7229726850426218, 67: 0.7228875714233821, 68: 0.7228734781314851, 69: 0.722969809155835, 70: 0.7229308604558806, 71: 0.7229770148790594, 72: 0.7227355295886426, 73: 0.7225411182645011, 74: 0.7227926408585094, 75: 0.7227240762049447, 76: 0.7227381795919469, 77: 0.7227951602269763, 78: 0.7227023834858219, 79: 0.7226967901159731, 80: 0.722674792769413, 81: 0.7226808694336391, 82: 0.7226803253600187, 83: 0.7227169535586337, 84: 0.7226991185114485, 85: 0.7227004758824918, 86: 0.7227151069056819, 87: 0.7227035744843564, 88: 0.7226491624189316, 89: 0.7226492863558626, 90: 0.7227411440266963, 91: 0.7228172599400825, 92: 0.7228185837894919, 93: 0.7228136120636359, 94: 0.7228006989354478, 95: 0.7227975167052755, 96: 0.7228584245642196, 97: 0.7228833089913344, 98: 0.722869064757155, 99: 0.7228635568022634, 100: 0.7228330951637465}, 'fold_2': {1: 5.368101915672432, 2: 5.253274412360862, 3: 5.141312089290392, 4: 4.653843480466608, 5: 3.8750033801821293, 6: 2.7863154858687844, 7: 2.540620957837608, 8: 1.2282439179201061, 9: 0.9790216825454359, 10: 0.8488426986790686, 11: 0.8051556252716493, 12: 0.7632953961067885, 13: 0.7524344434471986, 14: 0.7325880503438683, 15: 0.7218363379594525, 16: 0.7079799249338299, 17: 0.7040359274277529, 18: 0.7081656182399014, 19: 0.6991042963504123, 20: 0.6987337284393006, 21: 0.6965546647380501, 22: 0.6937836888428863, 23: 0.690693302472011, 24: 0.6902005750243717, 25: 0.6901685240165063, 26: 0.6867299737290584, 27: 0.6837051284898061, 28: 0.681841081370727, 29: 0.679876342373396, 30: 0.6789651195372324, 31: 0.6771505691470411, 32: 0.6759961767642968, 33: 0.6759816619353882, 34: 0.6789803052798753, 35: 0.678957516015824, 36: 0.6789433928115991, 37: 0.6788752532500678, 38: 0.6790630934117183, 39: 0.6792982021932873, 40: 0.6786338918474855, 41: 0.6782036824123394, 42: 0.6779723907674563, 43: 0.6782138734738652, 44: 0.6786202860879275, 45: 0.678821007040193, 46: 0.6783595471184908, 47: 0.6787398745365754, 48: 0.6791381717007917, 49: 0.6789580484092858, 50: 0.6788638249492802, 51: 0.6786934838291635, 52: 0.6780528222217127, 53: 0.6784014037775861, 54: 0.6781400496335243, 55: 0.6778968780743904, 56: 0.677928526708633, 57: 0.6778966395902538, 58: 0.6780706009409357, 59: 0.677868451912215, 60: 0.6777054462186093, 61: 0.6782370352254864, 62: 0.6779990748101089, 63: 0.6781684511867077, 64: 0.6785444510318915, 65: 0.6783695459896374, 66: 0.6782088982249987, 67: 0.6783986101285735, 68: 0.6784099320602602, 69: 0.6782831096898214, 70: 0.6782850311165939, 71: 0.6782147935363725, 72: 0.6781017907983391, 73: 0.6784365039158584, 74: 0.6784826169133147, 75: 0.6787441456696267, 76: 0.6790167284572365, 77: 0.6790232319094518, 78: 0.6793412236133922, 79: 0.6794891382846626, 80: 0.6794588364937264, 81: 0.6794689145848741, 82: 0.6794779518103818, 83: 0.6795117795153623, 84: 0.6795419130061974, 85: 0.6796600054849342, 86: 0.6797129567174809, 87: 0.6797047159972782, 88: 0.6797914581019253, 89: 0.6798433484937964, 90: 0.6798289652435734, 91: 0.6798726212976239, 92: 0.6798780391815187, 93: 0.6798544444990836, 94: 0.6798385275848071, 95: 0.6798512427462569, 96: 0.679865157797056, 97: 0.6798487805540999, 98: 0.6798813091872689, 99: 0.6798814769587873, 100: 0.6799072718920205}, 'fold_3': {1: 5.511611206840777, 2: 5.42710029874514, 3: 5.3454672083299615, 4: 4.977982670816769, 5: 3.9609072102478105, 6: 2.938403654108194, 7: 2.5749101140909323, 8: 1.268892179769614, 9: 1.040300806027242, 10: 0.9139516025450468, 11: 0.8703697286604188, 12: 0.860029348253666, 13: 0.8536271162250093, 14: 0.8388697746889692, 15: 0.8252622556973597, 16: 0.8224280380060125, 17: 0.8214257920269807, 18: 0.8117541141179411, 19: 0.8078831179511836, 20: 0.7969404938534321, 21: 0.796274983362064, 22: 0.7991887564838476, 23: 0.8021452005765312, 24: 0.8012373689562067, 25: 0.7967176855394774, 26: 0.7969555484410049, 27: 0.7955264380718822, 28: 0.7912139587039285, 29: 0.7904783834948907, 30: 0.7896168634518046, 31: 0.7885749381265524, 32: 0.7856700345374981, 33: 0.7860011655226838, 34: 0.7832195753189818, 35: 0.7799515484707169, 36: 0.7798227575682506, 37: 0.7818543763334249, 38: 0.7811653700306738, 39: 0.7812988276543555, 40: 0.78019486790625, 41: 0.7798922095628091, 42: 0.7792560748498053, 43: 0.7794425969920235, 44: 0.7791803088004141, 45: 0.7788326491579218, 46: 0.7787980059088008, 47: 0.7784631075759815, 48: 0.77876155694785, 49: 0.779054693582912, 50: 0.779193147441438, 51: 0.7792430208817449, 52: 0.7790640494435497, 53: 0.778876350549464, 54: 0.7793956056919307, 55: 0.7795344961422954, 56: 0.7796689711604813, 57: 0.7792259179100229, 58: 0.778885006326083, 59: 0.7786108431409612, 60: 0.7788429180008298, 61: 0.7795180392173007, 62: 0.7788955628918458, 63: 0.7786921792957279, 64: 0.7790439351940861, 65: 0.7788677755384437, 66: 0.7787108593227026, 67: 0.7788270883301249, 68: 0.7788303239183219, 69: 0.7790367683315649, 70: 0.7790203152480969, 71: 0.7790702285348372, 72: 0.7789897370555419, 73: 0.7791434687389175, 74: 0.7793476271178437, 75: 0.7792778961708613, 76: 0.7792938072774521, 77: 0.7794237925372712, 78: 0.7794187451465437, 79: 0.7793891088691545, 80: 0.7794436319425739, 81: 0.7795115694263932, 82: 0.779571064485248, 83: 0.7796099017928534, 84: 0.7795444865770521, 85: 0.7794279860321625, 86: 0.7795297926818624, 87: 0.7796991516307656, 88: 0.7794654539461658, 89: 0.7794672419394661, 90: 0.7795107107107309, 91: 0.7796231325036516, 92: 0.7796494007590677, 93: 0.7797400028378568, 94: 0.7797405150286877, 95: 0.7797249086053155, 96: 0.7799320851579447, 97: 0.7799820418771835, 98: 0.7799988498190153, 99: 0.7804038700668031, 100: 0.7800358033364131}, 'fold_4': {1: 4.9631524425041595, 2: 4.904939910925031, 3: 4.768252596846698, 4: 4.5295795782298365, 5: 3.556861290155156, 6: 2.861850964554895, 7: 2.5214660487451965, 8: 1.2208792164205806, 9: 1.0154000903661091, 10: 0.8926080252582234, 11: 0.8414598424281471, 12: 0.8250816568652324, 13: 0.8083049500539052, 14: 0.8042582649649107, 15: 0.7863235590082116, 16: 0.7723212703193535, 17: 0.7681164522603091, 18: 0.765130291129549, 19: 0.757994223807007, 20: 0.7544182451334948, 21: 0.7442404458125287, 22: 0.7415443142798164, 23: 0.7424016320687181, 24: 0.7402584587544324, 25: 0.734985924498437, 26: 0.7317915263281395, 27: 0.7306346528893214, 28: 0.7295759155710742, 29: 0.7241140008240874, 30: 0.7222789113281602, 31: 0.72330407229038, 32: 0.7232399451319745, 33: 0.7229817536410208, 34: 0.7221922909076186, 35: 0.72132493389361, 36: 0.7202022790256993, 37: 0.7194691988697197, 38: 0.7201523543123275, 39: 0.7200086118057832, 40: 0.7207589811295841, 41: 0.7202949860334663, 42: 0.7208138526119104, 43: 0.7208967011020094, 44: 0.7210385876822087, 45: 0.7209473862348491, 46: 0.7208029090716674, 47: 0.7211123241471401, 48: 0.7208755285919907, 49: 0.7208890791127924, 50: 0.7205287019446683, 51: 0.7206168728806522, 52: 0.7205958764300598, 53: 0.7207411650518833, 54: 0.7208790614539428, 55: 0.7204604418406872, 56: 0.7202432264292993, 57: 0.720029086590029, 58: 0.7201052656720665, 59: 0.719949486562663, 60: 0.7202240432748415, 61: 0.7203501240433162, 62: 0.7206260761297252, 63: 0.7210938100574832, 64: 0.7209092677264395, 65: 0.720958535143603, 66: 0.7206705353994444, 67: 0.7206956723732351, 68: 0.7206995977034175, 69: 0.7205907817356266, 70: 0.7203919568368783, 71: 0.7204719062995754, 72: 0.7203438351544591, 73: 0.7206055771751873, 74: 0.7207715555924831, 75: 0.7208236739448297, 76: 0.7207258816733858, 77: 0.7207259080936368, 78: 0.7206856027424353, 79: 0.7207214487668351, 80: 0.7207564103511944, 81: 0.7208858843318519, 82: 0.7207335071448886, 83: 0.7206986781636622, 84: 0.7208808138774042, 85: 0.7207542573688074, 86: 0.7206617064584906, 87: 0.7208330275559773, 88: 0.7209068009623893, 89: 0.7208716093655253, 90: 0.7208523182325128, 91: 0.720761031883728, 92: 0.7209221933767734, 93: 0.7208817503411643, 94: 0.7207543056632981, 95: 0.7208042589406414, 96: 0.720869143505042, 97: 0.7207969547787829, 98: 0.7202855500602687, 99: 0.7205327202962446, 100: 0.7212108901434919}, 'MSE': {1: 5.3323439102314945, 2: 5.177777031511034, 3: 5.040232789690463, 4: 4.681774773119627, 5: 3.7497898759351487, 6: 2.8268235417414305, 7: 2.5348213506218684, 8: 1.240982173989487, 9: 1.0024684766177323, 10: 0.8739567497663949, 11: 0.8367555207198749, 12: 0.8137487041145953, 13: 0.8017416195669886, 14: 0.7889308119172252, 15: 0.7763884806721201, 16: 0.7655960604852259, 17: 0.7618106584501472, 18: 0.7569730723130808, 19: 0.7477793497906583, 20: 0.7445864687755392, 21: 0.7420508020460274, 22: 0.7401929352883174, 23: 0.7387461220610619, 24: 0.7379746113344322, 25: 0.7352900612333831, 26: 0.732179105407231, 27: 0.7311882802901938, 28: 0.7298386631496974, 29: 0.7287300123227894, 30: 0.7271585247619551, 31: 0.7264762927894873, 32: 0.7256329114300716, 33: 0.7254631547722353, 34: 0.7254283455580193, 35: 0.7244816934935425, 36: 0.7238920885787359, 37: 0.7235215033537066, 38: 0.7235993922737507, 39: 0.7234647103805101, 40: 0.7230663606078723, 41: 0.722990981386404, 42: 0.7229816983109197, 43: 0.7228263925868902, 44: 0.722674508093956, 45: 0.7227424655456156, 46: 0.7225572022821914, 47: 0.7225304450083427, 48: 0.7226259062235222, 49: 0.7226604131998481, 50: 0.722529096355613, 51: 0.7225457408786018, 52: 0.7223961460746388, 53: 0.7223781735980397, 54: 0.7225386955308857, 55: 0.7224601775981386, 56: 0.7227036817335951, 57: 0.7223734953194556, 58: 0.7223958882506821, 59: 0.7222420010013837, 60: 0.7223933503571722, 61: 0.722619336011695, 62: 0.7223386649686766, 63: 0.722437179127616, 64: 0.7225209695886892, 65: 0.7224854889136884, 66: 0.7223268709495932, 67: 0.7223930486837409, 68: 0.7223868556981371, 69: 0.7224662704991841, 70: 0.7224409536300532, 71: 0.7224636242474787, 72: 0.722333968384517, 73: 0.7224440685159029, 74: 0.7225997634331909, 75: 0.7226472586795731, 76: 0.7226197869293535, 77: 0.7226459263261643, 78: 0.722661131069666, 79: 0.7226855854313324, 80: 0.7227014709664269, 81: 0.7227437447447417, 82: 0.7227270005526619, 83: 0.722740820325653, 84: 0.7227735355636304, 85: 0.7227508463690535, 86: 0.7227639274449001, 87: 0.7228092733068325, 88: 0.7227742568848128, 89: 0.7227644454770513, 90: 0.7227710874780093, 91: 0.7228029468414627, 92: 0.7228444741350624, 93: 0.722850386343886, 94: 0.7228220120712427, 95: 0.7228190928407828, 96: 0.7228887993797696, 97: 0.7228895557779421, 98: 0.7227971097232778, 99: 0.7229270728671319, 100: 0.7229655366829559}, 'R2': {1: 0.12183805570384032, 2: 0.14729304379650743, 3: 0.1699446433289633, 4: 0.228977630337996, 5: 0.38246241735553976, 6: 0.5344619740608514, 7: 0.5825506225443586, 8: 0.7956277132357407, 9: 0.8349075601007102, 10: 0.856071631626571, 11: 0.8621981501294408, 12: 0.8659870488093236, 13: 0.8679644465333195, 14: 0.8700742061330584, 15: 0.8721397514500202, 16: 0.8739171110604532, 17: 0.8745405134641671, 18: 0.8753371957711119, 19: 0.8768512723913612, 20: 0.8773770949278189, 21: 0.877794683527272, 22: 0.878100648017121, 23: 0.8783389177795499, 24: 0.8784659747307046, 25: 0.8789080823246606, 26: 0.8794204129362836, 27: 0.8795835878788384, 28: 0.8798058508146329, 29: 0.8799884299922127, 30: 0.8802472318615392, 31: 0.880359585859235, 32: 0.8804984788914242, 33: 0.880526435422213, 34: 0.880532168009561, 35: 0.8806880682724675, 36: 0.8807851678982737, 37: 0.8808461980657231, 38: 0.8808333708575353, 39: 0.8808555510685684, 40: 0.8809211536659887, 41: 0.8809335675621661, 42: 0.8809350963539608, 43: 0.8809606730194703, 44: 0.8809856862564013, 45: 0.8809744946211873, 46: 0.8810050048715301, 47: 0.8810094114176987, 48: 0.8809936902999804, 49: 0.8809880074869563, 50: 0.8810096335218096, 51: 0.8810068924032854, 52: 0.8810315285606894, 53: 0.8810344883744711, 54: 0.8810080526728316, 55: 0.8810209834705528, 56: 0.8809808817688177, 57: 0.8810352588210676, 58: 0.881031571020664, 59: 0.8810569140833624, 60: 0.8810319889759828, 61: 0.8809947723213372, 62: 0.88104099488931, 63: 0.8810247709947504, 64: 0.8810109718858692, 65: 0.8810168150533519, 66: 0.8810429371981526, 67: 0.8810320386573484, 68: 0.8810330585548588, 69: 0.8810199800555596, 70: 0.881024149387357, 71: 0.8810204158559471, 72: 0.881041768350577, 73: 0.8810236364095257, 74: 0.8809979956493676, 75: 0.8809901738511847, 76: 0.8809946980615493, 77: 0.880990393271027, 78: 0.8809878892639074, 79: 0.8809838619749424, 80: 0.8809812458510352, 81: 0.8809742839551067, 82: 0.8809770414877265, 83: 0.8809747655657447, 84: 0.8809693778267929, 85: 0.8809731144175978, 86: 0.8809709601486324, 87: 0.8809634923237977, 88: 0.8809692590353527, 89: 0.8809708748360328, 90: 0.8809697809920172, 91: 0.8809645342035457, 92: 0.8809576952431367, 93: 0.8809567215856077, 94: 0.8809613944287048, 95: 0.8809618751848488, 96: 0.8809503954995852, 97: 0.8809502709314588, 98: 0.880965495494704, 99: 0.8809440923952387, 100: 0.8809377579464752}}'\n",
      "1: {'fold_0': 5.397604977935402, 'fold_1': 5.414841974088104, 'fold_2': 5.368101915672432, 'fold_3': 5.511611206840777, 'fold_4': 4.9631524425041595, 'MSE': 5.3323439102314945, 'R2': 0.12183805570384032}'\n",
      "2: {'fold_0': 5.299072153140445, 'fold_1': 5.000051977511262, 'fold_2': 5.253274412360862, 'fold_3': 5.42710029874514, 'fold_4': 4.904939910925031, 'MSE': 5.177777031511034, 'R2': 0.14729304379650743}'\n",
      "3: {'fold_0': 5.065854861944807, 'fold_1': 4.872659348241044, 'fold_2': 5.141312089290392, 'fold_3': 5.3454672083299615, 'fold_4': 4.768252596846698, 'MSE': 5.040232789690463, 'R2': 0.1699446433289633}'\n",
      "4: {'fold_0': 4.8002302669665635, 'fold_1': 4.44267903405236, 'fold_2': 4.653843480466608, 'fold_3': 4.977982670816769, 'fold_4': 4.5295795782298365, 'MSE': 4.681774773119627, 'R2': 0.228977630337996}'\n",
      "5: {'fold_0': 3.715696823094543, 'fold_1': 3.634326814444159, 'fold_2': 3.8750033801821293, 'fold_3': 3.9609072102478105, 'fold_4': 3.556861290155156, 'MSE': 3.7497898759351487, 'R2': 0.38246241735553976}'\n",
      "6: {'fold_0': 2.8256721536359195, 'fold_1': 2.7200341869113904, 'fold_2': 2.7863154858687844, 'fold_3': 2.938403654108194, 'fold_4': 2.861850964554895, 'MSE': 2.8268235417414305, 'R2': 0.5344619740608514}'\n",
      "7: {'fold_0': 2.5519066661067207, 'fold_1': 2.48480485497013, 'fold_2': 2.540620957837608, 'fold_3': 2.5749101140909323, 'fold_4': 2.5214660487451965, 'MSE': 2.5348213506218684, 'R2': 0.5825506225443586}'\n",
      "8: {'fold_0': 1.238075338572452, 'fold_1': 1.2478552794700257, 'fold_2': 1.2282439179201061, 'fold_3': 1.268892179769614, 'fold_4': 1.2208792164205806, 'MSE': 1.240982173989487, 'R2': 0.7956277132357407}'\n",
      "9: {'fold_0': 1.0004213875299952, 'fold_1': 0.9764574392476754, 'fold_2': 0.9790216825454359, 'fold_3': 1.040300806027242, 'fold_4': 1.0154000903661091, 'MSE': 1.0024684766177323, 'R2': 0.8349075601007102}'\n",
      "10: {'fold_0': 0.861332833481699, 'fold_1': 0.8520660630293129, 'fold_2': 0.8488426986790686, 'fold_3': 0.9139516025450468, 'fold_4': 0.8926080252582234, 'MSE': 0.8739567497663949, 'R2': 0.856071631626571}'\n",
      "11: {'fold_0': 0.8273082334046411, 'fold_1': 0.8384343914580595, 'fold_2': 0.8051556252716493, 'fold_3': 0.8703697286604188, 'fold_4': 0.8414598424281471, 'MSE': 0.8367555207198749, 'R2': 0.8621981501294408}'\n",
      "12: {'fold_0': 0.8096803600403776, 'fold_1': 0.8094456104723539, 'fold_2': 0.7632953961067885, 'fold_3': 0.860029348253666, 'fold_4': 0.8250816568652324, 'MSE': 0.8137487041145953, 'R2': 0.8659870488093236}'\n",
      "13: {'fold_0': 0.7959040347250804, 'fold_1': 0.7970259206241639, 'fold_2': 0.7524344434471986, 'fold_3': 0.8536271162250093, 'fold_4': 0.8083049500539052, 'MSE': 0.8017416195669886, 'R2': 0.8679644465333195}'\n",
      "14: {'fold_0': 0.7858549326745825, 'fold_1': 0.7818366329652755, 'fold_2': 0.7325880503438683, 'fold_3': 0.8388697746889692, 'fold_4': 0.8042582649649107, 'MSE': 0.7889308119172252, 'R2': 0.8700742061330584}'\n",
      "15: {'fold_0': 0.7660080484520673, 'fold_1': 0.7810367616617813, 'fold_2': 0.7218363379594525, 'fold_3': 0.8252622556973597, 'fold_4': 0.7863235590082116, 'MSE': 0.7763884806721201, 'R2': 0.8721397514500202}'\n",
      "16: {'fold_0': 0.7621030308136848, 'fold_1': 0.7616361594356764, 'fold_2': 0.7079799249338299, 'fold_3': 0.8224280380060125, 'fold_4': 0.7723212703193535, 'MSE': 0.7655960604852259, 'R2': 0.8739171110604532}'\n",
      "17: {'fold_0': 0.759752968867632, 'fold_1': 0.7541911123061905, 'fold_2': 0.7040359274277529, 'fold_3': 0.8214257920269807, 'fold_4': 0.7681164522603091, 'MSE': 0.7618106584501472, 'R2': 0.8745405134641671}'\n",
      "18: {'fold_0': 0.7497423093108908, 'fold_1': 0.7486028366351876, 'fold_2': 0.7081656182399014, 'fold_3': 0.8117541141179411, 'fold_4': 0.765130291129549, 'MSE': 0.7569730723130808, 'R2': 0.8753371957711119}'\n",
      "19: {'fold_0': 0.7329961018489892, 'fold_1': 0.7391937788146206, 'fold_2': 0.6991042963504123, 'fold_3': 0.8078831179511836, 'fold_4': 0.757994223807007, 'MSE': 0.7477793497906583, 'R2': 0.8768512723913612}'\n",
      "20: {'fold_0': 0.7277582428821165, 'fold_1': 0.7434593789732655, 'fold_2': 0.6987337284393006, 'fold_3': 0.7969404938534321, 'fold_4': 0.7544182451334948, 'MSE': 0.7445864687755392, 'R2': 0.8773770949278189}'\n",
      "21: {'fold_0': 0.7263898636935519, 'fold_1': 0.7450629582359874, 'fold_2': 0.6965546647380501, 'fold_3': 0.796274983362064, 'fold_4': 0.7442404458125287, 'MSE': 0.7420508020460274, 'R2': 0.877794683527272}'\n",
      "22: {'fold_0': 0.7231989631092334, 'fold_1': 0.7413803683785616, 'fold_2': 0.6937836888428863, 'fold_3': 0.7991887564838476, 'fold_4': 0.7415443142798164, 'MSE': 0.7401929352883174, 'R2': 0.878100648017121}'\n",
      "23: {'fold_0': 0.7220427712185392, 'fold_1': 0.7345278407040161, 'fold_2': 0.690693302472011, 'fold_3': 0.8021452005765312, 'fold_4': 0.7424016320687181, 'MSE': 0.7387461220610619, 'R2': 0.8783389177795499}'\n",
      "24: {'fold_0': 0.7214327776684041, 'fold_1': 0.7348134361826781, 'fold_2': 0.6902005750243717, 'fold_3': 0.8012373689562067, 'fold_4': 0.7402584587544324, 'MSE': 0.7379746113344322, 'R2': 0.8784659747307046}'\n",
      "25: {'fold_0': 0.717577394855007, 'fold_1': 0.7350596641916012, 'fold_2': 0.6901685240165063, 'fold_3': 0.7967176855394774, 'fold_4': 0.734985924498437, 'MSE': 0.7352900612333831, 'R2': 0.8789080823246606}'\n",
      "26: {'fold_0': 0.7139848146901216, 'fold_1': 0.7294177814010651, 'fold_2': 0.6867299737290584, 'fold_3': 0.7969555484410049, 'fold_4': 0.7317915263281395, 'MSE': 0.732179105407231, 'R2': 0.8794204129362836}'\n",
      "27: {'fold_0': 0.7143222624236197, 'fold_1': 0.7297575249177523, 'fold_2': 0.6837051284898061, 'fold_3': 0.7955264380718822, 'fold_4': 0.7306346528893214, 'MSE': 0.7311882802901938, 'R2': 0.8795835878788384}'\n",
      "28: {'fold_0': 0.7150022282075326, 'fold_1': 0.729667602381068, 'fold_2': 0.681841081370727, 'fold_3': 0.7912139587039285, 'fold_4': 0.7295759155710742, 'MSE': 0.7298386631496974, 'R2': 0.8798058508146329}'\n",
      "29: {'fold_0': 0.7151272397616933, 'fold_1': 0.7321163981629701, 'fold_2': 0.679876342373396, 'fold_3': 0.7904783834948907, 'fold_4': 0.7241140008240874, 'MSE': 0.7287300123227894, 'R2': 0.8799884299922127}'\n",
      "30: {'fold_0': 0.7134088783136844, 'fold_1': 0.7295713543461257, 'fold_2': 0.6789651195372324, 'fold_3': 0.7896168634518046, 'fold_4': 0.7222789113281602, 'MSE': 0.7271585247619551, 'R2': 0.8802472318615392}'\n",
      "31: {'fold_0': 0.7144010200388925, 'fold_1': 0.7270624741765249, 'fold_2': 0.6771505691470411, 'fold_3': 0.7885749381265524, 'fold_4': 0.72330407229038, 'MSE': 0.7264762927894873, 'R2': 0.880359585859235}'\n",
      "32: {'fold_0': 0.715496632919228, 'fold_1': 0.725967117980724, 'fold_2': 0.6759961767642968, 'fold_3': 0.7856700345374981, 'fold_4': 0.7232399451319745, 'MSE': 0.7256329114300716, 'R2': 0.8804984788914242}'\n",
      "33: {'fold_0': 0.7156820455516821, 'fold_1': 0.7248750326685701, 'fold_2': 0.6759816619353882, 'fold_3': 0.7860011655226838, 'fold_4': 0.7229817536410208, 'MSE': 0.7254631547722353, 'R2': 0.880526435422213}'\n",
      "34: {'fold_0': 0.7159633452907412, 'fold_1': 0.7250650723328005, 'fold_2': 0.6789803052798753, 'fold_3': 0.7832195753189818, 'fold_4': 0.7221922909076186, 'MSE': 0.7254283455580193, 'R2': 0.880532168009561}'\n",
      "35: {'fold_0': 0.7146624479430996, 'fold_1': 0.7258318360753345, 'fold_2': 0.678957516015824, 'fold_3': 0.7799515484707169, 'fold_4': 0.72132493389361, 'MSE': 0.7244816934935425, 'R2': 0.8806880682724675}'\n",
      "36: {'fold_0': 0.7129683100817252, 'fold_1': 0.725804919333879, 'fold_2': 0.6789433928115991, 'fold_3': 0.7798227575682506, 'fold_4': 0.7202022790256993, 'MSE': 0.7238920885787359, 'R2': 0.8807851678982737}'\n",
      "37: {'fold_0': 0.7119653175424516, 'fold_1': 0.723664228062234, 'fold_2': 0.6788752532500678, 'fold_3': 0.7818543763334249, 'fold_4': 0.7194691988697197, 'MSE': 0.7235215033537066, 'R2': 0.8808461980657231}'\n",
      "38: {'fold_0': 0.7119212181841938, 'fold_1': 0.7239356812402492, 'fold_2': 0.6790630934117183, 'fold_3': 0.7811653700306738, 'fold_4': 0.7201523543123275, 'MSE': 0.7235993922737507, 'R2': 0.8808333708575353}'\n",
      "39: {'fold_0': 0.7108497388725766, 'fold_1': 0.7240835798317038, 'fold_2': 0.6792982021932873, 'fold_3': 0.7812988276543555, 'fold_4': 0.7200086118057832, 'MSE': 0.7234647103805101, 'R2': 0.8808555510685684}'\n",
      "40: {'fold_0': 0.7105113619176454, 'fold_1': 0.7234759175040051, 'fold_2': 0.6786338918474855, 'fold_3': 0.78019486790625, 'fold_4': 0.7207589811295841, 'MSE': 0.7230663606078723, 'R2': 0.8809211536659887}'\n",
      "41: {'fold_0': 0.7111863006810212, 'fold_1': 0.7236352550737485, 'fold_2': 0.6782036824123394, 'fold_3': 0.7798922095628091, 'fold_4': 0.7202949860334663, 'MSE': 0.722990981386404, 'R2': 0.8809335675621661}'\n",
      "42: {'fold_0': 0.7112423248081001, 'fold_1': 0.7238994676002842, 'fold_2': 0.6779723907674563, 'fold_3': 0.7792560748498053, 'fold_4': 0.7208138526119104, 'MSE': 0.7229816983109197, 'R2': 0.8809350963539608}'\n",
      "43: {'fold_0': 0.7101714497773479, 'fold_1': 0.723660663944263, 'fold_2': 0.6782138734738652, 'fold_3': 0.7794425969920235, 'fold_4': 0.7208967011020094, 'MSE': 0.7228263925868902, 'R2': 0.8809606730194703}'\n",
      "44: {'fold_0': 0.7094879059472369, 'fold_1': 0.7232964447402308, 'fold_2': 0.6786202860879275, 'fold_3': 0.7791803088004141, 'fold_4': 0.7210385876822087, 'MSE': 0.722674508093956, 'R2': 0.8809856862564013}'\n",
      "45: {'fold_0': 0.7104352204798654, 'fold_1': 0.7229557185390828, 'fold_2': 0.678821007040193, 'fold_3': 0.7788326491579218, 'fold_4': 0.7209473862348491, 'MSE': 0.7227424655456156, 'R2': 0.8809744946211873}'\n",
      "46: {'fold_0': 0.7104312855777075, 'fold_1': 0.722673976695227, 'fold_2': 0.6783595471184908, 'fold_3': 0.7787980059088008, 'fold_4': 0.7208029090716674, 'MSE': 0.7225572022821914, 'R2': 0.8810050048715301}'\n",
      "47: {'fold_0': 0.7103372522704192, 'fold_1': 0.722291580671801, 'fold_2': 0.6787398745365754, 'fold_3': 0.7784631075759815, 'fold_4': 0.7211123241471401, 'MSE': 0.7225304450083427, 'R2': 0.8810094114176987}'\n",
      "48: {'fold_0': 0.7105177995991266, 'fold_1': 0.7221250639193153, 'fold_2': 0.6791381717007917, 'fold_3': 0.77876155694785, 'fold_4': 0.7208755285919907, 'MSE': 0.7226259062235222, 'R2': 0.8809936902999804}'\n",
      "49: {'fold_0': 0.7102569289347965, 'fold_1': 0.7224175769993796, 'fold_2': 0.6789580484092858, 'fold_3': 0.779054693582912, 'fold_4': 0.7208890791127924, 'MSE': 0.7226604131998481, 'R2': 0.8809880074869563}'\n",
      "50: {'fold_0': 0.7099757030453538, 'fold_1': 0.7223469006473242, 'fold_2': 0.6788638249492802, 'fold_3': 0.779193147441438, 'fold_4': 0.7205287019446683, 'MSE': 0.722529096355613, 'R2': 0.8810096335218096}'\n",
      "51: {'fold_0': 0.7098883077451437, 'fold_1': 0.722545879359316, 'fold_2': 0.6786934838291635, 'fold_3': 0.7792430208817449, 'fold_4': 0.7206168728806522, 'MSE': 0.7225457408786018, 'R2': 0.8810068924032854}'\n",
      "52: {'fold_0': 0.7097982345015884, 'fold_1': 0.7227279617544952, 'fold_2': 0.6780528222217127, 'fold_3': 0.7790640494435497, 'fold_4': 0.7205958764300598, 'MSE': 0.7223961460746388, 'R2': 0.8810315285606894}'\n",
      "53: {'fold_0': 0.7097299770763308, 'fold_1': 0.7224075644052697, 'fold_2': 0.6784014037775861, 'fold_3': 0.778876350549464, 'fold_4': 0.7207411650518833, 'MSE': 0.7223781735980397, 'R2': 0.8810344883744711}'\n",
      "54: {'fold_0': 0.7098718407281466, 'fold_1': 0.7226615886553653, 'fold_2': 0.6781400496335243, 'fold_3': 0.7793956056919307, 'fold_4': 0.7208790614539428, 'MSE': 0.7225386955308857, 'R2': 0.8810080526728316}'\n",
      "55: {'fold_0': 0.7095817399100819, 'fold_1': 0.7230664211753914, 'fold_2': 0.6778968780743904, 'fold_3': 0.7795344961422954, 'fold_4': 0.7204604418406872, 'MSE': 0.7224601775981386, 'R2': 0.8810209834705528}'\n",
      "56: {'fold_0': 0.7101422315587961, 'fold_1': 0.7237759960904468, 'fold_2': 0.677928526708633, 'fold_3': 0.7796689711604813, 'fold_4': 0.7202432264292993, 'MSE': 0.7227036817335951, 'R2': 0.8809808817688177}'\n",
      "57: {'fold_0': 0.7096241375210741, 'fold_1': 0.7233338038833168, 'fold_2': 0.6778966395902538, 'fold_3': 0.7792259179100229, 'fold_4': 0.720029086590029, 'MSE': 0.7223734953194556, 'R2': 0.8810352588210676}'\n",
      "58: {'fold_0': 0.7098441765679, 'fold_1': 0.7233300397246964, 'fold_2': 0.6780706009409357, 'fold_3': 0.778885006326083, 'fold_4': 0.7201052656720665, 'MSE': 0.7223958882506821, 'R2': 0.881031571020664}'\n",
      "59: {'fold_0': 0.7100495337314221, 'fold_1': 0.722998145780305, 'fold_2': 0.677868451912215, 'fold_3': 0.7786108431409612, 'fold_4': 0.719949486562663, 'MSE': 0.7222420010013837, 'R2': 0.8810569140833624}'\n",
      "60: {'fold_0': 0.7100912110042046, 'fold_1': 0.7233642508626537, 'fold_2': 0.6777054462186093, 'fold_3': 0.7788429180008298, 'fold_4': 0.7202240432748415, 'MSE': 0.7223933503571722, 'R2': 0.8810319889759828}'\n",
      "61: {'fold_0': 0.7101782824889614, 'fold_1': 0.7230643356898583, 'fold_2': 0.6782370352254864, 'fold_3': 0.7795180392173007, 'fold_4': 0.7203501240433162, 'MSE': 0.722619336011695, 'R2': 0.8809947723213372}'\n",
      "62: {'fold_0': 0.7095730019600884, 'fold_1': 0.7228567534044732, 'fold_2': 0.6779990748101089, 'fold_3': 0.7788955628918458, 'fold_4': 0.7206260761297252, 'MSE': 0.7223386649686766, 'R2': 0.88104099488931}'\n",
      "63: {'fold_0': 0.7096068255426246, 'fold_1': 0.7228913103116156, 'fold_2': 0.6781684511867077, 'fold_3': 0.7786921792957279, 'fold_4': 0.7210938100574832, 'MSE': 0.722437179127616, 'R2': 0.8810247709947504}'\n",
      "64: {'fold_0': 0.7094961370253774, 'fold_1': 0.7228669508557205, 'fold_2': 0.6785444510318915, 'fold_3': 0.7790439351940861, 'fold_4': 0.7209092677264395, 'MSE': 0.7225209695886892, 'R2': 0.8810109718858692}'\n",
      "65: {'fold_0': 0.7094607949392564, 'fold_1': 0.7230290670638141, 'fold_2': 0.6783695459896374, 'fold_3': 0.7788677755384437, 'fold_4': 0.720958535143603, 'MSE': 0.7224854889136884, 'R2': 0.8810168150533519}'\n",
      "66: {'fold_0': 0.7093284207378824, 'fold_1': 0.7229726850426218, 'fold_2': 0.6782088982249987, 'fold_3': 0.7787108593227026, 'fold_4': 0.7206705353994444, 'MSE': 0.7223268709495932, 'R2': 0.8810429371981526}'\n",
      "67: {'fold_0': 0.7094135079063381, 'fold_1': 0.7228875714233821, 'fold_2': 0.6783986101285735, 'fold_3': 0.7788270883301249, 'fold_4': 0.7206956723732351, 'MSE': 0.7223930486837409, 'R2': 0.8810320386573484}'\n",
      "68: {'fold_0': 0.7093775339633598, 'fold_1': 0.7228734781314851, 'fold_2': 0.6784099320602602, 'fold_3': 0.7788303239183219, 'fold_4': 0.7206995977034175, 'MSE': 0.7223868556981371, 'R2': 0.8810330585548588}'\n",
      "69: {'fold_0': 0.7097070637132271, 'fold_1': 0.722969809155835, 'fold_2': 0.6782831096898214, 'fold_3': 0.7790367683315649, 'fold_4': 0.7205907817356266, 'MSE': 0.7224662704991841, 'R2': 0.8810199800555596}'\n",
      "70: {'fold_0': 0.7098342977908083, 'fold_1': 0.7229308604558806, 'fold_2': 0.6782850311165939, 'fold_3': 0.7790203152480969, 'fold_4': 0.7203919568368783, 'MSE': 0.7224409536300532, 'R2': 0.881024149387357}'\n",
      "71: {'fold_0': 0.7098409528357863, 'fold_1': 0.7229770148790594, 'fold_2': 0.6782147935363725, 'fold_3': 0.7790702285348372, 'fold_4': 0.7204719062995754, 'MSE': 0.7224636242474787, 'R2': 0.8810204158559471}'\n",
      "72: {'fold_0': 0.7097561934123456, 'fold_1': 0.7227355295886426, 'fold_2': 0.6781017907983391, 'fold_3': 0.7789897370555419, 'fold_4': 0.7203438351544591, 'MSE': 0.722333968384517, 'R2': 0.881041768350577}'\n",
      "73: {'fold_0': 0.7097514628160398, 'fold_1': 0.7225411182645011, 'fold_2': 0.6784365039158584, 'fold_3': 0.7791434687389175, 'fold_4': 0.7206055771751873, 'MSE': 0.7224440685159029, 'R2': 0.8810236364095257}'\n",
      "74: {'fold_0': 0.7098592348677172, 'fold_1': 0.7227926408585094, 'fold_2': 0.6784826169133147, 'fold_3': 0.7793476271178437, 'fold_4': 0.7207715555924831, 'MSE': 0.7225997634331909, 'R2': 0.8809979956493676}'\n",
      "75: {'fold_0': 0.7099259261083624, 'fold_1': 0.7227240762049447, 'fold_2': 0.6787441456696267, 'fold_3': 0.7792778961708613, 'fold_4': 0.7208236739448297, 'MSE': 0.7226472586795731, 'R2': 0.8809901738511847}'\n",
      "76: {'fold_0': 0.7095765300059955, 'fold_1': 0.7227381795919469, 'fold_2': 0.6790167284572365, 'fold_3': 0.7792938072774521, 'fold_4': 0.7207258816733858, 'MSE': 0.7226197869293535, 'R2': 0.8809946980615493}'\n",
      "77: {'fold_0': 0.7095089741210587, 'fold_1': 0.7227951602269763, 'fold_2': 0.6790232319094518, 'fold_3': 0.7794237925372712, 'fold_4': 0.7207259080936368, 'MSE': 0.7226459263261643, 'R2': 0.880990393271027}'\n",
      "78: {'fold_0': 0.7094044176612048, 'fold_1': 0.7227023834858219, 'fold_2': 0.6793412236133922, 'fold_3': 0.7794187451465437, 'fold_4': 0.7206856027424353, 'MSE': 0.722661131069666, 'R2': 0.8809878892639074}'\n",
      "79: {'fold_0': 0.7093791267432935, 'fold_1': 0.7226967901159731, 'fold_2': 0.6794891382846626, 'fold_3': 0.7793891088691545, 'fold_4': 0.7207214487668351, 'MSE': 0.7226855854313324, 'R2': 0.8809838619749424}'\n",
      "80: {'fold_0': 0.709421215046626, 'fold_1': 0.722674792769413, 'fold_2': 0.6794588364937264, 'fold_3': 0.7794436319425739, 'fold_4': 0.7207564103511944, 'MSE': 0.7227014709664269, 'R2': 0.8809812458510352}'\n",
      "81: {'fold_0': 0.7094183699293635, 'fold_1': 0.7226808694336391, 'fold_2': 0.6794689145848741, 'fold_3': 0.7795115694263932, 'fold_4': 0.7208858843318519, 'MSE': 0.7227437447447417, 'R2': 0.8809742839551067}'\n",
      "82: {'fold_0': 0.7094163843060898, 'fold_1': 0.7226803253600187, 'fold_2': 0.6794779518103818, 'fold_3': 0.779571064485248, 'fold_4': 0.7207335071448886, 'MSE': 0.7227270005526619, 'R2': 0.8809770414877265}'\n",
      "83: {'fold_0': 0.7094095428909806, 'fold_1': 0.7227169535586337, 'fold_2': 0.6795117795153623, 'fold_3': 0.7796099017928534, 'fold_4': 0.7206986781636622, 'MSE': 0.722740820325653, 'R2': 0.8809747655657447}'\n",
      "84: {'fold_0': 0.7094480717076858, 'fold_1': 0.7226991185114485, 'fold_2': 0.6795419130061974, 'fold_3': 0.7795444865770521, 'fold_4': 0.7208808138774042, 'MSE': 0.7227735355636304, 'R2': 0.8809693778267929}'\n",
      "85: {'fold_0': 0.7094605730395624, 'fold_1': 0.7227004758824918, 'fold_2': 0.6796600054849342, 'fold_3': 0.7794279860321625, 'fold_4': 0.7207542573688074, 'MSE': 0.7227508463690535, 'R2': 0.8809731144175978}'\n",
      "86: {'fold_0': 0.7094457375959111, 'fold_1': 0.7227151069056819, 'fold_2': 0.6797129567174809, 'fold_3': 0.7795297926818624, 'fold_4': 0.7206617064584906, 'MSE': 0.7227639274449001, 'R2': 0.8809709601486324}'\n",
      "87: {'fold_0': 0.7093470298853857, 'fold_1': 0.7227035744843564, 'fold_2': 0.6797047159972782, 'fold_3': 0.7796991516307656, 'fold_4': 0.7208330275559773, 'MSE': 0.7228092733068325, 'R2': 0.8809634923237977}'\n",
      "88: {'fold_0': 0.7093055101817832, 'fold_1': 0.7226491624189316, 'fold_2': 0.6797914581019253, 'fold_3': 0.7794654539461658, 'fold_4': 0.7209068009623893, 'MSE': 0.7227742568848128, 'R2': 0.8809692590353527}'\n",
      "89: {'fold_0': 0.709236326821753, 'fold_1': 0.7226492863558626, 'fold_2': 0.6798433484937964, 'fold_3': 0.7794672419394661, 'fold_4': 0.7208716093655253, 'MSE': 0.7227644454770513, 'R2': 0.8809708748360328}'\n",
      "90: {'fold_0': 0.7091646769486696, 'fold_1': 0.7227411440266963, 'fold_2': 0.6798289652435734, 'fold_3': 0.7795107107107309, 'fold_4': 0.7208523182325128, 'MSE': 0.7227710874780093, 'R2': 0.8809697809920172}'\n",
      "91: {'fold_0': 0.7091795764016068, 'fold_1': 0.7228172599400825, 'fold_2': 0.6798726212976239, 'fold_3': 0.7796231325036516, 'fold_4': 0.720761031883728, 'MSE': 0.7228029468414627, 'R2': 0.8809645342035457}'\n",
      "92: {'fold_0': 0.7091939943957873, 'fold_1': 0.7228185837894919, 'fold_2': 0.6798780391815187, 'fold_3': 0.7796494007590677, 'fold_4': 0.7209221933767734, 'MSE': 0.7228444741350624, 'R2': 0.8809576952431367}'\n",
      "93: {'fold_0': 0.7091995451060946, 'fold_1': 0.7228136120636359, 'fold_2': 0.6798544444990836, 'fold_3': 0.7797400028378568, 'fold_4': 0.7208817503411643, 'MSE': 0.722850386343886, 'R2': 0.8809567215856077}'\n",
      "94: {'fold_0': 0.7092126915969412, 'fold_1': 0.7228006989354478, 'fold_2': 0.6798385275848071, 'fold_3': 0.7797405150286877, 'fold_4': 0.7207543056632981, 'MSE': 0.7228220120712427, 'R2': 0.8809613944287048}'\n",
      "95: {'fold_0': 0.7091539087714552, 'fold_1': 0.7227975167052755, 'fold_2': 0.6798512427462569, 'fold_3': 0.7797249086053155, 'fold_4': 0.7208042589406414, 'MSE': 0.7228190928407828, 'R2': 0.8809618751848488}'\n",
      "96: {'fold_0': 0.7091507195729062, 'fold_1': 0.7228584245642196, 'fold_2': 0.679865157797056, 'fold_3': 0.7799320851579447, 'fold_4': 0.720869143505042, 'MSE': 0.7228887993797696, 'R2': 0.8809503954995852}'\n",
      "97: {'fold_0': 0.709166553862136, 'fold_1': 0.7228833089913344, 'fold_2': 0.6798487805540999, 'fold_3': 0.7799820418771835, 'fold_4': 0.7207969547787829, 'MSE': 0.7228895557779421, 'R2': 0.8809502709314588}'\n",
      "98: {'fold_0': 0.7091765049816587, 'fold_1': 0.722869064757155, 'fold_2': 0.6798813091872689, 'fold_3': 0.7799988498190153, 'fold_4': 0.7202855500602687, 'MSE': 0.7227971097232778, 'R2': 0.880965495494704}'\n",
      "99: {'fold_0': 0.7091717242872025, 'fold_1': 0.7228635568022634, 'fold_2': 0.6798814769587873, 'fold_3': 0.7804038700668031, 'fold_4': 0.7205327202962446, 'MSE': 0.7229270728671319, 'R2': 0.8809440923952387}'\n",
      "100: {'fold_0': 0.7090711669075844, 'fold_1': 0.7228330951637465, 'fold_2': 0.6799072718920205, 'fold_3': 0.7800358033364131, 'fold_4': 0.7212108901434919, 'MSE': 0.7229655366829559, 'R2': 0.8809377579464752}'\n",
      "Selected pls preprocessing with 59 components'\n"
     ]
    }
   ],
   "source": [
    "# set logging, in this case the root logger\n",
    "ut.setup_logger(logger_name=\"log\",file_name=log_dir/\"log.txt\")\n",
    "ut.setup_logger(logger_name=\"test_log\",file_name=log_dir/\"test_log.txt\")\n",
    "ut.setup_logger(logger_name=\"summary\",file_name=log_dir/\"summary.txt\")\n",
    "tb = SummaryWriter(log_dir/\"tb\")\n",
    "summary_logger = logging.getLogger(\"summary\")\n",
    "#step 1, run pls, set up pls - that runs best\n",
    "pls_models = {i:PLSRegression(n_components=i) for i in n_comps}\n",
    "\n",
    "pls_scheme = SKLearnScheme(logger=\"log\")\n",
    "scores_pls, preds_pls, model_states_pls , train_time_pls, test_time_pls,_ = eval.evaluate(pls_models,dataset,pls_scheme,logger_name=\"log\")\n",
    "summary_logger.info(f\"Train times: {train_time_pls}\")\n",
    "summary_logger.info(f\"Test times: {test_time_pls}\")\n",
    "from collections import defaultdict\n",
    "summary_logger.info(f\"Scores: {scores_pls}\")\n",
    "for key,value in flip_dicts(scores_pls).items():\n",
    "    summary_logger.info(f\"{key}: {value}\")\n",
    "\n",
    "selected_comps = min(scores_pls[\"MSE\"],key=scores_pls[\"MSE\"].get)\n",
    "summary_logger.info(f\"Selected pls preprocessing with {selected_comps} components\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval.preprocessing= PLSRegression(n_components=selected_comps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the deep learners\n",
    "The following cells setup our models and run a train-test evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% setup experiment\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Experiment'\n",
      "Seed: 1'\n",
      "bs: 32'\n",
      "epochs: 2'\n",
      "--------------------'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 5879 - Val 1929 - Test 1905-----------------------------------'\n",
      "Training extractors on 5879 instances, validating on 1929 instances, for 2 epochs'\n",
      "\n",
      "--- EPOCH 0---'\n",
      "Extractor Train Losses are random_0:nan(-0.091585),random_1:nan(-0.606802)'\n",
      "Tested (val) on 1929 instances with mean losses of: random_0:nan,random_1:nan'\n",
      "Testing (val) took 0:00:00.354003'\n",
      "Epoch 0 finished in 0:00:01.735001 '\n",
      "\n",
      "--- EPOCH 1---'\n",
      "Extractor Train Losses are random_0:40.8615(-2.981865),random_1:186.2757(-3.310007)'\n",
      "Tested (val) on 1929 instances with mean losses of: random_0:5.0672,random_1:18.7876'\n",
      "Testing (val) took 0:00:00.381999'\n",
      "Epoch 1 finished in 0:00:01.773000 '\n",
      "\n",
      "-----------'\n",
      "Finished training extractors with a best validation loss of random_0:5.0672,random_1:18.7876'\n",
      "Training took 0:00:03.514000'\n",
      "Tested (test) on 1905 instances with mean losses of: random_0:5.5036,random_1:19.0614'\n",
      "Testing (test) took 0:00:00.368998'\n",
      "-----------------------------------Fold 1 - Train 5855 - Val 1903 - Test 1955-----------------------------------'\n",
      "Training extractors on 5855 instances, validating on 1903 instances, for 2 epochs'\n",
      "\n",
      "--- EPOCH 0---'\n",
      "Extractor Train Losses are random_0:nan(-0.091585),random_1:nan(-0.606802)'\n",
      "Tested (val) on 1903 instances with mean losses of: random_0:nan,random_1:nan'\n",
      "Testing (val) took 0:00:00.385996'\n",
      "Epoch 0 finished in 0:00:01.779001 '\n",
      "\n",
      "--- EPOCH 1---'\n",
      "Extractor Train Losses are random_0:81.037(-3.311366),random_1:222.7863(-3.395458)'\n",
      "Tested (val) on 1903 instances with mean losses of: random_0:9.4022,random_1:122.4809'\n",
      "Testing (val) took 0:00:00.374999'\n",
      "Epoch 1 finished in 0:00:01.720000 '\n",
      "\n",
      "-----------'\n",
      "Finished training extractors with a best validation loss of random_0:9.4022,random_1:122.4809'\n",
      "Training took 0:00:03.504001'\n",
      "Tested (test) on 1955 instances with mean losses of: random_0:8.9555,random_1:120.9327'\n",
      "Testing (test) took 0:00:00.381000'\n",
      "-----------------------------------Fold 2 - Train 5821 - Val 1955 - Test 1937-----------------------------------'\n",
      "Training extractors on 5821 instances, validating on 1955 instances, for 2 epochs'\n",
      "\n",
      "--- EPOCH 0---'\n",
      "Extractor Train Losses are random_0:nan(-0.091585),random_1:nan(-0.606802)'\n",
      "Tested (val) on 1955 instances with mean losses of: random_0:nan,random_1:nan'\n",
      "Testing (val) took 0:00:00.364996'\n",
      "Epoch 0 finished in 0:00:01.774999 '\n",
      "\n",
      "--- EPOCH 1---'\n",
      "Extractor Train Losses are random_0:40.6102(-2.960828),random_1:29.9878(-2.168163)'\n",
      "Tested (val) on 1955 instances with mean losses of: random_0:5.9064,random_1:4.6155'\n",
      "Testing (val) took 0:00:00.384998'\n",
      "Epoch 1 finished in 0:00:01.774001 '\n",
      "\n",
      "-----------'\n",
      "Finished training extractors with a best validation loss of random_0:5.9064,random_1:4.6155'\n",
      "Training took 0:00:03.553002'\n",
      "Tested (test) on 1937 instances with mean losses of: random_0:6.0958,random_1:4.8498'\n",
      "Testing (test) took 0:00:00.390997'\n",
      "-----------------------------------Fold 3 - Train 5787 - Val 1937 - Test 1989-----------------------------------'\n",
      "Training extractors on 5787 instances, validating on 1937 instances, for 2 epochs'\n",
      "\n",
      "--- EPOCH 0---'\n",
      "Extractor Train Losses are random_0:nan(-0.091585),random_1:nan(-0.606802)'\n",
      "Tested (val) on 1937 instances with mean losses of: random_0:nan,random_1:nan'\n",
      "Testing (val) took 0:00:00.363998'\n",
      "Epoch 0 finished in 0:00:01.746998 '\n",
      "\n",
      "--- EPOCH 1---'\n",
      "Extractor Train Losses are random_0:125.1498(-3.534515),random_1:262.6227(-3.992505)'\n",
      "Tested (val) on 1937 instances with mean losses of: random_0:25.6554,random_1:250.1167'\n",
      "Testing (val) took 0:00:00.381997'\n",
      "Epoch 1 finished in 0:00:01.770002 '\n",
      "\n",
      "-----------'\n",
      "Finished training extractors with a best validation loss of random_0:25.6554,random_1:250.1167'\n",
      "Training took 0:00:03.522001'\n",
      "Tested (test) on 1989 instances with mean losses of: random_0:27.0574,random_1:248.5596'\n",
      "Testing (test) took 0:00:00.389000'\n",
      "-----------------------------------Fold 4 - Train 5797 - Val 1989 - Test 1927-----------------------------------'\n",
      "Training extractors on 5797 instances, validating on 1989 instances, for 2 epochs'\n",
      "\n",
      "--- EPOCH 0---'\n",
      "Extractor Train Losses are random_0:nan(-0.091585),random_1:nan(-0.606802)'\n",
      "Tested (val) on 1989 instances with mean losses of: random_0:nan,random_1:nan'\n",
      "Testing (val) took 0:00:00.394004'\n",
      "Epoch 0 finished in 0:00:01.770999 '\n",
      "\n",
      "--- EPOCH 1---'\n",
      "Extractor Train Losses are random_0:nan(-1.075139),random_1:213.6509(-3.368814)'\n",
      "Tested (val) on 1989 instances with mean losses of: random_0:nan,random_1:32.8131'\n",
      "Testing (val) took 0:00:00.409004'\n",
      "Epoch 1 finished in 0:00:01.756000 '\n",
      "\n",
      "-----------'\n",
      "Finished training extractors with a best validation loss of random_0:inf,random_1:32.8131'\n",
      "Training took 0:00:03.532000'\n",
      "Tested (test) on 1927 instances with mean losses of: random_0:273.0034,random_1:31.6362'\n",
      "Testing (test) took 0:00:00.383000'\n",
      "Train times: {'fold_0': 3, 'fold_1': 3, 'fold_2': 3, 'fold_3': 3, 'fold_4': 3, 'mean': 3.0}'\n",
      "Test times: {'fold_0': 0, 'fold_1': 0, 'fold_2': 0, 'fold_3': 0, 'fold_4': 0, 'mean': 0.0}'\n"
     ]
    }
   ],
   "source": [
    "n_models = 100\n",
    "epochs = 100\n",
    "bs = 32\n",
    "fixed_hyperparams = {'bs': bs,'loss': nn.MSELoss(),'epochs': epochs}\n",
    "device = \"cpu\" #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#setup models\n",
    "config_gen = RandomConfigGen(lr= (0,1),\n",
    "                             allow_increase_size=False,\n",
    "                             n_features=selected_comps,\n",
    "                             opt=[torch.optim.SGD,\n",
    "                                  torch.optim.Adam],\n",
    "                             lr_update = [None,\n",
    "                                          torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                                          torch.optim.lr_scheduler.ExponentialLR,\n",
    "                                          torch.optim.lr_scheduler.CosineAnnealingLR],\n",
    "                            dropout = [True,False],\n",
    "                            batch_norm = [True,False])\n",
    "configs = {f\"random_{i}\":config_gen.sample() for i in range(n_models)}\n",
    "config_gen.save(log_dir/'config_gen.txt')\n",
    "\n",
    "deep_models = {name:RandomNet(input_size=selected_comps,\n",
    "                             n_layers=config.n_layers,\n",
    "                             act_function=config.act_function,\n",
    "                             n_features = config.n_features,\n",
    "                             dropout=config.dropout,\n",
    "                             batch_norm=config.batch_norm,\n",
    "                             device=device,dtype=torch.float)\n",
    "              for name, config in configs.items()}\n",
    "\n",
    "ex.write_summary_head(seed,fixed_hyperparams)\n",
    "ex.save_models(deep_models,configs,log_dir)\n",
    "start = datetime.datetime.now()\n",
    "deep_scheme = DeepScheme(configs,fixed_hyperparams=fixed_hyperparams,logger=\"log\",device=device,adaptive_lr=True)\n",
    "scores_deep, preds_deep, model_states_deep , train_time_deep, test_time_deep, pp_states = eval.evaluate(deep_models,dataset,deep_scheme,logger_name=\"log\")\n",
    "\n",
    "\n",
    "summary_logger.info(f\"Train times: {train_time_deep}\")\n",
    "summary_logger.info(f\"Test times: {test_time_deep}\")\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building final model - Train 7413 - Test 1448'\n",
      "Training extractors on 7413 instances, validating on 2830 instances, for 2 epochs'\n",
      "\n",
      "--- EPOCH 0---'\n",
      "Extractor Train Losses are random_0:nan(-0.091585),random_1:nan(-0.606802)'\n",
      "Tested (val) on 2830 instances with mean losses of: random_0:nan,random_1:nan'\n",
      "Testing (val) took 0:00:00.566000'\n",
      "Epoch 0 finished in 0:00:02.341000 '\n",
      "\n",
      "--- EPOCH 1---'\n",
      "Extractor Train Losses are random_0:nan(-1.975805),random_1:41.7411(-2.697869)'\n",
      "Tested (val) on 2830 instances with mean losses of: random_0:nan,random_1:3.1165'\n",
      "Testing (val) took 0:00:00.572002'\n",
      "Epoch 1 finished in 0:00:02.365999 '\n",
      "\n",
      "-----------'\n",
      "Finished training extractors with a best validation loss of random_0:inf,random_1:3.1165'\n",
      "Training took 0:00:04.713000'\n",
      "Tested (test) on 1448 instances with mean losses of: random_0:270.5512,random_1:3.1818'\n",
      "Testing (test) took 0:00:00.294000'\n"
     ]
    }
   ],
   "source": [
    "scores_deep_final, preds_deep_final, model_states_deep_final , train_time_deep_final, test_time_deep_final,pp_states_final = eval.build(deep_models,dataset,deep_scheme,logger_name=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex.save_pp(pp_states,log_dir)\n",
    "PLSRegression(n_components=selected_comps).save_state(pp_states_final.state(),log_dir / \"preprocessing\"   / f\"_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% log results\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiments took 0:00:27.515998'\n",
      "Finished Random Deep Search'\n",
      "---Loss results---'\n",
      "0 - random_0 - fold_0:5.5036,fold_1:8.9555,fold_2:6.0958,fold_3:27.0574,fold_4:273.0034,MSE:63.8005,R2:-9.507'\n",
      "1 - random_1 - fold_0:19.0614,fold_1:120.9327,fold_2:4.8498,fold_3:248.5596,fold_4:31.6362,MSE:86.2223,R2:-13.1996'\n"
     ]
    }
   ],
   "source": [
    "ex.save_results(model_states_deep, preds_deep,configs, scores_deep, log_dir,tb,prefix=\"\")\n",
    "     \n",
    "for model, state_dict in model_states_deep_final.items():\n",
    "     torch.save(state_dict.state(), log_dir / \"models\" / f\"{model}\" / f\"_final\")\n",
    "        \n",
    "\n",
    "\n",
    "#summary_logger.info(f\"Scores: {scores_deep}\")\n",
    "#for key,value in flip_dicts(scores_deep).items():\n",
    "#    summary_logger.info(f\"{key}: {value}\")\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "diff = end - start\n",
    "ex.write_summary(diff, deep_models, scores_deep,prefix=\"\")\n",
    "ex.save_pred_plots(preds_deep, deep_models,log_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting deep results as a function of number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(scores_deep)\n",
    "scores_df.to_csv(log_dir / f\"scores.csv\", index=False)\n",
    "\n",
    "scores_df_final = pd.DataFrame(scores_deep_final)\n",
    "scores_df_final.to_csv(log_dir / f\"scores_final.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\numpy\\lib\\histograms.py:906: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return n/db/n.sum(), bin_edges\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "if True:\n",
    "    # plot deep results as a function of number of features\n",
    "\n",
    "    n_features_dict = {name:config.n_features for name,config in configs.items()}\n",
    "    to_plot = pd.DataFrame([[name, scores_deep[\"R2\"][name],n_features_dict[name]] for name in scores_deep[\"R2\"].keys()]\n",
    "                           ,columns = [\"name\",\"score\",\"n_features\"])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(to_plot[\"score\"],bins=100,density=True)\n",
    "    #ax.set_xscale('log')\n",
    "    #ax.legend(loc='upper right',bbox_to_anchor=(1.4, 1))\n",
    "    ax.set_xlabel(\"Number of Models\")\n",
    "    ax.set_ylabel(\"R2\")\n",
    "    ax.set_title(\"Distribution of R2 Scoes\")\n",
    "    plt.savefig(log_dir / f\"dist_plot.png\",bbox_inches='tight')\n",
    "    #plt.savefig(log_dir / f\"pp_deep_pls_compressed.png\",bbox_inches='tight')\n",
    "\n",
    "    n_features_dict = {name:config.n_features for name,config in configs.items()}\n",
    "    to_plot = pd.DataFrame([[name, scores_deep[\"R2\"][name],n_features_dict[name]] for name in scores_deep[\"R2\"].keys()]\n",
    "                           ,columns = [\"name\",\"score\",\"n_features\"])\n",
    "    to_plot = to_plot[to_plot[\"score\"]>=0]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(to_plot[\"score\"],bins=100,density=True)\n",
    "    #ax.set_xscale('log')\n",
    "    #ax.legend(loc='upper right',bbox_to_anchor=(1.4, 1))\n",
    "    ax.set_xlabel(\"Number of Models\")\n",
    "    ax.set_ylabel(\"R2\")\n",
    "    ax.set_title(\"Distribution of R2 Scoes\")\n",
    "    plt.savefig(log_dir / f\"dist_plot_compressed.png\",bbox_inches='tight')\n",
    "    #plt.savefig(log_dir / f\"pp_deep_pls_compressed.png\",bbox_inches='tight')\n",
    "    pass\n",
    "\n",
    "    #plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% Return our best models\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      " Top 5 performance on Test Set'\n",
      "Index - Model - Val MSE - Val R2 - Test MSE - Test R2'\n",
      "0 - random_0 - 63.80053504755648 --9.507049578953044 - 63.80053504755648 - 270.5511969901643 - -45.134402035975945'\n",
      "1 - random_1 - 86.22232620004317 --13.199602801469181 - 86.22232620004317 - 3.181777107121291 - 0.45744322744898247'\n"
     ]
    }
   ],
   "source": [
    "summary_logger.info(\"------------------\\n Top 5 performance on Test Set\")\n",
    "summary_logger.info(f\"Index - Model - Val MSE - Val R2 - Test MSE - Test R2\")\n",
    "for i,key in enumerate(sorted(scores_deep['MSE'],key=scores_deep['MSE'].get)):\n",
    "    if i <5:\n",
    "        summary_logger.info(f\"{i} - {key} - {scores_deep['MSE'][key]} -{scores_deep['R2'][key]} - {scores_deep['MSE'][key]} - {scores_deep_final['MSE'][key]} - {scores_deep_final['R2'][key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 5879 - Val 1929 - Test 1905-----------------------------------'\n",
      "Tested (test) on 1905 instances with mean losses of: random_0:5.5036,random_1:19.0614'\n",
      "Testing (test) took 0:00:00.382003'\n",
      "-----------------------------------Fold 1 - Train 5855 - Val 1903 - Test 1955-----------------------------------'\n",
      "Tested (test) on 1955 instances with mean losses of: random_0:8.9555,random_1:120.9327'\n",
      "Testing (test) took 0:00:00.421003'\n",
      "-----------------------------------Fold 2 - Train 5821 - Val 1955 - Test 1937-----------------------------------'\n",
      "Tested (test) on 1937 instances with mean losses of: random_0:6.0958,random_1:4.8498'\n",
      "Testing (test) took 0:00:00.403000'\n",
      "-----------------------------------Fold 3 - Train 5787 - Val 1937 - Test 1989-----------------------------------'\n",
      "Tested (test) on 1989 instances with mean losses of: random_0:27.0574,random_1:248.5596'\n",
      "Testing (test) took 0:00:00.394001'\n",
      "-----------------------------------Fold 4 - Train 5797 - Val 1989 - Test 1927-----------------------------------'\n",
      "Tested (test) on 1927 instances with mean losses of: random_0:273.0034,random_1:31.6362'\n",
      "Testing (test) took 0:00:00.394999'\n"
     ]
    }
   ],
   "source": [
    "model_path = Path('D:/workspace/lazydeep/experiments/1.01/')\n",
    "log_path = Path(\"D:/workspace/lazydeep/experiments/1.02\")\n",
    "\n",
    "log_dir = log_path / re.sub(r'\\.(?=csv$)[^.]+$', '',file_name)\n",
    "model_dir = model_path / re.sub(r'\\.(?=csv$)[^.]+$', '',file_name)\n",
    "\n",
    "if not log_dir.exists():\n",
    "    log_dir.mkdir()\n",
    "    \n",
    "ut.setup_logger(logger_name=\"\",file_name=log_dir/\"log.txt\")\n",
    "ut.setup_logger(logger_name=\"summary2\",file_name=log_dir/\"summary.txt\")\n",
    "summary_logger = logging.getLogger(\"summary2\")\n",
    "tb = SummaryWriter(log_dir/\"tb\")\n",
    "\n",
    "    \n",
    "    \n",
    "deep_scores_dict={}\n",
    "deep_preds_dict={}\n",
    "actual_y = None\n",
    "preprocessing=PLSRegression(n_components=selected_comps)\n",
    "\n",
    "load_fun_cv = lambda name,model, fold : model.load_state(model_dir/'models'/name/f\"_fold_{fold}\")\n",
    "load_fun_pp_cv = lambda fold : preprocessing.from_state(preprocessing.load_state(model_dir/'preprocessing'/f\"_fold_{fold}\"))\n",
    "load_fun_build = lambda name,model : model.load_state(model_dir/'models'/name/f\"_final\")\n",
    "load_fun_pp_build = lambda : preprocessing.from_state(preprocessing.load_state(model_dir/'preprocessing'/f\"_final\"))\n",
    "\n",
    "deep_scheme = DeepScheme(configs, fixed_hyperparams=fixed_hyperparams,loss_eval=loss_target,device=device,tensorboard=tb,adaptive_lr=False,update=False)\n",
    "deep_scores, deep_preds, _ , _, _,_ = eval.evaluate(deep_models,dataset,deep_scheme,logger_name=\"log\",load_fun=load_fun_cv,load_fun_pp=load_fun_pp_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building final model - Train 7413 - Test 1448'\n",
      "Tested (test) on 1448 instances with mean losses of: random_0:270.5512,random_1:3.1818'\n",
      "Testing (test) took 0:00:00.290999'\n"
     ]
    }
   ],
   "source": [
    "deep_scores_final, deep_preds_final, _ ,_, _,_ = eval.build(deep_models,dataset,deep_scheme,logger_name=\"test_log\",load_fun=load_fun_build,load_fun_pp=load_fun_pp_build)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - random_0 - deep - 5.503632525196226 - 8.955492423562443 - 6.095810025145728 - 27.057358468939395 - 273.0033813856646 - 63.80053504755648 - -9.507049578953044\n",
      "1 - random_1 - deep - 19.061410476404227 - 120.93269432438609 - 4.84977889947197 - 248.55960121365754 - 31.636215629637025 - 86.22232620004317 - -13.199602801469181\n"
     ]
    }
   ],
   "source": [
    "all_scores = []\n",
    "for k,v in ut.flip_dicts(deep_scores).items():\n",
    "    dict1 = {'model_num':k,\"predictor\":\"deep\"}\n",
    "    all_scores.append({**dict1,**v})\n",
    "    \n",
    "all_scores_final = []\n",
    "for k,v in ut.flip_dicts(deep_scores_final).items():\n",
    "    dict1 = {'model_num':k,\"predictor\":\"deep\"}\n",
    "    all_scores_final.append({**dict1,**v})  \n",
    "\n",
    "scores_df_sorted = pd.DataFrame(all_scores).sort_values(by='MSE')\n",
    "for i,(index,row) in enumerate(scores_df_sorted.iterrows()):\n",
    "    s = f\"{i} - \" + \" - \".join([f\"{i}\" for i in row.tolist()])\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_0'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 5879 - Val 1929 - Test 1905-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:1.9768,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0007,lwr_k=50:0.0216,lwr_k=100:0.3398,lwr_k=200:0.5565,lwr_k=300:0.6854,lwr_k=400:0.7966,lwr_k=500:0.8642,lwr_k=600:0.9318,lwr_k=700:0.9862,lwr_k=800:1.0324,lwr_k=900:1.0759,lwr_k=1000:1.1187'\n",
      "Tested (test) on 1905 instances with mean losses of: lr:2.0655,lwr_k=10:2.6799,lwr_k=20:3.6161,lwr_k=30:51.9147,lwr_k=40:117.7345,lwr_k=50:35528.3602,lwr_k=100:13.076,lwr_k=200:144.8676,lwr_k=300:6.7638,lwr_k=400:6.9832,lwr_k=500:7.566,lwr_k=600:3.3229,lwr_k=700:3.3125,lwr_k=800:1.2572,lwr_k=900:1.7083,lwr_k=1000:3.1208'\n",
      "-----------------------------------Fold 1 - Train 5855 - Val 1903 - Test 1955-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:2.6224,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0042,lwr_k=50:0.0369,lwr_k=100:0.4163,lwr_k=200:0.6384,lwr_k=300:0.8031,lwr_k=400:0.9307,lwr_k=500:1.0395,lwr_k=600:1.1296,lwr_k=700:1.2181,lwr_k=800:1.2838,lwr_k=900:1.3485,lwr_k=1000:1.4124'\n",
      "Tested (test) on 1955 instances with mean losses of: lr:2.4437,lwr_k=10:3.2969,lwr_k=20:5.5466,lwr_k=30:141.6553,lwr_k=40:7383.0781,lwr_k=50:118443.7189,lwr_k=100:336772.7754,lwr_k=200:3.2864,lwr_k=300:3.2411,lwr_k=400:1.485,lwr_k=500:1.3961,lwr_k=600:1.446,lwr_k=700:1.5446,lwr_k=800:1.5772,lwr_k=900:1.664,lwr_k=1000:1.7457'\n",
      "-----------------------------------Fold 2 - Train 5821 - Val 1955 - Test 1937-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:1.9639,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0009,lwr_k=50:0.0225,lwr_k=100:0.354,lwr_k=200:0.5801,lwr_k=300:0.7206,lwr_k=400:0.8303,lwr_k=500:0.9078,lwr_k=600:0.9696,lwr_k=700:1.0266,lwr_k=800:1.0752,lwr_k=900:1.1226,lwr_k=1000:1.1674'\n",
      "Tested (test) on 1937 instances with mean losses of: lr:2.0731,lwr_k=10:2.2611,lwr_k=20:3.5872,lwr_k=30:31.5361,lwr_k=40:816.5173,lwr_k=50:11239.1234,lwr_k=100:33817.4389,lwr_k=200:70.7841,lwr_k=300:26.7755,lwr_k=400:7.0955,lwr_k=500:171.1269,lwr_k=600:109.6631,lwr_k=700:1.1656,lwr_k=800:1.1908,lwr_k=900:1.2111,lwr_k=1000:1.2529'\n",
      "-----------------------------------Fold 3 - Train 5787 - Val 1937 - Test 1989-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:2.5562,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0001,lwr_k=40:0.0196,lwr_k=50:0.0731,lwr_k=100:0.4913,lwr_k=200:0.7706,lwr_k=300:0.9782,lwr_k=400:1.062,lwr_k=500:1.1588,lwr_k=600:1.2489,lwr_k=700:1.343,lwr_k=800:1.4227,lwr_k=900:1.5053,lwr_k=1000:1.5829'\n",
      "Tested (test) on 1989 instances with mean losses of: lr:2.7536,lwr_k=10:3.8181,lwr_k=20:6.8625,lwr_k=30:51.8273,lwr_k=40:28.3604,lwr_k=50:1495.1375,lwr_k=100:107745.9303,lwr_k=200:2.4285,lwr_k=300:1.5897,lwr_k=400:1.568,lwr_k=500:1.4848,lwr_k=600:1.5522,lwr_k=700:1.64,lwr_k=800:1.7035,lwr_k=900:1.7697,lwr_k=1000:1.8342'\n",
      "-----------------------------------Fold 4 - Train 5797 - Val 1989 - Test 1927-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:3.0496,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.0095,lwr_k=50:0.0402,lwr_k=100:0.3348,lwr_k=200:0.6342,lwr_k=300:0.8092,lwr_k=400:0.943,lwr_k=500:1.0367,lwr_k=600:1.1263,lwr_k=700:1.1959,lwr_k=800:1.264,lwr_k=900:1.3245,lwr_k=1000:1.3734'\n",
      "Tested (test) on 1927 instances with mean losses of: lr:3.2808,lwr_k=10:2.2287,lwr_k=20:3.6915,lwr_k=30:21.9977,lwr_k=40:1048.3653,lwr_k=50:999.2724,lwr_k=100:735.5145,lwr_k=200:3401.5581,lwr_k=300:144.7598,lwr_k=400:1.2558,lwr_k=500:1.2842,lwr_k=600:1.3218,lwr_k=700:1.3888,lwr_k=800:1.4685,lwr_k=900:1.5157,lwr_k=1000:1.5894'\n",
      "Building final model - Train 7413 - Test 1448'\n",
      "Finished training DeepLWR with a train loss of lr:3.0394,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0001,lwr_k=40:0.0085,lwr_k=50:0.037,lwr_k=100:0.2926,lwr_k=200:0.5574,lwr_k=300:0.7322,lwr_k=400:0.8405,lwr_k=500:0.9364,lwr_k=600:1.0195,lwr_k=700:1.1004,lwr_k=800:1.1705,lwr_k=900:1.2246,lwr_k=1000:1.2652'\n",
      "Tested (test) on 1448 instances with mean losses of: lr:2.9949,lwr_k=10:1.619,lwr_k=20:2.9641,lwr_k=30:14.9392,lwr_k=40:14.1927,lwr_k=50:59.7487,lwr_k=100:1.6474,lwr_k=200:1.1333,lwr_k=300:1.008,lwr_k=400:1.1096,lwr_k=500:1.177,lwr_k=600:1.1976,lwr_k=700:1.2545,lwr_k=800:1.3164,lwr_k=900:1.3706,lwr_k=1000:1.4036'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\stats.py:3845: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_1'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 5879 - Val 1929 - Test 1905-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.4726,lwr_k=10:0.0,lwr_k=20:0.0003,lwr_k=30:0.3783,lwr_k=40:0.6781,lwr_k=50:0.8332,lwr_k=100:3.0869,lwr_k=200:3.9087,lwr_k=300:2.5873,lwr_k=400:2.7311,lwr_k=500:2.7651,lwr_k=600:2.8619,lwr_k=700:2.9314,lwr_k=800:3.0052,lwr_k=900:3.0554,lwr_k=1000:3.1796'\n",
      "Tested (test) on 1905 instances with mean losses of: lr:4.5164,lwr_k=10:238.1665,lwr_k=20:600.4701,lwr_k=30:126408.3777,lwr_k=40:82237.3768,lwr_k=50:129804.9218,lwr_k=100:6010.4785,lwr_k=200:5.3204,lwr_k=300:3.6019,lwr_k=400:3.5836,lwr_k=500:3.3615,lwr_k=600:3.2101,lwr_k=700:3.1599,lwr_k=800:3.1377,lwr_k=900:3.1338,lwr_k=1000:3.2323'\n",
      "-----------------------------------Fold 1 - Train 5855 - Val 1903 - Test 1955-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:3.9594,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.4908,lwr_k=40:0.5347,lwr_k=50:0.7434,lwr_k=100:1.447,lwr_k=200:2.1234,lwr_k=300:2.5147,lwr_k=400:2.6994,lwr_k=500:2.9475,lwr_k=600:3.0835,lwr_k=700:3.1531,lwr_k=800:3.1787,lwr_k=900:3.2329,lwr_k=1000:3.2757'\n",
      "Tested (test) on 1955 instances with mean losses of: lr:3.8477,lwr_k=10:31.7701,lwr_k=20:6117.5135,lwr_k=30:96580.1975,lwr_k=40:7187.7262,lwr_k=50:4551.932,lwr_k=100:941.4154,lwr_k=200:2.9352,lwr_k=300:3.1923,lwr_k=400:2.9934,lwr_k=500:3.1474,lwr_k=600:3.2498,lwr_k=700:3.309,lwr_k=800:3.322,lwr_k=900:3.3425,lwr_k=1000:3.3705'\n",
      "-----------------------------------Fold 2 - Train 5821 - Val 1955 - Test 1937-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.4483,lwr_k=10:0.0448,lwr_k=20:0.2965,lwr_k=30:0.8558,lwr_k=40:2.0023,lwr_k=50:2.5122,lwr_k=100:3.3415,lwr_k=200:3.8274,lwr_k=300:4.3896,lwr_k=400:3.9383,lwr_k=500:3.7907,lwr_k=600:3.8362,lwr_k=700:3.87,lwr_k=800:4.0195,lwr_k=900:4.0492,lwr_k=1000:4.0911'\n",
      "Tested (test) on 1937 instances with mean losses of: lr:4.4207,lwr_k=10:24490.2383,lwr_k=20:2776718.8798,lwr_k=30:7766377.3578,lwr_k=40:4721.6497,lwr_k=50:12.0738,lwr_k=100:21.7814,lwr_k=200:5.3225,lwr_k=300:5.3517,lwr_k=400:5.1592,lwr_k=500:4.2631,lwr_k=600:4.2249,lwr_k=700:4.2774,lwr_k=800:4.3351,lwr_k=900:4.394,lwr_k=1000:4.4262'\n",
      "-----------------------------------Fold 3 - Train 5787 - Val 1937 - Test 1989-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:3.9897,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.365,lwr_k=40:0.5933,lwr_k=50:0.8088,lwr_k=100:1.125,lwr_k=200:1.487,lwr_k=300:1.7223,lwr_k=400:1.9269,lwr_k=500:2.0876,lwr_k=600:2.2189,lwr_k=700:2.3375,lwr_k=800:2.4367,lwr_k=900:2.5285,lwr_k=1000:2.6158'\n",
      "Tested (test) on 1989 instances with mean losses of: lr:4.5946,lwr_k=10:32.682,lwr_k=20:480.4099,lwr_k=30:681782.3492,lwr_k=40:1492186.3785,lwr_k=50:244747.8474,lwr_k=100:167749.2019,lwr_k=200:7259.6165,lwr_k=300:3322.6083,lwr_k=400:321.4097,lwr_k=500:304.0589,lwr_k=600:220.4129,lwr_k=700:2.8637,lwr_k=800:2.9269,lwr_k=900:2.9076,lwr_k=1000:2.9829'\n",
      "-----------------------------------Fold 4 - Train 5797 - Val 1989 - Test 1927-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.3831,lwr_k=10:0.0,lwr_k=20:0.0006,lwr_k=30:0.4549,lwr_k=40:0.7392,lwr_k=50:1.1921,lwr_k=100:2.1955,lwr_k=200:3.2003,lwr_k=300:3.4079,lwr_k=400:3.7638,lwr_k=500:3.3599,lwr_k=600:3.7562,lwr_k=700:6.0167,lwr_k=800:6.4226,lwr_k=900:5.2743,lwr_k=1000:4.5592'\n",
      "Tested (test) on 1927 instances with mean losses of: lr:4.2422,lwr_k=10:40.3934,lwr_k=20:1803.4439,lwr_k=30:3041665.5518,lwr_k=40:89034.1712,lwr_k=50:38450.0155,lwr_k=100:51648.1128,lwr_k=200:17049.7788,lwr_k=300:19252.5,lwr_k=400:19506.6267,lwr_k=500:4.1073,lwr_k=600:4.8117,lwr_k=700:7.3179,lwr_k=800:6.8436,lwr_k=900:5.2848,lwr_k=1000:4.8893'\n",
      "Building final model - Train 7413 - Test 1448'\n",
      "Finished training DeepLWR with a train loss of lr:5.8643,lwr_k=10:0.0,lwr_k=20:0.0001,lwr_k=30:0.3746,lwr_k=40:0.5567,lwr_k=50:0.7961,lwr_k=100:1.2885,lwr_k=200:1.7075,lwr_k=300:1.9317,lwr_k=400:2.0412,lwr_k=500:2.1581,lwr_k=600:2.1958,lwr_k=700:2.4107,lwr_k=800:2.2811,lwr_k=900:2.3013,lwr_k=1000:2.3117'\n",
      "Tested (test) on 1448 instances with mean losses of: lr:6.5772,lwr_k=10:35.8649,lwr_k=20:165.1559,lwr_k=30:3070777.1397,lwr_k=40:442.7754,lwr_k=50:19.4577,lwr_k=100:261.13,lwr_k=200:2.2108,lwr_k=300:2.1935,lwr_k=400:2.2219,lwr_k=500:2.2873,lwr_k=600:2.3539,lwr_k=700:2.6366,lwr_k=800:2.2373,lwr_k=900:2.2395,lwr_k=1000:2.2607'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\stats.py:3845: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    }
   ],
   "source": [
    "from sk_models import setup_pls_models_exh, StandardScaler, PLSRegression\n",
    "from plot import plot_preds_and_res\n",
    "\n",
    "for deep_name,deep_model in deep_models.items():\n",
    "    logging.getLogger().info(f\"Running model {deep_name}\")\n",
    "    temp_dict = {deep_name:deep_model}\n",
    "\n",
    "    lwr_scheme = DeepLWRScheme_1_to_n(lwr_models = setup_pls_models_exh(nrow),n_neighbours=500,loss_fun_sk = mean_squared_error)\n",
    "    lwr_scores, lwr_preds, _ , _, _,_= eval.evaluate(temp_dict,dataset,lwr_scheme,logger_name=\"log\",load_fun=load_fun_cv,load_fun_pp = load_fun_pp_cv)\n",
    "    lwr_scores_final, lwr_preds_final, _ , _, _,_= eval.build(temp_dict,dataset,lwr_scheme,logger_name=\"test_log\",load_fun=load_fun_build,load_fun_pp = load_fun_pp_build)\n",
    "\n",
    "    #scores\n",
    "    for k,v in ut.flip_dicts(lwr_scores).items():\n",
    "        dict1 = {'model_num':deep_name,\"predictor\":k}\n",
    "        all_scores.append({**dict1,**v})\n",
    "\n",
    "    for k,v in ut.flip_dicts(lwr_scores_final).items():\n",
    "        dict1 = {'model_num':deep_name,\"predictor\":k}\n",
    "        all_scores_final.append({**dict1,**v})\n",
    "\n",
    "    lwr_preds['deep'] = deep_preds[deep_name]\n",
    "    lwr_preds_final['deep'] = deep_preds_final[deep_name]\n",
    "\n",
    "    if not (log_dir/deep_name).exists():\n",
    "        (log_dir/deep_name).mkdir()    \n",
    "    \n",
    "    lwr_preds.to_csv(log_dir/deep_name/ f\"predictions.csv\",index=False)\n",
    "    lwr_preds_final.to_csv(log_dir/deep_name/ f\"predictions_test.csv\",index=False)\n",
    "\n",
    "    #preds\n",
    "    # todo save predictions - appending solns\n",
    "    plot_preds_and_res(lwr_preds,name_lambda=lambda x:f\"{deep_name} with {x} predictor\",save_lambda= lambda x:f\"deep_lwr{x}\",save_loc=log_dir/deep_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank - model_num - predictor - fold_0 - fold_1 - fold_2 - fold_3 - fold_4 - MSE - R2'\n",
      "0 - random_0 - lwr_k=800 - 1.2571931612516503 - 1.5772231719688674 - 1.1907652241735833 - 1.7034587606369873 - 1.4685063360482702 - 1.4416686602573607 - 0.7625774753025107'\n",
      "1 - random_0 - lwr_k=800 - 1.2571931612516503 - 1.5772231719688674 - 1.1907652241735833 - 1.7034587606369873 - 1.4685063360482702 - 1.4416686602573607 - 0.7625774753025107'\n",
      "2 - random_0 - lwr_k=900 - 1.708301955516447 - 1.6639608771138088 - 1.2111363305519127 - 1.76969731277089 - 1.515682657086325 - 1.5745885151448606 - 0.7406874471706788'\n",
      "3 - random_0 - lwr_k=900 - 1.708301955516447 - 1.6639608771138088 - 1.2111363305519127 - 1.76969731277089 - 1.515682657086325 - 1.5745885151448606 - 0.7406874471706788'\n",
      "4 - random_0 - lwr_k=700 - 3.312468650726873 - 1.5445622059367066 - 1.165572541126502 - 1.639965443617589 - 1.388838195882499 - 1.8043620277178987 - 0.7028469856502332'\n",
      "5 - random_0 - lwr_k=700 - 3.312468650726873 - 1.5445622059367066 - 1.165572541126502 - 1.639965443617589 - 1.388838195882499 - 1.8043620277178987 - 0.7028469856502332'\n",
      "6 - random_0 - lwr_k=1000 - 3.1208361898368358 - 1.745681912052799 - 1.2529027474713863 - 1.8341650587825384 - 1.5894300385661548 - 1.9042375875414308 - 0.6863988875382543'\n",
      "7 - random_0 - lwr_k=1000 - 3.1208361898368358 - 1.745681912052799 - 1.2529027474713863 - 1.8341650587825384 - 1.5894300385661548 - 1.9042375875414308 - 0.6863988875382543'\n",
      "8 - random_0 - lr - 2.065452535754627 - 2.4436654394496857 - 2.0731373428419304 - 2.753594456446533 - 3.2807784557212423 - 2.52513945288464 - 0.5841450947473026'\n",
      "9 - random_0 - lr - 2.065452535754627 - 2.4436654394496857 - 2.0731373428419304 - 2.753594456446533 - 3.2807784557212423 - 2.52513945288464 - 0.5841450947473026'\n",
      "10 - random_0 - lwr_k=10 - 2.679880629819873 - 3.296923612236123 - 2.2610945364233705 - 3.8181444065923755 - 2.2286601678307303 - 2.864132167909256 - 0.5283177687646228'\n",
      "11 - random_0 - lwr_k=10 - 2.679880629819873 - 3.296923612236123 - 2.2610945364233705 - 3.8181444065923755 - 2.2286601678307303 - 2.864132167909256 - 0.5283177687646228'\n",
      "12 - random_0 - lwr_k=400 - 6.983210986013615 - 1.4850013865725165 - 7.0955014960888905 - 1.5679839712847425 - 1.2557910428077796 - 3.6537434876356776 - 0.3982799048453941'\n",
      "13 - random_0 - lwr_k=400 - 6.983210986013615 - 1.4850013865725165 - 7.0955014960888905 - 1.5679839712847425 - 1.2557910428077796 - 3.6537434876356776 - 0.3982799048453941'\n",
      "14 - random_1 - lwr_k=1000 - 3.2322500028764596 - 3.370523956482269 - 4.426174917846835 - 2.9828537291280206 - 4.88932976673032 - 3.7758618278592375 - 0.37816873405628315'\n",
      "15 - random_1 - lwr_k=900 - 3.133792764086166 - 3.342537512571894 - 4.393972246320941 - 2.9076161346824643 - 5.284805756419739 - 3.8075496219798333 - 0.3729502006376104'\n",
      "16 - random_1 - lwr_k=800 - 3.137659671934367 - 3.321974227229829 - 4.33509279419442 - 2.9268876153395254 - 6.843611336678829 - 4.105631066026387 - 0.3238603848137912'\n",
      "17 - random_1 - lwr_k=700 - 3.1598722545421367 - 3.3090143454448637 - 4.277372706636547 - 2.8637352625615486 - 7.31794931957065 - 4.177041943688874 - 0.3121000189732349'\n",
      "18 - random_1 - lr - 4.516421937404487 - 3.847690567117039 - 4.420670967852906 - 4.594603399450706 - 4.242240786217413 - 4.324340849454221 - 0.2878419636682502'\n",
      "19 - random_0 - lwr_k=20 - 3.6160914484383073 - 5.546633544483912 - 3.5871626956507456 - 6.862542114985871 - 3.691500013130981 - 4.678654763947989 - 0.22949145190811826'\n",
      "20 - random_0 - lwr_k=20 - 3.6160914484383073 - 5.546633544483912 - 3.5871626956507456 - 6.862542114985871 - 3.691500013130981 - 4.678654763947989 - 0.22949145190811826'\n",
      "21 - random_0 - lwr_k=600 - 3.322927968509271 - 1.4460422390196754 - 109.66308833726929 - 1.5521721344118125 - 1.3218476491189093 - 23.392264311930557 - -2.85237648874054'\n",
      "22 - random_0 - lwr_k=600 - 3.322927968509271 - 1.4460422390196754 - 109.66308833726929 - 1.5521721344118125 - 1.3218476491189093 - 23.392264311930557 - -2.85237648874054'\n",
      "23 - random_0 - lwr_k=300 - 6.763750256394267 - 3.2410886849511336 - 26.775477914272425 - 1.5896944619312483 - 144.7598280209326 - 36.36358120236534 - -4.98856969989659'\n",
      "24 - random_0 - lwr_k=300 - 6.763750256394267 - 3.2410886849511336 - 26.775477914272425 - 1.5896944619312483 - 144.7598280209326 - 36.36358120236534 - -4.98856969989659'\n",
      "25 - random_0 - lwr_k=500 - 7.565963142448411 - 1.3960891146480245 - 171.12692913366925 - 1.4848363159095588 - 1.2842102474762689 - 36.45046724149586 - -5.002878606887499'\n",
      "26 - random_0 - lwr_k=500 - 7.565963142448411 - 1.3960891146480245 - 171.12692913366925 - 1.4848363159095588 - 1.2842102474762689 - 36.45046724149586 - -5.002878606887499'\n",
      "27 - random_1 - lwr_k=600 - 3.2100849037302703 - 3.2498357694882642 - 4.224909831865965 - 220.41293300140532 - 4.811654163411507 - 48.21637726016362 - -6.940558282532874'\n",
      "28 - random_0 - lwr_k=30 - 51.9146716554626 - 141.65529722661978 - 31.536092764853937 - 51.82731537493098 - 21.997706905312373 - 59.96016459945379 - -8.874594664448075'\n",
      "29 - random_0 - lwr_k=30 - 51.9146716554626 - 141.65529722661978 - 31.536092764853937 - 51.82731537493098 - 21.997706905312373 - 59.96016459945379 - -8.874594664448075'\n",
      "30 - random_0 - deep - 5.503632525196226 - 8.955492423562443 - 6.095810025145728 - 27.057358468939395 - 273.0033813856646 - 63.80053504755648 - -9.507049578953044'\n",
      "31 - random_1 - lwr_k=500 - 3.361505680597977 - 3.1474387090243403 - 4.263087876528495 - 304.05889053289246 - 4.107272870616742 - 65.22211060497469 - -9.741163065282716'\n",
      "32 - random_1 - deep - 19.061410476404227 - 120.93269432438609 - 4.84977889947197 - 248.55960121365754 - 31.636215629637025 - 86.22232620004317 - -13.199602801469181'\n",
      "33 - random_0 - lwr_k=200 - 144.86756637569744 - 3.286389679443722 - 70.78412660081877 - 2.428473667419411 - 3401.5581109657883 - 718.5358975548619 - -117.33274287366253'\n",
      "34 - random_0 - lwr_k=200 - 144.86756637569744 - 3.286389679443722 - 70.78412660081877 - 2.428473667419411 - 3401.5581109657883 - 718.5358975548619 - -117.33274287366253'\n",
      "35 - random_0 - lwr_k=40 - 117.73447656878187 - 7383.078076018716 - 816.5172918453892 - 28.360380441193573 - 1048.3652878767991 - 1885.761815836587 - -309.5584130642881'\n",
      "36 - random_0 - lwr_k=40 - 117.73447656878187 - 7383.078076018716 - 816.5172918453892 - 28.360380441193573 - 1048.3652878767991 - 1885.761815836587 - -309.5584130642881'\n",
      "37 - random_1 - lwr_k=400 - 3.5836267659989103 - 2.9933677120845705 - 5.159169121651597 - 321.40965833141365 - 19506.6266741229 - 3938.1473864831682 - -647.5574119109585'\n",
      "38 - random_1 - lwr_k=300 - 3.6018972178528186 - 3.1923072952191647 - 5.351690106931328 - 3322.608279803429 - 19252.49997749025 - 4502.388975996294 - -740.4800552440086'\n",
      "39 - random_1 - lwr_k=200 - 5.320432793350819 - 2.935201716563594 - 5.322527427196744 - 7259.616545000514 - 17049.77876406701 - 4871.871148544924 - -801.3285654845995'\n",
      "40 - random_1 - lwr_k=10 - 238.1664836382892 - 31.77009935490657 - 24490.238345923808 - 32.681974624705965 - 40.39342975998699 - 4951.740137784438 - -814.4818631826273'\n",
      "41 - random_0 - lwr_k=50 - 35528.36019720657 - 118443.71887330776 - 11239.123411969493 - 1495.137521144983 - 999.2724164351159 - 33553.85618223081 - -5524.84755966018'\n",
      "42 - random_0 - lwr_k=50 - 35528.36019720657 - 118443.71887330776 - 11239.123411969493 - 1495.137521144983 - 999.2724164351159 - 33553.85618223081 - -5524.84755966018'\n",
      "43 - random_1 - lwr_k=100 - 6010.47852010611 - 941.4154473164862 - 21.781406410811996 - 167749.20194229184 - 51648.11278982471 - 45970.523563650175 - -7569.697808051709'\n",
      "44 - random_1 - lwr_k=50 - 129804.92177652211 - 4551.932020819014 - 12.073829505337866 - 244747.8474030648 - 38450.01547720128 - 84124.10567301456 - -13853.055447969582'\n",
      "45 - random_0 - lwr_k=100 - 13.075959112751985 - 336772.7753841158 - 33817.43891485825 - 107745.93030299048 - 735.514495291401 - 96740.86858449617 - -15930.85861212997'\n",
      "46 - random_0 - lwr_k=100 - 13.075959112751985 - 336772.7753841158 - 33817.43891485825 - 107745.93030299048 - 735.514495291401 - 96740.86858449617 - -15930.85861212997'\n",
      "47 - random_1 - lwr_k=40 - 82237.37677026477 - 7187.726213956348 - 4721.649714769615 - 1492186.3785222652 - 89034.17121933121 - 341746.89568759233 - -56279.90074954753'\n",
      "48 - random_1 - lwr_k=20 - 600.4701083860865 - 6117.513515989054 - 2776718.879800923 - 480.40993799653154 - 1803.4439038546334 - 555548.1186475882 - -91489.95111542795'\n",
      "49 - random_1 - lwr_k=30 - 126408.37770116005 - 96580.19753009718 - 7766377.357808644 - 681782.3491586471 - 3041665.551804513 - 2336090.7853980474 - -384720.2521722799'\n"
     ]
    }
   ],
   "source": [
    "scores_df = pd.DataFrame(all_scores)\n",
    "scores_df.to_csv(log_dir/f\"scores.csv\",index=False)\n",
    "scores_df_final = pd.DataFrame(all_scores_final)\n",
    "scores_df_final.to_csv(log_dir/f\"test_scores.csv\",index=False)\n",
    "\n",
    "scores_df_sorted = pd.DataFrame(scores_df).sort_values(by='MSE')\n",
    "\n",
    "best_5 = []\n",
    "summary_logger.info(f\"Rank - \" +\" - \".join(list(scores_df_sorted.columns)))\n",
    "for i,(index,row) in enumerate(scores_df_sorted.iterrows()):\n",
    "    if i < 5:\n",
    "        best_5.append((row[\"model_num\"],row[\"predictor\"],row[\"MSE\"]))\n",
    "    s = f\"{i} - \" + \" - \".join([f\"{i}\" for i in row.tolist()])\n",
    "    summary_logger.info(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      " Best 5 on Test Sest \n",
      " ---------------------'\n",
      "Rank -  Deep Model - Predictor - Val Set - Test Set'\n",
      "0 - random_0 - lwr_k=800 - 1.4416686602573607 - 1.3164490013734322 - 0.7755190609688045'\n",
      "1 - random_0 - lwr_k=800 - 1.4416686602573607 - 1.3164490013734322 - 0.7755190609688045'\n",
      "2 - random_0 - lwr_k=900 - 1.5745885151448606 - 1.3705645264119901 - 0.766291279365303'\n",
      "3 - random_0 - lwr_k=900 - 1.5745885151448606 - 1.3705645264119901 - 0.766291279365303'\n",
      "4 - random_0 - lwr_k=700 - 1.8043620277178987 - 1.254542720124795 - 0.7860753226486151'\n"
     ]
    }
   ],
   "source": [
    "summary_logger.info(\"-----------------------\\n Best 5 on Test Sest \\n ---------------------\")\n",
    "summary_logger.info(f\"Rank -  Deep Model - Predictor - Val Set - Test Set\")\n",
    "for i, (j,k,v) in enumerate(best_5):\n",
    "\n",
    "    row = scores_df_final.loc[(scores_df_final['model_num']==j) & (scores_df_final['predictor'] == k)].iloc[0]\n",
    "    #print(row)\n",
    "    s = f\"{i} - {j} - {k} - {v} - {row['MSE']} - {row['R2']}\"\n",
    "    summary_logger.info(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (100) does not match length of index (2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-af2520b568eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdeep_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscores_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mscores_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"predictor\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"deep\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"R2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdeep_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"order\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mdeep_ordering\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model_num\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"order\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdeep_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\lazydeep\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3038\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3039\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3040\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\lazydeep\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3114\u001b[0m         \"\"\"\n\u001b[0;32m   3115\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3116\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3117\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\lazydeep\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[1;34m(self, key, value, broadcast)\u001b[0m\n\u001b[0;32m   3762\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3763\u001b[0m             \u001b[1;31m# turn me into an ndarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3764\u001b[1;33m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3765\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3766\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\lazydeep\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    745\u001b[0m     \"\"\"\n\u001b[0;32m    746\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 747\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m    748\u001b[0m             \u001b[1;34m\"Length of values \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[1;34mf\"({len(data)}) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (100) does not match length of index (2)"
     ]
    }
   ],
   "source": [
    "#take 1 is a scatter plot - lets, for each dataset\n",
    "#graph our deep models by rank - plot - then overlay our knn moels\n",
    "#plot points\n",
    "\n",
    "deep_set = scores_df[scores_df[\"predictor\"]==\"deep\"].sort_values(\"R2\")\n",
    "deep_set[\"order\"] = [i for i in range(0,100)]\n",
    "deep_ordering = {row[\"model_num\"]:row[\"order\"] for index, row in deep_set.iterrows()}\n",
    "\n",
    "def order_models(x):\n",
    "    x = [deep_ordering[i] for i in x]\n",
    "    return x\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "set_deep = False\n",
    "knn_models = scores_df[\"predictor\"].unique()\n",
    "for knn_model in knn_models:\n",
    "    subset = scores_df[scores_df[\"predictor\"]==knn_model]\n",
    "    s=3\n",
    "    if knn_model == \"deep\":\n",
    "        s=10\n",
    "    ax.scatter(x=order_models(subset[\"model_num\"].tolist()), y=subset[\"R2\"], s=s, label=knn_model)\n",
    "\n",
    "#ax.set_ylim(0,scores_db[\"deep_mean\"].max())\n",
    "ax.set_ylim(0,1)\n",
    "# plot residuals\n",
    "ax.legend(loc='upper right',bbox_to_anchor=(1.4, 1))\n",
    "ax.set_ylabel(\"R^2 Score\")\n",
    "ax.set_xlabel(\"Deep Model Rank\")\n",
    "#ax.set_ylim(0,200)\n",
    "#ax.set_yscale(\"symlog\")\n",
    "ax.set_title(\"Summary of LWR improvements over Deep Models\")\n",
    "plt.savefig(log_dir/f\"summary_plot.png\", bbox_inches='tight')\n",
    "logging.getLogger().info(\"Wrote Summary Graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df[\"n_features\"] = [deep_models[i].n_features for i in scores_df[\"model_num\"]] \n",
    "from matplotlib.colors import Colormap\n",
    "import seaborn as sns #heatmap of features - pls model - score\n",
    "class nlcmap(Colormap):\n",
    "    def __init__(self, cmap, levels):\n",
    "        self.cmap = cmap\n",
    "        self.N = cmap.N\n",
    "        self.monochrome = self.cmap.monochrome\n",
    "        self.levels = np.asarray(levels, dtype='float64')\n",
    "        self._x = self.levels\n",
    "        self.levmax = self.levels.max()\n",
    "        self.levmin = self.levels.min()\n",
    "        self.transformed_levels = np.linspace(self.levmin, self.levmax, #uniform spacing along levels (colour segments)\n",
    "             len(self.levels))\n",
    "\n",
    "    def __call__(self, xi, alpha=1.0, **kw):\n",
    "        yi = np.interp(xi, self._x, self.transformed_levels)\n",
    "        return self.cmap((yi-self.levmin) / (self.levmax-self.levmin), alpha)\n",
    "    \n",
    "levels = np.concatenate((\n",
    "    [0, 1],\n",
    "    [0.6,0.8,0.9,0.95,0.98]\n",
    "    ))\n",
    "\n",
    "levels = levels[levels <= 1]\n",
    "levels.sort()\n",
    "cmap_nonlin = nlcmap(plt.cm.YlGnBu, levels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = scores_df[[\"predictor\",\"n_features\",\"R2\"]]\n",
    "subset = subset[np.logical_not(subset[\"predictor\"]==\"deep\")]\n",
    "subset = subset[np.logical_not(subset[\"predictor\"]==\"lr\")]\n",
    "trans = subset[\"predictor\"].transform(lambda x: int(x.replace(\"lwr_k=\",\"\"))).tolist()\n",
    "subset.loc[:,\"predictor\"]=trans\n",
    "subset=subset.sort_values(\"predictor\",ascending=False)\n",
    "\n",
    "def rand_jitter(arr):\n",
    "    stdev = .01 * (max(arr) - min(arr))\n",
    "    return arr + np.random.randn(len(arr)) * stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sc = ax.scatter(x=rand_jitter(subset[\"n_features\"]), y=rand_jitter(subset[\"predictor\"]), s=20,c=subset[\"R2\"],cmap=cmap_nonlin,vmin=0)\n",
    "ax.set_xlabel(\"Number of Features\")\n",
    "ax.set_ylabel(\"Number of Neighbours\")\n",
    "\n",
    "cbar = fig.colorbar(sc,label=\"R2 Score\")\n",
    "\n",
    "ax.set_title(\"LWR performance as a function of the number of components\")\n",
    "plt.savefig(log_dir/f\"heat_scatter.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (lazydeep)",
   "language": "python",
   "name": "pycharm-12fcba0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
