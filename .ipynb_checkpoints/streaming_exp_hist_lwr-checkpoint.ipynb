{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import logging\n",
    "# set seed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils as ut\n",
    "import experiment as ex\n",
    "from evaluation import *\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import random\n",
    "#define fixed_hyperparams and create a config gen\n",
    "from configurations import RandomConfigGen, Configuration\n",
    "from torch import nn\n",
    "from deep_net import RandomNet\n",
    "from experiment import run_experiment\n",
    "import regex as re\n",
    "from pathlib import *\n",
    "from sk_models import PLSRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from river_models import StreamLocalWeightedRegression\n",
    "\n",
    "from river import stream,linear_model,preprocessing, ensemble, metrics, optim\n",
    "from river.neighbors import KNNRegressor \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from river.utils import dict2numpy, numpy2dict\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed + 1)\n",
    "np.random.seed(seed + 2)\n",
    "random_state = np.random.RandomState(seed)\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"GPU detected is {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup input and output directories\n",
    "\n",
    "#setup input and outpu t formats, load data\n",
    "\n",
    "#we need to set parametesr\n",
    "file_name = \"PLN7.csv\" #\"mango_684_990.csv\" #\"mango_729_975.csv\" #fitlered=513-1050\n",
    "id_cols =[\"db_id\",\"sample_id\"] #\n",
    "output_cols = None\n",
    "data_path = Path('D:/workspace/lazydeep/data/soil_data/')\n",
    "log_path = Path(\"D:/workspace/lazydeep/experiments/5.02\") #1.01/\")\n",
    "if not log_path.exists():\n",
    "    log_path.mkdir()\n",
    "\n",
    "data_file = data_path / file_name\n",
    "log_dir = log_path / re.sub(r'\\.(?=csv$)[^.]+$', '',file_name)\n",
    "if not log_dir.exists():\n",
    "    log_dir.mkdir()\n",
    "print(f\"Output directory is {log_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(models,X):\n",
    "    preds = {name:[] for name in models.keys()}\n",
    "    for i in range(0,len(X)):\n",
    "        xi = numpy2dict(X[i])\n",
    "        for name,model in river_models.items():\n",
    "            pred = model.predict_one(xi)\n",
    "            preds[name].append(pred)\n",
    "    return preds\n",
    "        \n",
    "def batch_score(models,X,y):\n",
    "    preds = {name:[] for name in models.keys()}\n",
    "    for i in range(0,len(X)):\n",
    "        xi = numpy2dict(X[i])\n",
    "        yi = y[i]\n",
    "        for name,model in river_models.items():\n",
    "            pred = model.predict_one(xi)\n",
    "            preds[name].append(pred)\n",
    "            \n",
    "    scores = {name:r2_score(y,pred) for name,pred in preds.items()}\n",
    "    mse = {name:mean_squared_error(y,pred) for name,pred in preds.items()}\n",
    "    return scores, mse\n",
    "        \n",
    "\n",
    "def batch_learn(models,X,y):\n",
    "    for i in range(0,len(X)):\n",
    "        xi = numpy2dict(X[i])\n",
    "        yi = y[i]\n",
    "        for name,model in river_models.items():\n",
    "             model.learn_one(xi,yi)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_file)\n",
    "data=data.sample(frac=1,random_state=random_state)\n",
    "\n",
    "pre_ind =[i for i in range(0,10000)]\n",
    "pretrain_ind,pretest_ind = train_test_split(pre_ind,train_size=5/6,random_state=random_state,shuffle=False)\n",
    "stream_ind = [i for i in range(10000,110000)]\n",
    "\n",
    "pretrain_data =  ut.TabularDataset(data.iloc[pretrain_ind,:],id_cols = id_cols, cat_cols=None, output_cols=output_cols, ignore_cols= None)\n",
    "pretest_data = ut.TabularDataset(data.iloc[pretest_ind,:],id_cols = id_cols, cat_cols=None, output_cols=output_cols, ignore_cols= None)\n",
    "stream_data = ut.TabularDataset(data.iloc[stream_ind,:],id_cols = id_cols, cat_cols=None, output_cols=output_cols, ignore_cols= None)\n",
    "\n",
    "nrow, ncol = data.shape\n",
    "nrow_train = len(pretrain_data)\n",
    "nrow_test = len(pretest_data)\n",
    "nrow_stream = len(stream_data)\n",
    "\n",
    "print(f\"train: {nrow_train}, test: {nrow_test}, stream: {nrow_stream}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup evaluation\n",
    "from river.neighbors import KNNRegressor\n",
    "\n",
    "def setup_models():\n",
    "    lwr1 =  StreamLocalWeightedRegression(n_neighbors= 500, window_size=10000)\n",
    "    lwr2 =  (preprocessing.StandardScaler() |StreamLocalWeightedRegression(n_neighbors= 500, window_size=10000))\n",
    "    lwr3 =  ExpHistLocalWightedregression(n_neighbors= 500, window_size=10000)\n",
    "    lwr4 =  (preprocessing.StandardScaler() |ExpHistLocalWightedregressionlWeightedRegression(n_neighbors= 500, window_size=10000))\n",
    "    \n",
    "    return {\n",
    "            'lwr1':lwr1,\n",
    "            'lwr2':lwr2,\n",
    "\n",
    "           }\n",
    "            #,'lin1':lin1,\n",
    "            #'lin2':lin2,\n",
    "            #'lin3':lin3\n",
    "            #'lwr1':(preprocessing.StandardScaler()|StreamLocalWeightedRegression(n_neighbors= 500, window_size=10000))\n",
    "            #'lwr2':(preprocessing.StandardScaler()|StreamLocalWeightedRegression(n_neighbors= 800, window_size=10000)),\n",
    "           # 'lwr3':(preprocessing.StandardScaler()|StreamLocalWeightedRegression(n_neighbors= 1000, window_size=10000))\n",
    "           \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup our metrics and stores of results\n",
    "river_models = setup_models()\n",
    "full_set = river_models.keys()\n",
    "metrics = {'R2':{name:metrics.R2() for name in full_set},\n",
    "           'R2_rolling':{name:metrics.Rolling(metrics.R2(), window_size=1000) for name in full_set},\n",
    "           'MSE':{name:metrics.MSE() for name in river_models.keys()},\n",
    "           'MSE_rolling':{name:metrics.Rolling(metrics.MSE(), window_size=1000) for name in full_set}\n",
    "          }\n",
    "if False:           \n",
    "    scores = {'R2':{name:[] for name in full_set},\n",
    "              'MSE':{name:[] for name in full_set},\n",
    "              'R2_rolling':{name:[] for name in full_set},\n",
    "              'MSE_rolling':{name:[] for name in full_set}\n",
    "             }    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "\n",
    "    train_X,train_y = pretrain_data[:]\n",
    "    test_X, test_y = stream_data[0:1000]\n",
    "\n",
    "    batch_learn(river_models,train_X,train_y)\n",
    "\n",
    "    batch_r2,batch_mse = batch_score(river_models,train_X,train_y)\n",
    "    preds = batch_predict(river_models,train_X)\n",
    "    preds['y'] = train_y.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for name in river_models.keys():\n",
    "        model_preds = preds[name]\n",
    "        mse = mean_squared_error(preds['y'],model_preds)\n",
    "        print(f\" {name} :  {mse}\")\n",
    "        for i, pred in enumerate(model_preds):\n",
    "            y=train_y[i]\n",
    "            for metric_k,metric_v in metrics.items():\n",
    "                metric_v[name].update(y, pred)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for name in river_models.keys():\n",
    "        scores['R2'][name] = scores['R2'][name] + [batch_r2[name] for _ in range(0,nrow_train)]\n",
    "        scores['R2_rolling'][name] = scores['R2_rolling'][name] + [batch_r2[name] for _ in range(0,nrow_train)]\n",
    "        scores['MSE'][name] = scores['MSE'][name] +  [batch_mse[name] for _ in range(0,nrow_test)]\n",
    "        scores['MSE_rolling'][ name] = scores['MSE_rolling'][name] +  [batch_mse[name] for _ in range(0,nrow_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for name in river_models.keys():\n",
    "        for m,v in metrics.items():\n",
    "            print(v[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so sofar we have establish our metrics and scores are correct\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prequential_evaluate(dataset,models,metrics,pretrain=1000,num_its=2000):\n",
    "    \"\"\"\n",
    "    only make prediction if after pretrain length\n",
    "    \"\"\"\n",
    "    X,y = dataset[0:num_its]\n",
    "    \n",
    "    preds = {name:[] for name in models.keys()}\n",
    "    preds['y'] = []\n",
    "    \n",
    "    scores = {k:{name:[] for name in full_set} for k in metrics.keys()}\n",
    "    \n",
    "    for i in tqdm(range(0,num_its)):\n",
    "        xi = numpy2dict(X[i])\n",
    "        yi = y[i]\n",
    "        \n",
    "\n",
    "        preds['y'].append(yi)    \n",
    "            \n",
    "        for name,model in river_models.items():  \n",
    "            pred = model.predict_one(xi)\n",
    "            preds[name].append(pred)\n",
    "            #predict if pretrained\n",
    "            if i >= pretrain:\n",
    "\n",
    "                for metric_k,metric_v in metrics.items():\n",
    "                    score = metric_v[name].update(yi, pred).get()\n",
    "                    scores[metric_k][name].append(score)\n",
    "                    \n",
    "            #learn\n",
    "            model.learn_one(xi,yi)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return preds, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds, scores = prequential_evaluate(stream_data,river_models,metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame(preds)\n",
    "preds_df.to_csv(log_dir/\"preds_df.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#findings\n",
    "#1) preprocessing works, random lr things for lr don't\n",
    "#) standardisation asks as regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlabel(\"Stream Index\")\n",
    "ax.set_ylabel(\"R^2 Score\")\n",
    "ax.set_title(\"Streaming performance \")\n",
    "\n",
    "scores_df = pd.DataFrame(scores['R2'])\n",
    "scores_df.to_csv(log_dir/\"r2_scores.csv\")\n",
    "for (columnName, columnData) in scores_df.iteritems():\n",
    "    ax.plot(columnData.index,columnData,'-',label = f\"{columnName}\")\n",
    "\n",
    "\n",
    "ax.legend(loc='upper right',bbox_to_anchor=(1.4, 1))\n",
    "plt.savefig(log_dir / f\"r2_plot.png\",bbox_inches='tight')\n",
    "#ax.set_ylim(0,1)\n",
    "plt.savefig(log_dir / f\"r2_plot_v2.png\",bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlabel(\"Stream Index\")\n",
    "ax.set_ylabel(\"R^2 Score\")\n",
    "ax.set_title(\"Streaming performance (rolling average) \")\n",
    "    \n",
    "scores_df = pd.DataFrame(scores['R2_rolling'])\n",
    "scores_df.to_csv(log_dir/\"r2_scores_rolling.csv\")\n",
    "for (columnName, columnData) in scores_df.iteritems():\n",
    "    ax.plot(columnData.index,columnData,label = f\"{columnName}\")\n",
    "\n",
    "\n",
    "ax.legend(loc='upper right',bbox_to_anchor=(1.4, 1))\n",
    "plt.savefig(log_dir / f\"r2_plot.png\",bbox_inches='tight')\n",
    "ax.set_ylim(-1,1)\n",
    "plt.savefig(log_dir / f\"r2_plot_v2.png\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlabel(\"Stream Index\")\n",
    "ax.set_ylabel(\"MSE\")\n",
    "ax.set_title(\"Streaming performance \")\n",
    "\n",
    "scores_df = pd.DataFrame(scores['MSE'])\n",
    "scores_df.to_csv(log_dir/\"MSE.csv\")\n",
    "for (columnName, columnData) in scores_df.iteritems():\n",
    "    ax.plot(columnData.index,columnData,'-',label = f\"{columnName}\")\n",
    "\n",
    "ax.legend(loc='upper right',bbox_to_anchor=(1.4, 1))\n",
    "plt.savefig(log_dir / f\"mse_plot.png\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlabel(\"Stream Index\")\n",
    "ax.set_ylabel(\"MSE\")\n",
    "ax.set_title(\"Streaming performance (rolling average) \")\n",
    "\n",
    "scores_df = pd.DataFrame(scores['MSE_rolling'])\n",
    "scores_df.to_csv(log_dir/\"MSE_rolling.csv\")\n",
    "for (columnName, columnData) in scores_df.iteritems():\n",
    "    ax.plot(columnData.index,columnData,label = f\"{columnName}\")\n",
    "\n",
    "ax.legend(loc='upper right',bbox_to_anchor=(1.4, 1))\n",
    "plt.savefig(log_dir / f\"mse_plot.png\",bbox_inches='tight')\n",
    "ax.set_ylim(-1,1000)\n",
    "plt.savefig(log_dir / f\"r2_plot_v2.png\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
