{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "# set seed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import utils as ut\n",
    "import experiment as exp\n",
    "from evaluation import *\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import random\n",
    "#define fixed_hyperparams and create a config gen\n",
    "from configurations import RandomConfigGen, Configuration\n",
    "from torch import nn\n",
    "from deep_net import RandomNet\n",
    "from experiment import run_experiment\n",
    "import regex as re\n",
    "from pathlib import *\n",
    "from plot import *\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from sk_models import setup_pls_models_exh, StandardScaler, PLSRegression\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed + 1)\n",
    "np.random.seed(seed + 2)\n",
    "random_state = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\workspace\\lazydeep\\experiments\\1.02\\A_C_OF_ALPHA\n"
     ]
    }
   ],
   "source": [
    "#setup input and output formats, load data\n",
    "\n",
    "file_name = \"A_C_OF_ALPHA.csv\"\n",
    "id_cols =[\"sample_id\"] #[\"db_id\", \"sample_id\"]#[\"sample_id\"]\n",
    "\n",
    "data_path = Path('D:/workspace/lazydeep/data/soil_data/')\n",
    "model_path = Path('D:/workspace/lazydeep/experiments/1.01/')\n",
    "log_path = Path(\"D:/workspace/lazydeep/experiments/1.02\")\n",
    "n_components = 36\n",
    "\n",
    "data_file = data_path / file_name\n",
    "log_dir = log_path / re.sub(r'\\.(?=csv$)[^.]+$', '',file_name)\n",
    "model_dir = model_path / re.sub(r'\\.(?=csv$)[^.]+$', '',file_name)\n",
    "\n",
    "if not log_dir.exists():\n",
    "    log_dir.mkdir()\n",
    "print(log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% load data\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7329, 1703)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(data_file)\n",
    "data = data.sample(frac=1)\n",
    "nrow, ncol = data.shape\n",
    "data = ut.sample_data(data,random_state)\n",
    "\n",
    "n_features = ncol - 1-len(id_cols)\n",
    "dataset = TabularDataset(data,id_cols = id_cols, cat_cols=None, output_cols=None, ignore_cols= None)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% load models\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 models\n"
     ]
    }
   ],
   "source": [
    "n_models = 100\n",
    "model_names = [f\"random_{i}\" for i in range(0,n_models)]\n",
    "deep_models = {name:torch.load(model_dir/\"models\"/name/\"_model\") for name in model_names}\n",
    "#for each model, load state\n",
    "print(f\"Loaded {len(deep_models)} models\")\n",
    "#print(deep_models)\n",
    "for name in model_names:\n",
    "    sub_path = log_dir / name\n",
    "    if not sub_path.exists():\n",
    "        sub_path.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% setup logging and tensorboard outputs\n"
    }
   },
   "outputs": [],
   "source": [
    "# set logging, in this case the root logger\n",
    "ut.setup_logger(logger_name=\"\",file_name=log_dir/\"log.txt\")\n",
    "ut.setup_logger(logger_name=\"summary\",file_name=log_dir/\"summary.txt\")\n",
    "summary_logger = logging.getLogger(\"summary\")\n",
    "tb = SummaryWriter(log_dir/\"tb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% setup experiment\n"
    }
   },
   "outputs": [],
   "source": [
    "fixed_hyperparams = {'bs': 32,'loss': nn.MSELoss(),'epochs': 100}\n",
    "preprocessing = PLSRegression(n_components=n_components)\n",
    "eval = CrossValEvaluation(preprocessing=preprocessing,tensorboard=None,time=True,random_state=random_state)\n",
    "scores={} #->model->knn:{fold_0:number,...,fold_n:number,mean:number,median:number\n",
    "preds={} #model-> foldsxknn_models\n",
    "deep_scores_dict={}\n",
    "deep_preds_dict={}\n",
    "actual_y = None\n",
    "\n",
    "device = \"cpu\" #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "load_fun_cv = lambda name,model, fold : model.load_state(model_dir/'models'/name/f\"_fold_{fold}\")\n",
    "load_fun_pp_cv = lambda fold : preprocessing.from_state(preprocessing.load_state(model_dir/'preprocessing'/f\"_fold_{fold}\"))\n",
    "load_fun_build = lambda name,model : model.load_state(model_dir/'models'/name/f\"_final\")\n",
    "load_fun_pp_build = lambda : preprocessing.from_state(preprocessing.load_state(model_dir/'preprocessing'/f\"_final\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% deep experiment\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Tested (test) on 1222 instances with mean losses of: random_0:10.7076,random_1:8.82,random_2:18.5217,random_3:9.5714,random_4:8.5565,random_5:6.5993,random_6:7.6571,random_7:8.3455,random_8:10.0416,random_9:9.7644,random_10:9.4256,random_11:257.8883,random_12:14.0352,random_13:257.8896,random_14:11.7894,random_15:9.5401,random_16:14.3834,random_17:13.6174,random_18:18.9218,random_19:14.6448,random_20:39.1621,random_21:11.0764,random_22:16.7366,random_23:11.3506,random_24:8.2135,random_25:10.563,random_26:41.0637,random_27:13.9577,random_28:9.0218,random_29:7.6515,random_30:257.0104,random_31:8.7433,random_32:7.6763,random_33:14.2807,random_34:9.46,random_35:11.1106,random_36:185.2397,random_37:12.1935,random_38:10.1135,random_39:6.8647,random_40:15.7635,random_41:11.1829,random_42:137.3898,random_43:275.1415,random_44:9.7403,random_45:17.2322,random_46:19.165,random_47:256.8611,random_48:12.1704,random_49:12.3581,random_50:12.6058,random_51:13.9655,random_52:43.8716,random_53:27.4289,random_54:9.7526,random_55:11.9132,random_56:11.114,random_57:13.8149,random_58:623.385,random_59:6.5455,random_60:9.538,random_61:11.1117,random_62:25.0679,random_63:8.3169,random_64:17.1165,random_65:15.1769,random_66:7.8386,random_67:9.0845,random_68:256.983,random_69:9.9477,random_70:13.3352,random_71:8.9785,random_72:10.4251,random_73:7.9652,random_74:9.2308,random_75:10.8834,random_76:14.1239,random_77:8.3764,random_78:8.1943,random_79:26.2064,random_80:11.8488,random_81:10.8392,random_82:8.3428,random_83:9.1326,random_84:17.7719,random_85:25.5258,random_86:14.6184,random_87:9.5911,random_88:11.0698,random_89:256.9932,random_90:7.5951,random_91:10.7757,random_92:17.4909,random_93:8.2121,random_94:8.3127,random_95:13.8873,random_96:33.8262,random_97:257.3188,random_98:10.8275,random_99:8.8789'\n",
      "Testing (test) took 0:00:01.592977'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Tested (test) on 1222 instances with mean losses of: random_0:14.4544,random_1:9.607,random_2:19.1362,random_3:12.3943,random_4:13.0206,random_5:6.6993,random_6:6.5641,random_7:10.6444,random_8:10.8843,random_9:11.9169,random_10:30.0773,random_11:202.8402,random_12:13.4619,random_13:202.806,random_14:14.1858,random_15:16.0399,random_16:16.8979,random_17:10.3417,random_18:60.6056,random_19:12.4466,random_20:7.319,random_21:14.0414,random_22:12.3095,random_23:12.7867,random_24:10.0797,random_25:10.3143,random_26:14.7858,random_27:22.633,random_28:9.4987,random_29:9.4391,random_30:203.1262,random_31:6.6165,random_32:202.964,random_33:14.5796,random_34:11.7746,random_35:9.9842,random_36:202.8258,random_37:13.6687,random_38:10.1591,random_39:8.5773,random_40:9.5057,random_41:17.7552,random_42:22.3204,random_43:280.4297,random_44:10.0725,random_45:20.9768,random_46:15.5844,random_47:203.3518,random_48:13.5393,random_49:10.9703,random_50:17.94,random_51:11.3951,random_52:32.7542,random_53:11.7835,random_54:29.576,random_55:9.5345,random_56:16.7035,random_57:10.7484,random_58:15.1917,random_59:6.7585,random_60:9.6114,random_61:22.7643,random_62:69.379,random_63:10.4704,random_64:10.8296,random_65:6.3449,random_66:18.4116,random_67:9.2598,random_68:203.1991,random_69:19.97,random_70:148.6904,random_71:9.8102,random_72:11.8984,random_73:13.0036,random_74:10.3948,random_75:12.2734,random_76:15.3987,random_77:10.5395,random_78:12.041,random_79:7.5248,random_80:14.0372,random_81:11.4389,random_82:12.1415,random_83:9.8849,random_84:12.2159,random_85:13.2135,random_86:12.0089,random_87:9.9282,random_88:16.3357,random_89:203.2179,random_90:14.6223,random_91:11.8541,random_92:13.7936,random_93:8.7859,random_94:12.0971,random_95:12.6864,random_96:13.4706,random_97:203.0589,random_98:7.9941,random_99:7.7749'\n",
      "Testing (test) took 0:00:01.565008'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Tested (test) on 1221 instances with mean losses of: random_0:8.1838,random_1:6.0096,random_2:22.2875,random_3:10.3141,random_4:12.0888,random_5:5.8089,random_6:9.6829,random_7:7.0611,random_8:9.1629,random_9:9.8593,random_10:7.7293,random_11:9.0627,random_12:10.601,random_13:153.7906,random_14:9.9352,random_15:11.5465,random_16:9.2229,random_17:6.8365,random_18:11.1423,random_19:9.8801,random_20:7.831,random_21:9.0819,random_22:8.0619,random_23:18.8824,random_24:8.8434,random_25:7.9606,random_26:15.3802,random_27:10.9519,random_28:7.0599,random_29:7.9952,random_30:154.0774,random_31:7.9751,random_32:154.2386,random_33:7.9013,random_34:7.079,random_35:7.6039,random_36:153.8746,random_37:11.2331,random_38:6.3329,random_39:7.0036,random_40:7.8927,random_41:8.135,random_42:32.1298,random_43:455.9252,random_44:6.501,random_45:9.9896,random_46:10.3998,random_47:154.6518,random_48:8.4745,random_49:7.1925,random_50:14.0121,random_51:153.818,random_52:197.3992,random_53:239.3804,random_54:9.3201,random_55:6.2381,random_56:6.7758,random_57:7.1415,random_58:462.1717,random_59:6.1033,random_60:8.5626,random_61:11.8952,random_62:69.1568,random_63:9.9099,random_64:7.9538,random_65:6.6916,random_66:14.0364,random_67:7.3462,random_68:154.3395,random_69:8.8313,random_70:9.8537,random_71:7.5362,random_72:9.1579,random_73:9.4839,random_74:6.9945,random_75:8.1428,random_76:7.885,random_77:9.7868,random_78:9.5827,random_79:13.2256,random_80:12.3755,random_81:9.9233,random_82:19.6038,random_83:7.65,random_84:9.3654,random_85:23.8309,random_86:10.8395,random_87:7.9648,random_88:9.6975,random_89:154.5312,random_90:6.5022,random_91:7.9049,random_92:8.6343,random_93:5.8525,random_94:7.4402,random_95:8.7052,random_96:7.7245,random_97:154.4451,random_98:7.1168,random_99:6.3594'\n",
      "Testing (test) took 0:00:01.553000'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Tested (test) on 1221 instances with mean losses of: random_0:13.64,random_1:11.4503,random_2:11.3095,random_3:14.2099,random_4:14.7001,random_5:12.4371,random_6:8.2102,random_7:11.6817,random_8:11.6903,random_9:11.4709,random_10:45.8048,random_11:14.9992,random_12:12.8922,random_13:239.5826,random_14:14.9542,random_15:14.3431,random_16:13.5177,random_17:11.9739,random_18:22.5463,random_19:16.8441,random_20:11.1257,random_21:11.821,random_22:15.1208,random_23:15.0771,random_24:13.5418,random_25:10.6956,random_26:15.2697,random_27:16.6473,random_28:20.2887,random_29:11.9189,random_30:239.2928,random_31:28.4459,random_32:23.4087,random_33:14.6357,random_34:14.0213,random_35:13.3153,random_36:239.429,random_37:11.0598,random_38:12.2406,random_39:10.4585,random_40:11.3108,random_41:18.3369,random_42:13.0804,random_43:386.1265,random_44:10.2181,random_45:22.3814,random_46:17.49,random_47:239.8815,random_48:15.7528,random_49:11.9245,random_50:16.0172,random_51:239.246,random_52:239.4531,random_53:236.6942,random_54:13.2886,random_55:17.4412,random_56:17.0794,random_57:13.1882,random_58:12.6803,random_59:9.5007,random_60:12.2905,random_61:16.1615,random_62:133.5792,random_63:10.6025,random_64:10.4574,random_65:11.2437,random_66:12.0294,random_67:11.1707,random_68:239.2023,random_69:28.9424,random_70:19.3987,random_71:9.9924,random_72:13.393,random_73:15.7667,random_74:9.8945,random_75:15.5963,random_76:12.5243,random_77:11.4127,random_78:12.7315,random_79:16.2636,random_80:11.8487,random_81:14.3276,random_82:17.1737,random_83:10.7573,random_84:13.6619,random_85:11.1333,random_86:12.1466,random_87:12.0023,random_88:13.2156,random_89:239.595,random_90:14.1187,random_91:11.3602,random_92:15.6683,random_93:9.9748,random_94:12.8234,random_95:10.9557,random_96:13.3402,random_97:240.8456,random_98:14.3839,random_99:10.8774'\n",
      "Testing (test) took 0:00:01.593001'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Tested (test) on 1221 instances with mean losses of: random_0:23.1247,random_1:17.9683,random_2:24.6473,random_3:32.7,random_4:29.4582,random_5:17.4119,random_6:17.9818,random_7:18.9071,random_8:28.5557,random_9:19.7035,random_10:22.1957,random_11:24.7838,random_12:18.9263,random_13:325.6178,random_14:32.3178,random_15:42.1497,random_16:33.466,random_17:20.7669,random_18:28.1445,random_19:32.4866,random_20:18.4272,random_21:25.6755,random_22:28.269,random_23:27.1366,random_24:29.1809,random_25:25.0539,random_26:31.9545,random_27:41.5007,random_28:33.1401,random_29:25.1698,random_30:325.7269,random_31:18.6502,random_32:325.7158,random_33:19.2276,random_34:17.484,random_35:16.5998,random_36:325.7246,random_37:32.1828,random_38:20.171,random_39:21.7828,random_40:26.3729,random_41:28.9787,random_42:56.2455,random_43:330.8626,random_44:24.1219,random_45:27.8946,random_46:28.5248,random_47:325.6313,random_48:30.8824,random_49:30.118,random_50:31.5541,random_51:16.8868,random_52:411.2854,random_53:324.2849,random_54:69.4817,random_55:21.7125,random_56:46.8397,random_57:21.5279,random_58:20.5076,random_59:25.3328,random_60:21.8582,random_61:24.7597,random_62:58.5801,random_63:23.511,random_64:14.8814,random_65:28.0035,random_66:19.6008,random_67:25.0623,random_68:325.7219,random_69:36.0506,random_70:25.4536,random_71:17.3246,random_72:22.3979,random_73:19.967,random_74:23.1758,random_75:35.2264,random_76:22.3222,random_77:25.1001,random_78:20.0708,random_79:37.0221,random_80:27.4029,random_81:28.7935,random_82:20.4643,random_83:17.1166,random_84:26.0229,random_85:27.343,random_86:26.3802,random_87:15.8441,random_88:20.656,random_89:325.7875,random_90:17.3612,random_91:22.9323,random_92:30.8284,random_93:33.7629,random_94:27.8863,random_95:27.3465,random_96:20.4867,random_97:325.7158,random_98:18.01,random_99:18.9325'\n",
      "Testing (test) took 0:00:01.548999'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Tested (test) on 1222 instances with mean losses of: random_0:10.676,random_1:10.8759,random_2:9.9551,random_3:561.5906,random_4:13.0128,random_5:8.3491,random_6:6.9174,random_7:9.7454,random_8:10.8295,random_9:11.7658,random_10:11.5504,random_11:14.2121,random_12:10.7761,random_13:237.6106,random_14:18.2338,random_15:12.9745,random_16:11.7361,random_17:12.4047,random_18:13.065,random_19:13.6008,random_20:10.758,random_21:12.9848,random_22:13.5265,random_23:15.1019,random_24:12.766,random_25:10.5247,random_26:279.7227,random_27:14.3187,random_28:13.5157,random_29:10.897,random_30:237.6115,random_31:16.2145,random_32:237.6115,random_33:11.6646,random_34:7.9327,random_35:9.9018,random_36:237.6145,random_37:15.2984,random_38:12.6852,random_39:8.6397,random_40:11.3701,random_41:18.5992,random_42:26.12,random_43:574.715,random_44:10.7809,random_45:13.4101,random_46:13.5689,random_47:237.516,random_48:20.0289,random_49:12.0172,random_50:12.8493,random_51:237.612,random_52:17.8771,random_53:264.277,random_54:11.6528,random_55:14.2713,random_56:25.9787,random_57:10.9847,random_58:11.0421,random_59:7.6728,random_60:10.6636,random_61:13.5615,random_62:45.5221,random_63:11.3609,random_64:9.6298,random_65:13.4054,random_66:12.6637,random_67:12.0291,random_68:237.6126,random_69:11.5638,random_70:237.6102,random_71:10.6919,random_72:11.5047,random_73:11.411,random_74:9.5842,random_75:13.2718,random_76:12.3804,random_77:11.5047,random_78:10.5959,random_79:9.6831,random_80:10.2462,random_81:12.6323,random_82:10.1896,random_83:11.1341,random_84:12.7635,random_85:13.979,random_86:9.8174,random_87:10.4213,random_88:9.9035,random_89:237.6104,random_90:12.2987,random_91:9.8772,random_92:15.3614,random_93:11.1727,random_94:11.2065,random_95:12.488,random_96:12.0183,random_97:13.0196,random_98:10.2431,random_99:7.8994'\n",
      "Testing (test) took 0:00:01.602999'\n"
     ]
    }
   ],
   "source": [
    "deep_scheme = DeepScheme(None, fixed_hyperparams=fixed_hyperparams,loss_eval=loss_target,device=device,tensorboard=tb,adaptive_lr=True,update=False)\n",
    "deep_scores, deep_preds, _ , _, _,_ = eval.evaluate(deep_models,dataset,deep_scheme,logger_name=\"log\",load_fun=load_fun_cv,load_fun_pp = load_fun_pp_cv)\n",
    "deep_scores_final, deep_preds_final, _ ,_, _,_ = eval.build(deep_models,dataset,deep_scheme,logger_name=\"test_log\",load_fun=load_fun_build,load_fun_pp = load_fun_pp_build)\n",
    "\n",
    "all_scores = []\n",
    "for k,v in ut.flip_dicts(deep_scores).items():\n",
    "    dict1 = {'model_num':k,\"predictor\":\"deep\"}\n",
    "    all_scores.append({**dict1,**v})\n",
    "    \n",
    "\n",
    "\n",
    "all_scores_final = []\n",
    "for k,v in ut.flip_dicts(deep_scores_final).items():\n",
    "    dict1 = {'model_num':k,\"predictor\":\"deep\"}\n",
    "    all_scores_final.append({**dict1,**v})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - random_5 - deep - 6.5993118223699145 - 6.699335931756102 - 5.808911881224236 - 12.437064430149338 - 17.411863611332222 - 9.790268489036634 - 0.958485857983749\n",
      "1 - random_6 - deep - 7.657056827201781 - 6.564094499753462 - 9.682900840390618 - 8.210204705949888 - 17.981783471665942 - 10.018255385589088 - 0.9575191142818861\n",
      "2 - random_99 - deep - 8.878893956810284 - 7.774931539295153 - 6.359399620574776 - 10.877428736284461 - 18.93252708988049 - 10.563903451673058 - 0.9552053767851393\n",
      "3 - random_71 - deep - 8.978512915378701 - 9.810193436430636 - 7.536236177986513 - 9.99243597140769 - 17.324588967775536 - 10.727956589197204 - 0.9545097344483622\n",
      "4 - random_1 - deep - 8.820022142070014 - 9.606977017928653 - 6.009644924666821 - 11.45034731278939 - 17.968321699475187 - 10.770552584248263 - 0.9543291126206918\n",
      "5 - random_59 - deep - 6.5454984639943525 - 6.758544790764292 - 6.103265019540998 - 9.500744471093068 - 25.332817496284903 - 10.84680016087757 - 0.954005796388034\n",
      "6 - random_83 - deep - 9.132638854793168 - 9.88492751940963 - 7.650007874334961 - 10.757322543473833 - 17.116600231886107 - 10.907841219023979 - 0.9537469610803455\n",
      "7 - random_39 - deep - 6.86467381507012 - 8.577284154252023 - 7.003635017526237 - 10.458490877635747 - 21.782771595572957 - 10.936317691033313 - 0.9536262109390761\n",
      "8 - random_87 - deep - 9.591051292887686 - 9.928160022791786 - 7.964778551793704 - 12.002320319877894 - 15.844098512116853 - 11.065653805796035 - 0.9530777808483076\n",
      "9 - random_7 - deep - 8.345489717349288 - 10.644410606296871 - 7.061051931857672 - 11.681684226310225 - 18.90712838090901 - 11.327352664624346 - 0.9519680867063077\n",
      "10 - random_98 - deep - 10.827458913899873 - 7.994082135576663 - 7.116821232045116 - 14.383903774836336 - 18.01002699971492 - 11.66571975795087 - 0.9505332925960427\n",
      "11 - random_35 - deep - 11.110560755097534 - 9.984226474980481 - 7.603945034630078 - 13.315264213875999 - 16.599770405368666 - 11.722368415456629 - 0.9502930825941048\n",
      "12 - random_38 - deep - 10.113525965014768 - 10.159051621994294 - 6.33290873815738 - 12.240612707794153 - 20.170954792544453 - 11.802864964336807 - 0.9499517492419325\n",
      "13 - random_74 - deep - 9.23080498028894 - 10.394778142389026 - 6.994467568924738 - 9.894535338849343 - 23.175846472419156 - 11.937390534733838 - 0.9493813140551413\n",
      "14 - random_34 - deep - 9.46002191958763 - 11.774634365169193 - 7.078968640334483 - 14.021272275020214 - 17.484042000516723 - 11.963347006061326 - 0.9492712495928504\n",
      "15 - random_90 - deep - 7.595128773472313 - 14.622320732391408 - 6.502217566937721 - 14.118738551299948 - 17.361157728159263 - 12.039607784390217 - 0.9489478773803967\n",
      "16 - random_44 - deep - 9.740283725694821 - 10.072496225322718 - 6.500976538287139 - 10.218064598623208 - 24.121856076320988 - 12.130007074525196 - 0.9485645529625797\n",
      "17 - random_64 - deep - 17.1164923260528 - 10.829608022873998 - 7.9538056664931585 - 10.457449399087391 - 14.88143995177248 - 12.248324163670285 - 0.9480628473712347\n",
      "18 - random_60 - deep - 9.537977507383655 - 9.611352725427006 - 8.562594127889348 - 12.290543086991557 - 21.85820404571358 - 12.371218274960283 - 0.9475417336147767\n",
      "19 - random_67 - deep - 9.084528671739145 - 9.259823190405013 - 7.346201403037531 - 11.170672286640514 - 25.062296899394067 - 12.383652479862825 - 0.947489008263193\n",
      "20 - random_29 - deep - 7.651454427628587 - 9.439089905415736 - 7.995207907624366 - 11.918936876270442 - 25.169781284972746 - 12.433620086847574 - 0.9472771282381559\n",
      "21 - random_78 - deep - 8.194250665592874 - 12.04101844153896 - 9.582719403734762 - 12.731468619527044 - 20.07083730517988 - 12.523270901041109 - 0.9468969776185411\n",
      "22 - random_9 - deep - 9.764407516891554 - 11.916917937476974 - 9.85930973384148 - 11.47092040497001 - 19.703502986979817 - 12.542454445737413 - 0.9468156327197991\n",
      "23 - random_63 - deep - 8.316906090844087 - 10.470420238039107 - 9.909878086399388 - 10.602487590252426 - 23.511038040548538 - 12.561108260208657 - 0.946736534061374\n",
      "24 - random_17 - deep - 13.617373124667353 - 10.3417157190326 - 6.836537090117184 - 11.973941703588626 - 20.766931151094052 - 12.707061358352181 - 0.9461176421841158\n",
      "25 - random_25 - deep - 10.562950493270948 - 10.314274984568893 - 7.960588215414761 - 10.695593128626118 - 25.053874358693466 - 12.916644365202666 - 0.9452289373727689\n",
      "26 - random_91 - deep - 10.775702972849144 - 11.854144468635273 - 7.904855595759736 - 11.360214189659075 - 22.932287254458465 - 12.964900416554617 - 0.9450243149386435\n",
      "27 - random_77 - deep - 8.376413025364354 - 10.53947632441544 - 9.786798172559434 - 11.412733754596195 - 25.100094699156664 - 13.041929150781968 - 0.9446976863261977\n",
      "28 - random_73 - deep - 7.9652308902646825 - 13.003598683009171 - 9.483937673545293 - 15.766737568876374 - 19.96698984525713 - 13.236397538178737 - 0.9438730727559874\n",
      "29 - random_57 - deep - 13.81493567483905 - 10.748424102358655 - 7.141529981471006 - 13.18816987536756 - 21.527939323906427 - 13.283871435637309 - 0.9436717669262886\n",
      "30 - random_93 - deep - 8.212067184198508 - 8.78585536819433 - 5.852539317125575 - 9.974807312017967 - 33.7628589305206 - 13.316047811423505 - 0.9435353279067201\n",
      "31 - random_55 - deep - 11.913200824038292 - 9.534548206142045 - 6.238147298202077 - 17.441172372113478 - 21.712506045092333 - 13.367049252273334 - 0.9433190640666789\n",
      "32 - random_72 - deep - 10.425143336313251 - 11.89836512532835 - 9.157874342459914 - 13.392998635231912 - 22.397916156478246 - 13.453708693395594 - 0.9429515979088481\n",
      "33 - random_65 - deep - 15.17693110027407 - 6.344937910620008 - 6.691614983610986 - 11.243705050853507 - 28.003481286163705 - 13.491239822392176 - 0.942792452874068\n",
      "34 - random_94 - deep - 8.312731803731715 - 12.097069780877296 - 7.440156336791392 - 12.823409493588503 - 27.886320496855166 - 13.710788955566503 - 0.9418614882223477\n",
      "35 - random_24 - deep - 8.213547367901342 - 10.079688495192707 - 8.843384348301493 - 13.541834973293088 - 29.180876710783938 - 13.970286214924116 - 0.9407611296274978\n",
      "36 - random_12 - deep - 14.035210795176205 - 13.461898888776814 - 10.60095466032661 - 12.892193795226813 - 18.926347332641203 - 13.98324422720537 - 0.9407061831505248\n",
      "37 - random_0 - deep - 10.707637811838703 - 14.454446158900783 - 8.183836109027034 - 13.639974670269565 - 23.124657241171448 - 14.021638737964489 - 0.9405433770769137\n",
      "38 - random_8 - deep - 10.041625729949502 - 10.88432570799283 - 9.162887204191316 - 11.690284599543203 - 28.5557421825637 - 14.065792701911594 - 0.9403561489194905\n",
      "39 - random_31 - deep - 8.74330190355766 - 6.616497465904723 - 7.975137802345844 - 28.445882393433166 - 18.65024067318703 - 14.084114149406338 - 0.9402784596125944\n",
      "40 - random_33 - deep - 14.280725569654798 - 14.57955609679027 - 7.901298767030483 - 14.635686588521672 - 19.227592967359087 - 14.125072120712169 - 0.9401047835750688\n",
      "41 - random_40 - deep - 15.763469237157445 - 9.505746225678589 - 7.892708075427306 - 11.310814607153166 - 26.37290060119879 - 14.168625286156745 - 0.9399201030121661\n",
      "42 - random_88 - deep - 11.06975191431623 - 16.335735538001927 - 9.697478103403377 - 13.215554891308246 - 20.6559527582267 - 14.194733308907097 - 0.9398093959191548\n",
      "43 - random_21 - deep - 11.076382857102613 - 14.041427347742008 - 9.081903980170772 - 11.820969692514531 - 25.675538370786974 - 14.338661524813547 - 0.9391990902465395\n",
      "44 - random_66 - deep - 7.838620371592221 - 18.411629753300094 - 14.036441267171323 - 12.029402654352586 - 19.600798533452913 - 14.382966583521727 - 0.9390112213948008\n",
      "45 - random_76 - deep - 14.123920518715916 - 15.398745425984433 - 7.884998857340395 - 12.524327021949512 - 22.322185216430363 - 14.450937148710258 - 0.9387230025681852\n",
      "46 - random_49 - deep - 12.358104440467448 - 10.970255800237828 - 7.192526347123629 - 11.924549981475755 - 30.118010431113152 - 14.51175635822424 - 0.9384651079758247\n",
      "47 - random_95 - deep - 13.887281623877792 - 12.686402309155504 - 8.705236733301462 - 10.955745172539663 - 27.346525450010557 - 14.715770223104796 - 0.937600018262563\n",
      "48 - random_81 - deep - 10.839223532356725 - 11.438874458525262 - 9.923302851184092 - 14.3276319013669 - 28.793481295661394 - 15.063217302114161 - 0.9361267218563124\n",
      "49 - random_86 - deep - 14.618445510364788 - 12.008941018249711 - 10.839531906309135 - 12.146646476006723 - 26.38024088635394 - 15.198143793148613 - 0.935554586626637\n",
      "50 - random_80 - deep - 11.848765964789008 - 14.037151750292988 - 12.375476183410944 - 11.848667316803866 - 27.40291676376805 - 15.501757385813377 - 0.9342671594413597\n",
      "51 - random_82 - deep - 8.342797227850133 - 12.141466226593367 - 19.603783872262266 - 17.173684197307903 - 20.464274763377546 - 15.543464267651935 - 0.9340903077628139\n",
      "52 - random_4 - deep - 8.556454881708673 - 13.020580602355558 - 12.08877030168763 - 14.700136087840937 - 29.458151466625818 - 15.563254508173994 - 0.9340063902628506\n",
      "53 - random_28 - deep - 9.021792635004571 - 9.498723345380368 - 7.059886680090652 - 20.28872149621337 - 33.140115017777674 - 15.799705734014557 - 0.933003754862151\n",
      "54 - random_84 - deep - 17.77190507648424 - 12.215879825835533 - 9.365415257571858 - 13.661853450522083 - 26.022885376463943 - 15.807321158882795 - 0.9329714628131793\n",
      "55 - random_3 - deep - 9.571443411020132 - 12.394262685323113 - 10.314060813378937 - 14.209947725478788 - 32.69996268470008 - 15.836345373595204 - 0.932848390052424\n",
      "56 - random_37 - deep - 12.193490716712565 - 13.668665629947244 - 11.233125335168488 - 11.0598222238914 - 32.18282595079127 - 16.066558964368948 - 0.9318722043929445\n",
      "57 - random_22 - deep - 16.736614255390073 - 12.309537467706809 - 8.061898582202309 - 15.120774216577717 - 28.269031366103015 - 16.099054904974818 - 0.9317344103074428\n",
      "58 - random_48 - deep - 12.170441250559156 - 13.53926678921314 - 8.47447781871324 - 15.752754500808528 - 30.88238018638867 - 16.162780426012915 - 0.9314641919438301\n",
      "59 - random_75 - deep - 10.883400013325844 - 12.273388917239325 - 8.142829727091621 - 15.596326254509592 - 35.22638329514512 - 16.422878688801067 - 0.9303612848854936\n",
      "60 - random_14 - deep - 11.789443225205043 - 14.185837790930915 - 9.935165691141414 - 14.95420842362075 - 32.3177764292724 - 16.635291122080933 - 0.9294605823224258\n",
      "61 - random_20 - deep - 39.16207127797428 - 7.318969784313645 - 7.831019791494521 - 11.125705932028268 - 18.427248875885883 - 16.77512115481405 - 0.9288676543712139\n",
      "62 - random_41 - deep - 11.182869110482024 - 17.755222688915296 - 8.1350486702259 - 18.33685722515097 - 28.978743638297168 - 16.876959291107994 - 0.9284358252689261\n",
      "63 - random_23 - deep - 11.350582731531022 - 12.786688141268716 - 18.882350515479636 - 15.077111395727309 - 27.136569926818797 - 17.045030146758197 - 0.92772314641052\n",
      "64 - random_19 - deep - 14.644790571371974 - 12.446621086320393 - 9.88012947594895 - 16.844071190831702 - 32.486593880680715 - 17.259224780931948 - 0.9268148866960734\n",
      "65 - random_92 - deep - 17.49085430388755 - 13.79361206810135 - 8.634275291905258 - 15.668314896285974 - 30.828361815062827 - 17.28254548881785 - 0.9267159987871076\n",
      "66 - random_61 - deep - 11.111676331627779 - 22.76433100130671 - 11.895224247088109 - 16.161543610250238 - 24.759658569004767 - 17.338355643954106 - 0.9264793443267247\n",
      "67 - random_16 - deep - 14.38338790743714 - 16.897919384040147 - 9.222873071487765 - 13.51767848711537 - 33.46603755341701 - 17.49697104110285 - 0.9258067598995897\n",
      "68 - random_96 - deep - 33.82618497982743 - 13.470626386995207 - 7.724528352907698 - 13.340178411188523 - 20.486696780655446 - 17.771568224691908 - 0.9246423723764545\n",
      "69 - random_46 - deep - 19.16498477798437 - 15.584358277765702 - 10.399757078297307 - 17.48997126164542 - 28.524801799452373 - 18.232493529023824 - 0.9226878888437151\n",
      "70 - random_50 - deep - 12.605820535637939 - 17.939984433974846 - 14.012108086563348 - 16.017238188922455 - 31.554113951986878 - 18.424820005043824 - 0.9218723577223857\n",
      "71 - random_15 - deep - 9.540106974725052 - 16.03993334590706 - 11.546460853845344 - 14.34311277372343 - 42.14972473225762 - 18.721924620818292 - 0.920612530861993\n",
      "72 - random_2 - deep - 18.52165936409159 - 19.136198137473748 - 22.287460430248363 - 11.309546325560186 - 24.64734511410575 - 19.180326942365646 - 0.9186687456533845\n",
      "73 - random_45 - deep - 17.232204743180844 - 20.97682205549824 - 9.98956318042202 - 22.3814328628715 - 27.89457114810928 - 19.694725516187283 - 0.9164875168678306\n",
      "74 - random_56 - deep - 11.113954569040851 - 16.70346545392675 - 6.77581204653372 - 17.079380891442202 - 46.83968665695503 - 19.70056254677312 - 0.9164627658288815\n",
      "75 - random_79 - deep - 26.206360743596004 - 7.524766065094943 - 13.22555059375185 - 16.263619559598105 - 37.022056963871385 - 20.04742841589449 - 0.9149919339546158\n",
      "76 - random_85 - deep - 25.525780847927162 - 13.213472968817928 - 23.830927468439675 - 11.133268950124142 - 27.343019333947982 - 20.209019155059014 - 0.9143067330429452\n",
      "77 - random_69 - deep - 9.947689936711239 - 19.96999430773496 - 8.831297386288155 - 28.942375776711103 - 36.05063212845386 - 20.74650121826745 - 0.9120276222373315\n",
      "78 - random_27 - deep - 13.957714427317418 - 22.632999891540226 - 10.951942775017116 - 16.64726336933764 - 41.500669299726304 - 21.137187327804668 - 0.9103709773094353\n",
      "79 - random_10 - deep - 9.425579701624994 - 30.077315219685605 - 7.729342117746964 - 45.80480305783384 - 22.19570557003037 - 23.0454701729803 - 0.9022791946243629\n",
      "80 - random_26 - deep - 41.06370960241839 - 14.78580060450028 - 15.380237275903875 - 15.269655804425161 - 31.954520563723904 - 23.69217131155899 - 0.8995369569687673\n",
      "81 - random_54 - deep - 9.752599347827868 - 29.575953610009726 - 9.320080328339149 - 13.28857665429049 - 69.48165791704457 - 26.281605162365786 - 0.8885568572151743\n",
      "82 - random_18 - deep - 18.921754627883338 - 60.605638357309196 - 11.142309173221573 - 22.546304057212662 - 28.144479325909188 - 28.275860438998787 - 0.8801005215320916\n",
      "83 - random_70 - deep - 13.335169500297884 - 148.690354436197 - 9.853681084760186 - 19.398673318820737 - 25.453612503314194 - 43.358635043103206 - 0.8161443136287975\n",
      "84 - random_42 - deep - 137.3897751279041 - 22.320417714001895 - 32.1298212056078 - 13.08043334173629 - 56.24546939559299 - 52.24222943500965 - 0.7784747849929526\n",
      "85 - random_62 - deep - 25.067888250522646 - 69.37897890406232 - 69.15678503784754 - 133.57924768106554 - 58.58007232797234 - 71.14475877878441 - 0.6983214890417162\n",
      "86 - random_51 - deep - 13.965525428909327 - 11.395110543933125 - 153.8179931656246 - 239.24597801564659 - 16.886806227163053 - 87.03792243329737 - 0.6309289498299653\n",
      "87 - random_11 - deep - 257.88830931018884 - 202.84016755598853 - 9.062741467837522 - 14.999221349914576 - 24.7838303474595 - 101.9569198301936 - 0.5676672142229804\n",
      "88 - random_32 - deep - 7.676251802428849 - 202.9640411152192 - 154.23861844455678 - 23.408693682258974 - 325.7158038087012 - 142.78841079436532 - 0.39452749731735215\n",
      "89 - random_53 - deep - 27.428854993829557 - 11.783475916436378 - 239.38039515039944 - 236.69417682619587 - 324.28489830648186 - 167.8657905690866 - 0.28819068882951027\n",
      "90 - random_52 - deep - 43.871636071493896 - 32.754218767980895 - 197.39922203420127 - 239.45309620700246 - 411.2853799705912 - 184.90468609436533 - 0.21593984816782696\n",
      "91 - random_36 - deep - 185.23965469085644 - 202.82576644323072 - 153.8746289371175 - 239.42895270370246 - 325.7245770922262 - 221.4097471176209 - 0.061145698310914764\n",
      "92 - random_58 - deep - 623.3850032225763 - 15.191739917386768 - 462.17170769068963 - 12.680299644485837 - 20.507616479703387 - 226.81756718130268 - 0.03821466119243133\n",
      "93 - random_30 - deep - 257.0104155345361 - 203.1262080290899 - 154.07742613246458 - 239.2928418683576 - 325.72690287815755 - 235.84486870526123 - -6.423564362578738e-05\n",
      "94 - random_68 - deep - 256.98296513534024 - 203.19908477982992 - 154.33951129335347 - 239.20229472692242 - 325.7218590673123 - 235.88724737305157 - -0.00024393592815608045\n",
      "95 - random_13 - deep - 257.8896043468419 - 202.80598166258167 - 153.7906374950862 - 239.58263992696976 - 325.61782553545476 - 235.93550832515012 - -0.00044857914315099556\n",
      "96 - random_89 - deep - 256.9932092145305 - 203.21792379026522 - 154.53123460686862 - 239.59503473755183 - 325.7874504045616 - 236.02303668299916 - -0.0008197298099854589\n",
      "97 - random_47 - deep - 256.8611107684196 - 203.3518000476294 - 154.6518052684587 - 239.88153429835864 - 325.6312650044759 - 236.07354856385138 - -0.0010339177454392612\n",
      "98 - random_97 - deep - 257.31881903665544 - 203.05891169971022 - 154.4450619624932 - 240.8455825324531 - 325.715822133932 - 236.2748457100667 - -0.0018874876270833507\n",
      "99 - random_43 - deep - 275.1415472452067 - 280.4297133225661 - 455.92521381300065 - 386.12649023132576 - 330.8625879131508 - 345.6748692824943 - -0.46578162089348063\n"
     ]
    }
   ],
   "source": [
    "scores_df_sorted = pd.DataFrame(all_scores).sort_values(by='MSE')\n",
    "\n",
    "for i,(index,row) in enumerate(scores_df_sorted.iterrows()):\n",
    "    s = f\"{i} - \" + \" - \".join([f\"{i}\" for i in row.tolist()])\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% lwr part\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691ad233e96f421385bfa608332fdda7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_0'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.2806,lwr_k=10:0.0,lwr_k=20:0.1466,lwr_k=30:1.8565,lwr_k=40:2.9958,lwr_k=50:3.6424,lwr_k=100:4.7932,lwr_k=200:5.6109,lwr_k=300:5.9273,lwr_k=400:6.1253,lwr_k=500:6.2913,lwr_k=600:6.442,lwr_k=700:6.5077,lwr_k=800:6.5856,lwr_k=900:6.6239,lwr_k=1000:6.6726'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.4396,lwr_k=10:63.867,lwr_k=20:14950877.5446,lwr_k=30:56.5157,lwr_k=40:38.5629,lwr_k=50:11.603,lwr_k=100:7.8087,lwr_k=200:7.2414,lwr_k=300:7.3506,lwr_k=400:7.5276,lwr_k=500:7.6354,lwr_k=600:7.693,lwr_k=700:7.7167,lwr_k=800:7.7521,lwr_k=900:7.7968,lwr_k=1000:7.8073'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.7962,lwr_k=10:0.0,lwr_k=20:0.0121,lwr_k=30:1.7634,lwr_k=40:3.0052,lwr_k=50:3.7662,lwr_k=100:5.4055,lwr_k=200:6.3487,lwr_k=300:6.6461,lwr_k=400:6.893,lwr_k=500:6.9855,lwr_k=600:7.0962,lwr_k=700:7.1664,lwr_k=800:7.2054,lwr_k=900:7.2373,lwr_k=1000:7.2748'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.7423,lwr_k=10:64.5784,lwr_k=20:60055.8032,lwr_k=30:297.7226,lwr_k=40:77.3355,lwr_k=50:70.7594,lwr_k=100:8.8418,lwr_k=200:7.5358,lwr_k=300:7.569,lwr_k=400:7.5838,lwr_k=500:7.5772,lwr_k=600:7.6279,lwr_k=700:7.6714,lwr_k=800:7.7053,lwr_k=900:7.7507,lwr_k=1000:7.818'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.7898,lwr_k=10:0.0,lwr_k=20:0.0707,lwr_k=30:1.9088,lwr_k=40:3.2045,lwr_k=50:3.9773,lwr_k=100:5.5102,lwr_k=200:6.4576,lwr_k=300:6.7731,lwr_k=400:7.0068,lwr_k=500:7.1318,lwr_k=600:7.2505,lwr_k=700:7.3692,lwr_k=800:7.4083,lwr_k=900:7.4791,lwr_k=1000:7.5369'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.6191,lwr_k=10:61.248,lwr_k=20:7039.4699,lwr_k=30:44.5137,lwr_k=40:42.4709,lwr_k=50:11.488,lwr_k=100:6.9906,lwr_k=200:6.7867,lwr_k=300:6.6593,lwr_k=400:6.6862,lwr_k=500:6.6191,lwr_k=600:6.5893,lwr_k=700:6.5625,lwr_k=800:6.5977,lwr_k=900:6.575,lwr_k=1000:6.6009'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.5549,lwr_k=10:0.0,lwr_k=20:0.1162,lwr_k=30:1.9764,lwr_k=40:2.9697,lwr_k=50:3.6288,lwr_k=100:4.9279,lwr_k=200:5.605,lwr_k=300:5.865,lwr_k=400:5.9819,lwr_k=500:6.0437,lwr_k=600:6.1445,lwr_k=700:6.2192,lwr_k=800:6.2797,lwr_k=900:6.3354,lwr_k=1000:6.4137'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.4392,lwr_k=10:191.6609,lwr_k=20:775155.7355,lwr_k=30:1846337.0778,lwr_k=40:491803.3726,lwr_k=50:30165.0985,lwr_k=100:2236.2879,lwr_k=200:11.8645,lwr_k=300:8.9533,lwr_k=400:8.7413,lwr_k=500:8.8409,lwr_k=600:8.9897,lwr_k=700:8.9249,lwr_k=800:8.9776,lwr_k=900:9.0142,lwr_k=1000:9.0393'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.263,lwr_k=10:0.0,lwr_k=20:0.2528,lwr_k=30:1.6671,lwr_k=40:2.5862,lwr_k=50:3.2437,lwr_k=100:4.4641,lwr_k=200:5.1423,lwr_k=300:5.3563,lwr_k=400:5.498,lwr_k=500:5.6019,lwr_k=600:5.6694,lwr_k=700:5.7173,lwr_k=800:5.7463,lwr_k=900:5.7834,lwr_k=1000:5.8092'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:19.94,lwr_k=10:66.4025,lwr_k=20:730741.971,lwr_k=30:744.4324,lwr_k=40:18.9117,lwr_k=50:375.9481,lwr_k=100:17.9181,lwr_k=200:17.8737,lwr_k=300:18.0581,lwr_k=400:17.9891,lwr_k=500:18.1548,lwr_k=600:18.3399,lwr_k=700:18.3261,lwr_k=800:18.2875,lwr_k=900:18.3531,lwr_k=1000:18.4722'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:7.592,lwr_k=10:0.0,lwr_k=20:0.0443,lwr_k=30:1.5697,lwr_k=40:2.6371,lwr_k=50:3.3549,lwr_k=100:4.902,lwr_k=200:5.687,lwr_k=300:5.9621,lwr_k=400:6.1278,lwr_k=500:6.2712,lwr_k=600:6.4258,lwr_k=700:6.5218,lwr_k=800:6.5895,lwr_k=900:6.6666,lwr_k=1000:6.7146'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.2015,lwr_k=10:78.8875,lwr_k=20:1400.6558,lwr_k=30:93.5668,lwr_k=40:21.1164,lwr_k=50:8.999,lwr_k=100:10.471,lwr_k=200:8.7151,lwr_k=300:9.0448,lwr_k=400:9.19,lwr_k=500:9.4569,lwr_k=600:9.4582,lwr_k=700:9.5011,lwr_k=800:9.4953,lwr_k=900:9.4977,lwr_k=1000:9.533'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_1'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.4501,lwr_k=10:0.0,lwr_k=20:0.0893,lwr_k=30:1.4376,lwr_k=40:2.1544,lwr_k=50:2.4698,lwr_k=100:3.3062,lwr_k=200:3.6581,lwr_k=300:4.0718,lwr_k=400:4.1036,lwr_k=500:4.0128,lwr_k=600:4.0429,lwr_k=700:4.077,lwr_k=800:4.1167,lwr_k=900:4.1328,lwr_k=1000:4.1314'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.2097,lwr_k=10:45.3894,lwr_k=20:597425.9027,lwr_k=30:141169.09,lwr_k=40:7115.8555,lwr_k=50:788.5794,lwr_k=100:11.1512,lwr_k=200:8.8789,lwr_k=300:9.3047,lwr_k=400:8.5475,lwr_k=500:8.3438,lwr_k=600:8.2897,lwr_k=700:8.3793,lwr_k=800:8.3503,lwr_k=900:8.2503,lwr_k=1000:8.7103'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.4414,lwr_k=10:0.0,lwr_k=20:0.0836,lwr_k=30:1.9629,lwr_k=40:2.9216,lwr_k=50:3.8597,lwr_k=100:5.3759,lwr_k=200:6.3298,lwr_k=300:6.6357,lwr_k=400:6.9515,lwr_k=500:7.0406,lwr_k=600:7.1114,lwr_k=700:7.1872,lwr_k=800:7.2301,lwr_k=900:7.2826,lwr_k=1000:7.3805'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.1796,lwr_k=10:65.519,lwr_k=20:45013.8367,lwr_k=30:37563493.5269,lwr_k=40:40051.133,lwr_k=50:7310.4585,lwr_k=100:4266.1194,lwr_k=200:28.2287,lwr_k=300:12.8944,lwr_k=400:11.2826,lwr_k=500:11.4652,lwr_k=600:8.9684,lwr_k=700:9.0749,lwr_k=800:9.0561,lwr_k=900:9.2615,lwr_k=1000:9.3474'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.4527,lwr_k=10:0.0,lwr_k=20:0.1389,lwr_k=30:2.4046,lwr_k=40:3.5838,lwr_k=50:3.8645,lwr_k=100:5.3376,lwr_k=200:6.4363,lwr_k=300:6.8023,lwr_k=400:7.0355,lwr_k=500:7.31,lwr_k=600:7.3419,lwr_k=700:7.5762,lwr_k=800:7.6482,lwr_k=900:7.6549,lwr_k=1000:7.7233'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:5.891,lwr_k=10:52.8021,lwr_k=20:7307.5348,lwr_k=30:3896790.6624,lwr_k=40:158490.512,lwr_k=50:49991.7062,lwr_k=100:24832.711,lwr_k=200:23.3438,lwr_k=300:6.0331,lwr_k=400:5.9203,lwr_k=500:5.8019,lwr_k=600:5.7015,lwr_k=700:5.6084,lwr_k=800:5.5981,lwr_k=900:5.5822,lwr_k=1000:5.571'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.1579,lwr_k=10:0.2254,lwr_k=20:3.1402,lwr_k=30:8.247,lwr_k=40:6.6175,lwr_k=50:6.9975,lwr_k=100:6.7862,lwr_k=200:6.8517,lwr_k=300:7.1743,lwr_k=400:7.5212,lwr_k=500:7.8198,lwr_k=600:7.4201,lwr_k=700:7.5588,lwr_k=800:7.3926,lwr_k=900:7.5117,lwr_k=1000:7.5773'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.5127,lwr_k=10:130204.7239,lwr_k=20:7376665.5771,lwr_k=30:97230916.3321,lwr_k=40:667118.9103,lwr_k=50:127710.2402,lwr_k=100:19.691,lwr_k=200:11.0768,lwr_k=300:10.2994,lwr_k=400:9.8336,lwr_k=500:9.868,lwr_k=600:9.7588,lwr_k=700:9.9434,lwr_k=800:9.844,lwr_k=900:9.934,lwr_k=1000:9.8919'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.8846,lwr_k=10:0.0,lwr_k=20:0.1472,lwr_k=30:1.6541,lwr_k=40:2.6235,lwr_k=50:2.8401,lwr_k=100:3.7869,lwr_k=200:4.2388,lwr_k=300:4.4021,lwr_k=400:4.4725,lwr_k=500:4.5012,lwr_k=600:4.5207,lwr_k=700:4.5715,lwr_k=800:4.622,lwr_k=900:4.6458,lwr_k=1000:4.6555'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:17.5362,lwr_k=10:73.8005,lwr_k=20:2447.2239,lwr_k=30:7797087.0571,lwr_k=40:808373.0942,lwr_k=50:2768256.004,lwr_k=100:19.2222,lwr_k=200:25.0151,lwr_k=300:24.4547,lwr_k=400:18.4024,lwr_k=500:17.6015,lwr_k=600:19.6666,lwr_k=700:16.6158,lwr_k=800:16.6548,lwr_k=900:16.7533,lwr_k=1000:16.9869'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:6.8363,lwr_k=10:0.0,lwr_k=20:0.2105,lwr_k=30:1.9555,lwr_k=40:3.3154,lwr_k=50:3.8575,lwr_k=100:4.7762,lwr_k=200:5.4719,lwr_k=300:5.8446,lwr_k=400:5.9399,lwr_k=500:6.0286,lwr_k=600:6.0854,lwr_k=700:6.1572,lwr_k=800:6.2317,lwr_k=900:6.3,lwr_k=1000:6.358'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.2713,lwr_k=10:133.1906,lwr_k=20:45180.6182,lwr_k=30:133908758.7309,lwr_k=40:164203623.7267,lwr_k=50:10814385.8758,lwr_k=100:10759.5836,lwr_k=200:397.7599,lwr_k=300:29.7379,lwr_k=400:16.384,lwr_k=500:65.5213,lwr_k=600:91.0275,lwr_k=700:57.5751,lwr_k=800:80.8266,lwr_k=900:62.2071,lwr_k=1000:66.3325'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_2'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.5547,lwr_k=10:0.0,lwr_k=20:0.0198,lwr_k=30:1.7082,lwr_k=40:2.96,lwr_k=50:3.6488,lwr_k=100:5.6975,lwr_k=200:7.2295,lwr_k=300:7.7223,lwr_k=400:8.3304,lwr_k=500:8.6106,lwr_k=600:8.8694,lwr_k=700:9.0405,lwr_k=800:9.2536,lwr_k=900:9.4115,lwr_k=1000:9.6033'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:14.6005,lwr_k=10:88.1887,lwr_k=20:1102.8928,lwr_k=30:84.707,lwr_k=40:403.114,lwr_k=50:229.9021,lwr_k=100:51.1724,lwr_k=200:11.3332,lwr_k=300:11.2144,lwr_k=400:11.2869,lwr_k=500:11.3756,lwr_k=600:11.6743,lwr_k=700:11.9157,lwr_k=800:12.029,lwr_k=900:12.1336,lwr_k=1000:12.2884'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:14.0798,lwr_k=10:0.0,lwr_k=20:0.0002,lwr_k=30:1.8661,lwr_k=40:3.1348,lwr_k=50:4.0978,lwr_k=100:6.1931,lwr_k=200:8.2518,lwr_k=300:9.2508,lwr_k=400:9.8621,lwr_k=500:10.3072,lwr_k=600:10.5605,lwr_k=700:10.7703,lwr_k=800:10.9473,lwr_k=900:11.0657,lwr_k=1000:11.2373'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:17.1827,lwr_k=10:72.3341,lwr_k=20:1309.5991,lwr_k=30:69.9641,lwr_k=40:143.8253,lwr_k=50:41.2175,lwr_k=100:12.7813,lwr_k=200:12.2837,lwr_k=300:12.109,lwr_k=400:12.6556,lwr_k=500:13.2708,lwr_k=600:13.4827,lwr_k=700:13.7905,lwr_k=800:14.0554,lwr_k=900:14.3748,lwr_k=1000:14.473'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:20.9875,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:2.1738,lwr_k=40:4.0843,lwr_k=50:5.1181,lwr_k=100:7.5865,lwr_k=200:9.1998,lwr_k=300:10.2086,lwr_k=400:11.3237,lwr_k=500:11.7293,lwr_k=600:12.4554,lwr_k=700:12.941,lwr_k=800:13.3233,lwr_k=900:13.6042,lwr_k=1000:13.8319'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:16.5141,lwr_k=10:90.69,lwr_k=20:478.8219,lwr_k=30:48.7045,lwr_k=40:26.5598,lwr_k=50:16.587,lwr_k=100:10.26,lwr_k=200:9.6086,lwr_k=300:9.7035,lwr_k=400:9.8354,lwr_k=500:9.9201,lwr_k=600:10.1291,lwr_k=700:10.1476,lwr_k=800:10.2696,lwr_k=900:10.467,lwr_k=1000:10.6365'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.698,lwr_k=10:0.0,lwr_k=20:0.0005,lwr_k=30:1.2305,lwr_k=40:2.1912,lwr_k=50:2.6676,lwr_k=100:3.8174,lwr_k=200:4.4445,lwr_k=300:4.7471,lwr_k=400:4.854,lwr_k=500:4.9588,lwr_k=600:5.0281,lwr_k=700:5.0743,lwr_k=800:5.114,lwr_k=900:5.1557,lwr_k=1000:5.1944'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.2887,lwr_k=10:89.3594,lwr_k=20:2952.5489,lwr_k=30:54.8754,lwr_k=40:39.0804,lwr_k=50:32.0392,lwr_k=100:11.1422,lwr_k=200:9.14,lwr_k=300:8.8536,lwr_k=400:8.6632,lwr_k=500:8.6785,lwr_k=600:8.8006,lwr_k=700:8.8496,lwr_k=800:8.8824,lwr_k=900:8.9512,lwr_k=1000:8.9429'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.5339,lwr_k=10:0.0,lwr_k=20:0.0082,lwr_k=30:1.4433,lwr_k=40:2.3267,lwr_k=50:3.0161,lwr_k=100:4.2415,lwr_k=200:4.9623,lwr_k=300:5.1999,lwr_k=400:5.3817,lwr_k=500:5.4716,lwr_k=600:5.561,lwr_k=700:5.6064,lwr_k=800:5.6848,lwr_k=900:5.7631,lwr_k=1000:5.826'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:20.0889,lwr_k=10:101.7058,lwr_k=20:1838.0157,lwr_k=30:311.0186,lwr_k=40:153.4787,lwr_k=50:29.2823,lwr_k=100:18.3554,lwr_k=200:48.7629,lwr_k=300:24.2605,lwr_k=400:24.9284,lwr_k=500:19.6738,lwr_k=600:18.4608,lwr_k=700:17.06,lwr_k=800:17.7225,lwr_k=900:17.7389,lwr_k=1000:18.095'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:5.6317,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.2411,lwr_k=40:2.0337,lwr_k=50:2.4845,lwr_k=100:3.5285,lwr_k=200:4.0991,lwr_k=300:4.4112,lwr_k=400:4.5635,lwr_k=500:4.6686,lwr_k=600:4.737,lwr_k=700:4.7959,lwr_k=800:4.8547,lwr_k=900:4.895,lwr_k=1000:4.9229'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.8972,lwr_k=10:38.7986,lwr_k=20:720.7517,lwr_k=30:70.3403,lwr_k=40:19.0794,lwr_k=50:17.9552,lwr_k=100:11.1337,lwr_k=200:9.6776,lwr_k=300:9.3336,lwr_k=400:9.2665,lwr_k=500:9.1693,lwr_k=600:9.326,lwr_k=700:9.3501,lwr_k=800:9.389,lwr_k=900:9.3921,lwr_k=1000:9.5138'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_3'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.7113,lwr_k=10:3.4732,lwr_k=20:4.734,lwr_k=30:5.2614,lwr_k=40:5.6813,lwr_k=50:5.8998,lwr_k=100:6.4276,lwr_k=200:6.7379,lwr_k=300:6.9017,lwr_k=400:7.0118,lwr_k=500:7.089,lwr_k=600:7.1604,lwr_k=700:7.2366,lwr_k=800:7.281,lwr_k=900:7.3196,lwr_k=1000:7.3479'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.8477,lwr_k=10:127.849,lwr_k=20:12.2591,lwr_k=30:11.2831,lwr_k=40:9.9138,lwr_k=50:8.5542,lwr_k=100:8.0593,lwr_k=200:8.0973,lwr_k=300:8.2026,lwr_k=400:8.3436,lwr_k=500:8.4243,lwr_k=600:8.3175,lwr_k=700:8.268,lwr_k=800:8.3043,lwr_k=900:8.3162,lwr_k=1000:8.3434'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.1858,lwr_k=10:2.0572,lwr_k=20:4.0505,lwr_k=30:4.9749,lwr_k=40:5.8146,lwr_k=50:6.1004,lwr_k=100:6.5827,lwr_k=200:7.0972,lwr_k=300:7.2393,lwr_k=400:7.3635,lwr_k=500:7.4299,lwr_k=600:7.5075,lwr_k=700:7.5416,lwr_k=800:7.5333,lwr_k=900:7.5592,lwr_k=1000:7.6333'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.8793,lwr_k=10:48773248.012,lwr_k=20:525.8756,lwr_k=30:18.0051,lwr_k=40:8.5437,lwr_k=50:8.6235,lwr_k=100:7.7293,lwr_k=200:7.2151,lwr_k=300:7.2754,lwr_k=400:7.3165,lwr_k=500:7.3963,lwr_k=600:7.4217,lwr_k=700:7.4511,lwr_k=800:7.4757,lwr_k=900:7.4819,lwr_k=1000:7.4705'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:14.526,lwr_k=10:3.5071,lwr_k=20:5.2684,lwr_k=30:6.0692,lwr_k=40:6.8746,lwr_k=50:7.3059,lwr_k=100:8.8321,lwr_k=200:10.1905,lwr_k=300:10.9135,lwr_k=400:11.1578,lwr_k=500:11.304,lwr_k=600:11.4226,lwr_k=700:11.5313,lwr_k=800:11.6476,lwr_k=900:11.7837,lwr_k=1000:11.8887'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.5954,lwr_k=10:19823217.1676,lwr_k=20:14.1693,lwr_k=30:9.9218,lwr_k=40:9.0015,lwr_k=50:8.4055,lwr_k=100:7.5594,lwr_k=200:7.1996,lwr_k=300:7.1655,lwr_k=400:7.2271,lwr_k=500:7.3101,lwr_k=600:7.3626,lwr_k=700:7.4672,lwr_k=800:7.5235,lwr_k=900:7.5915,lwr_k=1000:7.6093'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.8899,lwr_k=10:1.8856,lwr_k=20:3.9585,lwr_k=30:4.5649,lwr_k=40:4.9979,lwr_k=50:5.2664,lwr_k=100:5.7312,lwr_k=200:6.0482,lwr_k=300:6.2853,lwr_k=400:6.5059,lwr_k=500:6.6513,lwr_k=600:6.7808,lwr_k=700:7.0637,lwr_k=800:7.2024,lwr_k=900:7.4006,lwr_k=1000:7.4803'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.111,lwr_k=10:3643975119.7054,lwr_k=20:210406.9146,lwr_k=30:27535.9473,lwr_k=40:10.9524,lwr_k=50:10.9119,lwr_k=100:10.2681,lwr_k=200:9.8888,lwr_k=300:9.4364,lwr_k=400:9.4577,lwr_k=500:9.4915,lwr_k=600:9.4367,lwr_k=700:9.3351,lwr_k=800:9.3573,lwr_k=900:9.3466,lwr_k=1000:9.3622'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.1457,lwr_k=10:2.9979,lwr_k=20:4.1229,lwr_k=30:4.3582,lwr_k=40:4.9237,lwr_k=50:5.0885,lwr_k=100:5.768,lwr_k=200:6.1448,lwr_k=300:6.2946,lwr_k=400:6.4266,lwr_k=500:6.4771,lwr_k=600:6.5555,lwr_k=700:6.6059,lwr_k=800:6.66,lwr_k=900:6.6727,lwr_k=1000:6.7026'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:22.6601,lwr_k=10:12361138600.1935,lwr_k=20:165955102.9451,lwr_k=30:288227.5937,lwr_k=40:19.3905,lwr_k=50:20.243,lwr_k=100:19.179,lwr_k=200:19.7159,lwr_k=300:20.1249,lwr_k=400:20.3456,lwr_k=500:20.5454,lwr_k=600:20.6431,lwr_k=700:20.818,lwr_k=800:20.8912,lwr_k=900:20.9658,lwr_k=1000:21.004'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:64.7192,lwr_k=10:3.6857,lwr_k=20:16.782,lwr_k=30:21.3535,lwr_k=40:25.1419,lwr_k=50:26.4139,lwr_k=100:33.8012,lwr_k=200:37.6418,lwr_k=300:39.8623,lwr_k=400:41.8838,lwr_k=500:43.7284,lwr_k=600:45.0984,lwr_k=700:46.0806,lwr_k=800:46.9886,lwr_k=900:47.5892,lwr_k=1000:48.3384'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:74.7327,lwr_k=10:65358.8132,lwr_k=20:81.6745,lwr_k=30:75.2836,lwr_k=40:51.6146,lwr_k=50:49.7074,lwr_k=100:46.6715,lwr_k=200:47.4346,lwr_k=300:48.952,lwr_k=400:50.1904,lwr_k=500:51.7133,lwr_k=600:52.7348,lwr_k=700:53.603,lwr_k=800:54.5003,lwr_k=900:55.2401,lwr_k=1000:55.7962'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_4'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.5509,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.3014,lwr_k=40:1.4609,lwr_k=50:2.0286,lwr_k=100:3.4643,lwr_k=200:4.2253,lwr_k=300:4.5457,lwr_k=400:4.7246,lwr_k=500:4.8029,lwr_k=600:4.8627,lwr_k=700:4.9286,lwr_k=800:5.0044,lwr_k=900:5.0742,lwr_k=1000:5.1034'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.9244,lwr_k=10:28.3423,lwr_k=20:48.4619,lwr_k=30:101.6311,lwr_k=40:17.1693,lwr_k=50:11.5016,lwr_k=100:8.5109,lwr_k=200:8.5348,lwr_k=300:8.2467,lwr_k=400:8.2332,lwr_k=500:8.2895,lwr_k=600:8.4107,lwr_k=700:8.4586,lwr_k=800:8.4851,lwr_k=900:8.531,lwr_k=1000:8.5632'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.9189,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.3969,lwr_k=40:1.763,lwr_k=50:2.6385,lwr_k=100:5.0546,lwr_k=200:7.2702,lwr_k=300:7.9595,lwr_k=400:8.3864,lwr_k=500:8.6911,lwr_k=600:8.9061,lwr_k=700:9.0991,lwr_k=800:9.2545,lwr_k=900:9.4277,lwr_k=1000:9.5756'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.6344,lwr_k=10:36.0654,lwr_k=20:80.5235,lwr_k=30:212.1335,lwr_k=40:31.1382,lwr_k=50:18.448,lwr_k=100:10.788,lwr_k=200:8.7593,lwr_k=300:8.5312,lwr_k=400:8.8086,lwr_k=500:8.9355,lwr_k=600:9.1092,lwr_k=700:9.1752,lwr_k=800:9.2368,lwr_k=900:9.3687,lwr_k=1000:9.4363'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:15.4245,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.3428,lwr_k=40:1.8112,lwr_k=50:3.0043,lwr_k=100:5.4818,lwr_k=200:7.4898,lwr_k=300:8.6322,lwr_k=400:9.1908,lwr_k=500:9.671,lwr_k=600:10.0891,lwr_k=700:10.4447,lwr_k=800:10.711,lwr_k=900:10.9758,lwr_k=1000:11.1665'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.6684,lwr_k=10:34.0562,lwr_k=20:64.2409,lwr_k=30:129.1001,lwr_k=40:21.6374,lwr_k=50:13.5526,lwr_k=100:8.4691,lwr_k=200:7.5716,lwr_k=300:7.3439,lwr_k=400:7.3997,lwr_k=500:7.4683,lwr_k=600:7.5311,lwr_k=700:7.5736,lwr_k=800:7.5846,lwr_k=900:7.7289,lwr_k=1000:7.7183'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.5943,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.4425,lwr_k=40:1.7913,lwr_k=50:2.7798,lwr_k=100:5.0938,lwr_k=200:6.9277,lwr_k=300:7.5752,lwr_k=400:8.0851,lwr_k=500:8.3595,lwr_k=600:8.6625,lwr_k=700:8.9084,lwr_k=800:9.1494,lwr_k=900:9.2931,lwr_k=1000:9.4667'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:14.1856,lwr_k=10:32.2224,lwr_k=20:83.6922,lwr_k=30:261.0047,lwr_k=40:33.6399,lwr_k=50:21.8358,lwr_k=100:12.677,lwr_k=200:10.5764,lwr_k=300:10.6081,lwr_k=400:10.4361,lwr_k=500:10.6021,lwr_k=600:10.8314,lwr_k=700:10.9666,lwr_k=800:11.1428,lwr_k=900:11.2415,lwr_k=1000:11.2507'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.1864,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.4416,lwr_k=40:1.7437,lwr_k=50:2.6243,lwr_k=100:4.2272,lwr_k=200:5.1626,lwr_k=300:5.7072,lwr_k=400:6.0345,lwr_k=500:6.2486,lwr_k=600:6.3653,lwr_k=700:6.4611,lwr_k=800:6.545,lwr_k=900:6.6398,lwr_k=1000:6.715'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:27.9051,lwr_k=10:35.9899,lwr_k=20:147.7551,lwr_k=30:350.7193,lwr_k=40:26.6374,lwr_k=50:21.7525,lwr_k=100:18.2605,lwr_k=200:19.3856,lwr_k=300:21.0115,lwr_k=400:21.8031,lwr_k=500:22.4103,lwr_k=600:22.4923,lwr_k=700:22.7593,lwr_k=800:23.0675,lwr_k=900:23.2985,lwr_k=1000:23.515'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:10.8382,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.3267,lwr_k=40:1.6512,lwr_k=50:2.5038,lwr_k=100:4.7758,lwr_k=200:6.183,lwr_k=300:6.9339,lwr_k=400:7.2468,lwr_k=500:7.459,lwr_k=600:7.6202,lwr_k=700:7.7848,lwr_k=800:7.9603,lwr_k=900:8.0701,lwr_k=1000:8.1502'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.9681,lwr_k=10:31.209,lwr_k=20:148.7412,lwr_k=30:612.2403,lwr_k=40:66.9203,lwr_k=50:15.9848,lwr_k=100:9.6336,lwr_k=200:9.5045,lwr_k=300:10.2567,lwr_k=400:10.388,lwr_k=500:10.6858,lwr_k=600:10.6106,lwr_k=700:10.6041,lwr_k=800:10.6825,lwr_k=900:10.6923,lwr_k=1000:10.7595'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_5'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:3.4468,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0251,lwr_k=40:0.7391,lwr_k=50:1.189,lwr_k=100:2.1317,lwr_k=200:2.6149,lwr_k=300:2.7884,lwr_k=400:2.8759,lwr_k=500:2.9515,lwr_k=600:2.9971,lwr_k=700:3.0294,lwr_k=800:3.0587,lwr_k=900:3.0928,lwr_k=1000:3.1158'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:6.9563,lwr_k=10:23.2112,lwr_k=20:72.3859,lwr_k=30:754.6727,lwr_k=40:26.7953,lwr_k=50:15.1363,lwr_k=100:9.0334,lwr_k=200:7.5109,lwr_k=300:7.229,lwr_k=400:7.041,lwr_k=500:6.9374,lwr_k=600:6.9168,lwr_k=700:6.9086,lwr_k=800:6.9753,lwr_k=900:6.9663,lwr_k=1000:7.0165'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.8488,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0012,lwr_k=40:0.8258,lwr_k=50:1.6519,lwr_k=100:2.875,lwr_k=200:3.5862,lwr_k=300:3.9462,lwr_k=400:4.088,lwr_k=500:4.2001,lwr_k=600:4.2889,lwr_k=700:4.3593,lwr_k=800:4.4113,lwr_k=900:4.4698,lwr_k=1000:4.5131'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:6.5334,lwr_k=10:30.7237,lwr_k=20:58.3143,lwr_k=30:851.631,lwr_k=40:32.8072,lwr_k=50:21.3149,lwr_k=100:8.489,lwr_k=200:6.607,lwr_k=300:6.511,lwr_k=400:6.6289,lwr_k=500:6.5423,lwr_k=600:6.5347,lwr_k=700:6.4237,lwr_k=800:6.4641,lwr_k=900:6.4529,lwr_k=1000:6.4359'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.7245,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.6427,lwr_k=40:1.2102,lwr_k=50:1.9075,lwr_k=100:3.6126,lwr_k=200:4.447,lwr_k=300:4.721,lwr_k=400:4.8715,lwr_k=500:5.0003,lwr_k=600:5.0927,lwr_k=700:5.1558,lwr_k=800:5.2305,lwr_k=900:5.3307,lwr_k=1000:5.3551'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:5.9423,lwr_k=10:27.9482,lwr_k=20:64.0466,lwr_k=30:499.0612,lwr_k=40:53.3503,lwr_k=50:18.7436,lwr_k=100:7.4786,lwr_k=200:5.5664,lwr_k=300:5.4056,lwr_k=400:5.3855,lwr_k=500:5.2636,lwr_k=600:5.2019,lwr_k=700:5.1977,lwr_k=800:5.1577,lwr_k=900:5.2778,lwr_k=1000:5.3267'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.2082,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:1.0949,lwr_k=50:1.8417,lwr_k=100:3.509,lwr_k=200:4.4821,lwr_k=300:4.9364,lwr_k=400:5.1393,lwr_k=500:5.2722,lwr_k=600:5.3762,lwr_k=700:5.4878,lwr_k=800:5.5569,lwr_k=900:5.6042,lwr_k=1000:5.6602'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:11.03,lwr_k=10:26.2124,lwr_k=20:77.8957,lwr_k=30:671.4593,lwr_k=40:31.78,lwr_k=50:17.2486,lwr_k=100:11.3047,lwr_k=200:10.5044,lwr_k=300:10.5046,lwr_k=400:10.4158,lwr_k=500:10.4555,lwr_k=600:10.5151,lwr_k=700:10.5075,lwr_k=800:10.6098,lwr_k=900:10.5634,lwr_k=1000:10.5511'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:3.3337,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.2054,lwr_k=40:0.7529,lwr_k=50:1.2344,lwr_k=100:2.1225,lwr_k=200:2.6902,lwr_k=300:2.8532,lwr_k=400:2.9066,lwr_k=500:2.983,lwr_k=600:3.0146,lwr_k=700:3.0479,lwr_k=800:3.0706,lwr_k=900:3.0922,lwr_k=1000:3.1138'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:15.9413,lwr_k=10:33.6609,lwr_k=20:69.1502,lwr_k=30:11699.6566,lwr_k=40:164.9147,lwr_k=50:213.9708,lwr_k=100:60.3952,lwr_k=200:16.1916,lwr_k=300:15.077,lwr_k=400:15.0061,lwr_k=500:15.2702,lwr_k=600:15.6249,lwr_k=700:15.4665,lwr_k=800:15.403,lwr_k=900:15.4321,lwr_k=1000:15.4309'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:4.716,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0234,lwr_k=40:0.9894,lwr_k=50:1.6157,lwr_k=100:2.9022,lwr_k=200:3.5207,lwr_k=300:3.8002,lwr_k=400:3.9284,lwr_k=500:4.023,lwr_k=600:4.1035,lwr_k=700:4.1389,lwr_k=800:4.1889,lwr_k=900:4.2202,lwr_k=1000:4.2796'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.7877,lwr_k=10:29.5463,lwr_k=20:62.3927,lwr_k=30:6164.368,lwr_k=40:44.5827,lwr_k=50:16.755,lwr_k=100:8.064,lwr_k=200:7.5391,lwr_k=300:7.5857,lwr_k=400:7.5935,lwr_k=500:7.8486,lwr_k=600:7.87,lwr_k=700:7.763,lwr_k=800:7.8333,lwr_k=900:7.7657,lwr_k=1000:7.7722'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_6'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.6546,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.9099,lwr_k=40:1.7047,lwr_k=50:2.318,lwr_k=100:3.361,lwr_k=200:3.8591,lwr_k=300:3.9951,lwr_k=400:4.1263,lwr_k=500:4.2269,lwr_k=600:4.2789,lwr_k=700:4.3312,lwr_k=800:4.37,lwr_k=900:4.399,lwr_k=1000:4.4136'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.1086,lwr_k=10:75.6323,lwr_k=20:214.9019,lwr_k=30:101501.1175,lwr_k=40:176.3644,lwr_k=50:14.2847,lwr_k=100:11.1468,lwr_k=200:8.6334,lwr_k=300:8.414,lwr_k=400:8.3471,lwr_k=500:8.3424,lwr_k=600:8.3874,lwr_k=700:8.3419,lwr_k=800:8.3523,lwr_k=900:8.3215,lwr_k=1000:8.3387'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:3.4822,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.6663,lwr_k=40:1.3648,lwr_k=50:1.7386,lwr_k=100:2.4226,lwr_k=200:2.8907,lwr_k=300:3.0463,lwr_k=400:3.1501,lwr_k=500:3.2062,lwr_k=600:3.235,lwr_k=700:3.2599,lwr_k=800:3.2897,lwr_k=900:3.3002,lwr_k=1000:3.3127'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:6.1326,lwr_k=10:43.4013,lwr_k=20:303.5047,lwr_k=30:502.5809,lwr_k=40:78.5301,lwr_k=50:57.3046,lwr_k=100:7.8085,lwr_k=200:6.1297,lwr_k=300:5.8867,lwr_k=400:5.8696,lwr_k=500:5.9157,lwr_k=600:5.9747,lwr_k=700:6.0422,lwr_k=800:6.0432,lwr_k=900:6.0465,lwr_k=1000:6.0446'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.3727,lwr_k=10:0.0,lwr_k=20:0.0034,lwr_k=30:1.23,lwr_k=40:2.4473,lwr_k=50:3.0755,lwr_k=100:5.0362,lwr_k=200:5.9363,lwr_k=300:6.3886,lwr_k=400:6.7375,lwr_k=500:6.9534,lwr_k=600:7.0703,lwr_k=700:7.2019,lwr_k=800:7.3263,lwr_k=900:7.4291,lwr_k=1000:7.5408'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.7185,lwr_k=10:56.641,lwr_k=20:356.8994,lwr_k=30:15511170.0103,lwr_k=40:8736999.6267,lwr_k=50:704365.5937,lwr_k=100:15209.3357,lwr_k=200:6.8098,lwr_k=300:6.8096,lwr_k=400:6.5622,lwr_k=500:7.1228,lwr_k=600:7.0803,lwr_k=700:7.0069,lwr_k=800:6.8139,lwr_k=900:6.7803,lwr_k=1000:6.6991'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:3.5295,lwr_k=10:0.0,lwr_k=20:0.002,lwr_k=30:0.9421,lwr_k=40:1.5052,lwr_k=50:1.9132,lwr_k=100:2.6864,lwr_k=200:3.1068,lwr_k=300:3.2253,lwr_k=400:3.2808,lwr_k=500:3.3346,lwr_k=600:3.3773,lwr_k=700:3.3998,lwr_k=800:3.4118,lwr_k=900:3.4444,lwr_k=1000:3.4574'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.9558,lwr_k=10:73.5602,lwr_k=20:9509.4678,lwr_k=30:1505.5162,lwr_k=40:2387982.3552,lwr_k=50:20381.8226,lwr_k=100:13.0431,lwr_k=200:10.8824,lwr_k=300:8.0928,lwr_k=400:7.9493,lwr_k=500:7.9056,lwr_k=600:8.1025,lwr_k=700:7.9554,lwr_k=800:8.0647,lwr_k=900:8.0866,lwr_k=1000:8.007'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:3.5741,lwr_k=10:0.0,lwr_k=20:0.0001,lwr_k=30:0.6706,lwr_k=40:1.4444,lwr_k=50:1.9233,lwr_k=100:2.7321,lwr_k=200:3.1144,lwr_k=300:3.259,lwr_k=400:3.3274,lwr_k=500:3.3796,lwr_k=600:3.405,lwr_k=700:3.4349,lwr_k=800:3.4462,lwr_k=900:3.4586,lwr_k=1000:3.4589'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:18.9908,lwr_k=10:85.4815,lwr_k=20:1267.3182,lwr_k=30:94299.3909,lwr_k=40:409.6605,lwr_k=50:20.8839,lwr_k=100:20.4984,lwr_k=200:17.9561,lwr_k=300:18.0709,lwr_k=400:18.2123,lwr_k=500:18.0806,lwr_k=600:18.2849,lwr_k=700:18.2576,lwr_k=800:18.1018,lwr_k=900:18.1002,lwr_k=1000:18.1107'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:2.8572,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.6322,lwr_k=40:1.213,lwr_k=50:1.584,lwr_k=100:2.2448,lwr_k=200:2.5709,lwr_k=300:2.6473,lwr_k=400:2.6865,lwr_k=500:2.7217,lwr_k=600:2.7381,lwr_k=700:2.7658,lwr_k=800:2.7754,lwr_k=900:2.7889,lwr_k=1000:2.7978'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:6.8549,lwr_k=10:41.1294,lwr_k=20:279.4255,lwr_k=30:330.8688,lwr_k=40:26.886,lwr_k=50:16.3576,lwr_k=100:7.8236,lwr_k=200:7.024,lwr_k=300:6.7701,lwr_k=400:6.7172,lwr_k=500:6.7744,lwr_k=600:6.807,lwr_k=700:6.7914,lwr_k=800:6.7608,lwr_k=900:6.7596,lwr_k=1000:6.7429'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_7'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.9404,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.3851,lwr_k=40:2.2042,lwr_k=50:2.8515,lwr_k=100:4.0117,lwr_k=200:4.5314,lwr_k=300:4.7884,lwr_k=400:4.9492,lwr_k=500:5.0702,lwr_k=600:5.1286,lwr_k=700:5.1948,lwr_k=800:5.2375,lwr_k=900:5.2916,lwr_k=1000:5.3302'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.9698,lwr_k=10:32.0902,lwr_k=20:289.9287,lwr_k=30:24.1665,lwr_k=40:15.2358,lwr_k=50:10.9361,lwr_k=100:8.1987,lwr_k=200:7.8054,lwr_k=300:7.6732,lwr_k=400:7.5114,lwr_k=500:7.4861,lwr_k=600:7.4835,lwr_k=700:7.4956,lwr_k=800:7.5172,lwr_k=900:7.5231,lwr_k=1000:7.5122'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.1172,lwr_k=10:0.0,lwr_k=20:0.0001,lwr_k=30:1.9701,lwr_k=40:3.0309,lwr_k=50:3.7346,lwr_k=100:5.6946,lwr_k=200:6.8764,lwr_k=300:7.196,lwr_k=400:7.4851,lwr_k=500:7.6614,lwr_k=600:7.8339,lwr_k=700:7.9561,lwr_k=800:8.0743,lwr_k=900:8.1707,lwr_k=1000:8.1994'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.5237,lwr_k=10:80.0923,lwr_k=20:923.75,lwr_k=30:31.1526,lwr_k=40:17.2653,lwr_k=50:14.1187,lwr_k=100:8.8647,lwr_k=200:7.9338,lwr_k=300:8.0789,lwr_k=400:8.1835,lwr_k=500:8.3284,lwr_k=600:8.4172,lwr_k=700:8.4988,lwr_k=800:8.5732,lwr_k=900:8.6166,lwr_k=1000:8.629'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.2951,lwr_k=10:0.0,lwr_k=20:0.0005,lwr_k=30:1.99,lwr_k=40:3.2442,lwr_k=50:4.0304,lwr_k=100:5.3023,lwr_k=200:6.5222,lwr_k=300:6.9578,lwr_k=400:7.2158,lwr_k=500:7.4583,lwr_k=600:7.565,lwr_k=700:7.6776,lwr_k=800:7.7598,lwr_k=900:7.8343,lwr_k=1000:7.9216'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.0858,lwr_k=10:48.6413,lwr_k=20:1402.3906,lwr_k=30:26.6954,lwr_k=40:13.7884,lwr_k=50:10.6723,lwr_k=100:7.1956,lwr_k=200:6.5509,lwr_k=300:6.3703,lwr_k=400:6.1663,lwr_k=500:6.1522,lwr_k=600:6.1234,lwr_k=700:6.1482,lwr_k=800:6.1357,lwr_k=900:6.1503,lwr_k=1000:6.1379'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.0348,lwr_k=10:0.0,lwr_k=20:0.0001,lwr_k=30:2.0265,lwr_k=40:3.041,lwr_k=50:3.8019,lwr_k=100:5.5344,lwr_k=200:6.4513,lwr_k=300:6.8506,lwr_k=400:7.0443,lwr_k=500:7.2167,lwr_k=600:7.3133,lwr_k=700:7.4228,lwr_k=800:7.4915,lwr_k=900:7.541,lwr_k=1000:7.5853'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:11.1615,lwr_k=10:69.9332,lwr_k=20:837.8596,lwr_k=30:37.5358,lwr_k=40:16.6374,lwr_k=50:14.0241,lwr_k=100:10.8598,lwr_k=200:10.0749,lwr_k=300:9.9926,lwr_k=400:9.8958,lwr_k=500:9.8857,lwr_k=600:9.8827,lwr_k=700:9.8514,lwr_k=800:9.828,lwr_k=900:9.8245,lwr_k=1000:9.8651'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.3894,lwr_k=10:0.0,lwr_k=20:0.0001,lwr_k=30:1.3969,lwr_k=40:2.3708,lwr_k=50:2.9078,lwr_k=100:4.0306,lwr_k=200:4.5485,lwr_k=300:4.7289,lwr_k=400:4.8158,lwr_k=500:4.854,lwr_k=600:4.924,lwr_k=700:4.9434,lwr_k=800:4.9997,lwr_k=900:5.0256,lwr_k=1000:5.0498'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:19.3733,lwr_k=10:115.0434,lwr_k=20:1327.7553,lwr_k=30:44.9442,lwr_k=40:20.3462,lwr_k=50:22.5149,lwr_k=100:19.4076,lwr_k=200:18.236,lwr_k=300:18.4479,lwr_k=400:18.5528,lwr_k=500:18.395,lwr_k=600:18.5517,lwr_k=700:18.5423,lwr_k=800:18.3967,lwr_k=900:18.3542,lwr_k=1000:18.4215'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:7.1174,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.8673,lwr_k=40:2.7374,lwr_k=50:3.3146,lwr_k=100:4.7349,lwr_k=200:5.4724,lwr_k=300:5.7316,lwr_k=400:5.9352,lwr_k=500:6.0101,lwr_k=600:6.1169,lwr_k=700:6.1754,lwr_k=800:6.2401,lwr_k=900:6.2555,lwr_k=1000:6.2864'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.6474,lwr_k=10:28.6072,lwr_k=20:678.5791,lwr_k=30:27.429,lwr_k=40:13.4628,lwr_k=50:11.4302,lwr_k=100:9.5167,lwr_k=200:8.8476,lwr_k=300:9.0055,lwr_k=400:8.9675,lwr_k=500:8.8812,lwr_k=600:8.8474,lwr_k=700:8.8104,lwr_k=800:8.7731,lwr_k=900:8.7212,lwr_k=1000:8.78'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_8'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.992,lwr_k=10:2.5791,lwr_k=20:3.9608,lwr_k=30:4.4041,lwr_k=40:4.6433,lwr_k=50:4.8287,lwr_k=100:5.1847,lwr_k=200:5.4645,lwr_k=300:5.5081,lwr_k=400:5.5992,lwr_k=500:5.6126,lwr_k=600:5.6395,lwr_k=700:5.6616,lwr_k=800:5.669,lwr_k=900:5.6875,lwr_k=1000:5.7048'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.3842,lwr_k=10:243009.0625,lwr_k=20:15204.3789,lwr_k=30:5632.9051,lwr_k=40:9.3107,lwr_k=50:8.0881,lwr_k=100:7.8775,lwr_k=200:7.8966,lwr_k=300:7.8573,lwr_k=400:7.8892,lwr_k=500:7.9452,lwr_k=600:7.9036,lwr_k=700:7.8727,lwr_k=800:7.8714,lwr_k=900:7.8811,lwr_k=1000:7.8927'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.5471,lwr_k=10:3.4487,lwr_k=20:5.2001,lwr_k=30:5.7426,lwr_k=40:6.1919,lwr_k=50:6.4414,lwr_k=100:6.911,lwr_k=200:7.3619,lwr_k=300:7.5369,lwr_k=400:8.0271,lwr_k=500:8.0868,lwr_k=600:8.1796,lwr_k=700:8.3079,lwr_k=800:8.4013,lwr_k=900:8.4809,lwr_k=1000:8.5494'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.7657,lwr_k=10:32119.1535,lwr_k=20:169.7881,lwr_k=30:86.6635,lwr_k=40:69.978,lwr_k=50:25.3937,lwr_k=100:9.3674,lwr_k=200:7.861,lwr_k=300:7.8043,lwr_k=400:7.4992,lwr_k=500:7.4478,lwr_k=600:7.4168,lwr_k=700:7.4813,lwr_k=800:7.5243,lwr_k=900:7.5183,lwr_k=1000:7.551'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.6446,lwr_k=10:3.5558,lwr_k=20:5.3358,lwr_k=30:5.985,lwr_k=40:6.3732,lwr_k=50:6.7703,lwr_k=100:7.3101,lwr_k=200:7.5459,lwr_k=300:7.73,lwr_k=400:7.7864,lwr_k=500:7.8933,lwr_k=600:7.9768,lwr_k=700:7.9962,lwr_k=800:8.0302,lwr_k=900:8.085,lwr_k=1000:8.1084'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.5802,lwr_k=10:16043.1878,lwr_k=20:19483.687,lwr_k=30:4633.5648,lwr_k=40:107.6903,lwr_k=50:16913.7384,lwr_k=100:56.1627,lwr_k=200:7.2078,lwr_k=300:6.813,lwr_k=400:6.7737,lwr_k=500:6.8186,lwr_k=600:6.648,lwr_k=700:6.6265,lwr_k=800:6.6512,lwr_k=900:6.664,lwr_k=1000:6.7432'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.2015,lwr_k=10:2.8877,lwr_k=20:4.4587,lwr_k=30:4.9725,lwr_k=40:5.2521,lwr_k=50:5.395,lwr_k=100:5.7895,lwr_k=200:6.1118,lwr_k=300:6.3087,lwr_k=400:6.4016,lwr_k=500:6.4578,lwr_k=600:6.5308,lwr_k=700:6.5683,lwr_k=800:6.5976,lwr_k=900:6.6113,lwr_k=1000:6.609'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.2821,lwr_k=10:5705.2482,lwr_k=20:1313.546,lwr_k=30:902.5439,lwr_k=40:250.2932,lwr_k=50:480.0484,lwr_k=100:8.4526,lwr_k=200:10.6375,lwr_k=300:9.7092,lwr_k=400:9.5541,lwr_k=500:9.4624,lwr_k=600:9.4571,lwr_k=700:9.8228,lwr_k=800:9.339,lwr_k=900:9.3021,lwr_k=1000:9.338'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.2082,lwr_k=10:2.862,lwr_k=20:4.4515,lwr_k=30:5.0393,lwr_k=40:5.3496,lwr_k=50:5.5298,lwr_k=100:5.7888,lwr_k=200:5.9503,lwr_k=300:5.9941,lwr_k=400:6.0473,lwr_k=500:6.0691,lwr_k=600:6.0986,lwr_k=700:6.1061,lwr_k=800:6.1126,lwr_k=900:6.1159,lwr_k=1000:6.1399'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:21.6461,lwr_k=10:1539.9417,lwr_k=20:39.0982,lwr_k=30:22.8575,lwr_k=40:22.3156,lwr_k=50:20.3746,lwr_k=100:17.5168,lwr_k=200:17.8599,lwr_k=300:17.8175,lwr_k=400:17.6175,lwr_k=500:17.4912,lwr_k=600:17.3733,lwr_k=700:17.3302,lwr_k=800:17.3032,lwr_k=900:17.4352,lwr_k=1000:17.4889'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:8.0806,lwr_k=10:2.7964,lwr_k=20:4.6259,lwr_k=30:5.1571,lwr_k=40:5.4669,lwr_k=50:5.6698,lwr_k=100:6.1596,lwr_k=200:6.5522,lwr_k=300:6.6418,lwr_k=400:6.6784,lwr_k=500:6.767,lwr_k=600:6.8595,lwr_k=700:6.9688,lwr_k=800:7.0236,lwr_k=900:7.0909,lwr_k=1000:7.1325'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.4351,lwr_k=10:31333.3923,lwr_k=20:60.7974,lwr_k=30:18.9019,lwr_k=40:17.6792,lwr_k=50:12.1734,lwr_k=100:23.3993,lwr_k=200:10.3569,lwr_k=300:10.1093,lwr_k=400:9.6471,lwr_k=500:10.9851,lwr_k=600:9.8408,lwr_k=700:9.9318,lwr_k=800:10.0534,lwr_k=900:9.9764,lwr_k=1000:9.5339'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_9'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.6804,lwr_k=10:3.5659,lwr_k=20:5.0682,lwr_k=30:5.6748,lwr_k=40:5.9834,lwr_k=50:5.9709,lwr_k=100:6.3721,lwr_k=200:6.727,lwr_k=300:6.8961,lwr_k=400:6.9877,lwr_k=500:7.059,lwr_k=600:7.0763,lwr_k=700:7.1008,lwr_k=800:7.1205,lwr_k=900:7.1506,lwr_k=1000:7.1785'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.6072,lwr_k=10:128.4567,lwr_k=20:17.648,lwr_k=30:11.3607,lwr_k=40:8.3673,lwr_k=50:10.5844,lwr_k=100:7.4878,lwr_k=200:7.7277,lwr_k=300:7.7761,lwr_k=400:7.8669,lwr_k=500:7.939,lwr_k=600:7.9662,lwr_k=700:8.0122,lwr_k=800:8.0602,lwr_k=900:8.0801,lwr_k=1000:8.1445'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.6465,lwr_k=10:2.9752,lwr_k=20:4.617,lwr_k=30:5.4853,lwr_k=40:5.7298,lwr_k=50:5.8928,lwr_k=100:6.3113,lwr_k=200:6.7798,lwr_k=300:6.9135,lwr_k=400:6.9759,lwr_k=500:7.0313,lwr_k=600:7.0881,lwr_k=700:7.1196,lwr_k=800:7.1364,lwr_k=900:7.1614,lwr_k=1000:7.1731'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.539,lwr_k=10:408.9659,lwr_k=20:28.1197,lwr_k=30:11.223,lwr_k=40:9.8764,lwr_k=50:10.0403,lwr_k=100:9.3553,lwr_k=200:9.2747,lwr_k=300:9.2989,lwr_k=400:9.2353,lwr_k=500:9.0852,lwr_k=600:9.184,lwr_k=700:9.2131,lwr_k=800:9.2356,lwr_k=900:9.239,lwr_k=1000:9.2126'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.3173,lwr_k=10:3.8629,lwr_k=20:5.4159,lwr_k=30:5.9559,lwr_k=40:6.3163,lwr_k=50:6.7873,lwr_k=100:7.1546,lwr_k=200:7.4,lwr_k=300:7.482,lwr_k=400:7.5902,lwr_k=500:7.6957,lwr_k=600:7.7646,lwr_k=700:7.822,lwr_k=800:7.8721,lwr_k=900:7.9132,lwr_k=1000:7.9527'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.298,lwr_k=10:22.0851,lwr_k=20:9.651,lwr_k=30:9.3704,lwr_k=40:8.4476,lwr_k=50:8.2121,lwr_k=100:7.995,lwr_k=200:8.03,lwr_k=300:8.1388,lwr_k=400:8.1955,lwr_k=500:8.3045,lwr_k=600:8.3665,lwr_k=700:8.45,lwr_k=800:8.5467,lwr_k=900:8.6475,lwr_k=1000:8.6866'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.6059,lwr_k=10:3.7714,lwr_k=20:5.1074,lwr_k=30:5.7221,lwr_k=40:6.0467,lwr_k=50:6.5026,lwr_k=100:7.0779,lwr_k=200:7.4188,lwr_k=300:7.5848,lwr_k=400:7.6929,lwr_k=500:7.7947,lwr_k=600:7.789,lwr_k=700:7.8163,lwr_k=800:7.8465,lwr_k=900:7.8793,lwr_k=1000:7.9096'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.4873,lwr_k=10:15.2487,lwr_k=20:11.1029,lwr_k=30:10.0843,lwr_k=40:9.8499,lwr_k=50:9.8553,lwr_k=100:9.436,lwr_k=200:9.944,lwr_k=300:9.8066,lwr_k=400:9.9886,lwr_k=500:10.0274,lwr_k=600:10.0206,lwr_k=700:9.9209,lwr_k=800:9.9313,lwr_k=900:9.9662,lwr_k=1000:10.0087'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.1883,lwr_k=10:3.0011,lwr_k=20:4.1843,lwr_k=30:4.6567,lwr_k=40:4.8974,lwr_k=50:4.999,lwr_k=100:5.2682,lwr_k=200:5.5812,lwr_k=300:5.7398,lwr_k=400:5.8129,lwr_k=500:5.8512,lwr_k=600:5.8742,lwr_k=700:5.906,lwr_k=800:5.9225,lwr_k=900:5.9298,lwr_k=1000:5.9337'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:20.2043,lwr_k=10:31.4536,lwr_k=20:19.2793,lwr_k=30:17.5477,lwr_k=40:18.8659,lwr_k=50:18.5999,lwr_k=100:18.1907,lwr_k=200:19.4073,lwr_k=300:19.5157,lwr_k=400:19.5038,lwr_k=500:19.4685,lwr_k=600:19.5069,lwr_k=700:19.5396,lwr_k=800:19.5654,lwr_k=900:19.5713,lwr_k=1000:19.5971'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:9.0588,lwr_k=10:3.9393,lwr_k=20:5.7245,lwr_k=30:6.2328,lwr_k=40:6.4781,lwr_k=50:6.7557,lwr_k=100:7.4544,lwr_k=200:7.7665,lwr_k=300:7.8666,lwr_k=400:7.9531,lwr_k=500:8.0219,lwr_k=600:8.0618,lwr_k=700:8.1002,lwr_k=800:8.1327,lwr_k=900:8.1713,lwr_k=1000:8.1761'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.3469,lwr_k=10:17.7813,lwr_k=20:11.6501,lwr_k=30:11.1229,lwr_k=40:11.2841,lwr_k=50:10.2919,lwr_k=100:10.7367,lwr_k=200:10.7048,lwr_k=300:10.613,lwr_k=400:10.6926,lwr_k=500:10.7195,lwr_k=600:10.7305,lwr_k=700:10.6968,lwr_k=800:10.7232,lwr_k=900:10.7111,lwr_k=1000:10.6887'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_10'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.8298,lwr_k=10:0.2883,lwr_k=20:3.0021,lwr_k=30:4.015,lwr_k=40:4.5717,lwr_k=50:4.8837,lwr_k=100:5.5133,lwr_k=200:5.8811,lwr_k=300:6.0513,lwr_k=400:6.1496,lwr_k=500:6.1918,lwr_k=600:6.2734,lwr_k=700:6.3118,lwr_k=800:6.3523,lwr_k=900:6.3664,lwr_k=1000:6.3871'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.6646,lwr_k=10:15309755.0296,lwr_k=20:56.8233,lwr_k=30:1666.9007,lwr_k=40:8.8289,lwr_k=50:8.8569,lwr_k=100:7.92,lwr_k=200:8.0414,lwr_k=300:8.1451,lwr_k=400:8.1503,lwr_k=500:8.1911,lwr_k=600:8.2044,lwr_k=700:8.1992,lwr_k=800:8.2213,lwr_k=900:8.2297,lwr_k=1000:8.2571'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:35.5279,lwr_k=10:3.9863,lwr_k=20:9.4542,lwr_k=30:11.8757,lwr_k=40:13.2056,lwr_k=50:13.8688,lwr_k=100:16.3325,lwr_k=200:19.1652,lwr_k=300:20.4521,lwr_k=400:21.474,lwr_k=500:22.3829,lwr_k=600:23.2117,lwr_k=700:23.873,lwr_k=800:24.68,lwr_k=900:25.395,lwr_k=1000:25.9606'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:23.6612,lwr_k=10:87.6589,lwr_k=20:23.757,lwr_k=30:19.1054,lwr_k=40:17.6043,lwr_k=50:17.581,lwr_k=100:16.5241,lwr_k=200:66.3954,lwr_k=300:91.5451,lwr_k=400:78.7815,lwr_k=500:17.0549,lwr_k=600:17.1949,lwr_k=700:17.2249,lwr_k=800:17.3148,lwr_k=900:17.4409,lwr_k=1000:17.5626'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.5818,lwr_k=10:1.5507,lwr_k=20:4.1642,lwr_k=30:5.2004,lwr_k=40:5.9527,lwr_k=50:6.3583,lwr_k=100:7.3071,lwr_k=200:7.9674,lwr_k=300:8.1716,lwr_k=400:8.3551,lwr_k=500:8.4391,lwr_k=600:8.4722,lwr_k=700:8.5498,lwr_k=800:8.6369,lwr_k=900:8.6607,lwr_k=1000:8.6815'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.3597,lwr_k=10:453657890.1494,lwr_k=20:292943894.1082,lwr_k=30:1403425.1012,lwr_k=40:1142.1592,lwr_k=50:7.4795,lwr_k=100:6.63,lwr_k=200:6.4621,lwr_k=300:6.3187,lwr_k=400:6.3588,lwr_k=500:6.3602,lwr_k=600:6.3588,lwr_k=700:6.36,lwr_k=800:6.3644,lwr_k=900:6.3667,lwr_k=1000:6.3492'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:38.1271,lwr_k=10:1.7567,lwr_k=20:9.1314,lwr_k=30:11.4447,lwr_k=40:13.0734,lwr_k=50:14.3541,lwr_k=100:17.6997,lwr_k=200:19.9606,lwr_k=300:21.4313,lwr_k=400:22.1177,lwr_k=500:22.6971,lwr_k=600:23.2119,lwr_k=700:23.8751,lwr_k=800:24.4299,lwr_k=900:25.1073,lwr_k=1000:25.7578'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:33.6393,lwr_k=10:6021.6667,lwr_k=20:33.6792,lwr_k=30:25.8301,lwr_k=40:23.1075,lwr_k=50:21.1,lwr_k=100:20.1902,lwr_k=200:19.8958,lwr_k=300:19.3722,lwr_k=400:19.3317,lwr_k=500:19.6747,lwr_k=600:19.9208,lwr_k=700:20.185,lwr_k=800:20.4912,lwr_k=900:20.8869,lwr_k=1000:21.2298'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.3977,lwr_k=10:1.1198,lwr_k=20:3.149,lwr_k=30:3.9412,lwr_k=40:4.5601,lwr_k=50:4.8081,lwr_k=100:5.218,lwr_k=200:5.5735,lwr_k=300:5.7157,lwr_k=400:5.7808,lwr_k=500:5.8246,lwr_k=600:5.8593,lwr_k=700:5.8876,lwr_k=800:5.9173,lwr_k=900:5.96,lwr_k=1000:5.9853'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:20.316,lwr_k=10:1879280841.7434,lwr_k=20:3842170.4536,lwr_k=30:97585.3508,lwr_k=40:2055537.7702,lwr_k=50:18.7215,lwr_k=100:17.7593,lwr_k=200:18.861,lwr_k=300:19.2836,lwr_k=400:19.407,lwr_k=500:19.5215,lwr_k=600:19.563,lwr_k=700:19.6455,lwr_k=800:19.7682,lwr_k=900:19.7982,lwr_k=1000:19.8267'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:8.283,lwr_k=10:0.195,lwr_k=20:3.0546,lwr_k=30:4.2394,lwr_k=40:4.7905,lwr_k=50:5.0353,lwr_k=100:5.8998,lwr_k=200:6.487,lwr_k=300:6.7441,lwr_k=400:6.8781,lwr_k=500:6.9826,lwr_k=600:7.0624,lwr_k=700:7.1002,lwr_k=800:7.1433,lwr_k=900:7.228,lwr_k=1000:7.2881'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.8553,lwr_k=10:42630357.9156,lwr_k=20:1232252.6538,lwr_k=30:13.3672,lwr_k=40:12.6956,lwr_k=50:10.7708,lwr_k=100:9.2265,lwr_k=200:9.0745,lwr_k=300:9.1944,lwr_k=400:9.3602,lwr_k=500:9.4961,lwr_k=600:9.6238,lwr_k=700:9.6325,lwr_k=800:9.6386,lwr_k=900:9.7199,lwr_k=1000:9.7613'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_11'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:198.6529,lwr_k=10:204.1162,lwr_k=20:199.5629,lwr_k=30:199.1225,lwr_k=40:200.2207,lwr_k=50:201.3046,lwr_k=100:199.0461,lwr_k=200:198.7916,lwr_k=300:198.6531,lwr_k=400:198.9246,lwr_k=500:199.04,lwr_k=600:199.3729,lwr_k=700:198.9219,lwr_k=800:198.6668,lwr_k=900:198.6696,lwr_k=1000:198.66'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:258.1252,lwr_k=10:258.1761,lwr_k=20:256.8263,lwr_k=30:257.008,lwr_k=40:256.7936,lwr_k=50:257.0062,lwr_k=100:257.0664,lwr_k=200:259.1259,lwr_k=300:258.0959,lwr_k=400:259.6039,lwr_k=500:259.9529,lwr_k=600:260.81,lwr_k=700:259.5949,lwr_k=800:258.4118,lwr_k=900:258.4411,lwr_k=1000:257.9379'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:239.6366,lwr_k=10:241.2858,lwr_k=20:240.8066,lwr_k=30:239.6231,lwr_k=40:240.3896,lwr_k=50:239.9859,lwr_k=100:239.7813,lwr_k=200:239.5561,lwr_k=300:239.8289,lwr_k=400:239.5928,lwr_k=500:239.5661,lwr_k=600:239.5454,lwr_k=700:239.5585,lwr_k=800:239.5646,lwr_k=900:239.5798,lwr_k=1000:239.5659'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:202.7825,lwr_k=10:203.5279,lwr_k=20:203.239,lwr_k=30:203.0222,lwr_k=40:202.9423,lwr_k=50:202.724,lwr_k=100:202.6534,lwr_k=200:202.7033,lwr_k=300:203.4272,lwr_k=400:202.9609,lwr_k=500:202.8923,lwr_k=600:202.8002,lwr_k=700:202.8643,lwr_k=800:202.9044,lwr_k=900:202.9499,lwr_k=1000:202.9072'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.4193,lwr_k=10:7.6494,lwr_k=20:8.0624,lwr_k=30:8.3902,lwr_k=40:8.5303,lwr_k=50:8.7097,lwr_k=100:9.1544,lwr_k=200:9.3763,lwr_k=300:9.4929,lwr_k=400:9.572,lwr_k=500:9.6222,lwr_k=600:9.6908,lwr_k=700:9.7521,lwr_k=800:9.7597,lwr_k=900:9.8769,lwr_k=1000:10.0394'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.9814,lwr_k=10:9.1628,lwr_k=20:8.1715,lwr_k=30:8.1824,lwr_k=40:7.9858,lwr_k=50:7.8747,lwr_k=100:7.7022,lwr_k=200:7.6649,lwr_k=300:7.6408,lwr_k=400:7.6384,lwr_k=500:7.63,lwr_k=600:7.6199,lwr_k=700:7.611,lwr_k=800:7.602,lwr_k=900:7.6067,lwr_k=1000:7.6075'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.0325,lwr_k=10:6.2245,lwr_k=20:6.9619,lwr_k=30:7.2368,lwr_k=40:7.4723,lwr_k=50:7.5917,lwr_k=100:8.1597,lwr_k=200:8.8517,lwr_k=300:9.1454,lwr_k=400:9.3594,lwr_k=500:9.4976,lwr_k=600:9.5955,lwr_k=700:9.7349,lwr_k=800:9.8212,lwr_k=900:9.8989,lwr_k=1000:10.0039'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.6536,lwr_k=10:10.5578,lwr_k=20:9.8639,lwr_k=30:9.7088,lwr_k=40:9.591,lwr_k=50:9.54,lwr_k=100:9.2388,lwr_k=200:9.4808,lwr_k=300:10.0846,lwr_k=400:9.6048,lwr_k=500:9.6431,lwr_k=600:9.714,lwr_k=700:9.7335,lwr_k=800:9.7234,lwr_k=900:9.7765,lwr_k=1000:9.7473'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.6529,lwr_k=10:7.0809,lwr_k=20:7.5418,lwr_k=30:7.7276,lwr_k=40:7.848,lwr_k=50:7.9816,lwr_k=100:8.1695,lwr_k=200:8.2576,lwr_k=300:8.2883,lwr_k=400:8.2963,lwr_k=500:8.3155,lwr_k=600:8.3198,lwr_k=700:8.3217,lwr_k=800:8.3289,lwr_k=900:8.3539,lwr_k=1000:8.3963'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:20.2037,lwr_k=10:17.1311,lwr_k=20:16.0457,lwr_k=30:17.8257,lwr_k=40:19.4816,lwr_k=50:19.5146,lwr_k=100:19.8044,lwr_k=200:20.0335,lwr_k=300:20.2774,lwr_k=400:20.2146,lwr_k=500:20.2327,lwr_k=600:20.2055,lwr_k=700:20.1501,lwr_k=800:20.138,lwr_k=900:20.1549,lwr_k=1000:20.2652'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:10.2105,lwr_k=10:6.566,lwr_k=20:7.1619,lwr_k=30:7.3412,lwr_k=40:7.5622,lwr_k=50:7.8355,lwr_k=100:8.336,lwr_k=200:8.7789,lwr_k=300:9.0379,lwr_k=400:9.2334,lwr_k=500:9.3023,lwr_k=600:9.359,lwr_k=700:9.3853,lwr_k=800:9.4059,lwr_k=900:9.4106,lwr_k=1000:9.4635'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.5211,lwr_k=10:11.0628,lwr_k=20:9.9989,lwr_k=30:9.8237,lwr_k=40:10.0152,lwr_k=50:10.2967,lwr_k=100:10.1573,lwr_k=200:10.0857,lwr_k=300:10.0929,lwr_k=400:10.0992,lwr_k=500:10.1199,lwr_k=600:10.1297,lwr_k=700:10.0802,lwr_k=800:10.1359,lwr_k=900:10.1463,lwr_k=1000:10.2099'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_12'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.1,lwr_k=10:0.1589,lwr_k=20:3.183,lwr_k=30:4.3286,lwr_k=40:4.8736,lwr_k=50:5.1408,lwr_k=100:5.9507,lwr_k=200:6.4658,lwr_k=300:6.6701,lwr_k=400:6.7695,lwr_k=500:6.8667,lwr_k=600:6.9378,lwr_k=700:6.9915,lwr_k=800:7.0216,lwr_k=900:7.0488,lwr_k=1000:7.084'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.7677,lwr_k=10:549.792,lwr_k=20:13.3907,lwr_k=30:9.7503,lwr_k=40:8.6132,lwr_k=50:8.0836,lwr_k=100:7.3813,lwr_k=200:7.3111,lwr_k=300:7.4504,lwr_k=400:7.4452,lwr_k=500:7.4545,lwr_k=600:7.5071,lwr_k=700:7.4684,lwr_k=800:7.4829,lwr_k=900:7.5264,lwr_k=1000:7.5465'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.108,lwr_k=10:5.1084,lwr_k=20:4.9329,lwr_k=30:6.2304,lwr_k=40:6.791,lwr_k=50:7.2122,lwr_k=100:8.6038,lwr_k=200:9.5685,lwr_k=300:9.954,lwr_k=400:10.1609,lwr_k=500:10.3456,lwr_k=600:10.4652,lwr_k=700:10.681,lwr_k=800:10.7208,lwr_k=900:10.7382,lwr_k=1000:10.7725'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.3142,lwr_k=10:1125.8265,lwr_k=20:17.7965,lwr_k=30:38.3892,lwr_k=40:11.7929,lwr_k=50:9.3904,lwr_k=100:10.5758,lwr_k=200:9.0417,lwr_k=300:9.0794,lwr_k=400:9.2745,lwr_k=500:9.382,lwr_k=600:9.4576,lwr_k=700:9.5128,lwr_k=800:9.5538,lwr_k=900:9.592,lwr_k=1000:9.6303'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.0522,lwr_k=10:0.8333,lwr_k=20:4.0437,lwr_k=30:5.1427,lwr_k=40:6.1093,lwr_k=50:6.5899,lwr_k=100:7.6761,lwr_k=200:8.4942,lwr_k=300:8.7078,lwr_k=400:8.8302,lwr_k=500:8.94,lwr_k=600:9.0107,lwr_k=700:9.2163,lwr_k=800:9.3937,lwr_k=900:9.4851,lwr_k=1000:9.5273'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.7043,lwr_k=10:276460.9553,lwr_k=20:210.5555,lwr_k=30:10.4529,lwr_k=40:9.5708,lwr_k=50:8.6926,lwr_k=100:7.6893,lwr_k=200:7.5121,lwr_k=300:7.3897,lwr_k=400:7.327,lwr_k=500:7.2923,lwr_k=600:7.3043,lwr_k=700:7.1869,lwr_k=800:7.2222,lwr_k=900:7.2403,lwr_k=1000:7.1963'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.8004,lwr_k=10:1.0829,lwr_k=20:3.4057,lwr_k=30:4.433,lwr_k=40:5.3286,lwr_k=50:5.8548,lwr_k=100:7.003,lwr_k=200:7.5182,lwr_k=300:7.8685,lwr_k=400:8.0712,lwr_k=500:8.3059,lwr_k=600:8.4389,lwr_k=700:8.5632,lwr_k=800:8.615,lwr_k=900:8.7311,lwr_k=1000:8.8'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.6475,lwr_k=10:143502909.956,lwr_k=20:60713.2693,lwr_k=30:65.4175,lwr_k=40:13.1511,lwr_k=50:12.0462,lwr_k=100:10.6966,lwr_k=200:10.352,lwr_k=300:10.3805,lwr_k=400:10.4959,lwr_k=500:10.6221,lwr_k=600:10.6556,lwr_k=700:10.7018,lwr_k=800:10.6861,lwr_k=900:10.6051,lwr_k=1000:10.5385'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.7398,lwr_k=10:0.4997,lwr_k=20:3.0824,lwr_k=30:4.1302,lwr_k=40:4.5659,lwr_k=50:4.884,lwr_k=100:5.5817,lwr_k=200:6.007,lwr_k=300:6.1431,lwr_k=400:6.2721,lwr_k=500:6.3259,lwr_k=600:6.3805,lwr_k=700:6.4334,lwr_k=800:6.4197,lwr_k=900:6.4493,lwr_k=1000:6.47'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:20.5591,lwr_k=10:304404199.3181,lwr_k=20:205519.2199,lwr_k=30:6988.2925,lwr_k=40:102.7426,lwr_k=50:21.2612,lwr_k=100:25.9653,lwr_k=200:23.3742,lwr_k=300:22.8202,lwr_k=400:22.0812,lwr_k=500:20.6903,lwr_k=600:20.6021,lwr_k=700:20.4243,lwr_k=800:20.4658,lwr_k=900:20.428,lwr_k=1000:20.3502'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:10.6799,lwr_k=10:2.813,lwr_k=20:4.1139,lwr_k=30:4.8935,lwr_k=40:5.4672,lwr_k=50:5.9484,lwr_k=100:7.3615,lwr_k=200:8.337,lwr_k=300:8.7617,lwr_k=400:8.9572,lwr_k=500:9.1348,lwr_k=600:9.1889,lwr_k=700:9.2365,lwr_k=800:9.268,lwr_k=900:9.2994,lwr_k=1000:9.3504'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.8274,lwr_k=10:167.6803,lwr_k=20:14.8981,lwr_k=30:14.1031,lwr_k=40:11.1006,lwr_k=50:10.4156,lwr_k=100:9.8981,lwr_k=200:9.8603,lwr_k=300:9.8432,lwr_k=400:9.8327,lwr_k=500:9.654,lwr_k=600:9.6226,lwr_k=700:9.605,lwr_k=800:9.5997,lwr_k=900:9.5405,lwr_k=1000:9.5174'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_13'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:198.6529,lwr_k=10:204.1162,lwr_k=20:199.5629,lwr_k=30:199.1225,lwr_k=40:200.2207,lwr_k=50:201.3046,lwr_k=100:199.0461,lwr_k=200:198.7916,lwr_k=300:198.6531,lwr_k=400:198.9246,lwr_k=500:199.04,lwr_k=600:199.3729,lwr_k=700:198.9219,lwr_k=800:198.6668,lwr_k=900:198.6696,lwr_k=1000:198.66'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:258.1252,lwr_k=10:258.1761,lwr_k=20:256.8263,lwr_k=30:257.008,lwr_k=40:256.7936,lwr_k=50:257.0062,lwr_k=100:257.0664,lwr_k=200:259.1259,lwr_k=300:258.0959,lwr_k=400:259.6039,lwr_k=500:259.9529,lwr_k=600:260.81,lwr_k=700:259.5949,lwr_k=800:258.4118,lwr_k=900:258.4411,lwr_k=1000:257.9379'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:239.6367,lwr_k=10:241.3914,lwr_k=20:240.9123,lwr_k=30:239.7128,lwr_k=40:240.4918,lwr_k=50:240.0855,lwr_k=100:239.8793,lwr_k=200:239.6506,lwr_k=300:239.9838,lwr_k=400:239.6817,lwr_k=500:239.6714,lwr_k=600:239.6373,lwr_k=700:239.6579,lwr_k=800:239.6652,lwr_k=900:239.658,lwr_k=1000:239.6594'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:202.822,lwr_k=10:203.7798,lwr_k=20:203.4181,lwr_k=30:203.064,lwr_k=40:203.1208,lwr_k=50:202.8677,lwr_k=100:202.7683,lwr_k=200:202.7651,lwr_k=300:203.5235,lwr_k=400:202.9945,lwr_k=500:202.9686,lwr_k=600:202.8361,lwr_k=700:202.9308,lwr_k=800:202.952,lwr_k=900:202.9311,lwr_k=1000:202.9354'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:273.725,lwr_k=10:277.091,lwr_k=20:276.4127,lwr_k=30:273.7799,lwr_k=40:275.7834,lwr_k=50:275.1171,lwr_k=100:274.7301,lwr_k=200:274.1188,lwr_k=300:273.7313,lwr_k=400:273.8139,lwr_k=500:273.83,lwr_k=600:273.9627,lwr_k=700:273.8578,lwr_k=800:273.8415,lwr_k=900:273.8576,lwr_k=1000:273.8542'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:155.1861,lwr_k=10:154.056,lwr_k=20:153.8561,lwr_k=30:154.667,lwr_k=40:153.7285,lwr_k=50:153.6867,lwr_k=100:153.7343,lwr_k=200:154.042,lwr_k=300:155.3861,lwr_k=400:154.5445,lwr_k=500:154.4971,lwr_k=600:154.2291,lwr_k=700:154.4258,lwr_k=800:154.4663,lwr_k=900:154.4263,lwr_k=1000:154.4345'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:261.7626,lwr_k=10:264.7799,lwr_k=20:264.1397,lwr_k=30:261.7813,lwr_k=40:263.5504,lwr_k=50:262.9339,lwr_k=100:262.5815,lwr_k=200:262.0434,lwr_k=300:261.7939,lwr_k=400:261.8028,lwr_k=500:261.8138,lwr_k=600:261.9146,lwr_k=700:261.8338,lwr_k=800:261.822,lwr_k=900:261.8337,lwr_k=1000:261.8311'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:239.3147,lwr_k=10:240.8869,lwr_k=20:240.4092,lwr_k=30:239.2198,lwr_k=40:239.9902,lwr_k=50:239.5856,lwr_k=100:239.3808,lwr_k=200:239.1547,lwr_k=300:239.493,lwr_k=400:239.1882,lwr_k=500:239.1777,lwr_k=600:239.1424,lwr_k=700:239.164,lwr_k=800:239.1714,lwr_k=900:239.164,lwr_k=1000:239.1655'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:204.8268,lwr_k=10:209.4469,lwr_k=20:205.7566,lwr_k=30:220.4386,lwr_k=40:211.6878,lwr_k=50:210.0386,lwr_k=100:207.0361,lwr_k=200:206.8144,lwr_k=300:206.1592,lwr_k=400:206.0456,lwr_k=500:205.6606,lwr_k=600:205.6755,lwr_k=700:205.5579,lwr_k=800:205.6872,lwr_k=900:205.6737,lwr_k=1000:206.1653'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:325.691,lwr_k=10:333.6621,lwr_k=20:328.1241,lwr_k=30:335.143,lwr_k=40:328.4684,lwr_k=50:327.3437,lwr_k=100:325.5831,lwr_k=200:325.4807,lwr_k=300:325.2239,lwr_k=400:325.1887,lwr_k=500:325.1013,lwr_k=600:325.1035,lwr_k=700:325.0891,lwr_k=800:325.1053,lwr_k=900:325.1032,lwr_k=1000:325.2259'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:235.0387,lwr_k=10:237.1545,lwr_k=20:236.6246,lwr_k=30:235.06,lwr_k=40:236.151,lwr_k=50:235.6784,lwr_k=100:235.4262,lwr_k=200:235.1,lwr_k=300:235.2496,lwr_k=400:235.0455,lwr_k=500:235.0419,lwr_k=600:235.0503,lwr_k=700:235.039,lwr_k=800:235.0402,lwr_k=900:235.039,lwr_k=1000:235.0392'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:237.5599,lwr_k=10:240.0665,lwr_k=20:239.4842,lwr_k=30:237.5419,lwr_k=40:238.9555,lwr_k=50:238.4145,lwr_k=100:238.1146,lwr_k=200:237.6876,lwr_k=300:237.6473,lwr_k=400:237.5446,lwr_k=500:237.5479,lwr_k=600:237.6003,lwr_k=700:237.5559,lwr_k=800:237.5509,lwr_k=900:237.5558,lwr_k=1000:237.5547'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_14'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.9021,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0003,lwr_k=40:1.2002,lwr_k=50:2.2307,lwr_k=100:3.9167,lwr_k=200:5.1901,lwr_k=300:5.6238,lwr_k=400:5.9245,lwr_k=500:6.2675,lwr_k=600:6.4729,lwr_k=700:6.6791,lwr_k=800:6.8678,lwr_k=900:7.0355,lwr_k=1000:7.1638'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.3279,lwr_k=10:24.8089,lwr_k=20:65.4205,lwr_k=30:19291.8151,lwr_k=40:20.2408,lwr_k=50:12.2663,lwr_k=100:7.3355,lwr_k=200:7.2785,lwr_k=300:7.3521,lwr_k=400:7.4258,lwr_k=500:7.5694,lwr_k=600:7.7841,lwr_k=700:7.9034,lwr_k=800:7.9713,lwr_k=900:8.1484,lwr_k=1000:8.2855'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:15.3965,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0001,lwr_k=40:1.2887,lwr_k=50:2.1621,lwr_k=100:4.5034,lwr_k=200:6.4653,lwr_k=300:7.3887,lwr_k=400:7.9428,lwr_k=500:8.4105,lwr_k=600:8.8331,lwr_k=700:9.1459,lwr_k=800:9.4243,lwr_k=900:9.5884,lwr_k=1000:9.8525'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:13.4721,lwr_k=10:34.4817,lwr_k=20:79.8979,lwr_k=30:16288.3866,lwr_k=40:28.485,lwr_k=50:14.283,lwr_k=100:8.4719,lwr_k=200:7.7195,lwr_k=300:7.8764,lwr_k=400:8.0703,lwr_k=500:8.3937,lwr_k=600:8.5419,lwr_k=700:8.74,lwr_k=800:8.9283,lwr_k=900:9.2895,lwr_k=1000:9.4119'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:13.3057,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0013,lwr_k=40:1.3426,lwr_k=50:2.4862,lwr_k=100:4.5292,lwr_k=200:6.2906,lwr_k=300:7.2792,lwr_k=400:7.7603,lwr_k=500:8.1995,lwr_k=600:8.4401,lwr_k=700:8.7969,lwr_k=800:9.0371,lwr_k=900:9.231,lwr_k=1000:9.4162'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.8217,lwr_k=10:20.9098,lwr_k=20:51.9588,lwr_k=30:33388.7124,lwr_k=40:20.807,lwr_k=50:13.6776,lwr_k=100:6.9163,lwr_k=200:6.2228,lwr_k=300:6.6133,lwr_k=400:6.6433,lwr_k=500:6.6392,lwr_k=600:6.7338,lwr_k=700:6.799,lwr_k=800:6.8933,lwr_k=900:6.9102,lwr_k=1000:6.9498'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.7139,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0014,lwr_k=40:1.5267,lwr_k=50:2.4339,lwr_k=100:4.4404,lwr_k=200:6.1799,lwr_k=300:7.0387,lwr_k=400:7.6199,lwr_k=500:8.0601,lwr_k=600:8.3719,lwr_k=700:8.6587,lwr_k=800:8.9058,lwr_k=900:9.1274,lwr_k=1000:9.3114'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:14.6067,lwr_k=10:25.1445,lwr_k=20:82.3646,lwr_k=30:4493.9652,lwr_k=40:25.2003,lwr_k=50:14.7028,lwr_k=100:8.5931,lwr_k=200:8.5028,lwr_k=300:9.003,lwr_k=400:9.3936,lwr_k=500:9.7063,lwr_k=600:10.0047,lwr_k=700:10.1322,lwr_k=800:10.4235,lwr_k=900:10.5925,lwr_k=1000:10.7956'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.1513,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0001,lwr_k=40:1.3232,lwr_k=50:2.2705,lwr_k=100:3.6964,lwr_k=200:4.7566,lwr_k=300:5.2254,lwr_k=400:5.5421,lwr_k=500:5.7769,lwr_k=600:6.0201,lwr_k=700:6.1973,lwr_k=800:6.3969,lwr_k=900:6.5304,lwr_k=1000:6.6415'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:31.7668,lwr_k=10:45.1211,lwr_k=20:251.4764,lwr_k=30:32842.3159,lwr_k=40:44.1958,lwr_k=50:22.0678,lwr_k=100:17.8578,lwr_k=200:19.8209,lwr_k=300:20.6397,lwr_k=400:21.5614,lwr_k=500:22.3958,lwr_k=600:23.0138,lwr_k=700:23.4908,lwr_k=800:24.114,lwr_k=900:24.5235,lwr_k=1000:24.7546'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:16.3752,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0031,lwr_k=40:1.2855,lwr_k=50:2.1142,lwr_k=100:4.4177,lwr_k=200:6.1868,lwr_k=300:7.1872,lwr_k=400:7.9508,lwr_k=500:8.4717,lwr_k=600:8.826,lwr_k=700:9.2184,lwr_k=800:9.5446,lwr_k=900:9.7914,lwr_k=1000:10.0338'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:16.3146,lwr_k=10:27.7458,lwr_k=20:77.8447,lwr_k=30:10298.5844,lwr_k=40:54.0388,lwr_k=50:17.4863,lwr_k=100:9.4395,lwr_k=200:8.76,lwr_k=300:9.5235,lwr_k=400:9.5821,lwr_k=500:9.9875,lwr_k=600:10.1794,lwr_k=700:10.3737,lwr_k=800:10.3802,lwr_k=900:10.4649,lwr_k=1000:10.6271'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_15'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.2274,lwr_k=10:4.7172,lwr_k=20:5.5074,lwr_k=30:5.7442,lwr_k=40:5.743,lwr_k=50:5.8456,lwr_k=100:5.9369,lwr_k=200:6.029,lwr_k=300:6.0723,lwr_k=400:6.0909,lwr_k=500:6.1048,lwr_k=600:6.1008,lwr_k=700:6.0963,lwr_k=800:6.1029,lwr_k=900:6.1027,lwr_k=1000:6.1157'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.3112,lwr_k=10:9.4893,lwr_k=20:7.9586,lwr_k=30:7.4535,lwr_k=40:7.3702,lwr_k=50:7.3325,lwr_k=100:7.187,lwr_k=200:7.2316,lwr_k=300:7.2551,lwr_k=400:7.3002,lwr_k=500:7.4645,lwr_k=600:7.4617,lwr_k=700:7.4764,lwr_k=800:7.4667,lwr_k=900:7.4654,lwr_k=1000:7.4707'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.594,lwr_k=10:9.1743,lwr_k=20:9.6321,lwr_k=30:10.0789,lwr_k=40:10.379,lwr_k=50:10.4136,lwr_k=100:10.7234,lwr_k=200:11.0075,lwr_k=300:11.0944,lwr_k=400:11.1512,lwr_k=500:11.1559,lwr_k=600:11.1917,lwr_k=700:11.1948,lwr_k=800:11.2,lwr_k=900:11.2425,lwr_k=1000:11.2589'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.6494,lwr_k=10:12.6667,lwr_k=20:10.9203,lwr_k=30:10.6912,lwr_k=40:10.7254,lwr_k=50:10.5575,lwr_k=100:10.2144,lwr_k=200:9.9964,lwr_k=300:9.9696,lwr_k=400:9.9491,lwr_k=500:9.9585,lwr_k=600:9.9804,lwr_k=700:9.9816,lwr_k=800:10.1354,lwr_k=900:10.1434,lwr_k=1000:10.3437'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:16.0276,lwr_k=10:7.0155,lwr_k=20:8.6694,lwr_k=30:12.1658,lwr_k=40:12.952,lwr_k=50:13.133,lwr_k=100:13.7698,lwr_k=200:14.8448,lwr_k=300:14.9476,lwr_k=400:15.0701,lwr_k=500:15.1191,lwr_k=600:15.1792,lwr_k=700:15.2259,lwr_k=800:15.2629,lwr_k=900:15.3178,lwr_k=1000:15.3275'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.8317,lwr_k=10:7.5368,lwr_k=20:7.2295,lwr_k=30:7.5263,lwr_k=40:7.2054,lwr_k=50:7.1653,lwr_k=100:7.0031,lwr_k=200:7.3703,lwr_k=300:7.3261,lwr_k=400:7.3823,lwr_k=500:7.376,lwr_k=600:7.3692,lwr_k=700:7.3201,lwr_k=800:7.3068,lwr_k=900:7.3187,lwr_k=1000:7.3303'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:14.3499,lwr_k=10:7.1052,lwr_k=20:9.2326,lwr_k=30:9.889,lwr_k=40:10.6898,lwr_k=50:10.9021,lwr_k=100:12.038,lwr_k=200:12.7119,lwr_k=300:12.8955,lwr_k=400:13.1184,lwr_k=500:13.1676,lwr_k=600:13.2766,lwr_k=700:13.343,lwr_k=800:13.499,lwr_k=900:13.3434,lwr_k=1000:13.4571'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:11.8389,lwr_k=10:14.0092,lwr_k=20:12.9946,lwr_k=30:12.6958,lwr_k=40:12.0064,lwr_k=50:11.8473,lwr_k=100:11.2466,lwr_k=200:11.0809,lwr_k=300:11.0462,lwr_k=400:11.0828,lwr_k=500:11.1451,lwr_k=600:11.1968,lwr_k=700:11.2489,lwr_k=800:11.2657,lwr_k=900:11.2597,lwr_k=1000:11.3379'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.6414,lwr_k=10:5.2053,lwr_k=20:6.0595,lwr_k=30:6.1765,lwr_k=40:6.3402,lwr_k=50:6.479,lwr_k=100:6.8193,lwr_k=200:6.9657,lwr_k=300:7.0249,lwr_k=400:7.0967,lwr_k=500:7.1418,lwr_k=600:7.1662,lwr_k=700:7.2221,lwr_k=800:7.8391,lwr_k=900:7.9841,lwr_k=1000:8.0075'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:40.5421,lwr_k=10:147.8283,lwr_k=20:95.9722,lwr_k=30:72.6192,lwr_k=40:47.5598,lwr_k=50:36.6657,lwr_k=100:27.6019,lwr_k=200:27.6036,lwr_k=300:27.5532,lwr_k=400:27.6471,lwr_k=500:27.7064,lwr_k=600:27.6749,lwr_k=700:30.5215,lwr_k=800:37.6594,lwr_k=900:38.6754,lwr_k=1000:38.8221'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:9.3826,lwr_k=10:7.7882,lwr_k=20:8.5828,lwr_k=30:8.6992,lwr_k=40:8.7528,lwr_k=50:8.8383,lwr_k=100:8.8811,lwr_k=200:8.9714,lwr_k=300:9.0241,lwr_k=400:9.0545,lwr_k=500:9.0882,lwr_k=600:9.1027,lwr_k=700:9.0901,lwr_k=800:9.132,lwr_k=900:9.161,lwr_k=1000:9.185'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.1199,lwr_k=10:13.796,lwr_k=20:11.9896,lwr_k=30:11.6761,lwr_k=40:11.4944,lwr_k=50:11.4137,lwr_k=100:11.5702,lwr_k=200:11.7288,lwr_k=300:11.8381,lwr_k=400:11.8761,lwr_k=500:11.928,lwr_k=600:11.987,lwr_k=700:11.9689,lwr_k=800:11.9526,lwr_k=900:11.939,lwr_k=1000:11.9315'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_16'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.8106,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:0.8229,lwr_k=50:1.6011,lwr_k=100:3.285,lwr_k=200:4.1906,lwr_k=300:4.5637,lwr_k=400:4.7328,lwr_k=500:4.8719,lwr_k=600:4.9581,lwr_k=700:5.0616,lwr_k=800:5.1515,lwr_k=900:5.2219,lwr_k=1000:5.2932'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.7073,lwr_k=10:31.4259,lwr_k=20:58.7969,lwr_k=30:177.0838,lwr_k=40:48.4005,lwr_k=50:19.9597,lwr_k=100:8.9475,lwr_k=200:6.8854,lwr_k=300:6.5274,lwr_k=400:6.5174,lwr_k=500:6.5654,lwr_k=600:6.6025,lwr_k=700:6.7016,lwr_k=800:6.7466,lwr_k=900:6.8467,lwr_k=1000:6.9416'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.0431,lwr_k=10:0.0,lwr_k=20:0.0017,lwr_k=30:0.5787,lwr_k=40:8.2952,lwr_k=50:9.9914,lwr_k=100:4.2738,lwr_k=200:5.4702,lwr_k=300:5.8832,lwr_k=400:6.0114,lwr_k=500:6.0914,lwr_k=600:6.1555,lwr_k=700:6.2114,lwr_k=800:6.248,lwr_k=900:6.2819,lwr_k=1000:6.2916'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.3703,lwr_k=10:61.6543,lwr_k=20:5425.8617,lwr_k=30:38719.4949,lwr_k=40:2995.4544,lwr_k=50:1291.1852,lwr_k=100:10.0331,lwr_k=200:8.4931,lwr_k=300:8.0791,lwr_k=400:7.9265,lwr_k=500:7.9343,lwr_k=600:7.9592,lwr_k=700:7.8961,lwr_k=800:7.8495,lwr_k=900:7.8016,lwr_k=1000:7.9006'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.4174,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0018,lwr_k=40:0.9583,lwr_k=50:1.9082,lwr_k=100:3.9985,lwr_k=200:5.2211,lwr_k=300:5.6625,lwr_k=400:5.9039,lwr_k=500:6.0644,lwr_k=600:6.2104,lwr_k=700:6.3896,lwr_k=800:6.4807,lwr_k=900:6.5322,lwr_k=1000:6.609'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.2427,lwr_k=10:15.9532,lwr_k=20:42.206,lwr_k=30:199.0674,lwr_k=40:59.905,lwr_k=50:26.6328,lwr_k=100:7.3542,lwr_k=200:6.3141,lwr_k=300:6.0298,lwr_k=400:5.9771,lwr_k=500:6.0335,lwr_k=600:6.1002,lwr_k=700:6.1154,lwr_k=800:6.1415,lwr_k=900:6.1531,lwr_k=1000:6.1718'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.8125,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.1422,lwr_k=40:1.7356,lwr_k=50:2.1094,lwr_k=100:4.2594,lwr_k=200:5.304,lwr_k=300:5.7885,lwr_k=400:6.1331,lwr_k=500:6.3044,lwr_k=600:6.447,lwr_k=700:6.5582,lwr_k=800:6.6938,lwr_k=900:6.7731,lwr_k=1000:6.8357'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.1694,lwr_k=10:48.0599,lwr_k=20:335.3717,lwr_k=30:12522.6354,lwr_k=40:152.9346,lwr_k=50:35.9508,lwr_k=100:10.5433,lwr_k=200:9.1597,lwr_k=300:9.2979,lwr_k=400:9.3091,lwr_k=500:9.396,lwr_k=600:9.2843,lwr_k=700:9.3141,lwr_k=800:9.2671,lwr_k=900:9.1874,lwr_k=1000:9.2478'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.5019,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0672,lwr_k=40:0.848,lwr_k=50:1.7315,lwr_k=100:3.2659,lwr_k=200:4.3287,lwr_k=300:4.7184,lwr_k=400:5.0049,lwr_k=500:5.1913,lwr_k=600:5.3616,lwr_k=700:5.431,lwr_k=800:5.491,lwr_k=900:5.5553,lwr_k=1000:5.5942'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:22.924,lwr_k=10:106.4812,lwr_k=20:43929.6768,lwr_k=30:495.6971,lwr_k=40:152.7193,lwr_k=50:84.2578,lwr_k=100:47.4332,lwr_k=200:28.1765,lwr_k=300:19.1025,lwr_k=400:18.6005,lwr_k=500:18.2951,lwr_k=600:18.64,lwr_k=700:19.3913,lwr_k=800:20.0295,lwr_k=900:20.4358,lwr_k=1000:20.5775'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:6.5081,lwr_k=10:0.0002,lwr_k=20:0.1903,lwr_k=30:1.9936,lwr_k=40:12.2851,lwr_k=50:9.2161,lwr_k=100:5.6756,lwr_k=200:5.2435,lwr_k=300:5.4024,lwr_k=400:5.5372,lwr_k=500:5.6845,lwr_k=600:5.7577,lwr_k=700:5.8178,lwr_k=800:5.8722,lwr_k=900:5.9,lwr_k=1000:5.9185'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.3316,lwr_k=10:801.4668,lwr_k=20:5939.9948,lwr_k=30:123518.3671,lwr_k=40:38662.7048,lwr_k=50:1530512.2692,lwr_k=100:34.2784,lwr_k=200:35.7828,lwr_k=300:19.9692,lwr_k=400:14.6234,lwr_k=500:8.5569,lwr_k=600:8.6038,lwr_k=700:8.602,lwr_k=800:8.5075,lwr_k=900:8.6565,lwr_k=1000:8.6158'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_17'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.8598,lwr_k=10:0.0,lwr_k=20:0.0001,lwr_k=30:0.7933,lwr_k=40:8.4793,lwr_k=50:6.898,lwr_k=100:6.0423,lwr_k=200:5.2716,lwr_k=300:6.4378,lwr_k=400:5.9842,lwr_k=500:6.0231,lwr_k=600:6.5521,lwr_k=700:6.2687,lwr_k=800:6.1798,lwr_k=900:5.8786,lwr_k=1000:5.9258'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.0579,lwr_k=10:54.1789,lwr_k=20:12675.3115,lwr_k=30:38057.5051,lwr_k=40:562464.2004,lwr_k=50:3086440.2916,lwr_k=100:59.4585,lwr_k=200:9.4565,lwr_k=300:8.7963,lwr_k=400:8.4964,lwr_k=500:8.0846,lwr_k=600:8.0114,lwr_k=700:8.3903,lwr_k=800:8.0092,lwr_k=900:7.9313,lwr_k=1000:8.1642'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.9245,lwr_k=10:0.0,lwr_k=20:0.0053,lwr_k=30:1.2729,lwr_k=40:13.7435,lwr_k=50:10.7871,lwr_k=100:7.9918,lwr_k=200:8.3241,lwr_k=300:8.0721,lwr_k=400:8.0352,lwr_k=500:7.9781,lwr_k=600:7.5542,lwr_k=700:7.4959,lwr_k=800:7.5161,lwr_k=900:7.5649,lwr_k=1000:7.7356'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.3513,lwr_k=10:89.3472,lwr_k=20:620.9056,lwr_k=30:1848.3771,lwr_k=40:93193983.3102,lwr_k=50:1369159.9602,lwr_k=100:16.3335,lwr_k=200:12.7303,lwr_k=300:10.7531,lwr_k=400:9.3155,lwr_k=500:9.6913,lwr_k=600:9.3851,lwr_k=700:10.1559,lwr_k=800:9.3974,lwr_k=900:10.0868,lwr_k=1000:9.7512'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.446,lwr_k=10:0.0,lwr_k=20:0.0019,lwr_k=30:1.911,lwr_k=40:18.3067,lwr_k=50:18.2575,lwr_k=100:9.8865,lwr_k=200:7.7011,lwr_k=300:7.3511,lwr_k=400:7.3196,lwr_k=500:6.7384,lwr_k=600:7.1216,lwr_k=700:7.4055,lwr_k=800:7.8639,lwr_k=900:13.3173,lwr_k=1000:7.1819'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:5.9229,lwr_k=10:512.9276,lwr_k=20:1152.9695,lwr_k=30:16884.1364,lwr_k=40:135.1516,lwr_k=50:235.3167,lwr_k=100:110.2509,lwr_k=200:17.5875,lwr_k=300:29.3529,lwr_k=400:9.3905,lwr_k=500:7.0567,lwr_k=600:20.9472,lwr_k=700:6.9014,lwr_k=800:7.2233,lwr_k=900:7.0949,lwr_k=1000:6.0028'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.8951,lwr_k=10:0.0,lwr_k=20:0.0039,lwr_k=30:1.4618,lwr_k=40:5.2326,lwr_k=50:5.2047,lwr_k=100:4.8123,lwr_k=200:5.6185,lwr_k=300:5.3902,lwr_k=400:6.0589,lwr_k=500:5.859,lwr_k=600:6.4399,lwr_k=700:6.0906,lwr_k=800:6.1062,lwr_k=900:6.0404,lwr_k=1000:6.3071'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:1326940.4908,lwr_k=10:58.659,lwr_k=20:56258.6415,lwr_k=30:199.2265,lwr_k=40:393134287.8948,lwr_k=50:607136395.4177,lwr_k=100:1637737482.6684,lwr_k=200:1251803160.7904,lwr_k=300:3387241517.4895,lwr_k=400:441487090.4904,lwr_k=500:491221110.1639,lwr_k=600:4227028658.8491,lwr_k=700:3570093618.9661,lwr_k=800:3559870351.3716,lwr_k=900:397016992.8711,lwr_k=1000:299689151.3082'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.3362,lwr_k=10:0.0,lwr_k=20:0.0001,lwr_k=30:0.9625,lwr_k=40:4.3128,lwr_k=50:10.0566,lwr_k=100:3.9135,lwr_k=200:4.211,lwr_k=300:4.3855,lwr_k=400:4.4137,lwr_k=500:4.5255,lwr_k=600:4.5855,lwr_k=700:4.6182,lwr_k=800:4.8186,lwr_k=900:4.7849,lwr_k=1000:4.7582'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:282996865.801,lwr_k=10:50.7688,lwr_k=20:16140.4437,lwr_k=30:502.0583,lwr_k=40:163.2637,lwr_k=50:208365.986,lwr_k=100:1990246458.2598,lwr_k=200:54207824.0685,lwr_k=300:7204058838.1101,lwr_k=400:12754401199.9486,lwr_k=500:17627266216.339,lwr_k=600:10384207950.3759,lwr_k=700:13792712327.9429,lwr_k=800:15976299997.2819,lwr_k=900:19.4725,lwr_k=1000:19.1377'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:7.1471,lwr_k=10:0.0,lwr_k=20:0.0046,lwr_k=30:3.2778,lwr_k=40:18.4586,lwr_k=50:16.3236,lwr_k=100:11.1199,lwr_k=200:8.5151,lwr_k=300:6.7424,lwr_k=400:6.3136,lwr_k=500:6.0427,lwr_k=600:6.3387,lwr_k=700:6.5989,lwr_k=800:7.0754,lwr_k=900:6.4743,lwr_k=1000:6.813'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:601123.9727,lwr_k=10:55.2004,lwr_k=20:1785.2302,lwr_k=30:5602.5876,lwr_k=40:779.0016,lwr_k=50:161.2569,lwr_k=100:76.2269,lwr_k=200:12.443,lwr_k=300:12.8617,lwr_k=400:10.5082,lwr_k=500:9.3511,lwr_k=600:11.6753,lwr_k=700:9.5515,lwr_k=800:10.8224,lwr_k=900:9.1977,lwr_k=1000:3231.286'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_18'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.7235,lwr_k=10:8.3718,lwr_k=20:9.3297,lwr_k=30:9.6427,lwr_k=40:9.7486,lwr_k=50:9.7432,lwr_k=100:10.0524,lwr_k=200:10.162,lwr_k=300:10.1756,lwr_k=400:10.1921,lwr_k=500:10.232,lwr_k=600:10.2812,lwr_k=700:10.3179,lwr_k=800:10.3665,lwr_k=900:10.439,lwr_k=1000:10.4711'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:15.4607,lwr_k=10:16.4824,lwr_k=20:15.3129,lwr_k=30:15.2025,lwr_k=40:14.9399,lwr_k=50:14.9344,lwr_k=100:14.7749,lwr_k=200:14.7894,lwr_k=300:14.9383,lwr_k=400:14.5082,lwr_k=500:14.537,lwr_k=600:14.5677,lwr_k=700:14.6847,lwr_k=800:14.8624,lwr_k=900:15.023,lwr_k=1000:15.0963'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:17.6276,lwr_k=10:11.1015,lwr_k=20:11.5425,lwr_k=30:11.6834,lwr_k=40:11.6906,lwr_k=50:11.8602,lwr_k=100:12.0257,lwr_k=200:12.4759,lwr_k=300:12.9614,lwr_k=400:13.3527,lwr_k=500:13.5026,lwr_k=600:13.5771,lwr_k=700:13.6181,lwr_k=800:13.6737,lwr_k=900:13.744,lwr_k=1000:13.8258'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:16.913,lwr_k=10:14.1236,lwr_k=20:13.0094,lwr_k=30:12.953,lwr_k=40:12.6625,lwr_k=50:12.6859,lwr_k=100:12.5277,lwr_k=200:12.8714,lwr_k=300:13.3322,lwr_k=400:13.6226,lwr_k=500:13.694,lwr_k=600:13.7261,lwr_k=700:13.7157,lwr_k=800:13.7678,lwr_k=900:13.8206,lwr_k=1000:13.874'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.8528,lwr_k=10:8.977,lwr_k=20:9.1135,lwr_k=30:8.8699,lwr_k=40:8.9944,lwr_k=50:8.9655,lwr_k=100:9.0449,lwr_k=200:9.1472,lwr_k=300:9.3181,lwr_k=400:9.445,lwr_k=500:9.6336,lwr_k=600:9.7879,lwr_k=700:9.8921,lwr_k=800:9.9652,lwr_k=900:9.9972,lwr_k=1000:10.0109'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.6664,lwr_k=10:9.6271,lwr_k=20:8.8835,lwr_k=30:8.6169,lwr_k=40:8.6006,lwr_k=50:8.5147,lwr_k=100:8.367,lwr_k=200:8.4356,lwr_k=300:8.5444,lwr_k=400:8.5908,lwr_k=500:8.6293,lwr_k=600:8.6841,lwr_k=700:8.7151,lwr_k=800:8.7324,lwr_k=900:8.74,lwr_k=1000:8.7476'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.8005,lwr_k=10:5.7721,lwr_k=20:6.272,lwr_k=30:6.4364,lwr_k=40:6.5741,lwr_k=50:6.6648,lwr_k=100:6.7754,lwr_k=200:7.0054,lwr_k=300:7.2014,lwr_k=400:7.3592,lwr_k=500:7.4822,lwr_k=600:7.6255,lwr_k=700:7.7318,lwr_k=800:7.8262,lwr_k=900:7.8484,lwr_k=1000:7.8617'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:11.9705,lwr_k=10:12.0275,lwr_k=20:11.4147,lwr_k=30:10.9446,lwr_k=40:10.7951,lwr_k=50:10.7841,lwr_k=100:10.8143,lwr_k=200:10.914,lwr_k=300:10.8706,lwr_k=400:10.9281,lwr_k=500:11.1526,lwr_k=600:11.3014,lwr_k=700:11.4567,lwr_k=800:11.5532,lwr_k=900:11.5715,lwr_k=1000:11.5946'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.8511,lwr_k=10:5.6921,lwr_k=20:6.0277,lwr_k=30:6.1456,lwr_k=40:6.2522,lwr_k=50:6.2497,lwr_k=100:6.3389,lwr_k=200:6.3532,lwr_k=300:6.3896,lwr_k=400:6.4497,lwr_k=500:6.5377,lwr_k=600:6.6267,lwr_k=700:6.7331,lwr_k=800:6.836,lwr_k=900:6.9167,lwr_k=1000:6.9761'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:27.1492,lwr_k=10:19.994,lwr_k=20:19.0385,lwr_k=30:20.2827,lwr_k=40:20.122,lwr_k=50:19.7975,lwr_k=100:20.6496,lwr_k=200:20.9453,lwr_k=300:21.6113,lwr_k=400:22.2257,lwr_k=500:23.0029,lwr_k=600:23.7376,lwr_k=700:24.3233,lwr_k=800:24.9903,lwr_k=900:25.4457,lwr_k=1000:25.7095'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:9.9027,lwr_k=10:7.7278,lwr_k=20:8.4413,lwr_k=30:8.6258,lwr_k=40:8.6732,lwr_k=50:8.7137,lwr_k=100:8.8368,lwr_k=200:9.1217,lwr_k=300:9.3857,lwr_k=400:9.4256,lwr_k=500:9.4721,lwr_k=600:9.4984,lwr_k=700:9.5213,lwr_k=800:9.5358,lwr_k=900:9.5524,lwr_k=1000:9.5796'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.2582,lwr_k=10:12.8424,lwr_k=20:12.1153,lwr_k=30:11.7534,lwr_k=40:11.6921,lwr_k=50:11.3399,lwr_k=100:11.1563,lwr_k=200:11.0163,lwr_k=300:11.107,lwr_k=400:11.0501,lwr_k=500:11.0487,lwr_k=600:11.0976,lwr_k=700:11.0892,lwr_k=800:11.0643,lwr_k=900:11.0934,lwr_k=1000:11.0492'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_19'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.3171,lwr_k=10:2.5128,lwr_k=20:5.2355,lwr_k=30:6.2012,lwr_k=40:6.7406,lwr_k=50:7.0607,lwr_k=100:7.9425,lwr_k=200:8.5955,lwr_k=300:8.9591,lwr_k=400:9.2708,lwr_k=500:9.4526,lwr_k=600:9.6223,lwr_k=700:9.8097,lwr_k=800:9.9416,lwr_k=900:10.0714,lwr_k=1000:10.1869'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:14.2158,lwr_k=10:23.4276,lwr_k=20:11.4106,lwr_k=30:10.4837,lwr_k=40:10.01,lwr_k=50:9.5201,lwr_k=100:9.3988,lwr_k=200:9.5383,lwr_k=300:9.8328,lwr_k=400:10.1673,lwr_k=500:10.341,lwr_k=600:10.5878,lwr_k=700:10.8144,lwr_k=800:10.9668,lwr_k=900:11.1341,lwr_k=1000:11.287'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:14.5629,lwr_k=10:2.9906,lwr_k=20:5.9858,lwr_k=30:6.9916,lwr_k=40:7.6491,lwr_k=50:8.0459,lwr_k=100:8.899,lwr_k=200:9.739,lwr_k=300:10.1322,lwr_k=400:10.4573,lwr_k=500:10.6948,lwr_k=600:10.9109,lwr_k=700:11.1528,lwr_k=800:11.3246,lwr_k=900:11.4787,lwr_k=1000:11.6492'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.5635,lwr_k=10:35.1859,lwr_k=20:11.818,lwr_k=30:10.6634,lwr_k=40:10.0034,lwr_k=50:9.6873,lwr_k=100:9.4076,lwr_k=200:9.6709,lwr_k=300:9.7962,lwr_k=400:9.9758,lwr_k=500:10.1224,lwr_k=600:10.2272,lwr_k=700:10.3637,lwr_k=800:10.4464,lwr_k=900:10.5729,lwr_k=1000:10.669'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:14.1962,lwr_k=10:2.8098,lwr_k=20:5.6153,lwr_k=30:6.6953,lwr_k=40:7.4551,lwr_k=50:7.9354,lwr_k=100:9.0369,lwr_k=200:10.0404,lwr_k=300:10.5885,lwr_k=400:10.8719,lwr_k=500:11.0728,lwr_k=600:11.3201,lwr_k=700:11.4696,lwr_k=800:11.6186,lwr_k=900:11.7394,lwr_k=1000:11.8736'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.1259,lwr_k=10:35.0586,lwr_k=20:9.9341,lwr_k=30:8.8992,lwr_k=40:8.8936,lwr_k=50:8.4795,lwr_k=100:8.1931,lwr_k=200:8.0905,lwr_k=300:8.0943,lwr_k=400:8.2471,lwr_k=500:8.2448,lwr_k=600:8.2965,lwr_k=700:8.3128,lwr_k=800:8.358,lwr_k=900:8.3917,lwr_k=1000:8.43'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:15.5613,lwr_k=10:3.0824,lwr_k=20:6.2771,lwr_k=30:7.2642,lwr_k=40:7.8179,lwr_k=50:8.2032,lwr_k=100:9.5942,lwr_k=200:10.6191,lwr_k=300:11.2919,lwr_k=400:11.7114,lwr_k=500:12.0561,lwr_k=600:12.2777,lwr_k=700:12.5784,lwr_k=800:12.7201,lwr_k=900:12.9641,lwr_k=1000:13.1238'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:16.3853,lwr_k=10:28.5228,lwr_k=20:15.9635,lwr_k=30:14.746,lwr_k=40:13.7429,lwr_k=50:13.4172,lwr_k=100:12.5567,lwr_k=200:12.5041,lwr_k=300:12.8392,lwr_k=400:13.069,lwr_k=500:13.2253,lwr_k=600:13.3374,lwr_k=700:13.4629,lwr_k=800:13.5051,lwr_k=900:13.6106,lwr_k=1000:13.6811'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.9391,lwr_k=10:2.1283,lwr_k=20:4.2473,lwr_k=30:4.9536,lwr_k=40:5.481,lwr_k=50:5.8662,lwr_k=100:6.3556,lwr_k=200:6.822,lwr_k=300:7.0699,lwr_k=400:7.2433,lwr_k=500:7.3591,lwr_k=600:7.4681,lwr_k=700:7.5651,lwr_k=800:7.6574,lwr_k=900:7.7061,lwr_k=1000:7.7913'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:31.6657,lwr_k=10:38.9278,lwr_k=20:17.4533,lwr_k=30:17.1477,lwr_k=40:16.26,lwr_k=50:19.2074,lwr_k=100:20.4948,lwr_k=200:23.2828,lwr_k=300:24.4489,lwr_k=400:25.3732,lwr_k=500:26.0099,lwr_k=600:26.5335,lwr_k=700:26.9451,lwr_k=800:27.4453,lwr_k=900:27.7315,lwr_k=1000:28.1706'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:12.6105,lwr_k=10:2.5058,lwr_k=20:5.2559,lwr_k=30:6.0923,lwr_k=40:6.7966,lwr_k=50:7.0878,lwr_k=100:7.9592,lwr_k=200:8.7219,lwr_k=300:9.2142,lwr_k=400:9.5639,lwr_k=500:9.7815,lwr_k=600:9.941,lwr_k=700:10.1203,lwr_k=800:10.2241,lwr_k=900:10.315,lwr_k=1000:10.4541'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:13.2813,lwr_k=10:35.8713,lwr_k=20:13.3549,lwr_k=30:11.5408,lwr_k=40:11.1073,lwr_k=50:10.7261,lwr_k=100:10.4102,lwr_k=200:10.6427,lwr_k=300:10.9953,lwr_k=400:11.1074,lwr_k=500:11.224,lwr_k=600:11.2934,lwr_k=700:11.3314,lwr_k=800:11.394,lwr_k=900:11.4487,lwr_k=1000:11.484'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_20'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:25.3404,lwr_k=10:20.9774,lwr_k=20:22.11,lwr_k=30:22.7426,lwr_k=40:22.9384,lwr_k=50:23.2051,lwr_k=100:23.6951,lwr_k=200:24.183,lwr_k=300:24.3506,lwr_k=400:24.444,lwr_k=500:24.5351,lwr_k=600:24.5986,lwr_k=700:24.6351,lwr_k=800:24.6409,lwr_k=900:24.6832,lwr_k=1000:24.7351'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:28.1753,lwr_k=10:31.2762,lwr_k=20:28.8043,lwr_k=30:28.1098,lwr_k=40:27.5909,lwr_k=50:27.3761,lwr_k=100:27.4284,lwr_k=200:27.6448,lwr_k=300:27.6267,lwr_k=400:27.5085,lwr_k=500:27.4067,lwr_k=600:27.4527,lwr_k=700:27.4678,lwr_k=800:27.5644,lwr_k=900:27.6184,lwr_k=1000:27.7204'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.8931,lwr_k=10:4.2435,lwr_k=20:4.6824,lwr_k=30:4.7046,lwr_k=40:4.7162,lwr_k=50:4.7107,lwr_k=100:4.7877,lwr_k=200:4.827,lwr_k=300:4.8279,lwr_k=400:4.8205,lwr_k=500:4.845,lwr_k=600:4.8581,lwr_k=700:4.8673,lwr_k=800:4.8676,lwr_k=900:4.8633,lwr_k=1000:4.8669'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.3148,lwr_k=10:8.8089,lwr_k=20:7.631,lwr_k=30:7.6681,lwr_k=40:7.5178,lwr_k=50:8.3715,lwr_k=100:7.3138,lwr_k=200:7.2264,lwr_k=300:7.2099,lwr_k=400:7.2432,lwr_k=500:7.2303,lwr_k=600:7.2635,lwr_k=700:7.3023,lwr_k=800:7.3059,lwr_k=900:7.2969,lwr_k=1000:7.2866'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.4293,lwr_k=10:4.8125,lwr_k=20:4.9336,lwr_k=30:5.0135,lwr_k=40:5.0647,lwr_k=50:5.1123,lwr_k=100:5.1886,lwr_k=200:5.2506,lwr_k=300:5.2785,lwr_k=400:5.2966,lwr_k=500:5.3058,lwr_k=600:5.3295,lwr_k=700:5.3469,lwr_k=800:5.3564,lwr_k=900:5.3684,lwr_k=1000:5.3767'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.6362,lwr_k=10:8.2039,lwr_k=20:7.863,lwr_k=30:7.7218,lwr_k=40:7.6131,lwr_k=50:7.5334,lwr_k=100:7.4899,lwr_k=200:7.4575,lwr_k=300:7.4996,lwr_k=400:7.5259,lwr_k=500:7.4526,lwr_k=600:7.4728,lwr_k=700:7.5078,lwr_k=800:7.5211,lwr_k=900:7.5263,lwr_k=1000:7.5341'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.1325,lwr_k=10:5.2684,lwr_k=20:5.5876,lwr_k=30:5.6645,lwr_k=40:5.7497,lwr_k=50:5.7512,lwr_k=100:5.8133,lwr_k=200:5.8738,lwr_k=300:5.9175,lwr_k=400:5.9387,lwr_k=500:5.9626,lwr_k=600:5.991,lwr_k=700:6.0268,lwr_k=800:6.0465,lwr_k=900:6.0617,lwr_k=1000:6.0718'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:11.049,lwr_k=10:25995144.0919,lwr_k=20:11.1674,lwr_k=30:10.9729,lwr_k=40:10.9384,lwr_k=50:10.9199,lwr_k=100:10.8493,lwr_k=200:10.9703,lwr_k=300:10.957,lwr_k=400:10.9858,lwr_k=500:11.0446,lwr_k=600:11.0784,lwr_k=700:11.11,lwr_k=800:11.1273,lwr_k=900:11.1429,lwr_k=1000:11.1464'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:3.5213,lwr_k=10:3.1876,lwr_k=20:3.3104,lwr_k=30:3.3359,lwr_k=40:3.3767,lwr_k=50:3.4231,lwr_k=100:3.4239,lwr_k=200:3.4498,lwr_k=300:3.4575,lwr_k=400:3.4627,lwr_k=500:3.4595,lwr_k=600:3.4625,lwr_k=700:3.4672,lwr_k=800:3.4714,lwr_k=900:3.4779,lwr_k=1000:3.4791'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:18.1675,lwr_k=10:18.3461,lwr_k=20:16.6765,lwr_k=30:16.9523,lwr_k=40:16.9922,lwr_k=50:16.8966,lwr_k=100:17.0157,lwr_k=200:17.4942,lwr_k=300:17.4952,lwr_k=400:17.3931,lwr_k=500:17.4138,lwr_k=600:17.3786,lwr_k=700:17.4206,lwr_k=800:17.4507,lwr_k=900:17.492,lwr_k=1000:17.5065'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:4.3021,lwr_k=10:3.7753,lwr_k=20:3.9331,lwr_k=30:4.0344,lwr_k=40:4.1424,lwr_k=50:4.1067,lwr_k=100:4.1093,lwr_k=200:4.1441,lwr_k=300:4.1677,lwr_k=400:4.1638,lwr_k=500:4.17,lwr_k=600:4.1714,lwr_k=700:4.1856,lwr_k=800:4.1905,lwr_k=900:4.1931,lwr_k=1000:4.2003'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.3543,lwr_k=10:10.0712,lwr_k=20:10.0359,lwr_k=30:10.0365,lwr_k=40:10.0469,lwr_k=50:9.957,lwr_k=100:9.916,lwr_k=200:9.7529,lwr_k=300:9.7126,lwr_k=400:9.7291,lwr_k=500:9.7337,lwr_k=600:9.7535,lwr_k=700:9.7921,lwr_k=800:9.8453,lwr_k=900:9.8651,lwr_k=1000:9.8954'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_21'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.0716,lwr_k=10:2.4401,lwr_k=20:4.8038,lwr_k=30:5.3314,lwr_k=40:5.6013,lwr_k=50:5.8932,lwr_k=100:6.4753,lwr_k=200:6.8923,lwr_k=300:7.1039,lwr_k=400:7.2042,lwr_k=500:7.2755,lwr_k=600:7.321,lwr_k=700:7.3558,lwr_k=800:7.3964,lwr_k=900:7.4054,lwr_k=1000:7.4498'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.0885,lwr_k=10:19.4585,lwr_k=20:11.6952,lwr_k=30:9.1984,lwr_k=40:8.5931,lwr_k=50:8.5255,lwr_k=100:7.9498,lwr_k=200:8.051,lwr_k=300:8.1112,lwr_k=400:8.187,lwr_k=500:8.2661,lwr_k=600:8.3335,lwr_k=700:8.3618,lwr_k=800:8.3949,lwr_k=900:8.4225,lwr_k=1000:8.4722'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.1605,lwr_k=10:2.6039,lwr_k=20:4.8134,lwr_k=30:5.593,lwr_k=40:6.095,lwr_k=50:6.2844,lwr_k=100:6.7774,lwr_k=200:7.2074,lwr_k=300:7.4644,lwr_k=400:7.5573,lwr_k=500:7.6618,lwr_k=600:7.7451,lwr_k=700:7.7988,lwr_k=800:7.8732,lwr_k=900:8.0689,lwr_k=1000:8.1832'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.6914,lwr_k=10:162225.4918,lwr_k=20:10.8535,lwr_k=30:9.8431,lwr_k=40:9.1793,lwr_k=50:8.1787,lwr_k=100:7.2848,lwr_k=200:7.3012,lwr_k=300:7.4858,lwr_k=400:7.3925,lwr_k=500:7.4861,lwr_k=600:7.5468,lwr_k=700:7.5993,lwr_k=800:7.6279,lwr_k=900:7.7023,lwr_k=1000:7.7892'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.764,lwr_k=10:2.6319,lwr_k=20:5.2804,lwr_k=30:6.262,lwr_k=40:6.8231,lwr_k=50:7.3322,lwr_k=100:8.329,lwr_k=200:9.3443,lwr_k=300:9.6531,lwr_k=400:9.9066,lwr_k=500:10.0243,lwr_k=600:10.0955,lwr_k=700:10.1591,lwr_k=800:10.2324,lwr_k=900:10.2965,lwr_k=1000:10.3373'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.7149,lwr_k=10:26.8593,lwr_k=20:10.1252,lwr_k=30:9.3385,lwr_k=40:8.331,lwr_k=50:8.2083,lwr_k=100:7.9713,lwr_k=200:6.7592,lwr_k=300:6.6057,lwr_k=400:6.6294,lwr_k=500:6.6525,lwr_k=600:6.6924,lwr_k=700:6.7108,lwr_k=800:6.7449,lwr_k=900:6.7567,lwr_k=1000:6.7587'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.7994,lwr_k=10:2.5463,lwr_k=20:4.5821,lwr_k=30:5.4349,lwr_k=40:6.0322,lwr_k=50:6.3797,lwr_k=100:7.3296,lwr_k=200:8.0479,lwr_k=300:8.5534,lwr_k=400:8.811,lwr_k=500:8.9569,lwr_k=600:9.0958,lwr_k=700:9.1847,lwr_k=800:9.2588,lwr_k=900:9.3269,lwr_k=1000:9.389'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.8305,lwr_k=10:24.0454,lwr_k=20:10.7993,lwr_k=30:9.8621,lwr_k=40:9.9247,lwr_k=50:9.7658,lwr_k=100:9.4471,lwr_k=200:9.5357,lwr_k=300:9.5614,lwr_k=400:9.5447,lwr_k=500:9.6024,lwr_k=600:9.6505,lwr_k=700:9.6273,lwr_k=800:9.6589,lwr_k=900:9.6653,lwr_k=1000:9.6896'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.1293,lwr_k=10:2.585,lwr_k=20:4.4447,lwr_k=30:4.8541,lwr_k=40:5.2706,lwr_k=50:5.4811,lwr_k=100:5.8273,lwr_k=200:6.3265,lwr_k=300:6.4207,lwr_k=400:6.4505,lwr_k=500:6.5158,lwr_k=600:6.5805,lwr_k=700:6.6143,lwr_k=800:6.6588,lwr_k=900:6.7042,lwr_k=1000:6.741'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:24.2533,lwr_k=10:55.0046,lwr_k=20:18.3833,lwr_k=30:16.5487,lwr_k=40:16.6851,lwr_k=50:16.254,lwr_k=100:19.1667,lwr_k=200:21.3793,lwr_k=300:22.0306,lwr_k=400:22.0837,lwr_k=500:22.3354,lwr_k=600:22.5144,lwr_k=700:22.714,lwr_k=800:22.8096,lwr_k=900:22.9766,lwr_k=1000:23.0968'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:11.1424,lwr_k=10:2.5125,lwr_k=20:5.0784,lwr_k=30:5.9408,lwr_k=40:6.4293,lwr_k=50:6.7171,lwr_k=100:7.5668,lwr_k=200:8.216,lwr_k=300:8.6071,lwr_k=400:8.8514,lwr_k=500:9.019,lwr_k=600:9.1767,lwr_k=700:9.3358,lwr_k=800:9.4201,lwr_k=900:9.4937,lwr_k=1000:9.5736'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.2871,lwr_k=10:38.5294,lwr_k=20:15.0614,lwr_k=30:12.2838,lwr_k=40:11.1617,lwr_k=50:11.3347,lwr_k=100:10.1618,lwr_k=200:10.2398,lwr_k=300:10.3295,lwr_k=400:10.2484,lwr_k=500:10.302,lwr_k=600:10.2688,lwr_k=700:10.3189,lwr_k=800:10.3203,lwr_k=900:10.3463,lwr_k=1000:10.3227'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_22'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.8943,lwr_k=10:0.0,lwr_k=20:0.1461,lwr_k=30:2.0828,lwr_k=40:3.1555,lwr_k=50:3.9067,lwr_k=100:5.4192,lwr_k=200:6.2685,lwr_k=300:6.7011,lwr_k=400:6.8794,lwr_k=500:7.0821,lwr_k=600:7.1904,lwr_k=700:7.2582,lwr_k=800:7.319,lwr_k=900:7.4245,lwr_k=1000:7.4642'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.7294,lwr_k=10:128.4803,lwr_k=20:14120.5788,lwr_k=30:58.2733,lwr_k=40:42.1599,lwr_k=50:17.0581,lwr_k=100:7.9792,lwr_k=200:7.872,lwr_k=300:7.839,lwr_k=400:7.8912,lwr_k=500:7.9357,lwr_k=600:7.9684,lwr_k=700:8.0175,lwr_k=800:8.0297,lwr_k=900:8.0594,lwr_k=1000:8.0909'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.4823,lwr_k=10:0.0062,lwr_k=20:2.4558,lwr_k=30:3.5864,lwr_k=40:4.1668,lwr_k=50:4.8201,lwr_k=100:6.4662,lwr_k=200:7.2816,lwr_k=300:7.7114,lwr_k=400:7.8924,lwr_k=500:8.0321,lwr_k=600:8.1953,lwr_k=700:8.2752,lwr_k=800:8.3387,lwr_k=900:8.3969,lwr_k=1000:8.4649'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.0794,lwr_k=10:1278.8959,lwr_k=20:198095.7148,lwr_k=30:62121.8778,lwr_k=40:1341.3012,lwr_k=50:599.7748,lwr_k=100:11.2144,lwr_k=200:694.5897,lwr_k=300:9.4736,lwr_k=400:9.5594,lwr_k=500:9.1186,lwr_k=600:9.2855,lwr_k=700:9.2556,lwr_k=800:9.2215,lwr_k=900:9.189,lwr_k=1000:9.2218'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.7902,lwr_k=10:0.0,lwr_k=20:0.2322,lwr_k=30:2.4503,lwr_k=40:3.4449,lwr_k=50:4.0267,lwr_k=100:5.3505,lwr_k=200:6.2058,lwr_k=300:6.5563,lwr_k=400:6.7683,lwr_k=500:6.8321,lwr_k=600:6.921,lwr_k=700:7.0072,lwr_k=800:7.0925,lwr_k=900:7.1593,lwr_k=1000:7.2082'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.5721,lwr_k=10:88.4452,lwr_k=20:5127.0161,lwr_k=30:591.0535,lwr_k=40:19.1872,lwr_k=50:17.6089,lwr_k=100:11.2057,lwr_k=200:6.6844,lwr_k=300:6.1109,lwr_k=400:6.0976,lwr_k=500:6.0951,lwr_k=600:6.0458,lwr_k=700:6.0704,lwr_k=800:6.0622,lwr_k=900:6.0987,lwr_k=1000:6.0595'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.9187,lwr_k=10:0.0,lwr_k=20:0.663,lwr_k=30:2.1617,lwr_k=40:3.2066,lwr_k=50:3.7721,lwr_k=100:4.8867,lwr_k=200:5.6497,lwr_k=300:5.8926,lwr_k=400:6.108,lwr_k=500:6.2026,lwr_k=600:6.3864,lwr_k=700:6.5868,lwr_k=800:6.6783,lwr_k=900:6.7402,lwr_k=1000:6.7966'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.0195,lwr_k=10:274.833,lwr_k=20:3952.7943,lwr_k=30:248.2356,lwr_k=40:23.3603,lwr_k=50:18.0911,lwr_k=100:12.4985,lwr_k=200:11.4011,lwr_k=300:11.4013,lwr_k=400:10.768,lwr_k=500:10.5027,lwr_k=600:9.9889,lwr_k=700:9.8516,lwr_k=800:9.7367,lwr_k=900:9.6345,lwr_k=1000:9.6011'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.0178,lwr_k=10:0.0,lwr_k=20:0.4357,lwr_k=30:2.0544,lwr_k=40:3.0697,lwr_k=50:3.6785,lwr_k=100:4.8731,lwr_k=200:5.7262,lwr_k=300:5.9891,lwr_k=400:6.189,lwr_k=500:6.3189,lwr_k=600:6.3669,lwr_k=700:6.3942,lwr_k=800:6.452,lwr_k=900:6.4526,lwr_k=1000:6.4747'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:19.8768,lwr_k=10:117.2028,lwr_k=20:66249.9211,lwr_k=30:236.6063,lwr_k=40:52.3426,lwr_k=50:25.6783,lwr_k=100:18.0648,lwr_k=200:19.8806,lwr_k=300:19.7346,lwr_k=400:19.9871,lwr_k=500:20.1008,lwr_k=600:20.4238,lwr_k=700:20.4409,lwr_k=800:20.5271,lwr_k=900:20.6596,lwr_k=1000:20.7393'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:8.7223,lwr_k=10:0.0209,lwr_k=20:3.2525,lwr_k=30:3.5412,lwr_k=40:3.9871,lwr_k=50:4.6839,lwr_k=100:6.0986,lwr_k=200:7.1607,lwr_k=300:7.407,lwr_k=400:7.6285,lwr_k=500:7.7439,lwr_k=600:7.8072,lwr_k=700:7.8678,lwr_k=800:7.9152,lwr_k=900:7.9713,lwr_k=1000:7.9974'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.5032,lwr_k=10:292.9856,lwr_k=20:3164.664,lwr_k=30:125.4095,lwr_k=40:20.8346,lwr_k=50:15.5839,lwr_k=100:11.5092,lwr_k=200:10.0228,lwr_k=300:10.0462,lwr_k=400:10.1392,lwr_k=500:10.1176,lwr_k=600:10.0554,lwr_k=700:10.142,lwr_k=800:10.1485,lwr_k=900:10.1075,lwr_k=1000:10.1375'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_23'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.7878,lwr_k=10:6.0202,lwr_k=20:7.1153,lwr_k=30:7.5129,lwr_k=40:7.721,lwr_k=50:7.7664,lwr_k=100:7.8715,lwr_k=200:8.0769,lwr_k=300:8.2621,lwr_k=400:8.3987,lwr_k=500:8.5177,lwr_k=600:8.5693,lwr_k=700:8.6176,lwr_k=800:8.6583,lwr_k=900:8.6925,lwr_k=1000:8.7238'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.1319,lwr_k=10:11.032,lwr_k=20:9.5917,lwr_k=30:9.2008,lwr_k=40:9.092,lwr_k=50:8.982,lwr_k=100:8.9087,lwr_k=200:9.1874,lwr_k=300:9.4203,lwr_k=400:9.659,lwr_k=500:9.8442,lwr_k=600:9.9387,lwr_k=700:10.0619,lwr_k=800:10.1344,lwr_k=900:10.2035,lwr_k=1000:10.2696'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:13.7603,lwr_k=10:7.1453,lwr_k=20:8.4089,lwr_k=30:8.5803,lwr_k=40:8.7827,lwr_k=50:8.9888,lwr_k=100:9.5177,lwr_k=200:10.2767,lwr_k=300:10.7699,lwr_k=400:11.0922,lwr_k=500:11.3653,lwr_k=600:11.5738,lwr_k=700:11.7154,lwr_k=800:11.8305,lwr_k=900:11.924,lwr_k=1000:11.995'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.466,lwr_k=10:12.2728,lwr_k=20:11.5718,lwr_k=30:10.9948,lwr_k=40:10.9687,lwr_k=50:10.9311,lwr_k=100:10.6935,lwr_k=200:10.527,lwr_k=300:10.5557,lwr_k=400:10.6265,lwr_k=500:10.7135,lwr_k=600:10.741,lwr_k=700:10.8235,lwr_k=800:10.8859,lwr_k=900:10.9176,lwr_k=1000:10.9648'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:27.0314,lwr_k=10:11.9645,lwr_k=20:14.047,lwr_k=30:14.5397,lwr_k=40:15.1959,lwr_k=50:15.5256,lwr_k=100:16.4777,lwr_k=200:17.9255,lwr_k=300:18.9493,lwr_k=400:19.4781,lwr_k=500:19.9954,lwr_k=600:20.3796,lwr_k=700:20.6822,lwr_k=800:21.0114,lwr_k=900:21.3206,lwr_k=1000:21.5314'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:17.5788,lwr_k=10:17.4662,lwr_k=20:15.462,lwr_k=30:14.8306,lwr_k=40:14.3964,lwr_k=50:14.0991,lwr_k=100:13.7535,lwr_k=200:13.8513,lwr_k=300:13.7484,lwr_k=400:13.7684,lwr_k=500:13.8405,lwr_k=600:13.8334,lwr_k=700:13.8384,lwr_k=800:13.8852,lwr_k=900:13.9167,lwr_k=1000:13.9858'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:13.2725,lwr_k=10:6.6963,lwr_k=20:7.8717,lwr_k=30:8.2661,lwr_k=40:8.7348,lwr_k=50:9.0454,lwr_k=100:9.4587,lwr_k=200:10.1274,lwr_k=300:10.494,lwr_k=400:10.7813,lwr_k=500:10.9263,lwr_k=600:11.0734,lwr_k=700:11.1808,lwr_k=800:11.2819,lwr_k=900:11.3841,lwr_k=1000:11.5146'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:14.9239,lwr_k=10:14.9915,lwr_k=20:13.37,lwr_k=30:12.7826,lwr_k=40:12.6755,lwr_k=50:12.6506,lwr_k=100:12.3519,lwr_k=200:12.4042,lwr_k=300:12.5627,lwr_k=400:12.658,lwr_k=500:12.7414,lwr_k=600:12.8527,lwr_k=700:12.8618,lwr_k=800:12.9268,lwr_k=900:13.0337,lwr_k=1000:13.0309'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.167,lwr_k=10:5.3289,lwr_k=20:6.2188,lwr_k=30:6.3245,lwr_k=40:6.4458,lwr_k=50:6.5373,lwr_k=100:6.7522,lwr_k=200:6.9007,lwr_k=300:7.0141,lwr_k=400:7.1211,lwr_k=500:7.1917,lwr_k=600:7.2294,lwr_k=700:7.2593,lwr_k=800:7.2975,lwr_k=900:7.3338,lwr_k=1000:7.3698'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:26.3842,lwr_k=10:18.1774,lwr_k=20:16.5961,lwr_k=30:17.94,lwr_k=40:18.4126,lwr_k=50:18.3519,lwr_k=100:19.3975,lwr_k=200:21.3423,lwr_k=300:22.4176,lwr_k=400:22.9538,lwr_k=500:23.3506,lwr_k=600:23.6738,lwr_k=700:23.8242,lwr_k=800:24.0567,lwr_k=900:24.2224,lwr_k=1000:24.3764'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:12.6237,lwr_k=10:6.29,lwr_k=20:7.3521,lwr_k=30:7.779,lwr_k=40:8.0474,lwr_k=50:8.3011,lwr_k=100:9.042,lwr_k=200:9.4398,lwr_k=300:9.8258,lwr_k=400:10.0485,lwr_k=500:10.2786,lwr_k=600:10.4306,lwr_k=700:10.5538,lwr_k=800:10.6691,lwr_k=900:10.7392,lwr_k=1000:10.8182'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:13.0735,lwr_k=10:14.1338,lwr_k=20:12.2071,lwr_k=30:11.6266,lwr_k=40:11.2088,lwr_k=50:11.0902,lwr_k=100:11.2191,lwr_k=200:11.2337,lwr_k=300:11.4229,lwr_k=400:11.5843,lwr_k=500:11.6893,lwr_k=600:11.7444,lwr_k=700:11.7878,lwr_k=800:11.8096,lwr_k=900:11.8114,lwr_k=1000:11.8524'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_24'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.3579,lwr_k=10:0.0178,lwr_k=20:0.0542,lwr_k=30:1.1584,lwr_k=40:1.7816,lwr_k=50:2.2851,lwr_k=100:3.3235,lwr_k=200:4.1279,lwr_k=300:4.7452,lwr_k=400:5.0211,lwr_k=500:5.1591,lwr_k=600:5.237,lwr_k=700:5.3013,lwr_k=800:5.2901,lwr_k=900:5.3298,lwr_k=1000:5.1425'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.5275,lwr_k=10:13064.9812,lwr_k=20:16248520.3333,lwr_k=30:43440288.6824,lwr_k=40:99642.9437,lwr_k=50:5882.3517,lwr_k=100:12.2033,lwr_k=200:10.062,lwr_k=300:8.5642,lwr_k=400:7.5426,lwr_k=500:7.287,lwr_k=600:7.3525,lwr_k=700:7.2199,lwr_k=800:7.1648,lwr_k=900:7.2654,lwr_k=1000:17.573'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.6395,lwr_k=10:0.0151,lwr_k=20:0.1367,lwr_k=30:0.7375,lwr_k=40:2.261,lwr_k=50:3.0859,lwr_k=100:4.7283,lwr_k=200:6.0272,lwr_k=300:6.6265,lwr_k=400:7.1113,lwr_k=500:7.3011,lwr_k=600:7.4201,lwr_k=700:7.5447,lwr_k=800:7.6421,lwr_k=900:7.7414,lwr_k=1000:7.8118'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.766,lwr_k=10:17382883.7698,lwr_k=20:4291796.6751,lwr_k=30:12315.9098,lwr_k=40:24.5655,lwr_k=50:16.7155,lwr_k=100:10.2451,lwr_k=200:8.6306,lwr_k=300:8.5477,lwr_k=400:8.6201,lwr_k=500:8.7263,lwr_k=600:8.7913,lwr_k=700:8.6633,lwr_k=800:8.5689,lwr_k=900:8.6889,lwr_k=1000:8.6723'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.2818,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.2991,lwr_k=40:2.406,lwr_k=50:3.1226,lwr_k=100:4.6324,lwr_k=200:6.0197,lwr_k=300:6.755,lwr_k=400:7.0342,lwr_k=500:7.2493,lwr_k=600:7.4425,lwr_k=700:7.6566,lwr_k=800:7.7874,lwr_k=900:7.8667,lwr_k=1000:8.0338'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.6814,lwr_k=10:46.5591,lwr_k=20:237.3172,lwr_k=30:81.2335,lwr_k=40:3082.4611,lwr_k=50:18.5085,lwr_k=100:86.4547,lwr_k=200:9.3176,lwr_k=300:9.2302,lwr_k=400:8.6393,lwr_k=500:8.4822,lwr_k=600:8.4519,lwr_k=700:8.2074,lwr_k=800:8.0654,lwr_k=900:7.8292,lwr_k=1000:8.7332'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.2563,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.0778,lwr_k=40:1.9495,lwr_k=50:2.7125,lwr_k=100:3.9913,lwr_k=200:5.1085,lwr_k=300:5.868,lwr_k=400:6.2245,lwr_k=500:6.3087,lwr_k=600:6.4398,lwr_k=700:6.6487,lwr_k=800:6.8847,lwr_k=900:7.0766,lwr_k=1000:7.2835'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:12.487,lwr_k=10:29.9769,lwr_k=20:255.855,lwr_k=30:223.9255,lwr_k=40:98745.7709,lwr_k=50:20.2784,lwr_k=100:5977.6493,lwr_k=200:6986.1508,lwr_k=300:12.8903,lwr_k=400:11.0221,lwr_k=500:11.0052,lwr_k=600:11.0981,lwr_k=700:11.1436,lwr_k=800:11.1984,lwr_k=900:11.1546,lwr_k=1000:11.2488'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.1762,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.0453,lwr_k=40:2.0553,lwr_k=50:2.7633,lwr_k=100:4.0764,lwr_k=200:5.1948,lwr_k=300:5.7768,lwr_k=400:6.1308,lwr_k=500:6.3677,lwr_k=600:6.4828,lwr_k=700:6.5893,lwr_k=800:6.6428,lwr_k=900:6.7522,lwr_k=1000:6.8579'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:28.4311,lwr_k=10:46.2485,lwr_k=20:296.1993,lwr_k=30:1210.3356,lwr_k=40:30.9691,lwr_k=50:25.1988,lwr_k=100:21.8165,lwr_k=200:21.3543,lwr_k=300:21.6188,lwr_k=400:22.5996,lwr_k=500:23.1191,lwr_k=600:23.3061,lwr_k=700:23.7037,lwr_k=800:24.1109,lwr_k=900:24.0781,lwr_k=1000:24.3953'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:9.0746,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.188,lwr_k=40:1.9576,lwr_k=50:2.8523,lwr_k=100:3.9162,lwr_k=200:5.2244,lwr_k=300:5.9276,lwr_k=400:6.321,lwr_k=500:6.6431,lwr_k=600:6.8587,lwr_k=700:7.0471,lwr_k=800:7.0905,lwr_k=900:7.2227,lwr_k=1000:7.2536'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.7104,lwr_k=10:53.7047,lwr_k=20:196.9325,lwr_k=30:352.3325,lwr_k=40:1678.0253,lwr_k=50:21.7108,lwr_k=100:13.431,lwr_k=200:12.4814,lwr_k=300:10.5168,lwr_k=400:10.2773,lwr_k=500:10.7236,lwr_k=600:10.1508,lwr_k=700:10.0455,lwr_k=800:10.093,lwr_k=900:10.1375,lwr_k=1000:10.1559'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_25'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.0377,lwr_k=10:22.9457,lwr_k=20:4.3631,lwr_k=30:4.6743,lwr_k=40:4.6861,lwr_k=50:4.7869,lwr_k=100:5.1155,lwr_k=200:5.4356,lwr_k=300:5.5322,lwr_k=400:5.6126,lwr_k=500:5.6761,lwr_k=600:5.7301,lwr_k=700:5.777,lwr_k=800:5.8032,lwr_k=900:5.8412,lwr_k=1000:5.875'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.5107,lwr_k=10:39752920.5593,lwr_k=20:14281.0495,lwr_k=30:1347.0701,lwr_k=40:37885.5137,lwr_k=50:9.6953,lwr_k=100:8.8028,lwr_k=200:8.0344,lwr_k=300:7.9605,lwr_k=400:7.8805,lwr_k=500:7.6331,lwr_k=600:7.5671,lwr_k=700:7.4988,lwr_k=800:7.4824,lwr_k=900:7.4725,lwr_k=1000:7.4828'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.7586,lwr_k=10:1.8053,lwr_k=20:3.9698,lwr_k=30:4.8637,lwr_k=40:5.2559,lwr_k=50:5.5774,lwr_k=100:6.0898,lwr_k=200:6.3997,lwr_k=300:6.4874,lwr_k=400:6.5726,lwr_k=500:6.646,lwr_k=600:6.836,lwr_k=700:6.9244,lwr_k=800:6.9641,lwr_k=900:7.0139,lwr_k=1000:7.0701'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.4841,lwr_k=10:191572.8962,lwr_k=20:14.9905,lwr_k=30:8.8311,lwr_k=40:8.5824,lwr_k=50:7.8464,lwr_k=100:45.1381,lwr_k=200:7.3714,lwr_k=300:7.4723,lwr_k=400:7.5642,lwr_k=500:7.5893,lwr_k=600:7.6928,lwr_k=700:7.7692,lwr_k=800:7.8299,lwr_k=900:7.8724,lwr_k=1000:7.9072'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.7625,lwr_k=10:1.4665,lwr_k=20:3.5118,lwr_k=30:4.5307,lwr_k=40:4.9066,lwr_k=50:5.2108,lwr_k=100:5.9285,lwr_k=200:6.1841,lwr_k=300:6.2914,lwr_k=400:6.3162,lwr_k=500:6.3815,lwr_k=600:6.4667,lwr_k=700:6.4947,lwr_k=800:6.5383,lwr_k=900:6.5876,lwr_k=1000:6.5973'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.7495,lwr_k=10:2809249770.1385,lwr_k=20:4737336364.1859,lwr_k=30:130.1394,lwr_k=40:70.4471,lwr_k=50:65.8238,lwr_k=100:7.2428,lwr_k=200:6.9441,lwr_k=300:6.7367,lwr_k=400:6.7715,lwr_k=500:6.7317,lwr_k=600:6.7005,lwr_k=700:6.6721,lwr_k=800:6.6683,lwr_k=900:6.6176,lwr_k=1000:6.6257'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.803,lwr_k=10:4.2025,lwr_k=20:4.5205,lwr_k=30:5.1016,lwr_k=40:5.3131,lwr_k=50:5.3423,lwr_k=100:5.7281,lwr_k=200:6.0933,lwr_k=300:6.2551,lwr_k=400:6.3814,lwr_k=500:6.4922,lwr_k=600:6.5337,lwr_k=700:6.5458,lwr_k=800:6.561,lwr_k=900:6.5807,lwr_k=1000:6.5864'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.7901,lwr_k=10:13200.3964,lwr_k=20:15.3109,lwr_k=30:15.7156,lwr_k=40:12.1626,lwr_k=50:11.4268,lwr_k=100:10.4453,lwr_k=200:10.1251,lwr_k=300:10.1267,lwr_k=400:10.046,lwr_k=500:9.8895,lwr_k=600:9.8609,lwr_k=700:9.8803,lwr_k=800:9.8893,lwr_k=900:9.9163,lwr_k=1000:9.9266'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.1216,lwr_k=10:8.0171,lwr_k=20:3.2964,lwr_k=30:3.8194,lwr_k=40:4.0604,lwr_k=50:4.205,lwr_k=100:4.876,lwr_k=200:5.1988,lwr_k=300:5.324,lwr_k=400:5.3611,lwr_k=500:5.4391,lwr_k=600:5.5535,lwr_k=700:5.619,lwr_k=800:5.6627,lwr_k=900:5.7292,lwr_k=1000:5.7955'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:19.2494,lwr_k=10:1038126.5851,lwr_k=20:15971891.2815,lwr_k=30:21.0125,lwr_k=40:22.8304,lwr_k=50:18.1202,lwr_k=100:11.294,lwr_k=200:9.7054,lwr_k=300:13.4204,lwr_k=400:15.0481,lwr_k=500:16.0404,lwr_k=600:17.3175,lwr_k=700:17.34,lwr_k=800:16.6314,lwr_k=900:17.5662,lwr_k=1000:18.2987'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:6.7718,lwr_k=10:1.7632,lwr_k=20:3.863,lwr_k=30:4.4852,lwr_k=40:4.7114,lwr_k=50:4.9469,lwr_k=100:5.2889,lwr_k=200:5.5186,lwr_k=300:5.7494,lwr_k=400:5.8334,lwr_k=500:5.8527,lwr_k=600:5.9027,lwr_k=700:5.9831,lwr_k=800:6.1073,lwr_k=900:6.1498,lwr_k=1000:6.2027'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.7832,lwr_k=10:7605.7271,lwr_k=20:16.917,lwr_k=30:12.1187,lwr_k=40:11.3343,lwr_k=50:10.6559,lwr_k=100:9.3668,lwr_k=200:9.0723,lwr_k=300:8.8068,lwr_k=400:8.7918,lwr_k=500:8.7751,lwr_k=600:8.802,lwr_k=700:8.8092,lwr_k=800:8.8672,lwr_k=900:8.8817,lwr_k=1000:8.8745'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_26'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:32.9195,lwr_k=10:0.7084,lwr_k=20:15.1419,lwr_k=30:19.8777,lwr_k=40:22.061,lwr_k=50:23.4797,lwr_k=100:26.9143,lwr_k=200:28.9215,lwr_k=300:29.8829,lwr_k=400:30.4366,lwr_k=500:30.789,lwr_k=600:31.0023,lwr_k=700:31.2244,lwr_k=800:31.4809,lwr_k=900:31.6597,lwr_k=1000:31.7811'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:40.4443,lwr_k=10:266645.2028,lwr_k=20:241.8432,lwr_k=30:53.5271,lwr_k=40:46.4745,lwr_k=50:42.1153,lwr_k=100:38.6186,lwr_k=200:38.4741,lwr_k=300:38.8828,lwr_k=400:38.7303,lwr_k=500:38.8786,lwr_k=600:38.9697,lwr_k=700:39.1899,lwr_k=800:39.1783,lwr_k=900:39.2254,lwr_k=1000:39.2265'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.3173,lwr_k=10:32.7666,lwr_k=20:6.9994,lwr_k=30:7.3353,lwr_k=40:7.981,lwr_k=50:8.3185,lwr_k=100:9.4125,lwr_k=200:9.9451,lwr_k=300:10.2615,lwr_k=400:10.4058,lwr_k=500:10.546,lwr_k=600:10.582,lwr_k=700:10.6504,lwr_k=800:10.6845,lwr_k=900:10.7128,lwr_k=1000:10.7554'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:14.4134,lwr_k=10:73406167.4296,lwr_k=20:9199281.1184,lwr_k=30:1542627.3871,lwr_k=40:23.0312,lwr_k=50:34.5474,lwr_k=100:121.9189,lwr_k=200:13.7206,lwr_k=300:13.7554,lwr_k=400:13.7335,lwr_k=500:13.7723,lwr_k=600:13.8301,lwr_k=700:13.8527,lwr_k=800:13.8577,lwr_k=900:13.8563,lwr_k=1000:13.8876'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.7325,lwr_k=10:6.8245,lwr_k=20:8.4225,lwr_k=30:9.2309,lwr_k=40:9.5185,lwr_k=50:9.9935,lwr_k=100:11.0025,lwr_k=200:11.4827,lwr_k=300:11.7375,lwr_k=400:11.8984,lwr_k=500:12.0347,lwr_k=600:12.1428,lwr_k=700:12.2275,lwr_k=800:12.2989,lwr_k=900:12.3477,lwr_k=1000:12.4064'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:13.6873,lwr_k=10:288738720.0334,lwr_k=20:150442.4958,lwr_k=30:4276.1945,lwr_k=40:202889.9317,lwr_k=50:280073.4964,lwr_k=100:15.132,lwr_k=200:13.3478,lwr_k=300:13.1554,lwr_k=400:13.0754,lwr_k=500:12.961,lwr_k=600:13.1957,lwr_k=700:13.2434,lwr_k=800:13.2032,lwr_k=900:13.1877,lwr_k=1000:13.2203'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.3747,lwr_k=10:0.0,lwr_k=20:4.6063,lwr_k=30:6.9599,lwr_k=40:8.2477,lwr_k=50:9.0039,lwr_k=100:10.3571,lwr_k=200:11.165,lwr_k=300:11.5159,lwr_k=400:11.6067,lwr_k=500:11.751,lwr_k=600:11.7914,lwr_k=700:11.8573,lwr_k=800:11.9252,lwr_k=900:11.9296,lwr_k=1000:11.9691'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:14.7019,lwr_k=10:345.5132,lwr_k=20:37.9981,lwr_k=30:24.0215,lwr_k=40:20.5775,lwr_k=50:19.5801,lwr_k=100:16.3076,lwr_k=200:15.2467,lwr_k=300:14.4663,lwr_k=400:14.4322,lwr_k=500:14.336,lwr_k=600:14.4936,lwr_k=700:14.5449,lwr_k=800:14.5715,lwr_k=900:14.596,lwr_k=1000:14.6036'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.9487,lwr_k=10:45.8261,lwr_k=20:12.7246,lwr_k=30:10.5807,lwr_k=40:10.5128,lwr_k=50:11.0525,lwr_k=100:11.2967,lwr_k=200:11.621,lwr_k=300:11.8781,lwr_k=400:11.9872,lwr_k=500:12.047,lwr_k=600:12.1253,lwr_k=700:12.2055,lwr_k=800:12.27,lwr_k=900:12.297,lwr_k=1000:12.3757'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:30.8256,lwr_k=10:226869242502.2738,lwr_k=20:103097282.2391,lwr_k=30:5246740.6231,lwr_k=40:193015.894,lwr_k=50:176923.3959,lwr_k=100:4597345.3061,lwr_k=200:30.2665,lwr_k=300:28.5811,lwr_k=400:28.4355,lwr_k=500:28.5391,lwr_k=600:28.6796,lwr_k=700:28.6784,lwr_k=800:28.9803,lwr_k=900:29.0174,lwr_k=1000:29.1803'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:235.0387,lwr_k=10:267.4646,lwr_k=20:236.1389,lwr_k=30:235.2656,lwr_k=40:235.1973,lwr_k=50:235.872,lwr_k=100:238.564,lwr_k=200:235.4554,lwr_k=300:235.2143,lwr_k=400:235.0388,lwr_k=500:235.057,lwr_k=600:235.117,lwr_k=700:235.3828,lwr_k=800:235.1301,lwr_k=900:235.081,lwr_k=1000:235.0417'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:237.5599,lwr_k=10:271.5158,lwr_k=20:238.3782,lwr_k=30:237.6587,lwr_k=40:237.6114,lwr_k=50:238.1478,lwr_k=100:240.5807,lwr_k=200:237.8031,lwr_k=300:237.6228,lwr_k=400:237.5576,lwr_k=500:237.5418,lwr_k=600:237.563,lwr_k=700:237.7463,lwr_k=800:237.57,lwr_k=900:237.5469,lwr_k=1000:237.5773'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_27'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.8152,lwr_k=10:6.61,lwr_k=20:7.5457,lwr_k=30:7.9163,lwr_k=40:8.129,lwr_k=50:8.1867,lwr_k=100:8.3629,lwr_k=200:8.6808,lwr_k=300:8.9364,lwr_k=400:9.1135,lwr_k=500:9.3013,lwr_k=600:9.4589,lwr_k=700:9.6059,lwr_k=800:9.7183,lwr_k=900:9.8299,lwr_k=1000:9.9386'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:13.8704,lwr_k=10:11.218,lwr_k=20:9.8232,lwr_k=30:9.6953,lwr_k=40:9.4726,lwr_k=50:9.4784,lwr_k=100:9.2246,lwr_k=200:9.7124,lwr_k=300:10.0727,lwr_k=400:10.3376,lwr_k=500:10.6,lwr_k=600:10.8367,lwr_k=700:11.0801,lwr_k=800:11.2368,lwr_k=900:11.4341,lwr_k=1000:11.6052'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:30.0598,lwr_k=10:13.3161,lwr_k=20:14.6621,lwr_k=30:15.3008,lwr_k=40:15.8203,lwr_k=50:16.0769,lwr_k=100:17.0209,lwr_k=200:18.52,lwr_k=300:19.4761,lwr_k=400:20.2364,lwr_k=500:20.8627,lwr_k=600:21.4155,lwr_k=700:21.8846,lwr_k=800:22.3325,lwr_k=900:22.7674,lwr_k=1000:23.1408'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:21.4325,lwr_k=10:16.2227,lwr_k=20:15.5584,lwr_k=30:15.0902,lwr_k=40:14.7201,lwr_k=50:14.7728,lwr_k=100:14.6807,lwr_k=200:15.3711,lwr_k=300:15.4387,lwr_k=400:15.7,lwr_k=500:15.9405,lwr_k=600:16.1927,lwr_k=700:16.4198,lwr_k=800:16.6559,lwr_k=900:16.8814,lwr_k=1000:17.0647'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:15.5505,lwr_k=10:8.4636,lwr_k=20:9.3668,lwr_k=30:9.5954,lwr_k=40:9.8169,lwr_k=50:10.1391,lwr_k=100:10.8304,lwr_k=200:11.5629,lwr_k=300:11.9696,lwr_k=400:12.2904,lwr_k=500:12.4909,lwr_k=600:12.6788,lwr_k=700:12.8299,lwr_k=800:12.983,lwr_k=900:13.1032,lwr_k=1000:13.2696'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:11.6238,lwr_k=10:9.913,lwr_k=20:9.3202,lwr_k=30:9.47,lwr_k=40:9.459,lwr_k=50:9.2893,lwr_k=100:9.2057,lwr_k=200:9.1355,lwr_k=300:9.1339,lwr_k=400:9.1588,lwr_k=500:9.188,lwr_k=600:9.1755,lwr_k=700:9.2176,lwr_k=800:9.2444,lwr_k=900:9.2567,lwr_k=1000:9.2949'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:14.6455,lwr_k=10:7.4556,lwr_k=20:8.2389,lwr_k=30:8.5976,lwr_k=40:8.8432,lwr_k=50:9.1853,lwr_k=100:9.8262,lwr_k=200:10.6847,lwr_k=300:11.2419,lwr_k=400:11.5323,lwr_k=500:11.7528,lwr_k=600:11.9648,lwr_k=700:12.1237,lwr_k=800:12.337,lwr_k=900:12.4767,lwr_k=1000:12.6223'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:16.2468,lwr_k=10:13.9108,lwr_k=20:13.0029,lwr_k=30:12.9506,lwr_k=40:12.6915,lwr_k=50:12.7323,lwr_k=100:12.6188,lwr_k=200:12.6444,lwr_k=300:13.0542,lwr_k=400:13.2592,lwr_k=500:13.3085,lwr_k=600:13.2798,lwr_k=700:13.3881,lwr_k=800:13.467,lwr_k=900:13.6351,lwr_k=1000:13.5975'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.9897,lwr_k=10:8.4986,lwr_k=20:9.3262,lwr_k=30:9.5257,lwr_k=40:9.8505,lwr_k=50:10.0038,lwr_k=100:10.2557,lwr_k=200:10.5271,lwr_k=300:10.7173,lwr_k=400:10.8551,lwr_k=500:11.0019,lwr_k=600:11.1124,lwr_k=700:11.1851,lwr_k=800:11.2744,lwr_k=900:11.3663,lwr_k=1000:11.4466'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:40.7344,lwr_k=10:21.3386,lwr_k=20:22.452,lwr_k=30:24.011,lwr_k=40:24.8785,lwr_k=50:26.9428,lwr_k=100:28.7048,lwr_k=200:31.3694,lwr_k=300:32.5168,lwr_k=400:33.4131,lwr_k=500:34.2156,lwr_k=600:34.8674,lwr_k=700:35.3825,lwr_k=800:35.8612,lwr_k=900:36.2891,lwr_k=1000:36.6675'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:13.74,lwr_k=10:7.1453,lwr_k=20:7.9209,lwr_k=30:8.306,lwr_k=40:8.5411,lwr_k=50:8.6818,lwr_k=100:9.2056,lwr_k=200:9.8624,lwr_k=300:10.3438,lwr_k=400:10.5703,lwr_k=500:10.7742,lwr_k=600:11.0032,lwr_k=700:11.1214,lwr_k=800:11.2665,lwr_k=900:11.375,lwr_k=1000:11.452'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:14.0676,lwr_k=10:14.6111,lwr_k=20:12.4108,lwr_k=30:12.3119,lwr_k=40:11.9184,lwr_k=50:12.3381,lwr_k=100:11.4494,lwr_k=200:11.3155,lwr_k=300:11.3677,lwr_k=400:11.5604,lwr_k=500:11.6335,lwr_k=600:11.6238,lwr_k=700:11.6631,lwr_k=800:11.7149,lwr_k=900:11.7345,lwr_k=1000:11.824'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_28'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.9487,lwr_k=10:0.0,lwr_k=20:0.7562,lwr_k=30:2.08,lwr_k=40:2.805,lwr_k=50:3.2639,lwr_k=100:4.1733,lwr_k=200:4.725,lwr_k=300:4.9554,lwr_k=400:5.0747,lwr_k=500:5.1665,lwr_k=600:5.2336,lwr_k=700:5.301,lwr_k=800:5.3385,lwr_k=900:5.3579,lwr_k=1000:5.3817'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.2364,lwr_k=10:57.298,lwr_k=20:112338.4827,lwr_k=30:20.4047,lwr_k=40:11.4352,lwr_k=50:10.1903,lwr_k=100:8.5513,lwr_k=200:8.4727,lwr_k=300:8.4597,lwr_k=400:8.3898,lwr_k=500:8.3225,lwr_k=600:8.3649,lwr_k=700:8.4146,lwr_k=800:8.3618,lwr_k=900:8.4141,lwr_k=1000:8.4251'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.5325,lwr_k=10:0.0511,lwr_k=20:0.9647,lwr_k=30:2.4491,lwr_k=40:3.2756,lwr_k=50:3.877,lwr_k=100:5.11,lwr_k=200:5.8973,lwr_k=300:6.2346,lwr_k=400:6.4772,lwr_k=500:6.6809,lwr_k=600:6.7735,lwr_k=700:6.8276,lwr_k=800:6.8785,lwr_k=900:6.9178,lwr_k=1000:6.9408'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.4036,lwr_k=10:11583670.5304,lwr_k=20:62796367.3119,lwr_k=30:2079768.8825,lwr_k=40:6768091.2853,lwr_k=50:78932.4237,lwr_k=100:9.7165,lwr_k=200:8.6191,lwr_k=300:8.5882,lwr_k=400:8.5324,lwr_k=500:8.6826,lwr_k=600:8.7955,lwr_k=700:8.8676,lwr_k=800:8.9061,lwr_k=900:8.8908,lwr_k=1000:8.9026'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.7359,lwr_k=10:0.0,lwr_k=20:1.1257,lwr_k=30:3.0265,lwr_k=40:4.0287,lwr_k=50:4.7321,lwr_k=100:5.9927,lwr_k=200:6.9148,lwr_k=300:7.2718,lwr_k=400:7.4468,lwr_k=500:7.6146,lwr_k=600:7.7044,lwr_k=700:7.7556,lwr_k=800:7.8026,lwr_k=900:7.8911,lwr_k=1000:7.9218'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.9607,lwr_k=10:103.9048,lwr_k=20:1322.5209,lwr_k=30:24.2631,lwr_k=40:16.4233,lwr_k=50:28.513,lwr_k=100:7.4784,lwr_k=200:6.8422,lwr_k=300:6.4913,lwr_k=400:6.4152,lwr_k=500:6.4435,lwr_k=600:6.4362,lwr_k=700:6.4479,lwr_k=800:6.3717,lwr_k=900:6.373,lwr_k=1000:6.3801'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:18.1602,lwr_k=10:0.0,lwr_k=20:2.0995,lwr_k=30:4.0871,lwr_k=40:5.2932,lwr_k=50:6.0098,lwr_k=100:8.2245,lwr_k=200:9.8593,lwr_k=300:10.6262,lwr_k=400:11.2775,lwr_k=500:11.7495,lwr_k=600:11.9903,lwr_k=700:12.1699,lwr_k=800:12.4448,lwr_k=900:12.6452,lwr_k=1000:12.8645'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:17.7967,lwr_k=10:168.9571,lwr_k=20:689.6468,lwr_k=30:18.2206,lwr_k=40:15.8297,lwr_k=50:16.2985,lwr_k=100:11.9041,lwr_k=200:11.7395,lwr_k=300:11.7377,lwr_k=400:11.8057,lwr_k=500:12.0979,lwr_k=600:12.3599,lwr_k=700:12.4008,lwr_k=800:12.4918,lwr_k=900:12.5454,lwr_k=1000:12.6671'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.2215,lwr_k=10:0.0,lwr_k=20:1.547,lwr_k=30:3.2291,lwr_k=40:4.2263,lwr_k=50:4.766,lwr_k=100:5.9006,lwr_k=200:6.6862,lwr_k=300:7.102,lwr_k=400:7.2451,lwr_k=500:7.3804,lwr_k=600:7.5564,lwr_k=700:7.6539,lwr_k=800:7.7732,lwr_k=900:7.8602,lwr_k=1000:7.94'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:32.9199,lwr_k=10:106.3229,lwr_k=20:726.9693,lwr_k=30:37.0892,lwr_k=40:32.8423,lwr_k=50:25.0177,lwr_k=100:22.3628,lwr_k=200:22.5394,lwr_k=300:23.3976,lwr_k=400:23.9264,lwr_k=500:24.5594,lwr_k=600:25.2424,lwr_k=700:25.7046,lwr_k=800:26.2079,lwr_k=900:26.5581,lwr_k=1000:26.8513'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:10.931,lwr_k=10:0.0,lwr_k=20:1.6148,lwr_k=30:3.3381,lwr_k=40:4.3678,lwr_k=50:4.8978,lwr_k=100:6.5669,lwr_k=200:7.6205,lwr_k=300:8.0809,lwr_k=400:8.3262,lwr_k=500:8.5187,lwr_k=600:8.6777,lwr_k=700:8.7842,lwr_k=800:8.8933,lwr_k=900:8.9231,lwr_k=1000:9.0005'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:13.1443,lwr_k=10:378.8939,lwr_k=20:447.5286,lwr_k=30:28.324,lwr_k=40:13.3738,lwr_k=50:13.0571,lwr_k=100:11.3042,lwr_k=200:11.204,lwr_k=300:11.2981,lwr_k=400:11.3906,lwr_k=500:11.4364,lwr_k=600:11.4838,lwr_k=700:11.4893,lwr_k=800:11.4869,lwr_k=900:11.5176,lwr_k=1000:11.5894'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_29'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.0788,lwr_k=10:0.5969,lwr_k=20:3.1252,lwr_k=30:4.1063,lwr_k=40:4.6773,lwr_k=50:4.9486,lwr_k=100:5.5342,lwr_k=200:5.8071,lwr_k=300:5.9909,lwr_k=400:6.1272,lwr_k=500:6.2501,lwr_k=600:6.3342,lwr_k=700:6.3785,lwr_k=800:6.4197,lwr_k=900:6.4628,lwr_k=1000:6.5125'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.3851,lwr_k=10:374.6666,lwr_k=20:10.5058,lwr_k=30:8.2322,lwr_k=40:7.7955,lwr_k=50:7.2109,lwr_k=100:6.8336,lwr_k=200:6.9001,lwr_k=300:6.8525,lwr_k=400:6.9711,lwr_k=500:6.9655,lwr_k=600:7.027,lwr_k=700:7.0379,lwr_k=800:7.0365,lwr_k=900:7.0497,lwr_k=1000:7.0685'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.7356,lwr_k=10:0.7066,lwr_k=20:3.869,lwr_k=30:4.8268,lwr_k=40:5.3609,lwr_k=50:5.6978,lwr_k=100:6.5094,lwr_k=200:7.0597,lwr_k=300:7.3178,lwr_k=400:7.4587,lwr_k=500:7.5743,lwr_k=600:7.6555,lwr_k=700:7.7134,lwr_k=800:7.7725,lwr_k=900:7.8261,lwr_k=1000:7.8357'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.9612,lwr_k=10:467.6281,lwr_k=20:10.0274,lwr_k=30:8.7282,lwr_k=40:8.5648,lwr_k=50:8.3615,lwr_k=100:8.0278,lwr_k=200:7.7883,lwr_k=300:7.6464,lwr_k=400:7.6114,lwr_k=500:7.6161,lwr_k=600:7.6751,lwr_k=700:7.7226,lwr_k=800:7.7317,lwr_k=900:7.7731,lwr_k=1000:7.7923'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.4181,lwr_k=10:0.6994,lwr_k=20:3.9934,lwr_k=30:4.9968,lwr_k=40:5.6899,lwr_k=50:6.1043,lwr_k=100:7.0277,lwr_k=200:7.5566,lwr_k=300:7.9217,lwr_k=400:8.0618,lwr_k=500:8.1578,lwr_k=600:8.2676,lwr_k=700:8.347,lwr_k=800:8.4218,lwr_k=900:8.5084,lwr_k=1000:8.5829'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.5006,lwr_k=10:248.5498,lwr_k=20:12.0449,lwr_k=30:8.3577,lwr_k=40:7.6557,lwr_k=50:7.611,lwr_k=100:6.8064,lwr_k=200:6.5029,lwr_k=300:6.4196,lwr_k=400:6.5812,lwr_k=500:6.5923,lwr_k=600:6.6536,lwr_k=700:6.7079,lwr_k=800:6.757,lwr_k=900:6.8479,lwr_k=1000:6.9219'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.3818,lwr_k=10:0.7825,lwr_k=20:3.7471,lwr_k=30:4.6664,lwr_k=40:5.1837,lwr_k=50:5.5236,lwr_k=100:6.5914,lwr_k=200:7.5746,lwr_k=300:7.992,lwr_k=400:8.1973,lwr_k=500:8.3787,lwr_k=600:8.5264,lwr_k=700:8.6573,lwr_k=800:8.7904,lwr_k=900:8.9027,lwr_k=1000:9.068'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:11.6628,lwr_k=10:109.9879,lwr_k=20:13.534,lwr_k=30:11.4034,lwr_k=40:10.5834,lwr_k=50:10.3102,lwr_k=100:10.0815,lwr_k=200:9.9916,lwr_k=300:10.0293,lwr_k=400:10.0057,lwr_k=500:10.1132,lwr_k=600:10.258,lwr_k=700:10.3556,lwr_k=800:10.4364,lwr_k=900:10.515,lwr_k=1000:10.6262'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.6597,lwr_k=10:0.6246,lwr_k=20:3.0256,lwr_k=30:3.9929,lwr_k=40:4.4089,lwr_k=50:4.6715,lwr_k=100:5.6252,lwr_k=200:6.1513,lwr_k=300:6.4238,lwr_k=400:6.5827,lwr_k=500:6.6633,lwr_k=600:6.7432,lwr_k=700:6.8251,lwr_k=800:6.8969,lwr_k=900:6.9565,lwr_k=1000:7.0001'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:26.6699,lwr_k=10:129.1117,lwr_k=20:24.7044,lwr_k=30:17.2054,lwr_k=40:17.1897,lwr_k=50:17.081,lwr_k=100:21.2033,lwr_k=200:23.2222,lwr_k=300:24.2735,lwr_k=400:24.805,lwr_k=500:25.0176,lwr_k=600:25.1459,lwr_k=700:25.1887,lwr_k=800:25.2312,lwr_k=900:25.2526,lwr_k=1000:25.2749'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:8.8548,lwr_k=10:0.721,lwr_k=20:3.5659,lwr_k=30:4.4396,lwr_k=40:4.9853,lwr_k=50:5.2871,lwr_k=100:6.2942,lwr_k=200:6.9453,lwr_k=300:7.1531,lwr_k=400:7.3888,lwr_k=500:7.491,lwr_k=600:7.5721,lwr_k=700:7.682,lwr_k=800:7.7585,lwr_k=900:7.8246,lwr_k=1000:7.8875'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.2992,lwr_k=10:141.2709,lwr_k=20:12.9242,lwr_k=30:10.4978,lwr_k=40:9.9547,lwr_k=50:9.4168,lwr_k=100:8.5187,lwr_k=200:9.1299,lwr_k=300:9.1932,lwr_k=400:9.2494,lwr_k=500:9.3313,lwr_k=600:9.397,lwr_k=700:9.4225,lwr_k=800:9.4338,lwr_k=900:9.4823,lwr_k=1000:9.5367'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_30'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:198.6529,lwr_k=10:204.6077,lwr_k=20:198.9498,lwr_k=30:198.6535,lwr_k=40:198.6569,lwr_k=50:198.7108,lwr_k=100:199.279,lwr_k=200:199.0935,lwr_k=300:199.0971,lwr_k=400:198.9837,lwr_k=500:198.9108,lwr_k=600:198.7665,lwr_k=700:198.6869,lwr_k=800:198.6691,lwr_k=900:198.6586,lwr_k=1000:198.7115'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:258.1252,lwr_k=10:269.7306,lwr_k=20:259.6836,lwr_k=30:258.0686,lwr_k=40:258.2741,lwr_k=50:258.7401,lwr_k=100:260.5833,lwr_k=200:260.1028,lwr_k=300:260.1125,lwr_k=400:259.7876,lwr_k=500:259.5589,lwr_k=600:259.0192,lwr_k=700:258.5856,lwr_k=800:258.4357,lwr_k=900:258.3056,lwr_k=1000:258.7442'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:239.6367,lwr_k=10:247.3583,lwr_k=20:240.417,lwr_k=30:239.7352,lwr_k=40:239.7977,lwr_k=50:239.9721,lwr_k=100:240.913,lwr_k=200:240.6413,lwr_k=300:240.6466,lwr_k=400:240.4714,lwr_k=500:240.353,lwr_k=600:240.0931,lwr_k=700:239.91,lwr_k=800:239.8536,lwr_k=900:239.8081,lwr_k=1000:239.9738'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:202.822,lwr_k=10:208.8718,lwr_k=20:203.0709,lwr_k=30:202.7317,lwr_k=40:202.7416,lwr_k=50:202.809,lwr_k=100:203.4186,lwr_k=200:203.2236,lwr_k=300:203.2273,lwr_k=400:203.107,lwr_k=500:203.0291,lwr_k=600:202.872,lwr_k=700:202.7808,lwr_k=800:202.7587,lwr_k=900:202.7443,lwr_k=1000:202.8098'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:273.725,lwr_k=10:286.0596,lwr_k=20:279.7698,lwr_k=30:284.7255,lwr_k=40:280.9807,lwr_k=50:276.729,lwr_k=100:279.4766,lwr_k=200:277.9348,lwr_k=300:274.6314,lwr_k=400:274.8378,lwr_k=500:274.0095,lwr_k=600:274.1698,lwr_k=700:274.138,lwr_k=800:273.9476,lwr_k=900:273.8063,lwr_k=1000:273.8585'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:155.1861,lwr_k=10:158.9139,lwr_k=20:155.2057,lwr_k=30:158.0586,lwr_k=40:155.8406,lwr_k=50:153.9426,lwr_k=100:155.0605,lwr_k=200:154.3677,lwr_k=300:153.7594,lwr_k=400:153.7137,lwr_k=500:154.1635,lwr_k=600:153.9965,lwr_k=700:154.0242,lwr_k=800:154.2526,lwr_k=900:154.5687,lwr_k=1000:154.4242'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:261.7626,lwr_k=10:266.5313,lwr_k=20:270.2479,lwr_k=30:262.3543,lwr_k=40:262.3381,lwr_k=50:262.026,lwr_k=100:262.8406,lwr_k=200:262.9699,lwr_k=300:261.776,lwr_k=400:262.0761,lwr_k=500:261.9114,lwr_k=600:262.041,lwr_k=700:262.0314,lwr_k=800:261.8857,lwr_k=900:261.7905,lwr_k=1000:261.7883'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:239.3147,lwr_k=10:242.2668,lwr_k=20:245.3766,lwr_k=30:239.2665,lwr_k=40:239.2591,lwr_k=50:239.1512,lwr_k=100:239.5289,lwr_k=200:239.6079,lwr_k=300:239.4243,lwr_k=400:239.1624,lwr_k=500:239.1426,lwr_k=600:239.1542,lwr_k=700:239.1522,lwr_k=800:239.1459,lwr_k=900:239.2038,lwr_k=1000:239.2071'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:204.8268,lwr_k=10:211.2806,lwr_k=20:205.4066,lwr_k=30:206.4864,lwr_k=40:209.5979,lwr_k=50:206.8734,lwr_k=100:205.0478,lwr_k=200:205.3937,lwr_k=300:204.9822,lwr_k=400:205.3989,lwr_k=500:205.0975,lwr_k=600:204.9847,lwr_k=700:204.8698,lwr_k=800:204.8308,lwr_k=900:204.8269,lwr_k=1000:204.88'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:325.691,lwr_k=10:328.1843,lwr_k=20:327.4579,lwr_k=30:325.3422,lwr_k=40:327.0568,lwr_k=50:325.5073,lwr_k=100:326.6448,lwr_k=200:327.4316,lwr_k=300:326.461,lwr_k=400:327.4422,lwr_k=500:326.7727,lwr_k=600:326.4684,lwr_k=700:326.0573,lwr_k=800:325.7929,lwr_k=900:325.6835,lwr_k=1000:326.1039'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:235.0387,lwr_k=10:267.4646,lwr_k=20:236.1389,lwr_k=30:235.2656,lwr_k=40:235.1973,lwr_k=50:235.872,lwr_k=100:238.564,lwr_k=200:235.4554,lwr_k=300:235.2143,lwr_k=400:235.0388,lwr_k=500:235.057,lwr_k=600:235.117,lwr_k=700:235.3828,lwr_k=800:235.1301,lwr_k=900:235.081,lwr_k=1000:235.0417'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:237.5599,lwr_k=10:271.5158,lwr_k=20:238.3782,lwr_k=30:237.6587,lwr_k=40:237.6114,lwr_k=50:238.1478,lwr_k=100:240.5807,lwr_k=200:237.8031,lwr_k=300:237.6228,lwr_k=400:237.5576,lwr_k=500:237.5418,lwr_k=600:237.563,lwr_k=700:237.7463,lwr_k=800:237.57,lwr_k=900:237.5469,lwr_k=1000:237.5773'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_31'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.5275,lwr_k=10:0.0,lwr_k=20:0.2731,lwr_k=30:1.6341,lwr_k=40:2.2794,lwr_k=50:2.7515,lwr_k=100:3.7644,lwr_k=200:4.3348,lwr_k=300:4.5319,lwr_k=400:4.6265,lwr_k=500:4.7172,lwr_k=600:4.7901,lwr_k=700:4.8585,lwr_k=800:4.9019,lwr_k=900:4.9686,lwr_k=1000:5.0126'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.4939,lwr_k=10:55.4033,lwr_k=20:624.1541,lwr_k=30:19.5127,lwr_k=40:11.4328,lwr_k=50:11.6729,lwr_k=100:8.6378,lwr_k=200:8.0043,lwr_k=300:7.8094,lwr_k=400:7.9445,lwr_k=500:7.9676,lwr_k=600:8.047,lwr_k=700:8.1038,lwr_k=800:8.1569,lwr_k=900:8.1827,lwr_k=1000:8.1786'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.4816,lwr_k=10:0.0,lwr_k=20:0.1604,lwr_k=30:1.4681,lwr_k=40:2.1844,lwr_k=50:2.5116,lwr_k=100:3.3084,lwr_k=200:3.7114,lwr_k=300:3.9055,lwr_k=400:4.0201,lwr_k=500:4.0567,lwr_k=600:4.1072,lwr_k=700:4.1467,lwr_k=800:4.1958,lwr_k=900:4.215,lwr_k=1000:4.2489'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:6.8113,lwr_k=10:39.4495,lwr_k=20:276.3259,lwr_k=30:14.1752,lwr_k=40:9.7502,lwr_k=50:8.3583,lwr_k=100:6.9983,lwr_k=200:6.8101,lwr_k=300:6.6741,lwr_k=400:6.5377,lwr_k=500:6.5476,lwr_k=600:6.5619,lwr_k=700:6.5465,lwr_k=800:6.54,lwr_k=900:6.5381,lwr_k=1000:6.5306'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.9568,lwr_k=10:0.0,lwr_k=20:0.2656,lwr_k=30:2.2388,lwr_k=40:3.1171,lwr_k=50:3.6733,lwr_k=100:4.9727,lwr_k=200:5.676,lwr_k=300:5.9274,lwr_k=400:6.0605,lwr_k=500:6.1535,lwr_k=600:6.2318,lwr_k=700:6.2844,lwr_k=800:6.3375,lwr_k=900:6.4184,lwr_k=1000:6.4577'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.9256,lwr_k=10:33.1831,lwr_k=20:180.2941,lwr_k=30:18.7836,lwr_k=40:10.5109,lwr_k=50:9.2179,lwr_k=100:7.4949,lwr_k=200:6.8844,lwr_k=300:6.8238,lwr_k=400:6.8309,lwr_k=500:6.8001,lwr_k=600:6.8613,lwr_k=700:7.0482,lwr_k=800:7.0643,lwr_k=900:7.0891,lwr_k=1000:7.102'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:22.4523,lwr_k=10:0.0,lwr_k=20:0.474,lwr_k=30:3.6291,lwr_k=40:5.3028,lwr_k=50:6.5332,lwr_k=100:9.3643,lwr_k=200:11.7698,lwr_k=300:12.8777,lwr_k=400:14.0726,lwr_k=500:14.7673,lwr_k=600:15.3001,lwr_k=700:15.8862,lwr_k=800:16.2559,lwr_k=900:16.6415,lwr_k=1000:16.9392'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:26.9144,lwr_k=10:93.2952,lwr_k=20:548.8888,lwr_k=30:33.6234,lwr_k=40:23.0086,lwr_k=50:19.5194,lwr_k=100:17.4944,lwr_k=200:17.861,lwr_k=300:18.7647,lwr_k=400:19.5361,lwr_k=500:20.1193,lwr_k=600:20.6507,lwr_k=700:21.2153,lwr_k=800:21.6186,lwr_k=900:21.792,lwr_k=1000:22.1287'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:3.764,lwr_k=10:0.0,lwr_k=20:0.191,lwr_k=30:1.2993,lwr_k=40:1.8974,lwr_k=50:2.2236,lwr_k=100:2.8152,lwr_k=200:3.1457,lwr_k=300:3.2971,lwr_k=400:3.3863,lwr_k=500:3.4376,lwr_k=600:3.4696,lwr_k=700:3.4941,lwr_k=800:3.513,lwr_k=900:3.5245,lwr_k=1000:3.5403'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:18.2612,lwr_k=10:37.9921,lwr_k=20:753.9417,lwr_k=30:21.2849,lwr_k=40:18.8666,lwr_k=50:16.3873,lwr_k=100:16.6774,lwr_k=200:16.9642,lwr_k=300:16.9095,lwr_k=400:17.0457,lwr_k=500:17.0014,lwr_k=600:16.9833,lwr_k=700:16.9826,lwr_k=800:16.9535,lwr_k=900:16.9444,lwr_k=1000:17.0062'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:10.3049,lwr_k=10:0.0,lwr_k=20:0.3092,lwr_k=30:2.2248,lwr_k=40:3.2705,lwr_k=50:4.0059,lwr_k=100:5.4751,lwr_k=200:6.4735,lwr_k=300:6.8752,lwr_k=400:7.1813,lwr_k=500:7.378,lwr_k=600:7.5366,lwr_k=700:7.6878,lwr_k=800:7.8282,lwr_k=900:7.9299,lwr_k=1000:8.0287'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:14.0455,lwr_k=10:50.5327,lwr_k=20:555.8686,lwr_k=30:27.6054,lwr_k=40:14.5644,lwr_k=50:13.4533,lwr_k=100:11.1221,lwr_k=200:10.6614,lwr_k=300:10.4763,lwr_k=400:10.5364,lwr_k=500:10.7201,lwr_k=600:10.8842,lwr_k=700:11.0337,lwr_k=800:11.1344,lwr_k=900:11.3861,lwr_k=1000:11.597'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_32'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:3.4811,lwr_k=10:3.2249,lwr_k=20:3.3518,lwr_k=30:3.3755,lwr_k=40:3.3963,lwr_k=50:3.3974,lwr_k=100:3.4389,lwr_k=200:5.0891,lwr_k=300:5.1089,lwr_k=400:3.469,lwr_k=500:3.4795,lwr_k=600:3.4764,lwr_k=700:3.4774,lwr_k=800:3.4793,lwr_k=900:3.4819,lwr_k=1000:3.4817'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.7751,lwr_k=10:8.9532,lwr_k=20:8.3928,lwr_k=30:8.0993,lwr_k=40:8.0049,lwr_k=50:7.9809,lwr_k=100:7.8195,lwr_k=200:9.311,lwr_k=300:9.3443,lwr_k=400:7.7932,lwr_k=500:7.803,lwr_k=600:7.7982,lwr_k=700:7.7987,lwr_k=800:7.7901,lwr_k=900:7.7952,lwr_k=1000:7.7772'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:239.6367,lwr_k=10:241.3914,lwr_k=20:240.9123,lwr_k=30:239.7128,lwr_k=40:240.4918,lwr_k=50:240.0855,lwr_k=100:239.8793,lwr_k=200:239.6506,lwr_k=300:239.9838,lwr_k=400:239.6817,lwr_k=500:239.6714,lwr_k=600:239.6373,lwr_k=700:239.6579,lwr_k=800:239.6652,lwr_k=900:239.658,lwr_k=1000:239.6594'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:202.822,lwr_k=10:203.7798,lwr_k=20:203.4181,lwr_k=30:203.064,lwr_k=40:203.1208,lwr_k=50:202.8677,lwr_k=100:202.7683,lwr_k=200:202.7651,lwr_k=300:203.5235,lwr_k=400:202.9945,lwr_k=500:202.9686,lwr_k=600:202.8361,lwr_k=700:202.9308,lwr_k=800:202.952,lwr_k=900:202.9311,lwr_k=1000:202.9354'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:273.725,lwr_k=10:277.091,lwr_k=20:276.4127,lwr_k=30:273.7799,lwr_k=40:275.7834,lwr_k=50:275.1171,lwr_k=100:274.7301,lwr_k=200:274.1188,lwr_k=300:273.7313,lwr_k=400:273.8139,lwr_k=500:273.83,lwr_k=600:273.9627,lwr_k=700:273.8578,lwr_k=800:273.8415,lwr_k=900:273.8576,lwr_k=1000:273.8542'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:155.1861,lwr_k=10:154.056,lwr_k=20:153.8561,lwr_k=30:154.667,lwr_k=40:153.7285,lwr_k=50:153.6867,lwr_k=100:153.7343,lwr_k=200:154.042,lwr_k=300:155.3861,lwr_k=400:154.5445,lwr_k=500:154.4971,lwr_k=600:154.2291,lwr_k=700:154.4258,lwr_k=800:154.4663,lwr_k=900:154.4263,lwr_k=1000:154.4345'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:24.6977,lwr_k=10:14.4412,lwr_k=20:14.8916,lwr_k=30:15.3348,lwr_k=40:15.7295,lwr_k=50:15.9995,lwr_k=100:17.1051,lwr_k=200:18.4684,lwr_k=300:35.1262,lwr_k=400:19.353,lwr_k=500:19.604,lwr_k=600:19.9487,lwr_k=700:20.2178,lwr_k=800:20.3909,lwr_k=900:20.5219,lwr_k=1000:20.7143'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:22.7144,lwr_k=10:21.9733,lwr_k=20:19.8523,lwr_k=30:18.866,lwr_k=40:18.1949,lwr_k=50:18.0833,lwr_k=100:17.6738,lwr_k=200:17.7069,lwr_k=300:33.9441,lwr_k=400:18.2156,lwr_k=500:18.4026,lwr_k=600:18.6151,lwr_k=700:18.7848,lwr_k=800:18.7656,lwr_k=900:18.9425,lwr_k=1000:19.1461'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:204.8268,lwr_k=10:209.4469,lwr_k=20:205.7566,lwr_k=30:220.4386,lwr_k=40:211.6878,lwr_k=50:210.0386,lwr_k=100:207.0361,lwr_k=200:206.8144,lwr_k=300:206.1592,lwr_k=400:206.0456,lwr_k=500:205.6606,lwr_k=600:205.6755,lwr_k=700:205.5579,lwr_k=800:205.6872,lwr_k=900:205.6737,lwr_k=1000:206.1653'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:325.691,lwr_k=10:333.6621,lwr_k=20:328.1241,lwr_k=30:335.143,lwr_k=40:328.4684,lwr_k=50:327.3437,lwr_k=100:325.5831,lwr_k=200:325.4807,lwr_k=300:325.2239,lwr_k=400:325.1887,lwr_k=500:325.1013,lwr_k=600:325.1035,lwr_k=700:325.0891,lwr_k=800:325.1053,lwr_k=900:325.1032,lwr_k=1000:325.2259'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:235.0387,lwr_k=10:237.1545,lwr_k=20:236.6246,lwr_k=30:235.06,lwr_k=40:236.151,lwr_k=50:235.6784,lwr_k=100:235.4262,lwr_k=200:235.1,lwr_k=300:235.2496,lwr_k=400:235.0455,lwr_k=500:235.0419,lwr_k=600:235.0503,lwr_k=700:235.039,lwr_k=800:235.0402,lwr_k=900:235.039,lwr_k=1000:235.0392'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:237.5599,lwr_k=10:240.0665,lwr_k=20:239.4842,lwr_k=30:237.5419,lwr_k=40:238.9555,lwr_k=50:238.4145,lwr_k=100:238.1146,lwr_k=200:237.6876,lwr_k=300:237.6473,lwr_k=400:237.5446,lwr_k=500:237.5479,lwr_k=600:237.6003,lwr_k=700:237.5559,lwr_k=800:237.5509,lwr_k=900:237.5558,lwr_k=1000:237.5547'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_33'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.216,lwr_k=10:2.9222,lwr_k=20:3.9167,lwr_k=30:4.1909,lwr_k=40:4.364,lwr_k=50:4.5171,lwr_k=100:4.6604,lwr_k=200:4.8427,lwr_k=300:4.9057,lwr_k=400:4.9668,lwr_k=500:4.977,lwr_k=600:5.0027,lwr_k=700:5.0323,lwr_k=800:5.0492,lwr_k=900:5.0726,lwr_k=1000:5.0767'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.0055,lwr_k=10:17.0412,lwr_k=20:10.8777,lwr_k=30:9.3128,lwr_k=40:8.7151,lwr_k=50:8.652,lwr_k=100:8.0988,lwr_k=200:8.3564,lwr_k=300:8.8998,lwr_k=400:8.6584,lwr_k=500:8.5829,lwr_k=600:8.5657,lwr_k=700:8.5637,lwr_k=800:8.5869,lwr_k=900:8.574,lwr_k=1000:8.5724'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.7163,lwr_k=10:3.5171,lwr_k=20:5.3233,lwr_k=30:5.7802,lwr_k=40:6.0749,lwr_k=50:6.251,lwr_k=100:6.6682,lwr_k=200:6.8729,lwr_k=300:6.9907,lwr_k=400:7.0855,lwr_k=500:7.1255,lwr_k=600:7.1803,lwr_k=700:7.2048,lwr_k=800:7.2339,lwr_k=900:7.282,lwr_k=1000:7.3382'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.6545,lwr_k=10:27.9308,lwr_k=20:11.458,lwr_k=30:28.6402,lwr_k=40:26.0861,lwr_k=50:27.5302,lwr_k=100:17.1604,lwr_k=200:8.8644,lwr_k=300:8.9887,lwr_k=400:8.9188,lwr_k=500:8.8723,lwr_k=600:8.8916,lwr_k=700:8.9958,lwr_k=800:9.0129,lwr_k=900:9.0189,lwr_k=1000:9.1109'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.8547,lwr_k=10:5.4861,lwr_k=20:5.97,lwr_k=30:6.1264,lwr_k=40:6.2103,lwr_k=50:6.266,lwr_k=100:6.3044,lwr_k=200:6.3379,lwr_k=300:6.3598,lwr_k=400:6.3963,lwr_k=500:6.408,lwr_k=600:6.4235,lwr_k=700:6.4587,lwr_k=800:6.5278,lwr_k=900:6.5581,lwr_k=1000:6.5893'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.6407,lwr_k=10:6.4025,lwr_k=20:7.9858,lwr_k=30:7.8572,lwr_k=40:7.8537,lwr_k=50:6.3866,lwr_k=100:5.9829,lwr_k=200:6.1221,lwr_k=300:6.1318,lwr_k=400:6.1653,lwr_k=500:6.194,lwr_k=600:6.2373,lwr_k=700:6.277,lwr_k=800:6.3376,lwr_k=900:6.3972,lwr_k=1000:6.4598'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.3362,lwr_k=10:4.7574,lwr_k=20:6.1146,lwr_k=30:6.6194,lwr_k=40:6.9473,lwr_k=50:7.0303,lwr_k=100:7.4558,lwr_k=200:7.539,lwr_k=300:7.687,lwr_k=400:7.7823,lwr_k=500:7.8793,lwr_k=600:7.958,lwr_k=700:8.0002,lwr_k=800:8.0365,lwr_k=900:8.0824,lwr_k=1000:8.1248'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:12.2982,lwr_k=10:21.1223,lwr_k=20:17.0925,lwr_k=30:13.9969,lwr_k=40:13.3451,lwr_k=50:12.8054,lwr_k=100:12.677,lwr_k=200:12.4791,lwr_k=300:12.2658,lwr_k=400:12.2515,lwr_k=500:12.2509,lwr_k=600:12.2323,lwr_k=700:12.2673,lwr_k=800:12.2738,lwr_k=900:12.3102,lwr_k=1000:12.3272'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.9105,lwr_k=10:0.914,lwr_k=20:2.9447,lwr_k=30:3.8221,lwr_k=40:4.1885,lwr_k=50:4.5385,lwr_k=100:4.968,lwr_k=200:5.3687,lwr_k=300:5.4984,lwr_k=400:5.5674,lwr_k=500:5.6649,lwr_k=600:5.7059,lwr_k=700:5.7254,lwr_k=800:5.7445,lwr_k=900:5.7549,lwr_k=1000:5.7649'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:16.7282,lwr_k=10:10858.2602,lwr_k=20:34.4533,lwr_k=30:26.8695,lwr_k=40:18.643,lwr_k=50:17.6534,lwr_k=100:17.1352,lwr_k=200:16.4197,lwr_k=300:16.165,lwr_k=400:16.1456,lwr_k=500:16.2134,lwr_k=600:16.2547,lwr_k=700:16.251,lwr_k=800:16.3187,lwr_k=900:16.2293,lwr_k=1000:16.2437'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:7.2708,lwr_k=10:1.7299,lwr_k=20:3.8063,lwr_k=30:4.5566,lwr_k=40:5.0334,lwr_k=50:5.2916,lwr_k=100:5.9494,lwr_k=200:6.2449,lwr_k=300:6.3857,lwr_k=400:6.497,lwr_k=500:6.5532,lwr_k=600:6.5843,lwr_k=700:6.5891,lwr_k=800:6.6166,lwr_k=900:6.6545,lwr_k=1000:6.6848'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:18.7249,lwr_k=10:126.7178,lwr_k=20:4125.4297,lwr_k=30:26.377,lwr_k=40:22.429,lwr_k=50:21.6873,lwr_k=100:17.7111,lwr_k=200:15.443,lwr_k=300:14.8578,lwr_k=400:14.4012,lwr_k=500:14.5176,lwr_k=600:15.7581,lwr_k=700:16.9544,lwr_k=800:17.781,lwr_k=900:18.4873,lwr_k=1000:18.2784'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_34'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.4261,lwr_k=10:2.2158,lwr_k=20:3.5479,lwr_k=30:3.9001,lwr_k=40:4.2511,lwr_k=50:4.4335,lwr_k=100:4.7315,lwr_k=200:4.8717,lwr_k=300:5.0177,lwr_k=400:5.0851,lwr_k=500:5.1424,lwr_k=600:5.155,lwr_k=700:5.1772,lwr_k=800:5.2065,lwr_k=900:5.2368,lwr_k=1000:5.2576'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.6159,lwr_k=10:41.1782,lwr_k=20:25.3731,lwr_k=30:11.5935,lwr_k=40:9.5467,lwr_k=50:8.919,lwr_k=100:8.9506,lwr_k=200:8.4422,lwr_k=300:8.4481,lwr_k=400:8.3142,lwr_k=500:8.3357,lwr_k=600:8.4233,lwr_k=700:8.365,lwr_k=800:8.3733,lwr_k=900:8.3815,lwr_k=1000:8.4246'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.4108,lwr_k=10:3.2846,lwr_k=20:4.394,lwr_k=30:4.7282,lwr_k=40:4.8361,lwr_k=50:4.9635,lwr_k=100:5.2788,lwr_k=200:5.5241,lwr_k=300:5.614,lwr_k=400:5.6823,lwr_k=500:5.712,lwr_k=600:5.7804,lwr_k=700:5.8191,lwr_k=800:5.843,lwr_k=900:5.8714,lwr_k=1000:5.9037'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.8845,lwr_k=10:10637.2174,lwr_k=20:10.1396,lwr_k=30:9.5046,lwr_k=40:8.4611,lwr_k=50:15.2091,lwr_k=100:20.3115,lwr_k=200:63.8248,lwr_k=300:7.5912,lwr_k=400:7.5786,lwr_k=500:7.5514,lwr_k=600:7.57,lwr_k=700:7.6073,lwr_k=800:7.6237,lwr_k=900:7.7182,lwr_k=1000:7.7328'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.4391,lwr_k=10:3.6448,lwr_k=20:4.5787,lwr_k=30:4.7954,lwr_k=40:5.0432,lwr_k=50:5.2333,lwr_k=100:5.4855,lwr_k=200:5.6886,lwr_k=300:5.8015,lwr_k=400:5.8713,lwr_k=500:5.9025,lwr_k=600:5.9541,lwr_k=700:5.9846,lwr_k=800:6.0081,lwr_k=900:6.0203,lwr_k=1000:6.0465'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.4443,lwr_k=10:23.5243,lwr_k=20:26.5051,lwr_k=30:8.8053,lwr_k=40:8.1902,lwr_k=50:7.6136,lwr_k=100:6.2248,lwr_k=200:5.9062,lwr_k=300:5.9603,lwr_k=400:6.0272,lwr_k=500:5.7835,lwr_k=600:5.833,lwr_k=700:5.8472,lwr_k=800:5.8382,lwr_k=900:5.8108,lwr_k=1000:5.8103'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.5749,lwr_k=10:2.1108,lwr_k=20:3.5288,lwr_k=30:4.1691,lwr_k=40:4.5517,lwr_k=50:4.728,lwr_k=100:5.2237,lwr_k=200:5.7389,lwr_k=300:5.9808,lwr_k=400:6.0644,lwr_k=500:6.1284,lwr_k=600:6.1684,lwr_k=700:6.2549,lwr_k=800:6.2863,lwr_k=900:6.2948,lwr_k=1000:6.335'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:12.6214,lwr_k=10:1235043959.0245,lwr_k=20:17.9867,lwr_k=30:17.0677,lwr_k=40:17.4463,lwr_k=50:16.1013,lwr_k=100:10.2274,lwr_k=200:10.9603,lwr_k=300:11.3958,lwr_k=400:11.5709,lwr_k=500:11.7986,lwr_k=600:11.9593,lwr_k=700:12.0164,lwr_k=800:12.0856,lwr_k=900:11.9776,lwr_k=1000:12.0147'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.7284,lwr_k=10:0.0465,lwr_k=20:1.8053,lwr_k=30:2.9471,lwr_k=40:3.4648,lwr_k=50:3.7978,lwr_k=100:4.5659,lwr_k=200:4.9793,lwr_k=300:5.1535,lwr_k=400:5.2098,lwr_k=500:5.2354,lwr_k=600:5.2982,lwr_k=700:5.3183,lwr_k=800:5.3425,lwr_k=900:5.3744,lwr_k=1000:5.4119'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:19.1593,lwr_k=10:2411.146,lwr_k=20:19886.6318,lwr_k=30:14575.6103,lwr_k=40:14704.8434,lwr_k=50:19.7774,lwr_k=100:17.8734,lwr_k=200:17.9344,lwr_k=300:18.1171,lwr_k=400:18.2651,lwr_k=500:18.3803,lwr_k=600:18.4763,lwr_k=700:18.5084,lwr_k=800:18.6594,lwr_k=900:18.6716,lwr_k=1000:18.765'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:5.5222,lwr_k=10:4.0758,lwr_k=20:4.6135,lwr_k=30:4.7955,lwr_k=40:4.8436,lwr_k=50:4.924,lwr_k=100:5.1875,lwr_k=200:5.2873,lwr_k=300:5.3241,lwr_k=400:5.3511,lwr_k=500:5.3486,lwr_k=600:5.3845,lwr_k=700:5.3885,lwr_k=800:5.3912,lwr_k=900:5.3906,lwr_k=1000:5.4075'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.9117,lwr_k=10:13.2189,lwr_k=20:8.9071,lwr_k=30:8.2071,lwr_k=40:8.0388,lwr_k=50:8.1144,lwr_k=100:7.7562,lwr_k=200:7.9144,lwr_k=300:7.9629,lwr_k=400:7.9881,lwr_k=500:8.0137,lwr_k=600:7.9773,lwr_k=700:7.9881,lwr_k=800:7.9916,lwr_k=900:7.9903,lwr_k=1000:7.9216'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_35'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.4784,lwr_k=10:3.1007,lwr_k=20:3.7462,lwr_k=30:4.0595,lwr_k=40:4.2953,lwr_k=50:4.4202,lwr_k=100:4.6387,lwr_k=200:4.8795,lwr_k=300:4.9671,lwr_k=400:5.0205,lwr_k=500:5.0075,lwr_k=600:5.034,lwr_k=700:5.0882,lwr_k=800:5.0603,lwr_k=900:5.0935,lwr_k=1000:5.1741'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.1071,lwr_k=10:27.0066,lwr_k=20:9.86,lwr_k=30:9.6646,lwr_k=40:67233.3566,lwr_k=50:8921859429.8441,lwr_k=100:1181424.5564,lwr_k=200:3491813.8844,lwr_k=300:79508122.3587,lwr_k=400:30753.8266,lwr_k=500:8.2275,lwr_k=600:8.2622,lwr_k=700:8.2607,lwr_k=800:829.7955,lwr_k=900:8.5043,lwr_k=1000:8.312'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.3476,lwr_k=10:3.0604,lwr_k=20:4.2658,lwr_k=30:4.6141,lwr_k=40:5.0069,lwr_k=50:4.9424,lwr_k=100:5.3837,lwr_k=200:5.8251,lwr_k=300:5.9839,lwr_k=400:6.1632,lwr_k=500:6.2025,lwr_k=600:6.3388,lwr_k=700:6.3273,lwr_k=800:6.3527,lwr_k=900:6.4635,lwr_k=1000:6.4396'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.2166,lwr_k=10:15.5227,lwr_k=20:8.8734,lwr_k=30:7.8869,lwr_k=40:28754041222.9836,lwr_k=50:4002156162.5698,lwr_k=100:317535453.3631,lwr_k=200:3492.4365,lwr_k=300:7.8404,lwr_k=400:7.5726,lwr_k=500:7.6608,lwr_k=600:7.5678,lwr_k=700:7.8264,lwr_k=800:7.9442,lwr_k=900:7.8442,lwr_k=1000:7.8996'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.739,lwr_k=10:3.9901,lwr_k=20:4.8626,lwr_k=30:5.1877,lwr_k=40:5.361,lwr_k=50:5.7462,lwr_k=100:6.1168,lwr_k=200:6.1403,lwr_k=300:6.2482,lwr_k=400:6.3641,lwr_k=500:6.4917,lwr_k=600:6.4062,lwr_k=700:6.4726,lwr_k=800:6.5038,lwr_k=900:6.5205,lwr_k=1000:6.4938'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.8909,lwr_k=10:20.3665,lwr_k=20:9.2519,lwr_k=30:8.6541,lwr_k=40:22620227.4616,lwr_k=50:54716695.6213,lwr_k=100:29223.9269,lwr_k=200:5343.544,lwr_k=300:4904.2107,lwr_k=400:6.8688,lwr_k=500:6.9478,lwr_k=600:6.7697,lwr_k=700:6.736,lwr_k=800:6.7779,lwr_k=900:6.746,lwr_k=1000:6.6958'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.9705,lwr_k=10:3.5236,lwr_k=20:5.0768,lwr_k=30:5.4222,lwr_k=40:5.5518,lwr_k=50:5.6766,lwr_k=100:6.206,lwr_k=200:6.5621,lwr_k=300:6.5477,lwr_k=400:6.7045,lwr_k=500:6.6527,lwr_k=600:6.7106,lwr_k=700:6.7261,lwr_k=800:6.7495,lwr_k=900:6.7925,lwr_k=1000:6.807'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:13.19,lwr_k=10:23.0342,lwr_k=20:14.4987,lwr_k=30:13.6019,lwr_k=40:1265745.5806,lwr_k=50:12.051,lwr_k=100:10343269.7208,lwr_k=200:290.8454,lwr_k=300:12.7824,lwr_k=400:12.7858,lwr_k=500:13.1059,lwr_k=600:12.9621,lwr_k=700:12.9081,lwr_k=800:12.992,lwr_k=900:12.6618,lwr_k=1000:12.9478'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.8294,lwr_k=10:2.0362,lwr_k=20:3.454,lwr_k=30:4.0762,lwr_k=40:4.4,lwr_k=50:4.5957,lwr_k=100:4.8233,lwr_k=200:5.2276,lwr_k=300:5.2331,lwr_k=400:5.3065,lwr_k=500:5.3313,lwr_k=600:5.3297,lwr_k=700:5.3785,lwr_k=800:5.4044,lwr_k=900:5.4255,lwr_k=1000:5.5322'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:88080.997,lwr_k=10:78.1218,lwr_k=20:20.4274,lwr_k=30:19.2101,lwr_k=40:135019548.5747,lwr_k=50:216240410.0919,lwr_k=100:1582287528.9685,lwr_k=200:9941.7014,lwr_k=300:10245.7356,lwr_k=400:78.0845,lwr_k=500:78.549,lwr_k=600:19056.0153,lwr_k=700:88182.0067,lwr_k=800:157.2119,lwr_k=900:1318.2078,lwr_k=1000:8555.5561'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:6.3309,lwr_k=10:3.7546,lwr_k=20:4.7753,lwr_k=30:5.1283,lwr_k=40:5.3249,lwr_k=50:5.4836,lwr_k=100:5.7097,lwr_k=200:5.8517,lwr_k=300:5.8584,lwr_k=400:5.8979,lwr_k=500:5.9651,lwr_k=600:5.9783,lwr_k=700:5.9771,lwr_k=800:6.0061,lwr_k=900:6.0002,lwr_k=1000:6.0546'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.4823,lwr_k=10:23.4221,lwr_k=20:593.9142,lwr_k=30:583.8375,lwr_k=40:834.8252,lwr_k=50:301.2831,lwr_k=100:71.247,lwr_k=200:8.9675,lwr_k=300:8.6716,lwr_k=400:8.8449,lwr_k=500:8.8744,lwr_k=600:8.9436,lwr_k=700:8.9326,lwr_k=800:8.944,lwr_k=900:8.7231,lwr_k=1000:8.9255'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_36'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:133.8615,lwr_k=10:134.1802,lwr_k=20:143.825,lwr_k=30:142.3417,lwr_k=40:138.8944,lwr_k=50:140.4878,lwr_k=100:137.2689,lwr_k=200:134.4341,lwr_k=300:134.044,lwr_k=400:133.8845,lwr_k=500:133.8557,lwr_k=600:133.8223,lwr_k=700:133.7988,lwr_k=800:133.7969,lwr_k=900:133.7993,lwr_k=1000:133.8036'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:181.9591,lwr_k=10:181.0137,lwr_k=20:199.1118,lwr_k=30:196.5929,lwr_k=40:191.2177,lwr_k=50:193.8666,lwr_k=100:189.5091,lwr_k=200:184.204,lwr_k=300:182.9783,lwr_k=400:182.28,lwr_k=500:181.6973,lwr_k=600:181.714,lwr_k=700:182.3145,lwr_k=800:182.2194,lwr_k=900:182.3286,lwr_k=1000:182.3795'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:239.6367,lwr_k=10:247.3583,lwr_k=20:240.417,lwr_k=30:239.7352,lwr_k=40:239.7977,lwr_k=50:239.9721,lwr_k=100:240.913,lwr_k=200:240.6413,lwr_k=300:240.6466,lwr_k=400:240.4714,lwr_k=500:240.353,lwr_k=600:240.0931,lwr_k=700:239.91,lwr_k=800:239.8536,lwr_k=900:239.8081,lwr_k=1000:239.9738'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:202.822,lwr_k=10:208.8718,lwr_k=20:203.0709,lwr_k=30:202.7317,lwr_k=40:202.7416,lwr_k=50:202.809,lwr_k=100:203.4186,lwr_k=200:203.2236,lwr_k=300:203.2273,lwr_k=400:203.107,lwr_k=500:203.0291,lwr_k=600:202.872,lwr_k=700:202.7808,lwr_k=800:202.7587,lwr_k=900:202.7443,lwr_k=1000:202.8098'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:273.6847,lwr_k=10:286.0392,lwr_k=20:279.7442,lwr_k=30:284.7042,lwr_k=40:280.9563,lwr_k=50:276.6994,lwr_k=100:279.4506,lwr_k=200:277.9069,lwr_k=300:274.5971,lwr_k=400:274.8041,lwr_k=500:273.9726,lwr_k=600:274.1338,lwr_k=700:274.1018,lwr_k=800:273.9103,lwr_k=900:273.7678,lwr_k=1000:273.8205'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:155.1941,lwr_k=10:158.9139,lwr_k=20:155.2057,lwr_k=30:158.0586,lwr_k=40:155.8406,lwr_k=50:153.9426,lwr_k=100:155.0605,lwr_k=200:154.3677,lwr_k=300:153.7594,lwr_k=400:153.7137,lwr_k=500:154.1635,lwr_k=600:153.9965,lwr_k=700:154.0242,lwr_k=800:154.2526,lwr_k=900:154.5687,lwr_k=1000:154.4242'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:261.7626,lwr_k=10:266.5313,lwr_k=20:270.2479,lwr_k=30:262.3543,lwr_k=40:262.3381,lwr_k=50:262.026,lwr_k=100:262.8406,lwr_k=200:262.9699,lwr_k=300:261.776,lwr_k=400:262.0761,lwr_k=500:261.9114,lwr_k=600:262.041,lwr_k=700:262.0314,lwr_k=800:261.8857,lwr_k=900:261.7905,lwr_k=1000:261.7883'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:239.3147,lwr_k=10:242.2668,lwr_k=20:245.3767,lwr_k=30:239.2665,lwr_k=40:239.2591,lwr_k=50:239.1512,lwr_k=100:239.5289,lwr_k=200:239.6079,lwr_k=300:239.4243,lwr_k=400:239.1624,lwr_k=500:239.1426,lwr_k=600:239.1542,lwr_k=700:239.1522,lwr_k=800:239.1459,lwr_k=900:239.2038,lwr_k=1000:239.2071'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:204.8268,lwr_k=10:211.2806,lwr_k=20:205.4066,lwr_k=30:206.4864,lwr_k=40:209.5979,lwr_k=50:206.8734,lwr_k=100:205.0478,lwr_k=200:205.3937,lwr_k=300:204.9822,lwr_k=400:205.3989,lwr_k=500:205.0975,lwr_k=600:204.9847,lwr_k=700:204.8698,lwr_k=800:204.8308,lwr_k=900:204.8269,lwr_k=1000:204.88'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:325.691,lwr_k=10:328.1843,lwr_k=20:327.4579,lwr_k=30:325.3422,lwr_k=40:327.0568,lwr_k=50:325.5073,lwr_k=100:326.6448,lwr_k=200:327.4316,lwr_k=300:326.461,lwr_k=400:327.4422,lwr_k=500:326.7727,lwr_k=600:326.4684,lwr_k=700:326.0573,lwr_k=800:325.7929,lwr_k=900:325.6835,lwr_k=1000:326.1039'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:235.0387,lwr_k=10:267.4646,lwr_k=20:236.1389,lwr_k=30:235.2656,lwr_k=40:235.1973,lwr_k=50:235.872,lwr_k=100:238.564,lwr_k=200:235.4554,lwr_k=300:235.2143,lwr_k=400:235.0388,lwr_k=500:235.057,lwr_k=600:235.117,lwr_k=700:235.3828,lwr_k=800:235.1301,lwr_k=900:235.081,lwr_k=1000:235.0417'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:237.5599,lwr_k=10:271.5158,lwr_k=20:238.3782,lwr_k=30:237.6587,lwr_k=40:237.6114,lwr_k=50:238.1478,lwr_k=100:240.5807,lwr_k=200:237.8031,lwr_k=300:237.6228,lwr_k=400:237.5576,lwr_k=500:237.5418,lwr_k=600:237.563,lwr_k=700:237.7463,lwr_k=800:237.57,lwr_k=900:237.5469,lwr_k=1000:237.5773'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_37'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.9878,lwr_k=10:5.1606,lwr_k=20:6.2379,lwr_k=30:6.5184,lwr_k=40:6.6254,lwr_k=50:6.7371,lwr_k=100:7.0497,lwr_k=200:7.168,lwr_k=300:7.2766,lwr_k=400:7.3511,lwr_k=500:7.4346,lwr_k=600:7.4919,lwr_k=700:7.5126,lwr_k=800:7.5327,lwr_k=900:7.5587,lwr_k=1000:7.5982'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.8303,lwr_k=10:16.2223,lwr_k=20:11.3284,lwr_k=30:10.2624,lwr_k=40:10.0927,lwr_k=50:9.8917,lwr_k=100:9.5695,lwr_k=200:9.7329,lwr_k=300:9.7125,lwr_k=400:9.9244,lwr_k=500:10.196,lwr_k=600:10.3261,lwr_k=700:10.4098,lwr_k=800:10.484,lwr_k=900:10.5222,lwr_k=1000:10.5501'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.3406,lwr_k=10:6.38,lwr_k=20:7.8497,lwr_k=30:8.28,lwr_k=40:8.6675,lwr_k=50:8.703,lwr_k=100:9.2507,lwr_k=200:9.5005,lwr_k=300:9.689,lwr_k=400:9.8608,lwr_k=500:9.8924,lwr_k=600:9.9207,lwr_k=700:9.9334,lwr_k=800:9.9512,lwr_k=900:9.9522,lwr_k=1000:9.9852'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.5427,lwr_k=10:12.7041,lwr_k=20:10.9809,lwr_k=30:10.6727,lwr_k=40:10.375,lwr_k=50:10.1028,lwr_k=100:9.9253,lwr_k=200:9.8511,lwr_k=300:9.8164,lwr_k=400:10.0753,lwr_k=500:10.0425,lwr_k=600:10.0655,lwr_k=700:10.1468,lwr_k=800:10.1866,lwr_k=900:10.2069,lwr_k=1000:10.2428'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:13.3028,lwr_k=10:6.7397,lwr_k=20:7.5907,lwr_k=30:7.9463,lwr_k=40:8.3023,lwr_k=50:8.5305,lwr_k=100:8.7794,lwr_k=200:9.1346,lwr_k=300:9.1996,lwr_k=400:9.287,lwr_k=500:9.3424,lwr_k=600:9.3488,lwr_k=700:9.34,lwr_k=800:9.3641,lwr_k=900:9.3811,lwr_k=1000:9.3987'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.2605,lwr_k=10:8.5023,lwr_k=20:7.9721,lwr_k=30:7.7977,lwr_k=40:7.7639,lwr_k=50:7.6328,lwr_k=100:7.5111,lwr_k=200:7.6871,lwr_k=300:7.809,lwr_k=400:7.844,lwr_k=500:8.0909,lwr_k=600:8.2957,lwr_k=700:8.4357,lwr_k=800:8.485,lwr_k=900:8.4868,lwr_k=1000:8.4504'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.2335,lwr_k=10:5.9201,lwr_k=20:6.8398,lwr_k=30:7.3022,lwr_k=40:7.6824,lwr_k=50:8.0262,lwr_k=100:9.0778,lwr_k=200:9.7354,lwr_k=300:10.2141,lwr_k=400:10.3724,lwr_k=500:10.5297,lwr_k=600:10.6927,lwr_k=700:10.7674,lwr_k=800:10.8938,lwr_k=900:10.9942,lwr_k=1000:11.1206'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.9402,lwr_k=10:12.5103,lwr_k=20:11.2256,lwr_k=30:11.0012,lwr_k=40:10.5648,lwr_k=50:10.4033,lwr_k=100:9.9042,lwr_k=200:9.8394,lwr_k=300:9.856,lwr_k=400:9.8741,lwr_k=500:9.8736,lwr_k=600:9.917,lwr_k=700:9.9595,lwr_k=800:9.9985,lwr_k=900:10.0805,lwr_k=1000:10.1541'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.1976,lwr_k=10:5.0388,lwr_k=20:5.7209,lwr_k=30:5.9769,lwr_k=40:6.0447,lwr_k=50:6.1596,lwr_k=100:6.3534,lwr_k=200:6.4734,lwr_k=300:6.5213,lwr_k=400:6.7134,lwr_k=500:6.7618,lwr_k=600:6.7706,lwr_k=700:6.7833,lwr_k=800:6.7933,lwr_k=900:6.8123,lwr_k=1000:6.8264'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:25.6618,lwr_k=10:22.3359,lwr_k=20:19.6441,lwr_k=30:21.2885,lwr_k=40:21.0125,lwr_k=50:22.0571,lwr_k=100:22.1778,lwr_k=200:22.6401,lwr_k=300:22.8254,lwr_k=400:24.1635,lwr_k=500:24.2502,lwr_k=600:24.2992,lwr_k=700:24.3878,lwr_k=800:24.4312,lwr_k=900:24.5321,lwr_k=1000:24.5453'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:17.5939,lwr_k=10:6.0795,lwr_k=20:7.0356,lwr_k=30:7.511,lwr_k=40:7.6793,lwr_k=50:7.8261,lwr_k=100:8.0803,lwr_k=200:8.3967,lwr_k=300:8.7484,lwr_k=400:9.1587,lwr_k=500:10.0174,lwr_k=600:11.8851,lwr_k=700:13.2669,lwr_k=800:14.1116,lwr_k=900:15.0889,lwr_k=1000:15.5414'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:14.415,lwr_k=10:13.9286,lwr_k=20:11.3454,lwr_k=30:10.9472,lwr_k=40:10.9634,lwr_k=50:11.1423,lwr_k=100:10.9723,lwr_k=200:10.8214,lwr_k=300:11.0231,lwr_k=400:10.9992,lwr_k=500:11.0584,lwr_k=600:11.3412,lwr_k=700:11.6,lwr_k=800:11.8325,lwr_k=900:12.166,lwr_k=1000:12.3246'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_38'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.9798,lwr_k=10:0.0,lwr_k=20:0.6198,lwr_k=30:2.2919,lwr_k=40:3.1589,lwr_k=50:3.6947,lwr_k=100:4.8484,lwr_k=200:5.6043,lwr_k=300:5.8663,lwr_k=400:6.0404,lwr_k=500:6.1518,lwr_k=600:6.2513,lwr_k=700:6.3295,lwr_k=800:6.4066,lwr_k=900:6.4498,lwr_k=1000:6.4855'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.025,lwr_k=10:61.6692,lwr_k=20:269.0147,lwr_k=30:25.32,lwr_k=40:15.0967,lwr_k=50:11.0389,lwr_k=100:9.2077,lwr_k=200:9.0651,lwr_k=300:9.1358,lwr_k=400:9.306,lwr_k=500:9.3844,lwr_k=600:9.4312,lwr_k=700:9.4982,lwr_k=800:9.5266,lwr_k=900:9.518,lwr_k=1000:9.5857'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.4207,lwr_k=10:0.0,lwr_k=20:0.5506,lwr_k=30:2.2906,lwr_k=40:3.2087,lwr_k=50:3.8168,lwr_k=100:5.5682,lwr_k=200:6.3674,lwr_k=300:6.7541,lwr_k=400:7.0787,lwr_k=500:7.2742,lwr_k=600:7.3482,lwr_k=700:7.446,lwr_k=800:7.5224,lwr_k=900:7.623,lwr_k=1000:7.6555'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.0453,lwr_k=10:64.1518,lwr_k=20:149.9796,lwr_k=30:27.889,lwr_k=40:19.7484,lwr_k=50:16.4008,lwr_k=100:9.4668,lwr_k=200:8.4285,lwr_k=300:8.5308,lwr_k=400:8.6957,lwr_k=500:8.7135,lwr_k=600:8.7992,lwr_k=700:8.8953,lwr_k=800:8.9964,lwr_k=900:9.1129,lwr_k=1000:9.1528'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.7184,lwr_k=10:0.0,lwr_k=20:0.3892,lwr_k=30:1.6323,lwr_k=40:2.3786,lwr_k=50:2.8209,lwr_k=100:3.5401,lwr_k=200:3.9987,lwr_k=300:4.1498,lwr_k=400:4.2649,lwr_k=500:4.3559,lwr_k=600:4.4299,lwr_k=700:4.479,lwr_k=800:4.4941,lwr_k=900:4.5083,lwr_k=1000:4.5242'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.2632,lwr_k=10:48.3426,lwr_k=20:167.9562,lwr_k=30:32.4816,lwr_k=40:37.46,lwr_k=50:10.0346,lwr_k=100:7.114,lwr_k=200:6.3245,lwr_k=300:6.2379,lwr_k=400:6.1935,lwr_k=500:6.0938,lwr_k=600:6.0199,lwr_k=700:6.0795,lwr_k=800:6.0978,lwr_k=900:6.1195,lwr_k=1000:6.1012'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.161,lwr_k=10:0.0,lwr_k=20:0.5931,lwr_k=30:2.4216,lwr_k=40:3.4747,lwr_k=50:3.9663,lwr_k=100:5.371,lwr_k=200:6.1533,lwr_k=300:6.4646,lwr_k=400:6.7065,lwr_k=500:6.8551,lwr_k=600:6.9702,lwr_k=700:7.0955,lwr_k=800:7.1704,lwr_k=900:7.2524,lwr_k=1000:7.3224'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:11.7265,lwr_k=10:48.9426,lwr_k=20:426.1438,lwr_k=30:22.3644,lwr_k=40:14.8698,lwr_k=50:12.6497,lwr_k=100:11.19,lwr_k=200:10.4837,lwr_k=300:10.6577,lwr_k=400:10.7487,lwr_k=500:10.8437,lwr_k=600:10.9514,lwr_k=700:10.9783,lwr_k=800:11.0537,lwr_k=900:11.0721,lwr_k=1000:11.1171'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.1015,lwr_k=10:0.0,lwr_k=20:0.5225,lwr_k=30:1.7319,lwr_k=40:2.2874,lwr_k=50:2.6084,lwr_k=100:3.2452,lwr_k=200:3.6034,lwr_k=300:3.7164,lwr_k=400:3.7825,lwr_k=500:3.8353,lwr_k=600:3.8665,lwr_k=700:3.8994,lwr_k=800:3.933,lwr_k=900:3.954,lwr_k=1000:3.9726'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:19.2164,lwr_k=10:52.7073,lwr_k=20:185.4406,lwr_k=30:770.1561,lwr_k=40:232.656,lwr_k=50:413.195,lwr_k=100:15.3287,lwr_k=200:20.3668,lwr_k=300:20.4263,lwr_k=400:19.3233,lwr_k=500:19.2147,lwr_k=600:19.1028,lwr_k=700:18.8912,lwr_k=800:18.9998,lwr_k=900:19.0697,lwr_k=1000:19.0868'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:6.6883,lwr_k=10:0.0,lwr_k=20:0.4846,lwr_k=30:1.9613,lwr_k=40:2.7482,lwr_k=50:3.3561,lwr_k=100:4.3071,lwr_k=200:4.9603,lwr_k=300:5.203,lwr_k=400:5.3754,lwr_k=500:5.4932,lwr_k=600:5.5766,lwr_k=700:5.6406,lwr_k=800:5.693,lwr_k=900:5.7057,lwr_k=1000:5.7487'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.7278,lwr_k=10:49.349,lwr_k=20:116.6156,lwr_k=30:21.7281,lwr_k=40:13.1673,lwr_k=50:12.6622,lwr_k=100:10.4025,lwr_k=200:9.083,lwr_k=300:9.0016,lwr_k=400:9.0499,lwr_k=500:9.0632,lwr_k=600:9.0671,lwr_k=700:9.1383,lwr_k=800:9.2711,lwr_k=900:9.2271,lwr_k=1000:9.2937'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_39'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.2783,lwr_k=10:0.0,lwr_k=20:0.9309,lwr_k=30:1.9396,lwr_k=40:2.5396,lwr_k=50:2.9562,lwr_k=100:3.5433,lwr_k=200:3.8579,lwr_k=300:3.9979,lwr_k=400:4.0578,lwr_k=500:4.1073,lwr_k=600:4.1149,lwr_k=700:4.1351,lwr_k=800:4.1512,lwr_k=900:4.1643,lwr_k=1000:4.1799'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.0995,lwr_k=10:72.4331,lwr_k=20:1083.571,lwr_k=30:34.7555,lwr_k=40:21.2666,lwr_k=50:12.0969,lwr_k=100:8.2152,lwr_k=200:7.341,lwr_k=300:7.0669,lwr_k=400:7.0868,lwr_k=500:7.1303,lwr_k=600:7.1426,lwr_k=700:7.1382,lwr_k=800:7.1069,lwr_k=900:7.1296,lwr_k=1000:7.1222'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.5073,lwr_k=10:0.0,lwr_k=20:0.9261,lwr_k=30:2.2671,lwr_k=40:2.8679,lwr_k=50:3.3252,lwr_k=100:4.2715,lwr_k=200:4.7876,lwr_k=300:5.0262,lwr_k=400:5.1321,lwr_k=500:5.214,lwr_k=600:5.2581,lwr_k=700:5.2877,lwr_k=800:5.301,lwr_k=900:5.3147,lwr_k=1000:5.3263'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.6914,lwr_k=10:99.2637,lwr_k=20:1839.3384,lwr_k=30:75.0955,lwr_k=40:279.3103,lwr_k=50:333.1222,lwr_k=100:227.6018,lwr_k=200:203.9449,lwr_k=300:316.5641,lwr_k=400:324.8876,lwr_k=500:274.5786,lwr_k=600:168.6328,lwr_k=700:9.7415,lwr_k=800:11.9582,lwr_k=900:8.4508,lwr_k=1000:7.7857'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.4276,lwr_k=10:0.0,lwr_k=20:0.9513,lwr_k=30:2.2255,lwr_k=40:2.9766,lwr_k=50:3.2274,lwr_k=100:4.1368,lwr_k=200:4.7569,lwr_k=300:4.9531,lwr_k=400:5.0277,lwr_k=500:5.1379,lwr_k=600:5.1965,lwr_k=700:5.1871,lwr_k=800:5.2244,lwr_k=900:5.2422,lwr_k=1000:5.2668'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.0742,lwr_k=10:67.7026,lwr_k=20:340.3691,lwr_k=30:22.5152,lwr_k=40:10.8174,lwr_k=50:8.9682,lwr_k=100:7.7436,lwr_k=200:6.9755,lwr_k=300:6.8933,lwr_k=400:6.9079,lwr_k=500:6.862,lwr_k=600:6.8388,lwr_k=700:6.7745,lwr_k=800:6.7682,lwr_k=900:6.7785,lwr_k=1000:6.7396'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.8259,lwr_k=10:0.0,lwr_k=20:0.9579,lwr_k=30:2.224,lwr_k=40:2.6799,lwr_k=50:3.0597,lwr_k=100:3.8461,lwr_k=200:4.2284,lwr_k=300:4.4561,lwr_k=400:4.5568,lwr_k=500:4.6167,lwr_k=600:4.6485,lwr_k=700:4.6836,lwr_k=800:4.7102,lwr_k=900:4.725,lwr_k=1000:4.7301'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.1828,lwr_k=10:191.1001,lwr_k=20:140.9606,lwr_k=30:40.3853,lwr_k=40:19.9969,lwr_k=50:17.229,lwr_k=100:10.1629,lwr_k=200:9.1136,lwr_k=300:9.2746,lwr_k=400:9.5764,lwr_k=500:9.7245,lwr_k=600:9.876,lwr_k=700:9.9737,lwr_k=800:10.027,lwr_k=900:9.9453,lwr_k=1000:10.0238'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.0212,lwr_k=10:0.0,lwr_k=20:0.9443,lwr_k=30:1.8989,lwr_k=40:2.5174,lwr_k=50:2.7892,lwr_k=100:3.3306,lwr_k=200:3.6363,lwr_k=300:3.7453,lwr_k=400:3.7964,lwr_k=500:3.87,lwr_k=600:3.8863,lwr_k=700:3.9065,lwr_k=800:3.9053,lwr_k=900:3.9278,lwr_k=1000:3.9364'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:20.419,lwr_k=10:92.1221,lwr_k=20:82.1823,lwr_k=30:2149.3205,lwr_k=40:23.1757,lwr_k=50:21.6335,lwr_k=100:21.5803,lwr_k=200:23.3119,lwr_k=300:22.2128,lwr_k=400:19.3946,lwr_k=500:19.3502,lwr_k=600:19.4469,lwr_k=700:19.5865,lwr_k=800:19.5748,lwr_k=900:19.5147,lwr_k=1000:19.6177'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:4.7305,lwr_k=10:0.0,lwr_k=20:0.8224,lwr_k=30:2.0684,lwr_k=40:2.6932,lwr_k=50:3.0447,lwr_k=100:3.6754,lwr_k=200:4.1679,lwr_k=300:4.2698,lwr_k=400:4.3811,lwr_k=500:4.4385,lwr_k=600:4.4674,lwr_k=700:4.4867,lwr_k=800:4.5177,lwr_k=900:4.5367,lwr_k=1000:4.5479'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.6258,lwr_k=10:64.7074,lwr_k=20:929.7198,lwr_k=30:44.1285,lwr_k=40:18.6253,lwr_k=50:13.4628,lwr_k=100:8.5667,lwr_k=200:7.9639,lwr_k=300:8.0576,lwr_k=400:8.0514,lwr_k=500:8.079,lwr_k=600:8.1748,lwr_k=700:8.1747,lwr_k=800:8.2932,lwr_k=900:8.3514,lwr_k=1000:8.3435'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_40'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.7294,lwr_k=10:0.0,lwr_k=20:0.063,lwr_k=30:1.7461,lwr_k=40:14.3059,lwr_k=50:16.2464,lwr_k=100:6.693,lwr_k=200:6.2035,lwr_k=300:6.576,lwr_k=400:6.0597,lwr_k=500:5.6542,lwr_k=600:5.6161,lwr_k=700:5.5689,lwr_k=800:5.5348,lwr_k=900:5.5922,lwr_k=1000:5.5429'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.2908,lwr_k=10:162.7725,lwr_k=20:274.5593,lwr_k=30:1485.6266,lwr_k=40:1635.2843,lwr_k=50:61.2685,lwr_k=100:62.2452,lwr_k=200:11.3163,lwr_k=300:9.8394,lwr_k=400:8.6868,lwr_k=500:7.998,lwr_k=600:7.9526,lwr_k=700:8.0333,lwr_k=800:8.0365,lwr_k=900:7.9167,lwr_k=1000:7.93'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.1059,lwr_k=10:0.0,lwr_k=20:0.0871,lwr_k=30:1.155,lwr_k=40:9.2996,lwr_k=50:7.7178,lwr_k=100:5.1325,lwr_k=200:5.1521,lwr_k=300:5.2962,lwr_k=400:5.4059,lwr_k=500:5.4544,lwr_k=600:5.5986,lwr_k=700:5.5358,lwr_k=800:5.5654,lwr_k=900:5.5959,lwr_k=1000:5.5347'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.019,lwr_k=10:110.5298,lwr_k=20:1925.1325,lwr_k=30:888.6469,lwr_k=40:128.1125,lwr_k=50:99.8448,lwr_k=100:16.3989,lwr_k=200:8.6409,lwr_k=300:7.1481,lwr_k=400:6.9137,lwr_k=500:7.0197,lwr_k=600:6.9412,lwr_k=700:6.7389,lwr_k=800:6.6977,lwr_k=900:6.7509,lwr_k=1000:6.7727'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.2428,lwr_k=10:0.0007,lwr_k=20:0.4264,lwr_k=30:2.6254,lwr_k=40:9.9251,lwr_k=50:8.5308,lwr_k=100:10.6347,lwr_k=200:7.9059,lwr_k=300:7.611,lwr_k=400:7.012,lwr_k=500:7.1241,lwr_k=600:6.9877,lwr_k=700:6.6705,lwr_k=800:6.8327,lwr_k=900:7.0775,lwr_k=1000:6.9734'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.4442,lwr_k=10:31777.0942,lwr_k=20:521205.4831,lwr_k=30:414.6968,lwr_k=40:1038.9053,lwr_k=50:3382.1177,lwr_k=100:18.6332,lwr_k=200:10.0653,lwr_k=300:9.178,lwr_k=400:7.7803,lwr_k=500:7.1919,lwr_k=600:6.4621,lwr_k=700:6.4737,lwr_k=800:6.4314,lwr_k=900:6.5794,lwr_k=1000:6.7997'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.9305,lwr_k=10:0.0003,lwr_k=20:0.4927,lwr_k=30:1.7463,lwr_k=40:10.3862,lwr_k=50:10.1746,lwr_k=100:7.2564,lwr_k=200:6.8239,lwr_k=300:6.3013,lwr_k=400:6.2767,lwr_k=500:5.7467,lwr_k=600:5.9071,lwr_k=700:5.5889,lwr_k=800:5.6589,lwr_k=900:5.5317,lwr_k=1000:5.5308'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.7241,lwr_k=10:233.3589,lwr_k=20:532712.9518,lwr_k=30:5018384.4293,lwr_k=40:887954.2419,lwr_k=50:143071.1909,lwr_k=100:7107.2209,lwr_k=200:734.7994,lwr_k=300:23.2533,lwr_k=400:393.956,lwr_k=500:32.3362,lwr_k=600:11.0321,lwr_k=700:10.436,lwr_k=800:10.1568,lwr_k=900:9.7362,lwr_k=1000:9.4433'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.1461,lwr_k=10:0.0,lwr_k=20:0.0773,lwr_k=30:1.3299,lwr_k=40:3.9969,lwr_k=50:5.0397,lwr_k=100:4.9405,lwr_k=200:5.2004,lwr_k=300:5.0236,lwr_k=400:5.1031,lwr_k=500:4.9941,lwr_k=600:5.1076,lwr_k=700:5.0201,lwr_k=800:4.932,lwr_k=900:4.9525,lwr_k=1000:4.9815'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:16.097,lwr_k=10:95.5196,lwr_k=20:9205.5397,lwr_k=30:6489.4454,lwr_k=40:322.1628,lwr_k=50:2579.877,lwr_k=100:28.951,lwr_k=200:32.7817,lwr_k=300:21.4619,lwr_k=400:21.1476,lwr_k=500:19.5422,lwr_k=600:19.4262,lwr_k=700:18.51,lwr_k=800:17.3973,lwr_k=900:17.3919,lwr_k=1000:16.4733'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:8.2521,lwr_k=10:0.0003,lwr_k=20:0.4833,lwr_k=30:6.4935,lwr_k=40:17.5242,lwr_k=50:15.7861,lwr_k=100:11.9097,lwr_k=200:9.3793,lwr_k=300:11.7005,lwr_k=400:14.5083,lwr_k=500:8.6542,lwr_k=600:8.3902,lwr_k=700:9.0091,lwr_k=800:8.5096,lwr_k=900:8.0648,lwr_k=1000:7.8908'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.5804,lwr_k=10:110.1882,lwr_k=20:275.9589,lwr_k=30:476.3421,lwr_k=40:165.1226,lwr_k=50:72.1799,lwr_k=100:26.4952,lwr_k=200:17.0187,lwr_k=300:16.2619,lwr_k=400:18.3178,lwr_k=500:10.5273,lwr_k=600:11.4469,lwr_k=700:11.1537,lwr_k=800:15.8817,lwr_k=900:11.5423,lwr_k=1000:17.7296'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_41'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.5834,lwr_k=10:4.2117,lwr_k=20:5.8072,lwr_k=30:6.281,lwr_k=40:6.5245,lwr_k=50:6.6696,lwr_k=100:6.9454,lwr_k=200:7.2686,lwr_k=300:7.4463,lwr_k=400:7.5328,lwr_k=500:7.5886,lwr_k=600:7.6728,lwr_k=700:7.7427,lwr_k=800:7.8097,lwr_k=900:7.8595,lwr_k=1000:7.9087'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.9753,lwr_k=10:605.8057,lwr_k=20:11.5329,lwr_k=30:9.7737,lwr_k=40:9.6823,lwr_k=50:9.7057,lwr_k=100:8.9229,lwr_k=200:8.8776,lwr_k=300:8.9119,lwr_k=400:8.9685,lwr_k=500:9.0166,lwr_k=600:9.0268,lwr_k=700:9.0845,lwr_k=800:9.1543,lwr_k=900:9.2483,lwr_k=1000:9.3127'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.211,lwr_k=10:3.7338,lwr_k=20:5.8626,lwr_k=30:6.4379,lwr_k=40:6.931,lwr_k=50:7.1542,lwr_k=100:7.6832,lwr_k=200:7.9957,lwr_k=300:8.1605,lwr_k=400:8.3331,lwr_k=500:8.488,lwr_k=600:8.5739,lwr_k=700:8.6134,lwr_k=800:8.664,lwr_k=900:8.7131,lwr_k=1000:8.7531'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.2922,lwr_k=10:64.822,lwr_k=20:8.9916,lwr_k=30:8.3099,lwr_k=40:7.7719,lwr_k=50:7.847,lwr_k=100:8.1502,lwr_k=200:8.0141,lwr_k=300:8.1414,lwr_k=400:8.2378,lwr_k=500:8.3338,lwr_k=600:8.3938,lwr_k=700:8.4934,lwr_k=800:8.6717,lwr_k=900:8.7386,lwr_k=1000:8.7669'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.9487,lwr_k=10:4.8072,lwr_k=20:5.951,lwr_k=30:6.4865,lwr_k=40:6.78,lwr_k=50:6.9949,lwr_k=100:7.6089,lwr_k=200:7.9181,lwr_k=300:8.0815,lwr_k=400:8.2604,lwr_k=500:8.2974,lwr_k=600:8.2938,lwr_k=700:8.314,lwr_k=800:8.3762,lwr_k=900:8.3971,lwr_k=1000:8.4176'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.8781,lwr_k=10:53.4913,lwr_k=20:42.7429,lwr_k=30:8.3296,lwr_k=40:8.1723,lwr_k=50:7.7961,lwr_k=100:6.4019,lwr_k=200:6.3333,lwr_k=300:6.2791,lwr_k=400:6.298,lwr_k=500:6.2727,lwr_k=600:6.2892,lwr_k=700:6.3181,lwr_k=800:6.357,lwr_k=900:6.3809,lwr_k=1000:6.3889'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.1875,lwr_k=10:3.9961,lwr_k=20:5.346,lwr_k=30:5.9548,lwr_k=40:6.123,lwr_k=50:6.5595,lwr_k=100:7.0623,lwr_k=200:7.6081,lwr_k=300:7.8638,lwr_k=400:8.0087,lwr_k=500:8.1317,lwr_k=600:8.2154,lwr_k=700:8.277,lwr_k=800:8.4592,lwr_k=900:8.758,lwr_k=1000:8.8019'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.7496,lwr_k=10:357559.0918,lwr_k=20:188558.501,lwr_k=30:2785.0517,lwr_k=40:9.434,lwr_k=50:9.5541,lwr_k=100:9.0933,lwr_k=200:9.219,lwr_k=300:9.2411,lwr_k=400:9.2561,lwr_k=500:9.284,lwr_k=600:9.3433,lwr_k=700:9.4235,lwr_k=800:9.4623,lwr_k=900:9.5344,lwr_k=1000:9.5126'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.2956,lwr_k=10:2.7347,lwr_k=20:4.183,lwr_k=30:4.4983,lwr_k=40:4.8806,lwr_k=50:4.9958,lwr_k=100:5.3319,lwr_k=200:5.5879,lwr_k=300:5.7207,lwr_k=400:5.8018,lwr_k=500:5.8684,lwr_k=600:5.8996,lwr_k=700:5.9378,lwr_k=800:5.9735,lwr_k=900:6.0176,lwr_k=1000:6.0464'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:22.8372,lwr_k=10:155525385.7812,lwr_k=20:3606070.5655,lwr_k=30:76.473,lwr_k=40:19.6919,lwr_k=50:20.1288,lwr_k=100:19.0911,lwr_k=200:19.0877,lwr_k=300:19.0556,lwr_k=400:19.4908,lwr_k=500:19.62,lwr_k=600:20.2771,lwr_k=700:20.534,lwr_k=800:20.6617,lwr_k=900:21.2188,lwr_k=1000:21.5081'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:9.5551,lwr_k=10:4.3186,lwr_k=20:5.4047,lwr_k=30:5.7395,lwr_k=40:6.1226,lwr_k=50:6.3339,lwr_k=100:6.9354,lwr_k=200:7.3298,lwr_k=300:7.5848,lwr_k=400:7.8085,lwr_k=500:8.0519,lwr_k=600:8.1278,lwr_k=700:8.2186,lwr_k=800:8.2574,lwr_k=900:8.2645,lwr_k=1000:8.2722'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.9226,lwr_k=10:3987.415,lwr_k=20:247.3679,lwr_k=30:14.2765,lwr_k=40:11.0666,lwr_k=50:10.9161,lwr_k=100:9.8784,lwr_k=200:9.607,lwr_k=300:9.4386,lwr_k=400:9.3561,lwr_k=500:9.3796,lwr_k=600:9.3866,lwr_k=700:9.3271,lwr_k=800:9.3337,lwr_k=900:9.2909,lwr_k=1000:9.2814'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_42'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:49.6437,lwr_k=10:68.3902,lwr_k=20:18.7901,lwr_k=30:22.2783,lwr_k=40:25.1518,lwr_k=50:26.7766,lwr_k=100:30.5232,lwr_k=200:33.0902,lwr_k=300:33.7494,lwr_k=400:34.5637,lwr_k=500:35.185,lwr_k=600:36.2098,lwr_k=700:36.1099,lwr_k=800:36.7586,lwr_k=900:36.8179,lwr_k=1000:36.9906'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:56.0709,lwr_k=10:1064844089.225,lwr_k=20:62373911.1518,lwr_k=30:45274097.1248,lwr_k=40:114.9294,lwr_k=50:58.5307,lwr_k=100:43.6225,lwr_k=200:44.4589,lwr_k=300:39.0566,lwr_k=400:39.8005,lwr_k=500:39.6702,lwr_k=600:40.7714,lwr_k=700:41.8397,lwr_k=800:41.8987,lwr_k=900:42.3074,lwr_k=1000:42.247'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:25.7428,lwr_k=10:8.1253,lwr_k=20:6.7206,lwr_k=30:8.1058,lwr_k=40:8.7483,lwr_k=50:9.598,lwr_k=100:11.1723,lwr_k=200:13.1885,lwr_k=300:14.7421,lwr_k=400:17.055,lwr_k=500:17.1949,lwr_k=600:16.4901,lwr_k=700:16.991,lwr_k=800:17.5101,lwr_k=900:18.1385,lwr_k=1000:18.4918'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:19.6836,lwr_k=10:37970122.4896,lwr_k=20:21441362.6478,lwr_k=30:907818.0125,lwr_k=40:838130.7532,lwr_k=50:14.3105,lwr_k=100:14.0799,lwr_k=200:14.0518,lwr_k=300:13.396,lwr_k=400:13.8696,lwr_k=500:13.8192,lwr_k=600:13.5776,lwr_k=700:14.6952,lwr_k=800:14.8183,lwr_k=900:14.4101,lwr_k=1000:14.3175'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:30.4475,lwr_k=10:8.1671,lwr_k=20:9.8534,lwr_k=30:11.7726,lwr_k=40:12.7698,lwr_k=50:13.458,lwr_k=100:15.9795,lwr_k=200:18.2096,lwr_k=300:19.2117,lwr_k=400:20.1438,lwr_k=500:20.6625,lwr_k=600:21.1842,lwr_k=700:21.7206,lwr_k=800:21.7478,lwr_k=900:22.3692,lwr_k=1000:21.9117'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:24.4168,lwr_k=10:1466170.3501,lwr_k=20:529.9011,lwr_k=30:22.7599,lwr_k=40:21.1383,lwr_k=50:19.9355,lwr_k=100:16.9033,lwr_k=200:16.4958,lwr_k=300:16.5353,lwr_k=400:16.8016,lwr_k=500:16.7596,lwr_k=600:17.0961,lwr_k=700:17.1352,lwr_k=800:17.1921,lwr_k=900:17.7901,lwr_k=1000:17.3904'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.3409,lwr_k=10:1.3792,lwr_k=20:3.9181,lwr_k=30:4.8496,lwr_k=40:5.2949,lwr_k=50:5.6422,lwr_k=100:6.1852,lwr_k=200:6.5615,lwr_k=300:6.6538,lwr_k=400:6.7296,lwr_k=500:6.8076,lwr_k=600:6.878,lwr_k=700:6.9291,lwr_k=800:6.9333,lwr_k=900:6.9572,lwr_k=1000:6.9904'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.9952,lwr_k=10:15406617.3672,lwr_k=20:42.6943,lwr_k=30:11960.5705,lwr_k=40:37.9205,lwr_k=50:26.7323,lwr_k=100:11.8551,lwr_k=200:10.8539,lwr_k=300:10.7425,lwr_k=400:10.7499,lwr_k=500:10.7202,lwr_k=600:10.854,lwr_k=700:10.849,lwr_k=800:10.8398,lwr_k=900:10.8405,lwr_k=1000:10.8075'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:18.9324,lwr_k=10:2.9885,lwr_k=20:6.9249,lwr_k=30:8.3393,lwr_k=40:9.1162,lwr_k=50:9.767,lwr_k=100:11.1826,lwr_k=200:12.3262,lwr_k=300:12.9638,lwr_k=400:13.4353,lwr_k=500:13.8666,lwr_k=600:14.2176,lwr_k=700:14.5471,lwr_k=800:14.8273,lwr_k=900:15.0556,lwr_k=1000:15.3384'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:45.3827,lwr_k=10:731155.6201,lwr_k=20:2600.4856,lwr_k=30:77.3233,lwr_k=40:27.1992,lwr_k=50:28.4734,lwr_k=100:28.3921,lwr_k=200:31.2752,lwr_k=300:33.1911,lwr_k=400:34.824,lwr_k=500:36.1391,lwr_k=600:37.2983,lwr_k=700:38.1053,lwr_k=800:39.1954,lwr_k=900:40.1239,lwr_k=1000:40.7884'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:22.0254,lwr_k=10:10.4953,lwr_k=20:6.8125,lwr_k=30:8.3297,lwr_k=40:9.2106,lwr_k=50:9.6417,lwr_k=100:11.3704,lwr_k=200:12.979,lwr_k=300:13.9353,lwr_k=400:14.5683,lwr_k=500:14.8789,lwr_k=600:16.5532,lwr_k=700:15.725,lwr_k=800:15.9763,lwr_k=900:16.3322,lwr_k=1000:16.8876'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:21.6944,lwr_k=10:416032530.5672,lwr_k=20:221617.4953,lwr_k=30:1007.251,lwr_k=40:1135.0265,lwr_k=50:654.5966,lwr_k=100:130.2143,lwr_k=200:16.3219,lwr_k=300:17.7454,lwr_k=400:16.5371,lwr_k=500:16.3199,lwr_k=600:16.7909,lwr_k=700:16.7529,lwr_k=800:17.0737,lwr_k=900:16.9841,lwr_k=1000:17.1884'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_43'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:181.5256,lwr_k=10:0.0119,lwr_k=20:9.9227,lwr_k=30:19.6317,lwr_k=40:25.4022,lwr_k=50:30.1318,lwr_k=100:42.5275,lwr_k=200:56.7232,lwr_k=300:66.399,lwr_k=400:72.0212,lwr_k=500:77.7152,lwr_k=600:80.8873,lwr_k=700:85.7599,lwr_k=800:89.7759,lwr_k=900:93.3681,lwr_k=1000:97.0412'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:227.758,lwr_k=10:9311.4382,lwr_k=20:24933.2347,lwr_k=30:1080.5485,lwr_k=40:814.1744,lwr_k=50:387.2289,lwr_k=100:51.5475,lwr_k=200:64.9916,lwr_k=300:81.4231,lwr_k=400:89.4593,lwr_k=500:96.42,lwr_k=600:104.1897,lwr_k=700:108.4293,lwr_k=800:114.6078,lwr_k=900:119.0836,lwr_k=1000:124.1474'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:169.7058,lwr_k=10:0.0113,lwr_k=20:9.4039,lwr_k=30:19.255,lwr_k=40:25.3469,lwr_k=50:30.6981,lwr_k=100:45.9838,lwr_k=200:62.4395,lwr_k=300:72.2942,lwr_k=400:78.535,lwr_k=500:84.9436,lwr_k=600:90.5704,lwr_k=700:93.6156,lwr_k=800:97.0756,lwr_k=900:101.1831,lwr_k=1000:104.2173'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:134.4335,lwr_k=10:5070.2562,lwr_k=20:373.8427,lwr_k=30:306.6919,lwr_k=40:113.7916,lwr_k=50:131.5207,lwr_k=100:95.4803,lwr_k=200:81.3034,lwr_k=300:72.0187,lwr_k=400:68.4619,lwr_k=500:71.6386,lwr_k=600:73.2368,lwr_k=700:75.6545,lwr_k=800:77.4302,lwr_k=900:78.8086,lwr_k=1000:80.8837'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:171.3337,lwr_k=10:0.0011,lwr_k=20:8.3113,lwr_k=30:19.7529,lwr_k=40:28.5971,lwr_k=50:33.9102,lwr_k=100:51.2618,lwr_k=200:73.8495,lwr_k=300:85.0679,lwr_k=400:90.5899,lwr_k=500:94.7553,lwr_k=600:99.5515,lwr_k=700:103.9378,lwr_k=800:107.677,lwr_k=900:110.9774,lwr_k=1000:114.0285'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:111.4438,lwr_k=10:1292.1648,lwr_k=20:9987947.7025,lwr_k=30:675.9165,lwr_k=40:58.2504,lwr_k=50:56.4931,lwr_k=100:56.4399,lwr_k=200:65.213,lwr_k=300:68.3298,lwr_k=400:69.6036,lwr_k=500:72.1144,lwr_k=600:73.8193,lwr_k=700:76.5145,lwr_k=800:78.7223,lwr_k=900:80.0066,lwr_k=1000:80.7709'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:198.6678,lwr_k=10:0.0044,lwr_k=20:8.662,lwr_k=30:20.2279,lwr_k=40:29.2047,lwr_k=50:38.0007,lwr_k=100:67.9966,lwr_k=200:97.7239,lwr_k=300:111.8749,lwr_k=400:121.4519,lwr_k=500:128.9389,lwr_k=600:135.3793,lwr_k=700:142.5786,lwr_k=800:147.5949,lwr_k=900:153.5008,lwr_k=1000:156.473'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:178.2871,lwr_k=10:116091121.3511,lwr_k=20:112701752.7876,lwr_k=30:8985979.6653,lwr_k=40:52774.7036,lwr_k=50:101.4086,lwr_k=100:105.478,lwr_k=200:114.9621,lwr_k=300:114.549,lwr_k=400:120.1396,lwr_k=500:118.9638,lwr_k=600:124.221,lwr_k=700:127.8254,lwr_k=800:129.7729,lwr_k=900:132.5956,lwr_k=1000:134.9937'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:140.7105,lwr_k=10:0.0212,lwr_k=20:7.8099,lwr_k=30:15.8211,lwr_k=40:21.3207,lwr_k=50:25.7196,lwr_k=100:39.7568,lwr_k=200:56.9802,lwr_k=300:66.8667,lwr_k=400:74.516,lwr_k=500:80.7716,lwr_k=600:85.5416,lwr_k=700:89.7952,lwr_k=800:93.2754,lwr_k=900:96.5045,lwr_k=1000:99.5224'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:243.7814,lwr_k=10:9369.608,lwr_k=20:21320.5946,lwr_k=30:1503.2381,lwr_k=40:375.4247,lwr_k=50:139.6636,lwr_k=100:95.1788,lwr_k=200:117.6406,lwr_k=300:166.3705,lwr_k=400:176.2104,lwr_k=500:183.8408,lwr_k=600:188.7941,lwr_k=700:195.1047,lwr_k=800:200.8691,lwr_k=900:203.8322,lwr_k=1000:206.8232'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:151.0026,lwr_k=10:0.0111,lwr_k=20:8.4357,lwr_k=30:18.7943,lwr_k=40:25.5451,lwr_k=50:30.7732,lwr_k=100:45.4627,lwr_k=200:64.4567,lwr_k=300:71.9942,lwr_k=400:79.6873,lwr_k=500:82.3938,lwr_k=600:86.6609,lwr_k=700:89.7094,lwr_k=800:92.5629,lwr_k=900:95.0972,lwr_k=1000:98.0964'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:141.3233,lwr_k=10:141564.5448,lwr_k=20:9373487206.0625,lwr_k=30:428.5812,lwr_k=40:100.6898,lwr_k=50:72.9694,lwr_k=100:74.8804,lwr_k=200:70.9847,lwr_k=300:72.0212,lwr_k=400:72.0424,lwr_k=500:74.277,lwr_k=600:77.196,lwr_k=700:79.3859,lwr_k=800:81.6469,lwr_k=900:84.327,lwr_k=1000:86.5971'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_44'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.6218,lwr_k=10:0.0,lwr_k=20:0.2133,lwr_k=30:1.3825,lwr_k=40:2.1808,lwr_k=50:2.7315,lwr_k=100:3.8157,lwr_k=200:4.4781,lwr_k=300:4.7254,lwr_k=400:4.9014,lwr_k=500:5.041,lwr_k=600:5.0722,lwr_k=700:5.0797,lwr_k=800:5.1097,lwr_k=900:5.131,lwr_k=1000:5.1565'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.6069,lwr_k=10:76.2008,lwr_k=20:28372.8378,lwr_k=30:34896605.7481,lwr_k=40:1366533.8975,lwr_k=50:70469.9024,lwr_k=100:13.2732,lwr_k=200:8.7107,lwr_k=300:8.7645,lwr_k=400:8.5839,lwr_k=500:8.6857,lwr_k=600:8.7802,lwr_k=700:8.7638,lwr_k=800:8.7888,lwr_k=900:8.8659,lwr_k=1000:8.8964'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.9078,lwr_k=10:0.0,lwr_k=20:0.2434,lwr_k=30:1.7783,lwr_k=40:2.9305,lwr_k=50:3.7654,lwr_k=100:5.6606,lwr_k=200:6.5922,lwr_k=300:7.0771,lwr_k=400:7.3629,lwr_k=500:7.5136,lwr_k=600:7.6726,lwr_k=700:7.7886,lwr_k=800:7.8724,lwr_k=900:7.9693,lwr_k=1000:8.0578'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.5426,lwr_k=10:78.0233,lwr_k=20:238096813.704,lwr_k=30:39609556.0635,lwr_k=40:116360897.9994,lwr_k=50:150829363.6003,lwr_k=100:67696.1001,lwr_k=200:29.5358,lwr_k=300:8.5746,lwr_k=400:8.3971,lwr_k=500:8.3776,lwr_k=600:8.386,lwr_k=700:8.4159,lwr_k=800:8.4375,lwr_k=900:8.4861,lwr_k=1000:8.4863'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.6157,lwr_k=10:0.0,lwr_k=20:0.479,lwr_k=30:1.9273,lwr_k=40:2.9998,lwr_k=50:3.6938,lwr_k=100:4.9671,lwr_k=200:5.9096,lwr_k=300:6.2732,lwr_k=400:6.5403,lwr_k=500:6.691,lwr_k=600:6.8246,lwr_k=700:6.8977,lwr_k=800:6.9566,lwr_k=900:6.9712,lwr_k=1000:7.0111'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.1257,lwr_k=10:56.0275,lwr_k=20:410716.3713,lwr_k=30:260452.3136,lwr_k=40:5697.3595,lwr_k=50:72.6099,lwr_k=100:302.7801,lwr_k=200:368.45,lwr_k=300:7.8279,lwr_k=400:7.0724,lwr_k=500:7.5556,lwr_k=600:6.1615,lwr_k=700:5.8701,lwr_k=800:5.8517,lwr_k=900:5.8495,lwr_k=1000:5.8506'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.7421,lwr_k=10:0.0,lwr_k=20:0.2083,lwr_k=30:1.7638,lwr_k=40:2.901,lwr_k=50:3.4405,lwr_k=100:4.7467,lwr_k=200:5.5072,lwr_k=300:5.7805,lwr_k=400:5.9824,lwr_k=500:6.0724,lwr_k=600:6.1421,lwr_k=700:6.1836,lwr_k=800:6.2217,lwr_k=900:6.2709,lwr_k=1000:6.3315'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.2952,lwr_k=10:77.5814,lwr_k=20:192602.7105,lwr_k=30:3461.2044,lwr_k=40:301.2256,lwr_k=50:73.1053,lwr_k=100:11.1107,lwr_k=200:9.2954,lwr_k=300:9.1339,lwr_k=400:9.2897,lwr_k=500:9.1963,lwr_k=600:9.1923,lwr_k=700:9.0761,lwr_k=800:9.0858,lwr_k=900:9.1287,lwr_k=1000:9.0236'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.8711,lwr_k=10:0.0,lwr_k=20:0.2971,lwr_k=30:1.8276,lwr_k=40:2.9459,lwr_k=50:3.6642,lwr_k=100:4.939,lwr_k=200:5.7206,lwr_k=300:5.9556,lwr_k=400:6.1,lwr_k=500:6.1714,lwr_k=600:6.2328,lwr_k=700:6.3182,lwr_k=800:6.3678,lwr_k=900:6.4069,lwr_k=1000:6.4398'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:23.3016,lwr_k=10:84.3952,lwr_k=20:111572.6971,lwr_k=30:328.9182,lwr_k=40:501865.6902,lwr_k=50:121.4916,lwr_k=100:29.3996,lwr_k=200:25.2952,lwr_k=300:25.8516,lwr_k=400:21.3125,lwr_k=500:21.5566,lwr_k=600:21.5361,lwr_k=700:21.6777,lwr_k=800:21.7791,lwr_k=900:21.8455,lwr_k=1000:21.9741'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:6.4952,lwr_k=10:0.0,lwr_k=20:0.3577,lwr_k=30:1.8331,lwr_k=40:2.7563,lwr_k=50:3.2518,lwr_k=100:4.5211,lwr_k=200:5.0543,lwr_k=300:5.3702,lwr_k=400:5.5744,lwr_k=500:5.6778,lwr_k=600:5.7565,lwr_k=700:5.845,lwr_k=800:5.9012,lwr_k=900:5.9327,lwr_k=1000:5.9982'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.4512,lwr_k=10:108.0802,lwr_k=20:23248.1972,lwr_k=30:251963.1443,lwr_k=40:13693.0886,lwr_k=50:43360.483,lwr_k=100:593.649,lwr_k=200:9.7426,lwr_k=300:10.9918,lwr_k=400:11.2171,lwr_k=500:8.7162,lwr_k=600:8.7721,lwr_k=700:8.7553,lwr_k=800:8.7795,lwr_k=900:8.8822,lwr_k=1000:9.0167'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_45'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:13.4341,lwr_k=10:11.6266,lwr_k=20:12.2113,lwr_k=30:12.4485,lwr_k=40:12.581,lwr_k=50:12.5833,lwr_k=100:12.7366,lwr_k=200:12.7507,lwr_k=300:12.7392,lwr_k=400:12.7601,lwr_k=500:12.8287,lwr_k=600:12.9088,lwr_k=700:12.9813,lwr_k=800:13.0413,lwr_k=900:13.0878,lwr_k=1000:13.126'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:15.0398,lwr_k=10:15.6889,lwr_k=20:14.8388,lwr_k=30:14.5342,lwr_k=40:14.1679,lwr_k=50:14.13,lwr_k=100:14.0773,lwr_k=200:13.8666,lwr_k=300:13.9584,lwr_k=400:14.0179,lwr_k=500:14.0803,lwr_k=600:14.3023,lwr_k=700:14.6094,lwr_k=800:14.7388,lwr_k=900:14.7882,lwr_k=1000:14.8422'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.3603,lwr_k=10:8.4175,lwr_k=20:9.102,lwr_k=30:9.2911,lwr_k=40:9.294,lwr_k=50:9.3968,lwr_k=100:9.556,lwr_k=200:9.7462,lwr_k=300:9.9409,lwr_k=400:10.0224,lwr_k=500:10.0313,lwr_k=600:10.0583,lwr_k=700:10.0727,lwr_k=800:10.0915,lwr_k=900:10.0969,lwr_k=1000:10.1144'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.0279,lwr_k=10:11.4632,lwr_k=20:10.9097,lwr_k=30:10.9146,lwr_k=40:10.7342,lwr_k=50:10.5909,lwr_k=100:10.4967,lwr_k=200:10.4388,lwr_k=300:10.6204,lwr_k=400:10.6947,lwr_k=500:10.7197,lwr_k=600:10.7366,lwr_k=700:10.7546,lwr_k=800:10.7608,lwr_k=900:10.7728,lwr_k=1000:10.7928'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:15.6987,lwr_k=10:8.658,lwr_k=20:8.6098,lwr_k=30:8.6329,lwr_k=40:9.4438,lwr_k=50:9.7364,lwr_k=100:10.3479,lwr_k=200:11.1796,lwr_k=300:11.7097,lwr_k=400:12.0266,lwr_k=500:12.2385,lwr_k=600:12.3868,lwr_k=700:12.5134,lwr_k=800:12.6994,lwr_k=900:12.823,lwr_k=1000:12.9441'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.4628,lwr_k=10:8.2492,lwr_k=20:7.857,lwr_k=30:7.7179,lwr_k=40:7.7296,lwr_k=50:7.9382,lwr_k=100:8.3054,lwr_k=200:8.894,lwr_k=300:8.9606,lwr_k=400:8.8421,lwr_k=500:8.8492,lwr_k=600:8.9097,lwr_k=700:8.9143,lwr_k=800:8.9163,lwr_k=900:8.9154,lwr_k=1000:9.04'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:15.273,lwr_k=10:8.7118,lwr_k=20:9.6368,lwr_k=30:10.2305,lwr_k=40:10.8098,lwr_k=50:11.1832,lwr_k=100:12.5894,lwr_k=200:13.7166,lwr_k=300:13.9301,lwr_k=400:13.9579,lwr_k=500:13.9654,lwr_k=600:14.009,lwr_k=700:14.0427,lwr_k=800:14.0885,lwr_k=900:14.1537,lwr_k=1000:14.1983'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:12.8538,lwr_k=10:13.7466,lwr_k=20:12.6224,lwr_k=30:12.127,lwr_k=40:12.4213,lwr_k=50:11.8105,lwr_k=100:11.4878,lwr_k=200:11.6626,lwr_k=300:11.7689,lwr_k=400:11.7676,lwr_k=500:11.8099,lwr_k=600:11.8408,lwr_k=700:11.8888,lwr_k=800:11.9149,lwr_k=900:11.9733,lwr_k=1000:12.0259'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.1518,lwr_k=10:6.4889,lwr_k=20:6.6988,lwr_k=30:6.8356,lwr_k=40:6.8983,lwr_k=50:6.9767,lwr_k=100:7.0844,lwr_k=200:7.3138,lwr_k=300:7.4735,lwr_k=400:7.5477,lwr_k=500:7.5622,lwr_k=600:7.6027,lwr_k=700:7.6342,lwr_k=800:7.6765,lwr_k=900:7.7086,lwr_k=1000:7.7333'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:28.0043,lwr_k=10:20.1234,lwr_k=20:19.8101,lwr_k=30:20.0188,lwr_k=40:20.3495,lwr_k=50:20.5153,lwr_k=100:22.9249,lwr_k=200:25.959,lwr_k=300:27.0467,lwr_k=400:27.56,lwr_k=500:27.5835,lwr_k=600:27.5569,lwr_k=700:27.55,lwr_k=800:27.5109,lwr_k=900:27.483,lwr_k=1000:27.4336'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:8.4786,lwr_k=10:6.631,lwr_k=20:6.8935,lwr_k=30:7.1731,lwr_k=40:7.214,lwr_k=50:7.315,lwr_k=100:7.3886,lwr_k=200:7.6582,lwr_k=300:7.7153,lwr_k=400:7.7752,lwr_k=500:7.8233,lwr_k=600:7.8531,lwr_k=700:7.8668,lwr_k=800:7.8883,lwr_k=900:7.9059,lwr_k=1000:7.9275'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.2894,lwr_k=10:11.0724,lwr_k=20:10.8162,lwr_k=30:10.5485,lwr_k=40:10.742,lwr_k=50:10.4751,lwr_k=100:10.354,lwr_k=200:11.2071,lwr_k=300:11.2805,lwr_k=400:11.2919,lwr_k=500:11.3405,lwr_k=600:11.3941,lwr_k=700:11.434,lwr_k=800:11.4271,lwr_k=900:11.4626,lwr_k=1000:11.4492'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_46'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.5481,lwr_k=10:0.0301,lwr_k=20:1.7844,lwr_k=30:3.3612,lwr_k=40:4.407,lwr_k=50:5.0054,lwr_k=100:6.4131,lwr_k=200:7.3013,lwr_k=300:7.6395,lwr_k=400:7.7876,lwr_k=500:7.9102,lwr_k=600:8.0591,lwr_k=700:8.1655,lwr_k=800:8.2673,lwr_k=900:8.3201,lwr_k=1000:8.4102'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.9411,lwr_k=10:910.7603,lwr_k=20:62953.5867,lwr_k=30:6059.2708,lwr_k=40:787.8482,lwr_k=50:332.6063,lwr_k=100:13.2118,lwr_k=200:8.7166,lwr_k=300:8.5276,lwr_k=400:8.6441,lwr_k=500:8.7151,lwr_k=600:8.7657,lwr_k=700:8.8828,lwr_k=800:8.9493,lwr_k=900:8.966,lwr_k=1000:9.0409'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.8362,lwr_k=10:0.1596,lwr_k=20:9.581,lwr_k=30:6.285,lwr_k=40:5.8716,lwr_k=50:5.9076,lwr_k=100:6.7667,lwr_k=200:7.2547,lwr_k=300:7.5523,lwr_k=400:7.7752,lwr_k=500:7.8738,lwr_k=600:7.9814,lwr_k=700:8.0867,lwr_k=800:8.189,lwr_k=900:8.2615,lwr_k=1000:8.3137'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.348,lwr_k=10:153228323.5615,lwr_k=20:3246348.154,lwr_k=30:44198.7893,lwr_k=40:1331.4539,lwr_k=50:233.1961,lwr_k=100:327456.203,lwr_k=200:9.1019,lwr_k=300:8.7883,lwr_k=400:8.8318,lwr_k=500:8.8638,lwr_k=600:8.9734,lwr_k=700:9.0741,lwr_k=800:9.0702,lwr_k=900:9.0785,lwr_k=1000:9.1448'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:15.5903,lwr_k=10:0.0414,lwr_k=20:4.204,lwr_k=30:4.8636,lwr_k=40:6.2414,lwr_k=50:7.4249,lwr_k=100:9.4733,lwr_k=200:11.2635,lwr_k=300:12.0464,lwr_k=400:12.3498,lwr_k=500:12.735,lwr_k=600:12.8589,lwr_k=700:12.9061,lwr_k=800:13.0203,lwr_k=900:13.1097,lwr_k=1000:13.1833'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.9522,lwr_k=10:1331.1399,lwr_k=20:853742.86,lwr_k=30:934.1004,lwr_k=40:43319.1043,lwr_k=50:19957.7836,lwr_k=100:13.4575,lwr_k=200:2583041.3821,lwr_k=300:8.6775,lwr_k=400:8.6478,lwr_k=500:8.6467,lwr_k=600:8.686,lwr_k=700:8.7792,lwr_k=800:8.7848,lwr_k=900:8.8056,lwr_k=1000:8.8259'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.6779,lwr_k=10:0.072,lwr_k=20:15.0363,lwr_k=30:6.5705,lwr_k=40:5.8132,lwr_k=50:6.1148,lwr_k=100:6.4209,lwr_k=200:6.9609,lwr_k=300:7.1145,lwr_k=400:7.2513,lwr_k=500:7.3197,lwr_k=600:7.3974,lwr_k=700:7.4259,lwr_k=800:7.4464,lwr_k=900:7.4754,lwr_k=1000:7.5027'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.3349,lwr_k=10:128331.3681,lwr_k=20:425376.5411,lwr_k=30:8333928.4889,lwr_k=40:224.0155,lwr_k=50:17.096,lwr_k=100:11.1924,lwr_k=200:10.8756,lwr_k=300:10.8687,lwr_k=400:10.7401,lwr_k=500:10.762,lwr_k=600:10.8129,lwr_k=700:10.8127,lwr_k=800:10.8559,lwr_k=900:10.8887,lwr_k=1000:10.8795'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.0194,lwr_k=10:0.159,lwr_k=20:15.4121,lwr_k=30:4.9198,lwr_k=40:4.835,lwr_k=50:5.208,lwr_k=100:5.8139,lwr_k=200:6.386,lwr_k=300:6.6094,lwr_k=400:6.6921,lwr_k=500:6.7733,lwr_k=600:6.7904,lwr_k=700:6.8467,lwr_k=800:6.8958,lwr_k=900:6.9426,lwr_k=1000:7.0233'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:22.5272,lwr_k=10:43692.2679,lwr_k=20:171857.7756,lwr_k=30:337432.6313,lwr_k=40:26554.4892,lwr_k=50:18895.5616,lwr_k=100:21.9276,lwr_k=200:13215.1766,lwr_k=300:7436.3766,lwr_k=400:10524.7897,lwr_k=500:6680.1247,lwr_k=600:574.025,lwr_k=700:681.5674,lwr_k=800:37.8747,lwr_k=900:158.0222,lwr_k=1000:38.6771'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:11.055,lwr_k=10:7.6096,lwr_k=20:12.7675,lwr_k=30:8.1436,lwr_k=40:7.0578,lwr_k=50:6.9579,lwr_k=100:7.4416,lwr_k=200:7.6998,lwr_k=300:8.0642,lwr_k=400:8.2035,lwr_k=500:8.2899,lwr_k=600:8.3331,lwr_k=700:8.3852,lwr_k=800:8.4702,lwr_k=900:8.5414,lwr_k=1000:8.5412'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.3291,lwr_k=10:19419913.3295,lwr_k=20:2945.3409,lwr_k=30:8852.5563,lwr_k=40:1375.7666,lwr_k=50:210.3427,lwr_k=100:12.3497,lwr_k=200:11.5254,lwr_k=300:11.2614,lwr_k=400:11.2627,lwr_k=500:11.1269,lwr_k=600:11.1537,lwr_k=700:11.1409,lwr_k=800:11.2157,lwr_k=900:11.1528,lwr_k=1000:11.2256'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_47'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:508.9616,lwr_k=10:204.6077,lwr_k=20:198.9498,lwr_k=30:198.6535,lwr_k=40:198.6569,lwr_k=50:508.9616,lwr_k=100:199.279,lwr_k=200:199.0935,lwr_k=300:66820759.3048,lwr_k=400:508.9616,lwr_k=500:198.9108,lwr_k=600:508.9616,lwr_k=700:508.9616,lwr_k=800:198.6691,lwr_k=900:16633418.1332,lwr_k=1000:198.7115'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:609.2243,lwr_k=10:269.7306,lwr_k=20:259.6836,lwr_k=30:258.0686,lwr_k=40:258.2741,lwr_k=50:609.2243,lwr_k=100:260.5833,lwr_k=200:260.1028,lwr_k=300:66801890.2554,lwr_k=400:609.2243,lwr_k=500:259.5589,lwr_k=600:609.2243,lwr_k=700:609.2243,lwr_k=800:258.4357,lwr_k=900:16624033.7398,lwr_k=1000:258.7442'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:239.63,lwr_k=10:247.3429,lwr_k=20:240.4074,lwr_k=30:239.7266,lwr_k=40:239.7889,lwr_k=50:239.963,lwr_k=100:240.9029,lwr_k=200:240.6315,lwr_k=300:240.6368,lwr_k=400:240.4618,lwr_k=500:240.3435,lwr_k=600:240.0839,lwr_k=700:239.901,lwr_k=800:239.8447,lwr_k=900:239.7993,lwr_k=1000:239.9648'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:202.8212,lwr_k=10:208.8718,lwr_k=20:203.0709,lwr_k=30:202.7317,lwr_k=40:202.7416,lwr_k=50:202.809,lwr_k=100:203.4186,lwr_k=200:203.2236,lwr_k=300:203.2273,lwr_k=400:203.107,lwr_k=500:203.0291,lwr_k=600:202.872,lwr_k=700:202.7808,lwr_k=800:202.7587,lwr_k=900:202.7443,lwr_k=1000:202.8098'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:273.725,lwr_k=10:286.0596,lwr_k=20:279.7698,lwr_k=30:284.7255,lwr_k=40:280.9807,lwr_k=50:276.729,lwr_k=100:279.4766,lwr_k=200:277.9348,lwr_k=300:274.6314,lwr_k=400:274.8378,lwr_k=500:274.0095,lwr_k=600:274.1698,lwr_k=700:274.138,lwr_k=800:273.9476,lwr_k=900:273.8063,lwr_k=1000:273.8585'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:155.1861,lwr_k=10:158.9139,lwr_k=20:155.2057,lwr_k=30:158.0586,lwr_k=40:155.8406,lwr_k=50:153.9426,lwr_k=100:155.0605,lwr_k=200:154.3677,lwr_k=300:153.7594,lwr_k=400:153.7137,lwr_k=500:154.1635,lwr_k=600:153.9965,lwr_k=700:154.0242,lwr_k=800:154.2526,lwr_k=900:154.5687,lwr_k=1000:154.4242'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:260.9945,lwr_k=10:262.217,lwr_k=20:261.0202,lwr_k=30:263.0856,lwr_k=40:266.939,lwr_k=50:264.9451,lwr_k=100:263.9088,lwr_k=200:261.6572,lwr_k=300:261.0101,lwr_k=400:261.2484,lwr_k=500:261.1934,lwr_k=600:261.0599,lwr_k=700:261.1737,lwr_k=800:261.0575,lwr_k=900:260.9964,lwr_k=1000:261.0034'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:238.6588,lwr_k=10:238.9688,lwr_k=20:238.5508,lwr_k=30:239.5148,lwr_k=40:242.5303,lwr_k=50:240.9189,lwr_k=100:240.1185,lwr_k=200:238.6266,lwr_k=300:238.5676,lwr_k=400:238.4823,lwr_k=500:238.4767,lwr_k=600:238.5057,lwr_k=700:238.4763,lwr_k=800:238.5073,lwr_k=900:238.699,lwr_k=1000:238.5869'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:204.7755,lwr_k=10:219.8131,lwr_k=20:204.8632,lwr_k=30:206.8097,lwr_k=40:207.3487,lwr_k=50:524.87,lwr_k=100:205.2779,lwr_k=200:524.87,lwr_k=300:524.87,lwr_k=400:524.87,lwr_k=500:524.87,lwr_k=600:204.9166,lwr_k=700:204.8148,lwr_k=800:524.87,lwr_k=900:524.87,lwr_k=1000:524.87'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:325.6202,lwr_k=10:334.7116,lwr_k=20:326.2393,lwr_k=30:325.5078,lwr_k=40:325.7711,lwr_k=50:673.6287,lwr_k=100:327.2914,lwr_k=200:673.6287,lwr_k=300:673.6287,lwr_k=400:673.6287,lwr_k=500:673.6287,lwr_k=600:326.415,lwr_k=700:326.0414,lwr_k=800:673.6287,lwr_k=900:673.6287,lwr_k=1000:673.6287'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:234.9846,lwr_k=10:235.0093,lwr_k=20:234.9969,lwr_k=30:234.9861,lwr_k=40:237.5148,lwr_k=50:235.1171,lwr_k=100:240.3455,lwr_k=200:235.6702,lwr_k=300:235.296,lwr_k=400:235.1004,lwr_k=500:235.1938,lwr_k=600:235.2334,lwr_k=700:235.1785,lwr_k=800:235.0655,lwr_k=900:235.0112,lwr_k=1000:235.0368'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:237.4568,lwr_k=10:237.4385,lwr_k=20:237.4387,lwr_k=30:237.4689,lwr_k=40:240.4231,lwr_k=50:237.486,lwr_k=100:242.1812,lwr_k=200:237.9155,lwr_k=300:237.6155,lwr_k=400:237.4745,lwr_k=500:237.5301,lwr_k=600:237.5691,lwr_k=700:237.5304,lwr_k=800:237.4599,lwr_k=900:237.4388,lwr_k=1000:237.4616'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_48'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.7656,lwr_k=10:3.0454,lwr_k=20:4.9782,lwr_k=30:6.1206,lwr_k=40:6.7055,lwr_k=50:7.0379,lwr_k=100:7.8699,lwr_k=200:8.3975,lwr_k=300:8.5698,lwr_k=400:8.665,lwr_k=500:8.7489,lwr_k=600:8.8486,lwr_k=700:8.9018,lwr_k=800:8.9402,lwr_k=900:8.984,lwr_k=1000:8.9986'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.2021,lwr_k=10:25797.1274,lwr_k=20:26901.7663,lwr_k=30:169.6711,lwr_k=40:162.3389,lwr_k=50:17.3897,lwr_k=100:12.0749,lwr_k=200:11.7071,lwr_k=300:36.1434,lwr_k=400:26.0198,lwr_k=500:11.1792,lwr_k=600:10.6692,lwr_k=700:10.6799,lwr_k=800:10.764,lwr_k=900:10.8607,lwr_k=1000:10.9617'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:13.9223,lwr_k=10:3.8106,lwr_k=20:7.0233,lwr_k=30:8.5044,lwr_k=40:9.1173,lwr_k=50:9.5915,lwr_k=100:10.4349,lwr_k=200:11.2157,lwr_k=300:11.6214,lwr_k=400:12.0405,lwr_k=500:12.1224,lwr_k=600:12.3516,lwr_k=700:12.522,lwr_k=800:12.5478,lwr_k=900:12.7033,lwr_k=1000:12.6967'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:13.2772,lwr_k=10:90813.4593,lwr_k=20:111849.5375,lwr_k=30:5148.4337,lwr_k=40:1977.9369,lwr_k=50:425.8244,lwr_k=100:167.9416,lwr_k=200:11.4492,lwr_k=300:11.2621,lwr_k=400:11.5636,lwr_k=500:11.5884,lwr_k=600:11.7683,lwr_k=700:11.6798,lwr_k=800:11.8108,lwr_k=900:11.8904,lwr_k=1000:11.8883'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.1658,lwr_k=10:3.4765,lwr_k=20:5.9881,lwr_k=30:6.8812,lwr_k=40:7.5023,lwr_k=50:8.0607,lwr_k=100:9.0227,lwr_k=200:9.734,lwr_k=300:9.8832,lwr_k=400:9.9752,lwr_k=500:10.0984,lwr_k=600:10.175,lwr_k=700:10.1788,lwr_k=800:10.2061,lwr_k=900:10.216,lwr_k=1000:10.2477'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.2518,lwr_k=10:174659137.7275,lwr_k=20:52380420.8665,lwr_k=30:2922.0709,lwr_k=40:1325.3398,lwr_k=50:288.7947,lwr_k=100:7.9029,lwr_k=200:7.2785,lwr_k=300:7.2085,lwr_k=400:7.1765,lwr_k=500:7.2253,lwr_k=600:7.2584,lwr_k=700:7.3078,lwr_k=800:7.3272,lwr_k=900:7.377,lwr_k=1000:7.3822'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:14.6515,lwr_k=10:3.8009,lwr_k=20:6.7832,lwr_k=30:8.161,lwr_k=40:9.1605,lwr_k=50:9.6678,lwr_k=100:10.765,lwr_k=200:11.7289,lwr_k=300:12.2048,lwr_k=400:12.4799,lwr_k=500:12.6895,lwr_k=600:12.8794,lwr_k=700:13.0284,lwr_k=800:13.1949,lwr_k=900:13.3135,lwr_k=1000:13.4106'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:15.0845,lwr_k=10:95379.5646,lwr_k=20:495849.0708,lwr_k=30:69149.792,lwr_k=40:2870.0445,lwr_k=50:19408.5,lwr_k=100:14.2222,lwr_k=200:13.4184,lwr_k=300:13.4,lwr_k=400:13.4686,lwr_k=500:13.5892,lwr_k=600:13.6265,lwr_k=700:13.6151,lwr_k=800:13.6142,lwr_k=900:13.5803,lwr_k=1000:13.6317'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.3677,lwr_k=10:2.661,lwr_k=20:5.4696,lwr_k=30:6.3726,lwr_k=40:6.8697,lwr_k=50:7.1514,lwr_k=100:7.6531,lwr_k=200:7.9975,lwr_k=300:8.1807,lwr_k=400:8.266,lwr_k=500:8.3682,lwr_k=600:8.4046,lwr_k=700:8.4845,lwr_k=800:8.5299,lwr_k=900:8.5786,lwr_k=1000:8.6007'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:30.0394,lwr_k=10:50819609459.4175,lwr_k=20:207435845.2926,lwr_k=30:31997.7517,lwr_k=40:66959.5254,lwr_k=50:1020.6848,lwr_k=100:23.9082,lwr_k=200:24.8282,lwr_k=300:25.6544,lwr_k=400:26.1583,lwr_k=500:26.6405,lwr_k=600:26.9071,lwr_k=700:27.2016,lwr_k=800:27.4018,lwr_k=900:27.7245,lwr_k=1000:27.742'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:21.8061,lwr_k=10:4.2499,lwr_k=20:8.5357,lwr_k=30:10.3035,lwr_k=40:11.135,lwr_k=50:11.7663,lwr_k=100:13.3325,lwr_k=200:14.8805,lwr_k=300:15.6088,lwr_k=400:16.1785,lwr_k=500:16.5213,lwr_k=600:16.7663,lwr_k=700:17.0022,lwr_k=800:17.2273,lwr_k=900:17.2964,lwr_k=1000:17.4926'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:19.6753,lwr_k=10:5991.1278,lwr_k=20:231280.6623,lwr_k=30:24.8883,lwr_k=40:561.3215,lwr_k=50:17.7913,lwr_k=100:16.1778,lwr_k=200:15.7884,lwr_k=300:15.698,lwr_k=400:15.8624,lwr_k=500:15.9578,lwr_k=600:16.0259,lwr_k=700:16.118,lwr_k=800:16.1951,lwr_k=900:16.1666,lwr_k=1000:16.3216'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_49'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.9114,lwr_k=10:0.0,lwr_k=20:0.0007,lwr_k=30:1.6622,lwr_k=40:2.8209,lwr_k=50:3.4817,lwr_k=100:4.8015,lwr_k=200:5.3775,lwr_k=300:5.6443,lwr_k=400:5.8293,lwr_k=500:5.9583,lwr_k=600:6.0391,lwr_k=700:6.1454,lwr_k=800:6.203,lwr_k=900:6.2398,lwr_k=1000:6.3001'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.1833,lwr_k=10:49.3756,lwr_k=20:1349.2491,lwr_k=30:33.7198,lwr_k=40:18.0408,lwr_k=50:13.0721,lwr_k=100:8.5768,lwr_k=200:7.6268,lwr_k=300:7.6093,lwr_k=400:7.5317,lwr_k=500:7.5178,lwr_k=600:7.4916,lwr_k=700:7.6045,lwr_k=800:7.5544,lwr_k=900:7.585,lwr_k=1000:7.6868'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.8024,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.8561,lwr_k=40:3.0011,lwr_k=50:3.6366,lwr_k=100:4.9515,lwr_k=200:5.7478,lwr_k=300:6.1077,lwr_k=400:6.2742,lwr_k=500:6.477,lwr_k=600:6.6051,lwr_k=700:6.6916,lwr_k=800:6.8156,lwr_k=900:6.8718,lwr_k=1000:6.9738'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.4335,lwr_k=10:39.5397,lwr_k=20:2119.7737,lwr_k=30:24.414,lwr_k=40:13.335,lwr_k=50:10.6649,lwr_k=100:7.8192,lwr_k=200:7.3012,lwr_k=300:7.5007,lwr_k=400:7.5298,lwr_k=500:7.5244,lwr_k=600:7.6717,lwr_k=700:7.7187,lwr_k=800:7.7438,lwr_k=900:7.8065,lwr_k=1000:7.8177'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.5696,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.7854,lwr_k=40:2.9969,lwr_k=50:3.7861,lwr_k=100:5.263,lwr_k=200:6.408,lwr_k=300:7.0084,lwr_k=400:7.2696,lwr_k=500:7.4535,lwr_k=600:7.5844,lwr_k=700:7.6812,lwr_k=800:7.7825,lwr_k=900:7.8871,lwr_k=1000:7.9681'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.4884,lwr_k=10:47.3572,lwr_k=20:339.3221,lwr_k=30:16.0171,lwr_k=40:9.2524,lwr_k=50:8.4266,lwr_k=100:6.2265,lwr_k=200:5.7551,lwr_k=300:5.9114,lwr_k=400:5.971,lwr_k=500:6.0599,lwr_k=600:6.1147,lwr_k=700:6.1997,lwr_k=800:6.236,lwr_k=900:6.2947,lwr_k=1000:6.3508'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.8186,lwr_k=10:0.0,lwr_k=20:0.0004,lwr_k=30:1.7461,lwr_k=40:2.7224,lwr_k=50:3.4088,lwr_k=100:4.5555,lwr_k=200:5.2776,lwr_k=300:5.6035,lwr_k=400:5.6775,lwr_k=500:5.7578,lwr_k=600:5.8964,lwr_k=700:5.9241,lwr_k=800:5.9791,lwr_k=900:6.0186,lwr_k=1000:6.0439'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.1306,lwr_k=10:74.6523,lwr_k=20:1133.3902,lwr_k=30:35.8786,lwr_k=40:19.1685,lwr_k=50:13.5615,lwr_k=100:11.9744,lwr_k=200:10.3597,lwr_k=300:10.0523,lwr_k=400:9.8388,lwr_k=500:9.6798,lwr_k=600:9.6047,lwr_k=700:9.5253,lwr_k=800:9.3173,lwr_k=900:9.2435,lwr_k=1000:9.2317'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.7549,lwr_k=10:0.0,lwr_k=20:0.0001,lwr_k=30:1.4932,lwr_k=40:2.4034,lwr_k=50:3.0374,lwr_k=100:4.1051,lwr_k=200:4.6638,lwr_k=300:4.8681,lwr_k=400:4.988,lwr_k=500:5.0592,lwr_k=600:5.1163,lwr_k=700:5.1773,lwr_k=800:5.2299,lwr_k=900:5.2637,lwr_k=1000:5.3012'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:20.1962,lwr_k=10:53.3912,lwr_k=20:523.8076,lwr_k=30:47.4606,lwr_k=40:24.5137,lwr_k=50:20.8106,lwr_k=100:16.8659,lwr_k=200:16.9086,lwr_k=300:17.4628,lwr_k=400:17.363,lwr_k=500:17.5673,lwr_k=600:17.6781,lwr_k=700:17.9404,lwr_k=800:18.0664,lwr_k=900:18.1495,lwr_k=1000:18.4179'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:7.0213,lwr_k=10:0.0,lwr_k=20:0.1085,lwr_k=30:1.8079,lwr_k=40:2.9182,lwr_k=50:3.5575,lwr_k=100:4.8234,lwr_k=200:5.6095,lwr_k=300:5.8828,lwr_k=400:6.0064,lwr_k=500:6.0599,lwr_k=600:6.0992,lwr_k=700:6.1374,lwr_k=800:6.1755,lwr_k=900:6.2121,lwr_k=1000:6.2252'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.4294,lwr_k=10:445.506,lwr_k=20:1022042.4355,lwr_k=30:48.2912,lwr_k=40:24.1764,lwr_k=50:112.2318,lwr_k=100:9.3333,lwr_k=200:8.3288,lwr_k=300:8.2074,lwr_k=400:8.1777,lwr_k=500:8.2576,lwr_k=600:8.1222,lwr_k=700:8.0664,lwr_k=800:8.0591,lwr_k=900:8.0422,lwr_k=1000:8.0485'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_50'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.2766,lwr_k=10:5.1828,lwr_k=20:6.5026,lwr_k=30:6.9903,lwr_k=40:7.0971,lwr_k=50:7.2652,lwr_k=100:7.5496,lwr_k=200:7.8451,lwr_k=300:8.0376,lwr_k=400:8.2357,lwr_k=500:8.3726,lwr_k=600:8.4704,lwr_k=700:8.5715,lwr_k=800:8.6803,lwr_k=900:8.7468,lwr_k=1000:8.804'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.209,lwr_k=10:11.3243,lwr_k=20:9.1102,lwr_k=30:8.8579,lwr_k=40:8.7722,lwr_k=50:8.8263,lwr_k=100:8.549,lwr_k=200:8.9131,lwr_k=300:9.1697,lwr_k=400:9.4339,lwr_k=500:9.5993,lwr_k=600:9.7582,lwr_k=700:9.9103,lwr_k=800:10.0169,lwr_k=900:10.1236,lwr_k=1000:10.2129'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:20.2423,lwr_k=10:8.7804,lwr_k=20:10.8019,lwr_k=30:11.3941,lwr_k=40:11.7904,lwr_k=50:12.0022,lwr_k=100:12.8801,lwr_k=200:13.7174,lwr_k=300:14.3332,lwr_k=400:14.783,lwr_k=500:15.134,lwr_k=600:15.5351,lwr_k=700:15.8523,lwr_k=800:16.0691,lwr_k=900:16.2533,lwr_k=1000:16.4548'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:16.201,lwr_k=10:15.9692,lwr_k=20:12.9937,lwr_k=30:12.6991,lwr_k=40:12.7351,lwr_k=50:12.6692,lwr_k=100:12.6175,lwr_k=200:12.6789,lwr_k=300:13.0695,lwr_k=400:13.3355,lwr_k=500:13.3417,lwr_k=600:13.3301,lwr_k=700:13.3975,lwr_k=800:13.5269,lwr_k=900:13.5979,lwr_k=1000:13.701'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:21.6833,lwr_k=10:9.3549,lwr_k=20:10.9858,lwr_k=30:11.7561,lwr_k=40:12.4114,lwr_k=50:12.9824,lwr_k=100:13.6738,lwr_k=200:14.5952,lwr_k=300:15.2563,lwr_k=400:15.6368,lwr_k=500:16.0515,lwr_k=600:16.391,lwr_k=700:16.6832,lwr_k=800:16.8819,lwr_k=900:17.0946,lwr_k=1000:17.3194'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:14.3479,lwr_k=10:15.3975,lwr_k=20:13.0058,lwr_k=30:12.8686,lwr_k=40:12.3756,lwr_k=50:12.0452,lwr_k=100:11.7137,lwr_k=200:11.4438,lwr_k=300:11.2675,lwr_k=400:11.5327,lwr_k=500:11.5477,lwr_k=600:11.544,lwr_k=700:11.4794,lwr_k=800:11.4793,lwr_k=900:11.5356,lwr_k=1000:11.5616'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:14.0309,lwr_k=10:6.7208,lwr_k=20:7.7452,lwr_k=30:8.1061,lwr_k=40:8.476,lwr_k=50:8.71,lwr_k=100:9.3378,lwr_k=200:10.1146,lwr_k=300:10.642,lwr_k=400:10.938,lwr_k=500:11.1682,lwr_k=600:11.3916,lwr_k=700:11.5632,lwr_k=800:11.656,lwr_k=900:11.8051,lwr_k=1000:11.9125'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:15.2621,lwr_k=10:14.9334,lwr_k=20:13.0465,lwr_k=30:12.5945,lwr_k=40:12.0704,lwr_k=50:12.1197,lwr_k=100:12.2665,lwr_k=200:12.1749,lwr_k=300:12.4116,lwr_k=400:12.53,lwr_k=500:12.6087,lwr_k=600:12.6778,lwr_k=700:12.7848,lwr_k=800:12.8618,lwr_k=900:12.9167,lwr_k=1000:13.0123'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.3846,lwr_k=10:5.129,lwr_k=20:6.2745,lwr_k=30:6.6578,lwr_k=40:6.8191,lwr_k=50:7.1306,lwr_k=100:7.3392,lwr_k=200:7.7009,lwr_k=300:7.8587,lwr_k=400:8.0049,lwr_k=500:8.1307,lwr_k=600:8.1768,lwr_k=700:8.2524,lwr_k=800:8.3237,lwr_k=900:8.3695,lwr_k=1000:8.4117'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:31.8958,lwr_k=10:18.1072,lwr_k=20:17.8051,lwr_k=30:18.3056,lwr_k=40:18.6983,lwr_k=50:20.9403,lwr_k=100:22.3013,lwr_k=200:24.9455,lwr_k=300:26.067,lwr_k=400:26.8376,lwr_k=500:27.4753,lwr_k=600:27.8642,lwr_k=700:28.1477,lwr_k=800:28.5267,lwr_k=900:28.7899,lwr_k=1000:29.0672'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:11.4296,lwr_k=10:5.1631,lwr_k=20:6.39,lwr_k=30:6.7135,lwr_k=40:7.0005,lwr_k=50:7.1553,lwr_k=100:7.742,lwr_k=200:8.3118,lwr_k=300:8.6671,lwr_k=400:8.9005,lwr_k=500:9.1029,lwr_k=600:9.2471,lwr_k=700:9.3518,lwr_k=800:9.4749,lwr_k=900:9.5409,lwr_k=1000:9.6267'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.5639,lwr_k=10:13.3642,lwr_k=20:11.0427,lwr_k=30:11.0194,lwr_k=40:11.0428,lwr_k=50:10.7325,lwr_k=100:10.1633,lwr_k=200:10.3159,lwr_k=300:10.5314,lwr_k=400:10.696,lwr_k=500:10.8561,lwr_k=600:10.9108,lwr_k=700:10.9824,lwr_k=800:11.0472,lwr_k=900:11.1054,lwr_k=1000:11.1768'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_51'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.3574,lwr_k=10:0.0394,lwr_k=20:0.8953,lwr_k=30:1.8678,lwr_k=40:2.6264,lwr_k=50:2.9992,lwr_k=100:3.7724,lwr_k=200:4.329,lwr_k=300:4.5685,lwr_k=400:4.7087,lwr_k=500:4.7664,lwr_k=600:4.7999,lwr_k=700:4.8464,lwr_k=800:4.8966,lwr_k=900:4.9353,lwr_k=1000:4.9754'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.2607,lwr_k=10:50.8999,lwr_k=20:2509972.2398,lwr_k=30:32.168,lwr_k=40:10.4144,lwr_k=50:10.5351,lwr_k=100:11.2338,lwr_k=200:6.8478,lwr_k=300:6.6645,lwr_k=400:6.6828,lwr_k=500:6.6413,lwr_k=600:6.6041,lwr_k=700:6.6065,lwr_k=800:6.5965,lwr_k=900:6.6704,lwr_k=1000:6.687'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.2179,lwr_k=10:0.0559,lwr_k=20:0.8973,lwr_k=30:2.0473,lwr_k=40:2.7139,lwr_k=50:3.1472,lwr_k=100:4.1872,lwr_k=200:4.949,lwr_k=300:5.2562,lwr_k=400:5.4489,lwr_k=500:5.5689,lwr_k=600:5.6186,lwr_k=700:5.7216,lwr_k=800:5.7606,lwr_k=900:5.8029,lwr_k=1000:5.8283'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.012,lwr_k=10:64.5431,lwr_k=20:482.7166,lwr_k=30:37.5583,lwr_k=40:11.1868,lwr_k=50:11.6269,lwr_k=100:8.3129,lwr_k=200:8.0898,lwr_k=300:87.9227,lwr_k=400:8.1992,lwr_k=500:8.3106,lwr_k=600:8.3841,lwr_k=700:8.4138,lwr_k=800:8.4087,lwr_k=900:8.481,lwr_k=1000:8.4872'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:273.725,lwr_k=10:286.0596,lwr_k=20:279.7698,lwr_k=30:284.7255,lwr_k=40:280.9807,lwr_k=50:276.729,lwr_k=100:279.4766,lwr_k=200:277.9348,lwr_k=300:274.6314,lwr_k=400:274.8378,lwr_k=500:274.0095,lwr_k=600:274.1698,lwr_k=700:274.138,lwr_k=800:273.9476,lwr_k=900:273.8063,lwr_k=1000:273.8585'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:155.1861,lwr_k=10:158.9139,lwr_k=20:155.2057,lwr_k=30:158.0586,lwr_k=40:155.8406,lwr_k=50:153.9426,lwr_k=100:155.0605,lwr_k=200:154.3677,lwr_k=300:153.7594,lwr_k=400:153.7137,lwr_k=500:154.1635,lwr_k=600:153.9965,lwr_k=700:154.0242,lwr_k=800:154.2526,lwr_k=900:154.5687,lwr_k=1000:154.4242'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:261.7626,lwr_k=10:266.5313,lwr_k=20:270.2479,lwr_k=30:262.3543,lwr_k=40:262.3381,lwr_k=50:262.026,lwr_k=100:262.8406,lwr_k=200:262.9699,lwr_k=300:261.776,lwr_k=400:262.0761,lwr_k=500:261.9114,lwr_k=600:262.041,lwr_k=700:262.0314,lwr_k=800:261.8857,lwr_k=900:261.7905,lwr_k=1000:261.7883'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:239.3147,lwr_k=10:242.2668,lwr_k=20:245.3766,lwr_k=30:239.2665,lwr_k=40:239.2591,lwr_k=50:239.1512,lwr_k=100:239.5289,lwr_k=200:239.6079,lwr_k=300:239.4243,lwr_k=400:239.1624,lwr_k=500:239.1426,lwr_k=600:239.1542,lwr_k=700:239.1522,lwr_k=800:239.1459,lwr_k=900:239.2038,lwr_k=1000:239.2071'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.6734,lwr_k=10:0.0407,lwr_k=20:0.8294,lwr_k=30:2.0571,lwr_k=40:2.6701,lwr_k=50:3.181,lwr_k=100:4.0328,lwr_k=200:4.587,lwr_k=300:4.8476,lwr_k=400:4.9777,lwr_k=500:5.0822,lwr_k=600:5.1591,lwr_k=700:5.2538,lwr_k=800:5.2826,lwr_k=900:5.2952,lwr_k=1000:5.3159'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:17.8238,lwr_k=10:77.0083,lwr_k=20:274.6301,lwr_k=30:88.8954,lwr_k=40:14.4968,lwr_k=50:18.9049,lwr_k=100:14.768,lwr_k=200:14.3002,lwr_k=300:14.8207,lwr_k=400:15.655,lwr_k=500:15.9273,lwr_k=600:16.3432,lwr_k=700:16.5269,lwr_k=800:16.628,lwr_k=900:16.6977,lwr_k=1000:16.8142'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:235.0387,lwr_k=10:267.4646,lwr_k=20:236.1389,lwr_k=30:235.2656,lwr_k=40:235.1973,lwr_k=50:235.872,lwr_k=100:238.564,lwr_k=200:235.4554,lwr_k=300:235.2143,lwr_k=400:235.0388,lwr_k=500:235.057,lwr_k=600:235.117,lwr_k=700:235.3828,lwr_k=800:235.1301,lwr_k=900:235.081,lwr_k=1000:235.0417'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:237.5599,lwr_k=10:271.5158,lwr_k=20:238.3782,lwr_k=30:237.6587,lwr_k=40:237.6114,lwr_k=50:238.1478,lwr_k=100:240.5807,lwr_k=200:237.8031,lwr_k=300:237.6228,lwr_k=400:237.5576,lwr_k=500:237.5418,lwr_k=600:237.563,lwr_k=700:237.7463,lwr_k=800:237.57,lwr_k=900:237.5469,lwr_k=1000:237.5773'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_52'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:17.1349,lwr_k=10:0.6374,lwr_k=20:3.8513,lwr_k=30:5.591,lwr_k=40:6.5477,lwr_k=50:7.2783,lwr_k=100:8.9962,lwr_k=200:10.1453,lwr_k=300:10.801,lwr_k=400:11.3114,lwr_k=500:11.6338,lwr_k=600:11.8709,lwr_k=700:12.2376,lwr_k=800:12.4018,lwr_k=900:12.6268,lwr_k=1000:12.886'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:18.7556,lwr_k=10:3056.8088,lwr_k=20:51.9457,lwr_k=30:20.141,lwr_k=40:15.0894,lwr_k=50:12.9826,lwr_k=100:11.4315,lwr_k=200:11.2719,lwr_k=300:11.555,lwr_k=400:12.0396,lwr_k=500:12.3333,lwr_k=600:12.6945,lwr_k=700:13.0649,lwr_k=800:13.282,lwr_k=900:13.5684,lwr_k=1000:13.8456'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:31.5189,lwr_k=10:0.0004,lwr_k=20:6.5475,lwr_k=30:10.2105,lwr_k=40:11.9193,lwr_k=50:13.2823,lwr_k=100:16.4868,lwr_k=200:19.6335,lwr_k=300:21.4355,lwr_k=400:22.5825,lwr_k=500:23.2097,lwr_k=600:23.8347,lwr_k=700:24.2504,lwr_k=800:24.6367,lwr_k=900:25.009,lwr_k=1000:25.3346'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:26.6402,lwr_k=10:773.0019,lwr_k=20:47.6252,lwr_k=30:34.378,lwr_k=40:25.0474,lwr_k=50:21.4367,lwr_k=100:17.3228,lwr_k=200:17.7402,lwr_k=300:18.5325,lwr_k=400:19.2815,lwr_k=500:19.8235,lwr_k=600:20.4111,lwr_k=700:20.7808,lwr_k=800:21.1287,lwr_k=900:21.6632,lwr_k=1000:21.9705'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:116.829,lwr_k=10:0.0004,lwr_k=20:10.8095,lwr_k=30:18.2535,lwr_k=40:22.2956,lwr_k=50:25.8326,lwr_k=100:36.5991,lwr_k=200:51.3837,lwr_k=300:59.5911,lwr_k=400:65.5134,lwr_k=500:71.4204,lwr_k=600:75.0218,lwr_k=700:78.3265,lwr_k=800:81.602,lwr_k=900:84.2302,lwr_k=1000:86.288'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:86.9918,lwr_k=10:801.2078,lwr_k=20:57.7785,lwr_k=30:38.317,lwr_k=40:34.6891,lwr_k=50:29.7834,lwr_k=100:32.5405,lwr_k=200:39.4675,lwr_k=300:45.5153,lwr_k=400:49.2684,lwr_k=500:52.6027,lwr_k=600:55.6727,lwr_k=700:57.9359,lwr_k=800:60.0225,lwr_k=900:61.9143,lwr_k=1000:63.0647'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:132.0164,lwr_k=10:0.0065,lwr_k=20:13.0978,lwr_k=30:19.9846,lwr_k=40:23.8549,lwr_k=50:28.0701,lwr_k=100:39.9094,lwr_k=200:55.2789,lwr_k=300:66.7171,lwr_k=400:75.1533,lwr_k=500:81.9576,lwr_k=600:86.5385,lwr_k=700:90.2089,lwr_k=800:93.2551,lwr_k=900:96.1585,lwr_k=1000:99.4405'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:110.9466,lwr_k=10:2209.4383,lwr_k=20:89.7867,lwr_k=30:54.9825,lwr_k=40:44.5482,lwr_k=50:42.4372,lwr_k=100:40.7734,lwr_k=200:48.8485,lwr_k=300:54.2873,lwr_k=400:60.3453,lwr_k=500:66.4108,lwr_k=600:71.1697,lwr_k=700:73.9195,lwr_k=800:77.0948,lwr_k=900:80.047,lwr_k=1000:82.5472'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:112.5557,lwr_k=10:0.0007,lwr_k=20:12.5612,lwr_k=30:18.3247,lwr_k=40:22.9637,lwr_k=50:26.0199,lwr_k=100:35.4374,lwr_k=200:46.4777,lwr_k=300:53.8633,lwr_k=400:59.6683,lwr_k=500:64.9624,lwr_k=600:69.2045,lwr_k=700:72.9136,lwr_k=800:76.8907,lwr_k=900:79.9525,lwr_k=1000:82.4821'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:193.1765,lwr_k=10:757.5344,lwr_k=20:94.831,lwr_k=30:78.6469,lwr_k=40:73.7171,lwr_k=50:72.4905,lwr_k=100:76.3979,lwr_k=200:91.0544,lwr_k=300:100.7373,lwr_k=400:110.1492,lwr_k=500:118.5663,lwr_k=600:125.9038,lwr_k=700:130.7657,lwr_k=800:136.0542,lwr_k=900:141.0047,lwr_k=1000:145.6714'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:17.4298,lwr_k=10:0.0,lwr_k=20:4.2244,lwr_k=30:6.5318,lwr_k=40:7.9528,lwr_k=50:8.6173,lwr_k=100:10.4005,lwr_k=200:11.791,lwr_k=300:12.5253,lwr_k=400:13.0285,lwr_k=500:13.2967,lwr_k=600:13.5333,lwr_k=700:13.6899,lwr_k=800:13.8389,lwr_k=900:13.9427,lwr_k=1000:14.0837'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:17.8321,lwr_k=10:1044.1884,lwr_k=20:27.7269,lwr_k=30:19.8681,lwr_k=40:16.0029,lwr_k=50:14.5502,lwr_k=100:14.4181,lwr_k=200:14.6263,lwr_k=300:14.5985,lwr_k=400:14.7248,lwr_k=500:14.7567,lwr_k=600:14.7839,lwr_k=700:14.8708,lwr_k=800:14.894,lwr_k=900:15.0614,lwr_k=1000:15.1686'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_53'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.533,lwr_k=10:4.0037,lwr_k=20:5.1189,lwr_k=30:6.4808,lwr_k=40:7.0085,lwr_k=50:7.8372,lwr_k=100:9.044,lwr_k=200:9.5287,lwr_k=300:9.7913,lwr_k=400:9.9737,lwr_k=500:10.0305,lwr_k=600:10.1626,lwr_k=700:10.2864,lwr_k=800:10.3717,lwr_k=900:10.3633,lwr_k=1000:10.3791'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:14.6168,lwr_k=10:593270977.0157,lwr_k=20:795875493.1373,lwr_k=30:10761464.2408,lwr_k=40:8164125.3635,lwr_k=50:6703456.7504,lwr_k=100:257901.2566,lwr_k=200:19586.173,lwr_k=300:53729.8985,lwr_k=400:4571.7014,lwr_k=500:2057.9886,lwr_k=600:4971.1757,lwr_k=700:9894.1115,lwr_k=800:10799.4068,lwr_k=900:212.9738,lwr_k=1000:41.0573'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.6789,lwr_k=10:3.16,lwr_k=20:5.5822,lwr_k=30:6.1595,lwr_k=40:7.0148,lwr_k=50:7.1982,lwr_k=100:7.7093,lwr_k=200:8.4094,lwr_k=300:8.6618,lwr_k=400:8.8843,lwr_k=500:9.0498,lwr_k=600:9.1333,lwr_k=700:9.1685,lwr_k=800:9.2196,lwr_k=900:9.2562,lwr_k=1000:9.2794'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.9185,lwr_k=10:368587.9966,lwr_k=20:643035.3519,lwr_k=30:119831.9502,lwr_k=40:282493.2328,lwr_k=50:365215.5213,lwr_k=100:50.0446,lwr_k=200:121.3066,lwr_k=300:116.2833,lwr_k=400:10.6426,lwr_k=500:50.0185,lwr_k=600:63.0055,lwr_k=700:43.5754,lwr_k=800:14.9449,lwr_k=900:45.0786,lwr_k=1000:26.1095'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:148.0236,lwr_k=10:0.0,lwr_k=20:8.8637,lwr_k=30:18.4962,lwr_k=40:24.7474,lwr_k=50:29.6206,lwr_k=100:45.4708,lwr_k=200:58.917,lwr_k=300:66.4416,lwr_k=400:71.5566,lwr_k=500:75.5305,lwr_k=600:79.6369,lwr_k=700:83.5063,lwr_k=800:86.9721,lwr_k=900:88.7055,lwr_k=1000:90.9069'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:123.5852,lwr_k=10:564.7732,lwr_k=20:170100.4838,lwr_k=30:14617.5911,lwr_k=40:478.2713,lwr_k=50:75.3505,lwr_k=100:52.9014,lwr_k=200:56.8247,lwr_k=300:60.6794,lwr_k=400:63.3664,lwr_k=500:67.6049,lwr_k=600:68.9638,lwr_k=700:70.9943,lwr_k=800:73.4555,lwr_k=900:76.5101,lwr_k=1000:79.2833'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:80.2831,lwr_k=10:0.0,lwr_k=20:4.735,lwr_k=30:10.4644,lwr_k=40:13.2264,lwr_k=50:15.5294,lwr_k=100:21.9554,lwr_k=200:27.9994,lwr_k=300:31.1853,lwr_k=400:34.052,lwr_k=500:37.3002,lwr_k=600:39.6114,lwr_k=700:41.5921,lwr_k=800:43.597,lwr_k=900:44.9849,lwr_k=1000:46.4818'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:81.6029,lwr_k=10:1022.2995,lwr_k=20:120371.374,lwr_k=30:79.29,lwr_k=40:55.388,lwr_k=50:45.977,lwr_k=100:36.9049,lwr_k=200:36.1722,lwr_k=300:39.3762,lwr_k=400:41.2586,lwr_k=500:43.9405,lwr_k=600:45.7702,lwr_k=700:47.31,lwr_k=800:49.0694,lwr_k=900:50.1474,lwr_k=1000:51.1743'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:93.028,lwr_k=10:0.0,lwr_k=20:7.0704,lwr_k=30:15.6008,lwr_k=40:21.131,lwr_k=50:24.5497,lwr_k=100:35.2567,lwr_k=200:43.7037,lwr_k=300:49.2715,lwr_k=400:54.263,lwr_k=500:57.4822,lwr_k=600:60.6316,lwr_k=700:62.0113,lwr_k=800:63.5362,lwr_k=900:65.5243,lwr_k=1000:66.836'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:194.8998,lwr_k=10:3884.0163,lwr_k=20:11612.6411,lwr_k=30:8193.2267,lwr_k=40:125.7551,lwr_k=50:1292.7209,lwr_k=100:1418.9147,lwr_k=200:202.009,lwr_k=300:95.0178,lwr_k=400:114.1386,lwr_k=500:171.2685,lwr_k=600:640.7545,lwr_k=700:1818.9254,lwr_k=800:1425.6722,lwr_k=900:1503.1474,lwr_k=1000:1487.1323'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:127.9513,lwr_k=10:0.0,lwr_k=20:8.5727,lwr_k=30:19.2278,lwr_k=40:24.95,lwr_k=50:29.2477,lwr_k=100:39.8753,lwr_k=200:49.1109,lwr_k=300:55.1958,lwr_k=400:60.3625,lwr_k=500:64.4737,lwr_k=600:67.9482,lwr_k=700:70.6401,lwr_k=800:72.8535,lwr_k=900:75.1768,lwr_k=1000:76.737'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:133.0315,lwr_k=10:556.8245,lwr_k=20:1737.3713,lwr_k=30:205.5656,lwr_k=40:171.7274,lwr_k=50:119.6674,lwr_k=100:72.2507,lwr_k=200:61.6876,lwr_k=300:66.5635,lwr_k=400:70.0938,lwr_k=500:72.4967,lwr_k=600:74.7661,lwr_k=700:78.371,lwr_k=800:80.51,lwr_k=900:82.4262,lwr_k=1000:82.8142'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_54'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.2613,lwr_k=10:0.0004,lwr_k=20:4.8197,lwr_k=30:3.8158,lwr_k=40:4.3389,lwr_k=50:4.8288,lwr_k=100:5.8197,lwr_k=200:6.381,lwr_k=300:6.6518,lwr_k=400:6.7727,lwr_k=500:6.8606,lwr_k=600:6.9777,lwr_k=700:7.0558,lwr_k=800:7.1046,lwr_k=900:7.1552,lwr_k=1000:7.1848'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.4464,lwr_k=10:73.4669,lwr_k=20:758669.0189,lwr_k=30:31569.9824,lwr_k=40:194.5955,lwr_k=50:156572.4726,lwr_k=100:39166.6089,lwr_k=200:9.5979,lwr_k=300:9.1873,lwr_k=400:8.9837,lwr_k=500:8.8402,lwr_k=600:8.6038,lwr_k=700:8.4686,lwr_k=800:8.5414,lwr_k=900:8.5253,lwr_k=1000:8.4772'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:31.2505,lwr_k=10:0.0,lwr_k=20:13.2473,lwr_k=30:8.2129,lwr_k=40:8.5156,lwr_k=50:9.7036,lwr_k=100:13.5619,lwr_k=200:16.6907,lwr_k=300:18.1868,lwr_k=400:19.0563,lwr_k=500:19.6767,lwr_k=600:20.1044,lwr_k=700:20.6074,lwr_k=800:21.0881,lwr_k=900:21.3739,lwr_k=1000:21.9136'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:22.0004,lwr_k=10:149.2749,lwr_k=20:8262854.9448,lwr_k=30:1888743.0048,lwr_k=40:23444.6293,lwr_k=50:6816.0668,lwr_k=100:23143.3195,lwr_k=200:14.7017,lwr_k=300:14.9977,lwr_k=400:14.6876,lwr_k=500:15.0879,lwr_k=600:15.16,lwr_k=700:15.0609,lwr_k=800:15.2054,lwr_k=900:15.3466,lwr_k=1000:15.61'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.6048,lwr_k=10:0.0,lwr_k=20:9.2375,lwr_k=30:5.1252,lwr_k=40:5.5208,lwr_k=50:6.0381,lwr_k=100:7.6631,lwr_k=200:9.2955,lwr_k=300:9.9909,lwr_k=400:10.3294,lwr_k=500:10.4289,lwr_k=600:10.5671,lwr_k=700:10.6674,lwr_k=800:10.7411,lwr_k=900:10.858,lwr_k=1000:10.8374'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.8266,lwr_k=10:126.6072,lwr_k=20:3309030.6123,lwr_k=30:1053804.8052,lwr_k=40:44995.9366,lwr_k=50:116709.6541,lwr_k=100:783.4928,lwr_k=200:524.2377,lwr_k=300:9.9916,lwr_k=400:83.2667,lwr_k=500:8.5845,lwr_k=600:10.3899,lwr_k=700:9.0268,lwr_k=800:7.9269,lwr_k=900:7.9396,lwr_k=1000:7.9253'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.09,lwr_k=10:0.0028,lwr_k=20:5.7197,lwr_k=30:3.8352,lwr_k=40:4.718,lwr_k=50:5.2836,lwr_k=100:6.8404,lwr_k=200:7.7073,lwr_k=300:8.1218,lwr_k=400:8.3086,lwr_k=500:8.5383,lwr_k=600:8.6119,lwr_k=700:8.6892,lwr_k=800:8.853,lwr_k=900:8.8026,lwr_k=1000:8.8538'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:11.9625,lwr_k=10:1073.961,lwr_k=20:2197249.9271,lwr_k=30:46791.9702,lwr_k=40:8388.3751,lwr_k=50:1577.1753,lwr_k=100:11.9829,lwr_k=200:11.5987,lwr_k=300:11.1168,lwr_k=400:11.0881,lwr_k=500:10.9355,lwr_k=600:10.9696,lwr_k=700:10.8907,lwr_k=800:10.9209,lwr_k=900:10.9128,lwr_k=1000:10.9367'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:22.9246,lwr_k=10:0.0,lwr_k=20:10.9158,lwr_k=30:7.4129,lwr_k=40:8.3715,lwr_k=50:9.4402,lwr_k=100:11.7084,lwr_k=200:13.4093,lwr_k=300:14.4354,lwr_k=400:14.8653,lwr_k=500:15.3652,lwr_k=600:15.6831,lwr_k=700:16.0814,lwr_k=800:16.3927,lwr_k=900:16.6465,lwr_k=1000:16.9732'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:57.7929,lwr_k=10:207.1966,lwr_k=20:79043551.1896,lwr_k=30:9187860.0133,lwr_k=40:89130.4721,lwr_k=50:26452.5963,lwr_k=100:7325.9665,lwr_k=200:4755.6714,lwr_k=300:18289.3131,lwr_k=400:11408.6657,lwr_k=500:2881.5169,lwr_k=600:44.4198,lwr_k=700:45.7663,lwr_k=800:46.5454,lwr_k=900:47.2967,lwr_k=1000:48.2807'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:9.6951,lwr_k=10:0.0432,lwr_k=20:4.8811,lwr_k=30:3.7962,lwr_k=40:4.4971,lwr_k=50:5.3805,lwr_k=100:6.7289,lwr_k=200:7.5048,lwr_k=300:8.0059,lwr_k=400:8.2406,lwr_k=500:8.3611,lwr_k=600:8.4357,lwr_k=700:8.484,lwr_k=800:8.6733,lwr_k=900:8.6406,lwr_k=1000:8.6885'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.7232,lwr_k=10:65275.9179,lwr_k=20:2979099.6027,lwr_k=30:822769.6806,lwr_k=40:147557.9379,lwr_k=50:77877.6205,lwr_k=100:1358.257,lwr_k=200:10.4102,lwr_k=300:9.9163,lwr_k=400:9.9677,lwr_k=500:9.8988,lwr_k=600:9.8811,lwr_k=700:9.908,lwr_k=800:9.9432,lwr_k=900:9.8862,lwr_k=1000:9.824'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_55'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.2563,lwr_k=10:0.0,lwr_k=20:0.0044,lwr_k=30:0.6706,lwr_k=40:1.8157,lwr_k=50:2.3292,lwr_k=100:3.4766,lwr_k=200:4.0071,lwr_k=300:4.271,lwr_k=400:4.3958,lwr_k=500:4.5171,lwr_k=600:4.5373,lwr_k=700:4.5834,lwr_k=800:4.6255,lwr_k=900:4.6534,lwr_k=1000:4.6812'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.0622,lwr_k=10:48.9901,lwr_k=20:19434.4167,lwr_k=30:7004842.8919,lwr_k=40:40.3677,lwr_k=50:18.1947,lwr_k=100:11.4412,lwr_k=200:9.5417,lwr_k=300:9.5596,lwr_k=400:9.3597,lwr_k=500:9.091,lwr_k=600:9.1745,lwr_k=700:9.2767,lwr_k=800:9.2941,lwr_k=900:9.2739,lwr_k=1000:9.2654'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.0869,lwr_k=10:0.0,lwr_k=20:0.0001,lwr_k=30:0.8173,lwr_k=40:2.0459,lwr_k=50:2.7363,lwr_k=100:4.3894,lwr_k=200:5.2162,lwr_k=300:5.6311,lwr_k=400:5.8979,lwr_k=500:6.0526,lwr_k=600:6.1196,lwr_k=700:6.196,lwr_k=800:6.2562,lwr_k=900:6.3447,lwr_k=1000:6.4056'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.757,lwr_k=10:61.9852,lwr_k=20:690.4152,lwr_k=30:62837.2342,lwr_k=40:1496.2204,lwr_k=50:174.2908,lwr_k=100:14.9948,lwr_k=200:10.2334,lwr_k=300:9.442,lwr_k=400:8.7481,lwr_k=500:8.2073,lwr_k=600:8.1952,lwr_k=700:8.1135,lwr_k=800:8.1681,lwr_k=900:8.1509,lwr_k=1000:8.1486'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.8816,lwr_k=10:0.0,lwr_k=20:0.0006,lwr_k=30:0.9457,lwr_k=40:2.3296,lwr_k=50:3.1406,lwr_k=100:5.0791,lwr_k=200:6.0617,lwr_k=300:6.5204,lwr_k=400:6.7304,lwr_k=500:6.9061,lwr_k=600:6.9598,lwr_k=700:7.0818,lwr_k=800:7.1586,lwr_k=900:7.2195,lwr_k=1000:7.2445'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.0133,lwr_k=10:34.3158,lwr_k=20:598.9286,lwr_k=30:11032.9072,lwr_k=40:1237.3069,lwr_k=50:16.6751,lwr_k=100:7.409,lwr_k=200:6.526,lwr_k=300:6.3648,lwr_k=400:6.1028,lwr_k=500:5.8266,lwr_k=600:5.6666,lwr_k=700:5.6437,lwr_k=800:5.6273,lwr_k=900:5.6512,lwr_k=1000:5.6397'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:14.8707,lwr_k=10:0.0,lwr_k=20:0.0103,lwr_k=30:1.173,lwr_k=40:2.9054,lwr_k=50:4.2175,lwr_k=100:7.1056,lwr_k=200:9.0331,lwr_k=300:9.8055,lwr_k=400:10.1427,lwr_k=500:10.4636,lwr_k=600:10.7954,lwr_k=700:10.9868,lwr_k=800:11.1445,lwr_k=900:11.3141,lwr_k=1000:11.476'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:15.6762,lwr_k=10:95.3032,lwr_k=20:1437.6757,lwr_k=30:71268.4523,lwr_k=40:67.3703,lwr_k=50:2693.3651,lwr_k=100:13.4967,lwr_k=200:12.7221,lwr_k=300:12.6035,lwr_k=400:12.3625,lwr_k=500:12.3343,lwr_k=600:12.6648,lwr_k=700:12.7655,lwr_k=800:12.8919,lwr_k=900:12.9308,lwr_k=1000:12.9973'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.1638,lwr_k=10:0.0,lwr_k=20:0.0002,lwr_k=30:0.7241,lwr_k=40:1.7769,lwr_k=50:2.4016,lwr_k=100:3.9918,lwr_k=200:4.8752,lwr_k=300:5.2129,lwr_k=400:5.3425,lwr_k=500:5.4278,lwr_k=600:5.5019,lwr_k=700:5.5817,lwr_k=800:5.6307,lwr_k=900:5.6461,lwr_k=1000:5.6729'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:21.6485,lwr_k=10:93.9748,lwr_k=20:7811.4344,lwr_k=30:2224154.375,lwr_k=40:51086.2267,lwr_k=50:175383.9304,lwr_k=100:84.7328,lwr_k=200:19.1676,lwr_k=300:19.0472,lwr_k=400:19.283,lwr_k=500:19.5784,lwr_k=600:19.6518,lwr_k=700:19.8601,lwr_k=800:20.0346,lwr_k=900:20.1097,lwr_k=1000:20.2091'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:9.8233,lwr_k=10:0.0,lwr_k=20:0.1188,lwr_k=30:1.2968,lwr_k=40:2.8201,lwr_k=50:3.8895,lwr_k=100:5.8902,lwr_k=200:7.1805,lwr_k=300:7.6446,lwr_k=400:7.9629,lwr_k=500:8.1247,lwr_k=600:8.2945,lwr_k=700:8.3732,lwr_k=800:8.4104,lwr_k=900:8.475,lwr_k=1000:8.5658'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:13.9498,lwr_k=10:1264.8364,lwr_k=20:291229.3811,lwr_k=30:332296.8761,lwr_k=40:153876.3051,lwr_k=50:133574.8773,lwr_k=100:19.5116,lwr_k=200:13.0406,lwr_k=300:13.0299,lwr_k=400:12.9786,lwr_k=500:13.0144,lwr_k=600:12.5238,lwr_k=700:12.5084,lwr_k=800:12.6038,lwr_k=900:12.7018,lwr_k=1000:12.7663'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_56'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.1869,lwr_k=10:0.0115,lwr_k=20:1.0753,lwr_k=30:2.0824,lwr_k=40:2.6765,lwr_k=50:3.0263,lwr_k=100:3.9199,lwr_k=200:4.3727,lwr_k=300:4.5212,lwr_k=400:4.6301,lwr_k=500:4.6423,lwr_k=600:4.7046,lwr_k=700:4.7561,lwr_k=800:4.7552,lwr_k=900:4.8086,lwr_k=1000:4.8253'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.8343,lwr_k=10:480.7049,lwr_k=20:71722964.9816,lwr_k=30:50325268.1611,lwr_k=40:479438.7109,lwr_k=50:167196.7403,lwr_k=100:4088134.5838,lwr_k=200:8.4291,lwr_k=300:7.902,lwr_k=400:61198.2478,lwr_k=500:7.844,lwr_k=600:7.8111,lwr_k=700:7.7259,lwr_k=800:7.7162,lwr_k=900:7.7025,lwr_k=1000:7.6632'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:13.8029,lwr_k=10:0.9272,lwr_k=20:10.2016,lwr_k=30:6.6993,lwr_k=40:7.0318,lwr_k=50:7.5404,lwr_k=100:9.1645,lwr_k=200:10.2104,lwr_k=300:10.6519,lwr_k=400:10.9762,lwr_k=500:11.1487,lwr_k=600:11.2751,lwr_k=700:11.5205,lwr_k=800:11.7656,lwr_k=900:11.8717,lwr_k=1000:12.0224'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:16.4704,lwr_k=10:6065248.6384,lwr_k=20:451300658.5209,lwr_k=30:123820706.9848,lwr_k=40:150858737.1475,lwr_k=50:958749.0002,lwr_k=100:1022.3868,lwr_k=200:9013.6433,lwr_k=300:13.0864,lwr_k=400:12.8999,lwr_k=500:65.0547,lwr_k=600:24.3105,lwr_k=700:26.2743,lwr_k=800:13.2372,lwr_k=900:13.3319,lwr_k=1000:12.8229'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.3779,lwr_k=10:0.0216,lwr_k=20:1.9184,lwr_k=30:3.1178,lwr_k=40:3.7541,lwr_k=50:4.3094,lwr_k=100:5.2192,lwr_k=200:5.6624,lwr_k=300:5.7648,lwr_k=400:5.7991,lwr_k=500:5.873,lwr_k=600:5.9388,lwr_k=700:5.9949,lwr_k=800:6.0194,lwr_k=900:6.0394,lwr_k=1000:6.0595'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.8526,lwr_k=10:2478.9225,lwr_k=20:101983.4584,lwr_k=30:24137.3121,lwr_k=40:55.0846,lwr_k=50:37.8364,lwr_k=100:221.5429,lwr_k=200:7.1976,lwr_k=300:6.7954,lwr_k=400:6.7226,lwr_k=500:6.6846,lwr_k=600:6.6514,lwr_k=700:6.7242,lwr_k=800:6.7539,lwr_k=900:6.721,lwr_k=1000:6.705'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.2394,lwr_k=10:0.1524,lwr_k=20:2.0805,lwr_k=30:3.541,lwr_k=40:4.4058,lwr_k=50:4.8753,lwr_k=100:6.3476,lwr_k=200:7.7828,lwr_k=300:8.3103,lwr_k=400:8.5874,lwr_k=500:8.7711,lwr_k=600:9.0229,lwr_k=700:9.2048,lwr_k=800:9.4253,lwr_k=900:9.6242,lwr_k=1000:9.7487'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:15.676,lwr_k=10:1502387841.3341,lwr_k=20:66424197.7885,lwr_k=30:111724987.9113,lwr_k=40:32519.7971,lwr_k=50:2006.672,lwr_k=100:30378.6792,lwr_k=200:99.94,lwr_k=300:13.5878,lwr_k=400:13.1605,lwr_k=500:13.0058,lwr_k=600:13.1703,lwr_k=700:13.1856,lwr_k=800:13.4846,lwr_k=900:13.5321,lwr_k=1000:13.6569'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:13.3306,lwr_k=10:0.7929,lwr_k=20:21.2911,lwr_k=30:7.5636,lwr_k=40:6.9782,lwr_k=50:6.7296,lwr_k=100:7.438,lwr_k=200:8.5959,lwr_k=300:8.8744,lwr_k=400:9.166,lwr_k=500:9.4924,lwr_k=600:10.0016,lwr_k=700:9.7967,lwr_k=800:9.962,lwr_k=900:10.2306,lwr_k=1000:10.5786'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:40.4252,lwr_k=10:2080397.1797,lwr_k=20:9056645.6386,lwr_k=30:15062.1759,lwr_k=40:62339.3108,lwr_k=50:4515.9066,lwr_k=100:1328.904,lwr_k=200:56.5163,lwr_k=300:28.1316,lwr_k=400:28.7452,lwr_k=500:29.6523,lwr_k=600:31.0998,lwr_k=700:31.1074,lwr_k=800:31.8374,lwr_k=900:32.6596,lwr_k=1000:34.5628'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:15.7316,lwr_k=10:0.065,lwr_k=20:2.1056,lwr_k=30:4.7386,lwr_k=40:6.1885,lwr_k=50:7.1985,lwr_k=100:9.5176,lwr_k=200:10.535,lwr_k=300:10.9671,lwr_k=400:11.3544,lwr_k=500:11.7686,lwr_k=600:12.0497,lwr_k=700:12.2658,lwr_k=800:12.4641,lwr_k=900:12.6475,lwr_k=1000:12.7769'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:21.0187,lwr_k=10:3748.1739,lwr_k=20:239377.0772,lwr_k=30:29015.2237,lwr_k=40:25541.4803,lwr_k=50:10777.1634,lwr_k=100:89.7044,lwr_k=200:982.0052,lwr_k=300:1729.6482,lwr_k=400:1005.2973,lwr_k=500:381.752,lwr_k=600:85.8759,lwr_k=700:17.3304,lwr_k=800:17.455,lwr_k=900:17.5526,lwr_k=1000:17.7343'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_57'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.1517,lwr_k=10:0.0001,lwr_k=20:0.392,lwr_k=30:1.7341,lwr_k=40:2.732,lwr_k=50:5.3369,lwr_k=100:7.7646,lwr_k=200:6.0369,lwr_k=300:7.2373,lwr_k=400:12.1265,lwr_k=500:12.4838,lwr_k=600:10.2148,lwr_k=700:9.4069,lwr_k=800:9.324,lwr_k=900:9.0367,lwr_k=1000:9.9245'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.6249,lwr_k=10:52457.0265,lwr_k=20:166510.4103,lwr_k=30:19501.2087,lwr_k=40:114835.3854,lwr_k=50:31865.7964,lwr_k=100:34862.3229,lwr_k=200:926.7539,lwr_k=300:778.594,lwr_k=400:3039.8758,lwr_k=500:2515.2201,lwr_k=600:3457.4806,lwr_k=700:2070.3821,lwr_k=800:1828.5757,lwr_k=900:1702.3113,lwr_k=1000:1206.6206'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.9932,lwr_k=10:0.0,lwr_k=20:0.5517,lwr_k=30:1.8979,lwr_k=40:2.4271,lwr_k=50:2.9801,lwr_k=100:4.2533,lwr_k=200:4.8427,lwr_k=300:5.0747,lwr_k=400:5.1977,lwr_k=500:5.306,lwr_k=600:5.3774,lwr_k=700:5.4211,lwr_k=800:5.4518,lwr_k=900:5.5392,lwr_k=1000:5.5741'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.5591,lwr_k=10:158.8287,lwr_k=20:23262.8599,lwr_k=30:414.8154,lwr_k=40:315.4867,lwr_k=50:2915.425,lwr_k=100:168.3652,lwr_k=200:11.5144,lwr_k=300:9.834,lwr_k=400:9.6438,lwr_k=500:9.8748,lwr_k=600:10.3239,lwr_k=700:9.3024,lwr_k=800:9.0772,lwr_k=900:8.9566,lwr_k=1000:8.6605'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.6093,lwr_k=10:0.0,lwr_k=20:0.1984,lwr_k=30:1.6265,lwr_k=40:2.5206,lwr_k=50:3.1532,lwr_k=100:4.3858,lwr_k=200:5.1908,lwr_k=300:5.4261,lwr_k=400:5.5597,lwr_k=500:5.6063,lwr_k=600:5.6592,lwr_k=700:5.7035,lwr_k=800:5.7318,lwr_k=900:5.7767,lwr_k=1000:5.8156'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.5201,lwr_k=10:190.2867,lwr_k=20:883490.6804,lwr_k=30:209962.8685,lwr_k=40:3675.3625,lwr_k=50:3426.6138,lwr_k=100:42.1568,lwr_k=200:14.4881,lwr_k=300:71.16,lwr_k=400:19.4257,lwr_k=500:36.0716,lwr_k=600:36.6747,lwr_k=700:24.6026,lwr_k=800:7.3613,lwr_k=900:6.5944,lwr_k=1000:6.616'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.713,lwr_k=10:0.0001,lwr_k=20:1.7941,lwr_k=30:2.8243,lwr_k=40:3.2077,lwr_k=50:3.5183,lwr_k=100:4.3035,lwr_k=200:8.8529,lwr_k=300:6.2641,lwr_k=400:6.9914,lwr_k=500:5.254,lwr_k=600:5.3928,lwr_k=700:5.3019,lwr_k=800:5.3067,lwr_k=900:5.498,lwr_k=1000:5.4514'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.0222,lwr_k=10:301.1224,lwr_k=20:3463465.6662,lwr_k=30:30.1325,lwr_k=40:20.7062,lwr_k=50:17.1217,lwr_k=100:109.1792,lwr_k=200:9.5551,lwr_k=300:10.857,lwr_k=400:10.5737,lwr_k=500:16.7856,lwr_k=600:10.9012,lwr_k=700:12.7713,lwr_k=800:9.2539,lwr_k=900:9.9091,lwr_k=1000:10.1803'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.6479,lwr_k=10:0.0013,lwr_k=20:1.1904,lwr_k=30:2.5574,lwr_k=40:3.2486,lwr_k=50:3.7307,lwr_k=100:4.7282,lwr_k=200:5.3605,lwr_k=300:5.4899,lwr_k=400:5.6257,lwr_k=500:5.7263,lwr_k=600:5.806,lwr_k=700:5.8797,lwr_k=800:5.9466,lwr_k=900:6.0194,lwr_k=1000:6.0717'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:18.1212,lwr_k=10:1674.4775,lwr_k=20:212.4347,lwr_k=30:95.6809,lwr_k=40:97.8381,lwr_k=50:84.7619,lwr_k=100:76.5524,lwr_k=200:20.5445,lwr_k=300:15.2712,lwr_k=400:16.5371,lwr_k=500:16.4431,lwr_k=600:16.8661,lwr_k=700:18.0088,lwr_k=800:20.8037,lwr_k=900:20.6282,lwr_k=1000:20.6555'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:8.06,lwr_k=10:0.0044,lwr_k=20:7.8223,lwr_k=30:5.65,lwr_k=40:4.9193,lwr_k=50:4.9562,lwr_k=100:5.413,lwr_k=200:5.584,lwr_k=300:5.9019,lwr_k=400:6.0196,lwr_k=500:6.1011,lwr_k=600:6.1097,lwr_k=700:6.3145,lwr_k=800:6.3352,lwr_k=900:6.4454,lwr_k=1000:6.4542'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.0113,lwr_k=10:324.3959,lwr_k=20:153.4398,lwr_k=30:72.4017,lwr_k=40:30.2045,lwr_k=50:16.4011,lwr_k=100:11.0786,lwr_k=200:10.0632,lwr_k=300:9.576,lwr_k=400:9.5238,lwr_k=500:9.4146,lwr_k=600:9.3979,lwr_k=700:9.1451,lwr_k=800:9.2415,lwr_k=900:9.2984,lwr_k=1000:9.3877'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_58'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:141.4605,lwr_k=10:55.9725,lwr_k=20:73.101,lwr_k=30:81.7465,lwr_k=40:89.1517,lwr_k=50:92.7573,lwr_k=100:104.11,lwr_k=200:111.8472,lwr_k=300:114.9626,lwr_k=400:116.68,lwr_k=500:118.1864,lwr_k=600:119.4121,lwr_k=700:120.2105,lwr_k=800:121.0481,lwr_k=900:121.9302,lwr_k=1000:122.7845'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:187.0052,lwr_k=10:186.9762,lwr_k=20:155.9922,lwr_k=30:130.8175,lwr_k=40:128.864,lwr_k=50:134.7427,lwr_k=100:140.8649,lwr_k=200:146.1914,lwr_k=300:147.5967,lwr_k=400:149.8712,lwr_k=500:152.0533,lwr_k=600:153.9413,lwr_k=700:155.0525,lwr_k=800:156.4952,lwr_k=900:157.2012,lwr_k=1000:158.7073'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:13.1119,lwr_k=10:6.3122,lwr_k=20:7.4173,lwr_k=30:7.7202,lwr_k=40:7.9943,lwr_k=50:8.286,lwr_k=100:9.471,lwr_k=200:9.6893,lwr_k=300:9.8886,lwr_k=400:10.0073,lwr_k=500:10.0599,lwr_k=600:10.1325,lwr_k=700:10.225,lwr_k=800:10.2792,lwr_k=900:10.3565,lwr_k=1000:10.4584'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.6337,lwr_k=10:183494147.9528,lwr_k=20:249796.4027,lwr_k=30:10.7461,lwr_k=40:9.7834,lwr_k=50:9.5383,lwr_k=100:9.1011,lwr_k=200:9.2314,lwr_k=300:9.2743,lwr_k=400:9.4302,lwr_k=500:9.5001,lwr_k=600:9.5472,lwr_k=700:9.6401,lwr_k=800:9.6395,lwr_k=900:9.709,lwr_k=1000:9.7934'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:192.5876,lwr_k=10:80.7266,lwr_k=20:105.8021,lwr_k=30:118.3271,lwr_k=40:122.6795,lwr_k=50:127.7222,lwr_k=100:138.9291,lwr_k=200:147.6291,lwr_k=300:152.3015,lwr_k=400:155.6207,lwr_k=500:157.3672,lwr_k=600:159.3811,lwr_k=700:160.9689,lwr_k=800:161.8345,lwr_k=900:163.1877,lwr_k=1000:164.5296'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:115.1147,lwr_k=10:101824.651,lwr_k=20:655.7557,lwr_k=30:124.8406,lwr_k=40:126.2703,lwr_k=50:89.4847,lwr_k=100:88.716,lwr_k=200:92.4589,lwr_k=300:94.3737,lwr_k=400:95.9141,lwr_k=500:96.4666,lwr_k=600:97.5136,lwr_k=700:98.1244,lwr_k=800:98.628,lwr_k=900:99.5564,lwr_k=1000:100.372'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.9043,lwr_k=10:5.8287,lwr_k=20:5.5774,lwr_k=30:5.7459,lwr_k=40:5.7715,lwr_k=50:5.9003,lwr_k=100:6.0714,lwr_k=200:6.311,lwr_k=300:6.3827,lwr_k=400:6.3948,lwr_k=500:6.4488,lwr_k=600:6.4632,lwr_k=700:6.4789,lwr_k=800:6.487,lwr_k=900:6.4939,lwr_k=1000:6.5104'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.342,lwr_k=10:313863217.4587,lwr_k=20:10.4534,lwr_k=30:9.6903,lwr_k=40:9.4035,lwr_k=50:9.3067,lwr_k=100:9.4263,lwr_k=200:9.7005,lwr_k=300:9.6576,lwr_k=400:9.7243,lwr_k=500:9.7208,lwr_k=600:9.7483,lwr_k=700:9.7609,lwr_k=800:9.7749,lwr_k=900:9.7894,lwr_k=1000:9.7836'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.5385,lwr_k=10:3.3542,lwr_k=20:4.4698,lwr_k=30:4.9247,lwr_k=40:5.3075,lwr_k=50:5.4994,lwr_k=100:5.8016,lwr_k=200:5.9846,lwr_k=300:6.0786,lwr_k=400:6.1713,lwr_k=500:6.2233,lwr_k=600:6.2533,lwr_k=700:6.2725,lwr_k=800:6.3011,lwr_k=900:6.3231,lwr_k=1000:6.3495'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:20.3837,lwr_k=10:33654150.2179,lwr_k=20:18.0914,lwr_k=30:17.5532,lwr_k=40:18.4881,lwr_k=50:19.1451,lwr_k=100:18.4679,lwr_k=200:18.7594,lwr_k=300:19.03,lwr_k=400:19.318,lwr_k=500:19.4928,lwr_k=600:19.6059,lwr_k=700:19.6441,lwr_k=800:19.7553,lwr_k=900:19.8291,lwr_k=1000:19.8999'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:8.9465,lwr_k=10:5.8747,lwr_k=20:5.7896,lwr_k=30:5.7614,lwr_k=40:6.2192,lwr_k=50:6.4082,lwr_k=100:7.0466,lwr_k=200:7.4631,lwr_k=300:7.6238,lwr_k=400:7.6946,lwr_k=500:7.7482,lwr_k=600:7.7824,lwr_k=700:7.8246,lwr_k=800:7.8746,lwr_k=900:7.9073,lwr_k=1000:7.9688'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.1142,lwr_k=10:53278769434.5438,lwr_k=20:248235518.8554,lwr_k=30:130867206.7162,lwr_k=40:99681692.7748,lwr_k=50:2441849.0446,lwr_k=100:10.9878,lwr_k=200:9.5085,lwr_k=300:9.077,lwr_k=400:9.13,lwr_k=500:9.2149,lwr_k=600:9.2486,lwr_k=700:9.286,lwr_k=800:9.2826,lwr_k=900:9.2937,lwr_k=1000:9.332'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_59'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:3.5162,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.9904,lwr_k=40:1.5833,lwr_k=50:1.9411,lwr_k=100:2.6803,lwr_k=200:3.0484,lwr_k=300:3.1503,lwr_k=400:3.2091,lwr_k=500:3.2653,lwr_k=600:3.3033,lwr_k=700:3.3369,lwr_k=800:3.3591,lwr_k=900:3.3696,lwr_k=1000:3.387'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:6.7394,lwr_k=10:24.5349,lwr_k=20:630.6482,lwr_k=30:65.3291,lwr_k=40:151.6471,lwr_k=50:90.3574,lwr_k=100:8.1726,lwr_k=200:6.9734,lwr_k=300:7.039,lwr_k=400:6.9087,lwr_k=500:6.8366,lwr_k=600:6.8685,lwr_k=700:6.6851,lwr_k=800:6.7973,lwr_k=900:6.7719,lwr_k=1000:6.7767'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:3.6425,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.9034,lwr_k=40:1.54,lwr_k=50:1.9116,lwr_k=100:2.7476,lwr_k=200:3.1321,lwr_k=300:3.2857,lwr_k=400:3.3475,lwr_k=500:3.3853,lwr_k=600:3.4105,lwr_k=700:3.4128,lwr_k=800:3.4308,lwr_k=900:3.4616,lwr_k=1000:3.4707'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:6.7302,lwr_k=10:34.0414,lwr_k=20:537.0086,lwr_k=30:323.7151,lwr_k=40:285.6883,lwr_k=50:44.9148,lwr_k=100:9.0629,lwr_k=200:7.1243,lwr_k=300:6.9203,lwr_k=400:6.8353,lwr_k=500:6.8602,lwr_k=600:6.8396,lwr_k=700:6.8087,lwr_k=800:6.7777,lwr_k=900:6.7707,lwr_k=1000:6.7326'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.7113,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.0218,lwr_k=40:1.9395,lwr_k=50:2.4362,lwr_k=100:3.385,lwr_k=200:3.9981,lwr_k=300:4.1326,lwr_k=400:4.2062,lwr_k=500:4.2844,lwr_k=600:4.3398,lwr_k=700:4.3599,lwr_k=800:4.3733,lwr_k=900:4.3912,lwr_k=1000:4.4117'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:5.9772,lwr_k=10:25.5965,lwr_k=20:921.6083,lwr_k=30:572.0271,lwr_k=40:320.083,lwr_k=50:91.0999,lwr_k=100:31.5965,lwr_k=200:5.9581,lwr_k=300:5.9126,lwr_k=400:5.8521,lwr_k=500:5.848,lwr_k=600:5.8569,lwr_k=700:5.8411,lwr_k=800:5.8351,lwr_k=900:5.81,lwr_k=1000:5.8027'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:3.602,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.0285,lwr_k=40:1.5971,lwr_k=50:1.9912,lwr_k=100:2.7203,lwr_k=200:3.0993,lwr_k=300:3.2438,lwr_k=400:3.3255,lwr_k=500:3.3904,lwr_k=600:3.4136,lwr_k=700:3.4481,lwr_k=800:3.4705,lwr_k=900:3.4683,lwr_k=1000:3.4844'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.9404,lwr_k=10:51.0211,lwr_k=20:12556.1198,lwr_k=30:4432.0282,lwr_k=40:1792.5947,lwr_k=50:226.8515,lwr_k=100:11.8061,lwr_k=200:8.711,lwr_k=300:8.6622,lwr_k=400:8.703,lwr_k=500:8.7796,lwr_k=600:8.8153,lwr_k=700:8.8072,lwr_k=800:8.8274,lwr_k=900:8.8534,lwr_k=1000:8.8789'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.981,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.1838,lwr_k=40:1.9589,lwr_k=50:2.4175,lwr_k=100:3.2657,lwr_k=200:3.8512,lwr_k=300:4.0345,lwr_k=400:4.1889,lwr_k=500:4.2773,lwr_k=600:4.3315,lwr_k=700:4.391,lwr_k=800:4.4187,lwr_k=900:4.4615,lwr_k=1000:4.51'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:23.9383,lwr_k=10:52.1444,lwr_k=20:362.0836,lwr_k=30:47.0634,lwr_k=40:44.8065,lwr_k=50:17.8517,lwr_k=100:18.5292,lwr_k=200:18.3196,lwr_k=300:18.8398,lwr_k=400:19.2972,lwr_k=500:19.6594,lwr_k=600:19.7052,lwr_k=700:20.0971,lwr_k=800:20.4612,lwr_k=900:20.7489,lwr_k=1000:20.9886'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:3.6236,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.0565,lwr_k=40:1.6031,lwr_k=50:2.0411,lwr_k=100:2.7439,lwr_k=200:3.1333,lwr_k=300:3.2716,lwr_k=400:3.3628,lwr_k=500:3.4083,lwr_k=600:3.4386,lwr_k=700:3.472,lwr_k=800:3.4894,lwr_k=900:3.5101,lwr_k=1000:3.5182'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.6662,lwr_k=10:31.9641,lwr_k=20:477.8205,lwr_k=30:37.2373,lwr_k=40:16.0744,lwr_k=50:9.0478,lwr_k=100:6.6084,lwr_k=200:6.637,lwr_k=300:6.8375,lwr_k=400:6.9752,lwr_k=500:7.0881,lwr_k=600:7.1727,lwr_k=700:7.2584,lwr_k=800:7.3039,lwr_k=900:7.3279,lwr_k=1000:7.3609'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_60'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.2082,lwr_k=10:0.0861,lwr_k=20:2.3247,lwr_k=30:3.3085,lwr_k=40:3.7871,lwr_k=50:4.032,lwr_k=100:4.7015,lwr_k=200:5.1856,lwr_k=300:5.4172,lwr_k=400:5.5289,lwr_k=500:5.6306,lwr_k=600:5.6879,lwr_k=700:5.7529,lwr_k=800:5.7903,lwr_k=900:5.8613,lwr_k=1000:5.8816'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.8277,lwr_k=10:4621.6249,lwr_k=20:631.778,lwr_k=30:267.9349,lwr_k=40:154.9641,lwr_k=50:200.2572,lwr_k=100:10.377,lwr_k=200:8.3112,lwr_k=300:8.1294,lwr_k=400:7.861,lwr_k=500:7.8409,lwr_k=600:7.8146,lwr_k=700:7.8817,lwr_k=800:7.8288,lwr_k=900:7.7989,lwr_k=1000:7.7695'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.8489,lwr_k=10:0.0018,lwr_k=20:2.5826,lwr_k=30:3.6606,lwr_k=40:4.1791,lwr_k=50:4.658,lwr_k=100:5.2134,lwr_k=200:5.6789,lwr_k=300:5.8007,lwr_k=400:5.9176,lwr_k=500:6.0143,lwr_k=600:6.0875,lwr_k=700:6.1975,lwr_k=800:6.249,lwr_k=900:6.3059,lwr_k=1000:6.3477'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.7095,lwr_k=10:199873.479,lwr_k=20:23.3729,lwr_k=30:12.8565,lwr_k=40:12.1862,lwr_k=50:8.0416,lwr_k=100:7.0551,lwr_k=200:6.8371,lwr_k=300:6.9059,lwr_k=400:7.0543,lwr_k=500:7.1983,lwr_k=600:7.2285,lwr_k=700:7.23,lwr_k=800:7.168,lwr_k=900:7.2,lwr_k=1000:7.1756'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.8612,lwr_k=10:0.035,lwr_k=20:2.868,lwr_k=30:4.0986,lwr_k=40:4.8308,lwr_k=50:5.2864,lwr_k=100:6.0417,lwr_k=200:6.5611,lwr_k=300:6.7309,lwr_k=400:6.823,lwr_k=500:6.9023,lwr_k=600:6.9549,lwr_k=700:6.9855,lwr_k=800:7.0256,lwr_k=900:7.0419,lwr_k=1000:7.0627'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.3683,lwr_k=10:93029.0753,lwr_k=20:81675.5806,lwr_k=30:68284.0536,lwr_k=40:1558.6122,lwr_k=50:2427.1434,lwr_k=100:1002.7198,lwr_k=200:6.6781,lwr_k=300:6.7088,lwr_k=400:6.6904,lwr_k=500:6.5908,lwr_k=600:6.5322,lwr_k=700:6.6011,lwr_k=800:6.6287,lwr_k=900:6.6556,lwr_k=1000:6.7214'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.0609,lwr_k=10:0.1604,lwr_k=20:2.825,lwr_k=30:3.8753,lwr_k=40:4.4838,lwr_k=50:4.7364,lwr_k=100:5.3913,lwr_k=200:5.7358,lwr_k=300:5.8045,lwr_k=400:5.9003,lwr_k=500:5.9676,lwr_k=600:6.0379,lwr_k=700:6.0899,lwr_k=800:6.0953,lwr_k=900:6.1295,lwr_k=1000:6.1485'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.3458,lwr_k=10:1675110.961,lwr_k=20:66061.1645,lwr_k=30:40389.6899,lwr_k=40:30489.3665,lwr_k=50:14764.3372,lwr_k=100:70.3772,lwr_k=200:10.0369,lwr_k=300:9.912,lwr_k=400:9.9224,lwr_k=500:9.8725,lwr_k=600:9.8294,lwr_k=700:9.8368,lwr_k=800:9.8513,lwr_k=900:9.8784,lwr_k=1000:9.9126'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.0115,lwr_k=10:0.0834,lwr_k=20:2.4502,lwr_k=30:3.3961,lwr_k=40:3.9649,lwr_k=50:4.2741,lwr_k=100:4.9769,lwr_k=200:5.3764,lwr_k=300:5.5413,lwr_k=400:5.5965,lwr_k=500:5.6621,lwr_k=600:5.7048,lwr_k=700:5.7447,lwr_k=800:5.7487,lwr_k=900:5.788,lwr_k=1000:5.8087'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:12.5093,lwr_k=10:22879.2333,lwr_k=20:136.4194,lwr_k=30:135.6432,lwr_k=40:168.3556,lwr_k=50:131.8107,lwr_k=100:69.465,lwr_k=200:118.2765,lwr_k=300:58.5462,lwr_k=400:59.813,lwr_k=500:60.0456,lwr_k=600:18.7148,lwr_k=700:26.8327,lwr_k=800:23.5191,lwr_k=900:26.9894,lwr_k=1000:32.6331'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:7.6685,lwr_k=10:0.1906,lwr_k=20:2.4715,lwr_k=30:3.878,lwr_k=40:4.4914,lwr_k=50:4.8224,lwr_k=100:5.7688,lwr_k=200:6.1786,lwr_k=300:6.3779,lwr_k=400:6.5928,lwr_k=500:6.7434,lwr_k=600:6.8164,lwr_k=700:6.8095,lwr_k=800:6.8873,lwr_k=900:6.9075,lwr_k=1000:6.9427'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.0737,lwr_k=10:13525.2431,lwr_k=20:23.6877,lwr_k=30:15.468,lwr_k=40:14.6577,lwr_k=50:12.9836,lwr_k=100:9.7206,lwr_k=200:9.1065,lwr_k=300:8.7771,lwr_k=400:9.0977,lwr_k=500:9.1415,lwr_k=600:9.1531,lwr_k=700:9.1656,lwr_k=800:9.4324,lwr_k=900:9.4319,lwr_k=1000:9.4414'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_61'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.0837,lwr_k=10:0.0,lwr_k=20:1.0653,lwr_k=30:2.7354,lwr_k=40:3.6302,lwr_k=50:4.1396,lwr_k=100:5.467,lwr_k=200:6.2795,lwr_k=300:6.6104,lwr_k=400:6.8363,lwr_k=500:6.9616,lwr_k=600:6.9696,lwr_k=700:7.0651,lwr_k=800:7.1308,lwr_k=900:7.1932,lwr_k=1000:7.2498'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.9781,lwr_k=10:44.2545,lwr_k=20:4129.4382,lwr_k=30:245.5268,lwr_k=40:171.5887,lwr_k=50:352.88,lwr_k=100:10.3975,lwr_k=200:9.8614,lwr_k=300:9.8627,lwr_k=400:9.9026,lwr_k=500:9.8485,lwr_k=600:9.8354,lwr_k=700:9.8581,lwr_k=800:9.9369,lwr_k=900:9.9747,lwr_k=1000:9.959'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:25.3377,lwr_k=10:0.0,lwr_k=20:1.8839,lwr_k=30:4.8434,lwr_k=40:6.358,lwr_k=50:7.7203,lwr_k=100:11.2954,lwr_k=200:14.0714,lwr_k=300:15.1918,lwr_k=400:16.1143,lwr_k=500:16.7713,lwr_k=600:17.2722,lwr_k=700:17.7192,lwr_k=800:18.2035,lwr_k=900:18.628,lwr_k=1000:18.9505'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:18.9025,lwr_k=10:114.7238,lwr_k=20:224.0383,lwr_k=30:32.4082,lwr_k=40:25.055,lwr_k=50:26.0531,lwr_k=100:15.7195,lwr_k=200:13.4833,lwr_k=300:13.4238,lwr_k=400:13.7423,lwr_k=500:13.7191,lwr_k=600:13.7636,lwr_k=700:13.7625,lwr_k=800:14.027,lwr_k=900:14.1267,lwr_k=1000:14.338'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:13.8376,lwr_k=10:0.0,lwr_k=20:1.1812,lwr_k=30:3.5641,lwr_k=40:4.8256,lwr_k=50:5.7215,lwr_k=100:8.0731,lwr_k=200:9.8969,lwr_k=300:10.5598,lwr_k=400:11.0359,lwr_k=500:11.2325,lwr_k=600:11.4475,lwr_k=700:11.5844,lwr_k=800:11.7157,lwr_k=900:11.848,lwr_k=1000:11.9666'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.2493,lwr_k=10:41.3187,lwr_k=20:4306.7514,lwr_k=30:26.0645,lwr_k=40:18.4937,lwr_k=50:16.201,lwr_k=100:12.7608,lwr_k=200:9.6086,lwr_k=300:9.1682,lwr_k=400:9.1152,lwr_k=500:8.9767,lwr_k=600:9.0024,lwr_k=700:8.9523,lwr_k=800:8.951,lwr_k=900:8.8614,lwr_k=1000:8.8706'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:13.6737,lwr_k=10:0.1371,lwr_k=20:3.6638,lwr_k=30:5.4408,lwr_k=40:6.4137,lwr_k=50:7.3837,lwr_k=100:9.4147,lwr_k=200:10.8561,lwr_k=300:11.4487,lwr_k=400:11.6905,lwr_k=500:11.9245,lwr_k=600:12.154,lwr_k=700:12.2863,lwr_k=800:12.321,lwr_k=900:12.3823,lwr_k=1000:12.4389'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:14.7471,lwr_k=10:3337.7612,lwr_k=20:185281.7884,lwr_k=30:2264.5516,lwr_k=40:79491.9945,lwr_k=50:47674.5495,lwr_k=100:13.3684,lwr_k=200:13.8103,lwr_k=300:13.7781,lwr_k=400:13.9968,lwr_k=500:13.9599,lwr_k=600:14.0881,lwr_k=700:13.8518,lwr_k=800:13.7269,lwr_k=900:13.7351,lwr_k=1000:13.5781'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.4126,lwr_k=10:0.0,lwr_k=20:0.5143,lwr_k=30:2.1422,lwr_k=40:3.126,lwr_k=50:3.6242,lwr_k=100:5.075,lwr_k=200:5.8321,lwr_k=300:6.1393,lwr_k=400:6.3131,lwr_k=500:6.4557,lwr_k=600:6.5456,lwr_k=700:6.6397,lwr_k=800:6.6884,lwr_k=900:6.734,lwr_k=1000:6.7603'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:23.6326,lwr_k=10:62.0299,lwr_k=20:704.5509,lwr_k=30:30.5228,lwr_k=40:22.8956,lwr_k=50:30.4018,lwr_k=100:20.6076,lwr_k=200:20.7838,lwr_k=300:20.9951,lwr_k=400:21.2851,lwr_k=500:21.3235,lwr_k=600:21.424,lwr_k=700:21.5072,lwr_k=800:21.6421,lwr_k=900:21.7934,lwr_k=1000:21.8012'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:11.5864,lwr_k=10:0.0,lwr_k=20:0.9499,lwr_k=30:2.9033,lwr_k=40:4.0812,lwr_k=50:4.9885,lwr_k=100:6.892,lwr_k=200:8.1505,lwr_k=300:8.6925,lwr_k=400:8.9708,lwr_k=500:9.2021,lwr_k=600:9.3637,lwr_k=700:9.5489,lwr_k=800:9.663,lwr_k=900:9.7657,lwr_k=1000:9.8329'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.9092,lwr_k=10:49.9413,lwr_k=20:1156.6214,lwr_k=30:27.5053,lwr_k=40:21.0575,lwr_k=50:19.8346,lwr_k=100:13.2708,lwr_k=200:11.3536,lwr_k=300:11.2326,lwr_k=400:11.4419,lwr_k=500:11.5261,lwr_k=600:11.5885,lwr_k=700:12.0658,lwr_k=800:11.7185,lwr_k=900:11.7297,lwr_k=1000:11.8013'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_62'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:17.6924,lwr_k=10:6.635,lwr_k=20:8.7668,lwr_k=30:9.5122,lwr_k=40:9.7431,lwr_k=50:10.0056,lwr_k=100:10.4906,lwr_k=200:11.0661,lwr_k=300:11.4855,lwr_k=400:11.8723,lwr_k=500:12.1729,lwr_k=600:12.3523,lwr_k=700:12.6083,lwr_k=800:12.8076,lwr_k=900:13.0112,lwr_k=1000:13.1851'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:21.4199,lwr_k=10:486464.1,lwr_k=20:11788.6099,lwr_k=30:8208.9963,lwr_k=40:13336.8774,lwr_k=50:11616.8095,lwr_k=100:12.18,lwr_k=200:12.7912,lwr_k=300:13.1772,lwr_k=400:13.7395,lwr_k=500:14.2217,lwr_k=600:14.5291,lwr_k=700:14.9439,lwr_k=800:15.225,lwr_k=900:15.5087,lwr_k=1000:15.8759'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:42.1261,lwr_k=10:10.4156,lwr_k=20:13.8458,lwr_k=30:15.372,lwr_k=40:16.2351,lwr_k=50:16.5014,lwr_k=100:18.5731,lwr_k=200:20.5848,lwr_k=300:22.0585,lwr_k=400:23.3619,lwr_k=500:24.4521,lwr_k=600:25.7537,lwr_k=700:26.8648,lwr_k=800:27.7675,lwr_k=900:28.9328,lwr_k=1000:29.9489'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:29.2982,lwr_k=10:100.8963,lwr_k=20:59.4986,lwr_k=30:19.9207,lwr_k=40:19.172,lwr_k=50:16.3015,lwr_k=100:16.111,lwr_k=200:16.1166,lwr_k=300:16.2818,lwr_k=400:16.5982,lwr_k=500:16.7909,lwr_k=600:17.008,lwr_k=700:17.435,lwr_k=800:17.8571,lwr_k=900:18.3795,lwr_k=1000:19.155'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:151.1917,lwr_k=10:7.3595,lwr_k=20:10.6067,lwr_k=30:11.5523,lwr_k=40:11.9521,lwr_k=50:12.2512,lwr_k=100:14.1839,lwr_k=200:18.7001,lwr_k=300:20.4767,lwr_k=400:21.7385,lwr_k=500:26.5261,lwr_k=600:26.7167,lwr_k=700:26.9153,lwr_k=800:27.7218,lwr_k=900:27.7847,lwr_k=1000:28.0505'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:64.672,lwr_k=10:1313659428.5816,lwr_k=20:66886380.6309,lwr_k=30:1038723.4012,lwr_k=40:1196188.6903,lwr_k=50:276676.9651,lwr_k=100:11.6403,lwr_k=200:12.8086,lwr_k=300:13.2062,lwr_k=400:13.6443,lwr_k=500:16.8037,lwr_k=600:16.9831,lwr_k=700:17.0736,lwr_k=800:17.0733,lwr_k=900:17.1074,lwr_k=1000:17.1314'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:28.0541,lwr_k=10:6.6567,lwr_k=20:9.0318,lwr_k=30:10.1076,lwr_k=40:10.7242,lwr_k=50:11.143,lwr_k=100:14.5116,lwr_k=200:16.5894,lwr_k=300:17.8976,lwr_k=400:18.9277,lwr_k=500:21.3036,lwr_k=600:22.9393,lwr_k=700:23.1333,lwr_k=800:23.5247,lwr_k=900:23.544,lwr_k=1000:23.5735'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:23.3057,lwr_k=10:37104.7124,lwr_k=20:18.478,lwr_k=30:17.3315,lwr_k=40:14.262,lwr_k=50:13.4878,lwr_k=100:12.5912,lwr_k=200:13.1309,lwr_k=300:13.8715,lwr_k=400:14.3774,lwr_k=500:16.4373,lwr_k=600:18.1314,lwr_k=700:18.2461,lwr_k=800:18.7554,lwr_k=900:18.8005,lwr_k=1000:18.835'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:22.1375,lwr_k=10:7.2056,lwr_k=20:9.2006,lwr_k=30:9.8244,lwr_k=40:10.053,lwr_k=50:10.7257,lwr_k=100:11.4131,lwr_k=200:12.5372,lwr_k=300:13.2677,lwr_k=400:13.9851,lwr_k=500:14.7882,lwr_k=600:15.4155,lwr_k=700:15.9257,lwr_k=800:16.252,lwr_k=900:16.5687,lwr_k=1000:16.8826'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:53.1399,lwr_k=10:2268.4045,lwr_k=20:29.2847,lwr_k=30:29.6013,lwr_k=40:33.3494,lwr_k=50:35.4616,lwr_k=100:25.8282,lwr_k=200:31.0634,lwr_k=300:33.0799,lwr_k=400:35.0192,lwr_k=500:37.2329,lwr_k=600:38.7764,lwr_k=700:40.1573,lwr_k=800:41.2915,lwr_k=900:42.1953,lwr_k=1000:43.1723'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:41.9996,lwr_k=10:4.609,lwr_k=20:6.1373,lwr_k=30:6.8296,lwr_k=40:7.2812,lwr_k=50:7.595,lwr_k=100:8.6879,lwr_k=200:8.9867,lwr_k=300:9.1317,lwr_k=400:9.2157,lwr_k=500:9.2858,lwr_k=600:9.3263,lwr_k=700:9.4181,lwr_k=800:9.4665,lwr_k=900:9.4868,lwr_k=1000:9.528'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:38.0007,lwr_k=10:20.1173,lwr_k=20:12.7141,lwr_k=30:11.6225,lwr_k=40:10.7049,lwr_k=50:10.7515,lwr_k=100:10.4787,lwr_k=200:10.1542,lwr_k=300:10.2195,lwr_k=400:10.2553,lwr_k=500:10.2812,lwr_k=600:10.2848,lwr_k=700:10.3338,lwr_k=800:10.3629,lwr_k=900:10.3937,lwr_k=1000:10.4296'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_63'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.9506,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0,lwr_k=40:1.0825,lwr_k=50:1.7066,lwr_k=100:3.2005,lwr_k=200:4.1323,lwr_k=300:4.4819,lwr_k=400:4.7145,lwr_k=500:4.8756,lwr_k=600:5.0172,lwr_k=700:5.0917,lwr_k=800:5.1735,lwr_k=900:5.249,lwr_k=1000:5.2865'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:6.8964,lwr_k=10:24.2817,lwr_k=20:39.0041,lwr_k=30:10600.3523,lwr_k=40:22.9043,lwr_k=50:11.9964,lwr_k=100:7.0603,lwr_k=200:6.198,lwr_k=300:6.1306,lwr_k=400:6.1278,lwr_k=500:6.1568,lwr_k=600:6.1772,lwr_k=700:6.2264,lwr_k=800:6.3034,lwr_k=900:6.3301,lwr_k=1000:6.3598'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.6546,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.008,lwr_k=40:1.1597,lwr_k=50:1.7938,lwr_k=100:3.428,lwr_k=200:4.5503,lwr_k=300:4.9768,lwr_k=400:5.1852,lwr_k=500:5.2821,lwr_k=600:5.3768,lwr_k=700:5.4529,lwr_k=800:5.5142,lwr_k=900:5.5799,lwr_k=1000:5.6531'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.8088,lwr_k=10:34.2713,lwr_k=20:72.0344,lwr_k=30:4044.6459,lwr_k=40:39.1416,lwr_k=50:17.1593,lwr_k=100:12.2773,lwr_k=200:7.7017,lwr_k=300:7.0183,lwr_k=400:6.9436,lwr_k=500:6.8831,lwr_k=600:6.8181,lwr_k=700:6.7099,lwr_k=800:6.8041,lwr_k=900:6.8123,lwr_k=1000:6.7153'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.04,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0012,lwr_k=40:1.1172,lwr_k=50:1.9633,lwr_k=100:3.6069,lwr_k=200:4.6375,lwr_k=300:5.1154,lwr_k=400:5.397,lwr_k=500:5.5948,lwr_k=600:5.7617,lwr_k=700:5.807,lwr_k=800:5.8633,lwr_k=900:5.9601,lwr_k=1000:6.0216'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.5527,lwr_k=10:17.8471,lwr_k=20:47.881,lwr_k=30:116825.2376,lwr_k=40:22.261,lwr_k=50:11.3131,lwr_k=100:6.9137,lwr_k=200:5.9435,lwr_k=300:5.7586,lwr_k=400:5.6682,lwr_k=500:5.7248,lwr_k=600:5.6891,lwr_k=700:5.7156,lwr_k=800:5.7683,lwr_k=900:5.823,lwr_k=1000:5.8633'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.138,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0002,lwr_k=40:1.1924,lwr_k=50:1.9211,lwr_k=100:3.569,lwr_k=200:4.6358,lwr_k=300:5.1012,lwr_k=400:5.3129,lwr_k=500:5.5406,lwr_k=600:5.6835,lwr_k=700:5.7749,lwr_k=800:5.8634,lwr_k=900:5.9516,lwr_k=1000:6.0171'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.5668,lwr_k=10:24.0455,lwr_k=20:59.8457,lwr_k=30:46403.9616,lwr_k=40:24.3965,lwr_k=50:15.5422,lwr_k=100:9.0473,lwr_k=200:8.2053,lwr_k=300:8.3491,lwr_k=400:8.3673,lwr_k=500:8.4643,lwr_k=600:8.6336,lwr_k=700:8.7269,lwr_k=800:8.8555,lwr_k=900:8.7915,lwr_k=1000:8.8978'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.7891,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0424,lwr_k=40:1.2324,lwr_k=50:1.7631,lwr_k=100:3.0355,lwr_k=200:3.9703,lwr_k=300:4.3323,lwr_k=400:4.579,lwr_k=500:4.7584,lwr_k=600:4.8678,lwr_k=700:4.9425,lwr_k=800:5.0251,lwr_k=900:5.0853,lwr_k=1000:5.1544'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:23.4767,lwr_k=10:41.0649,lwr_k=20:87.8733,lwr_k=30:2362.192,lwr_k=40:64.5051,lwr_k=50:48.1623,lwr_k=100:21.5614,lwr_k=200:19.701,lwr_k=300:20.3973,lwr_k=400:20.4837,lwr_k=500:20.6676,lwr_k=600:20.7767,lwr_k=700:21.0697,lwr_k=800:21.4408,lwr_k=900:21.542,lwr_k=1000:21.8025'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:6.3087,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0745,lwr_k=40:1.1538,lwr_k=50:1.799,lwr_k=100:3.3649,lwr_k=200:4.4647,lwr_k=300:4.8414,lwr_k=400:5.074,lwr_k=500:5.2323,lwr_k=600:5.3441,lwr_k=700:5.4299,lwr_k=800:5.5175,lwr_k=900:5.572,lwr_k=1000:5.6103'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.8069,lwr_k=10:34.305,lwr_k=20:88.9998,lwr_k=30:6207.0038,lwr_k=40:26.1173,lwr_k=50:20.2681,lwr_k=100:8.5565,lwr_k=200:7.5586,lwr_k=300:7.7995,lwr_k=400:7.7548,lwr_k=500:7.8043,lwr_k=600:7.9041,lwr_k=700:8.0334,lwr_k=800:8.1176,lwr_k=900:8.2217,lwr_k=1000:8.3998'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_64'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.724,lwr_k=10:0.1011,lwr_k=20:3.2861,lwr_k=30:4.0127,lwr_k=40:4.3326,lwr_k=50:4.5835,lwr_k=100:5.4208,lwr_k=200:5.9837,lwr_k=300:6.1958,lwr_k=400:6.4529,lwr_k=500:6.6217,lwr_k=600:6.8263,lwr_k=700:6.886,lwr_k=800:6.9833,lwr_k=900:7.2321,lwr_k=1000:7.3227'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.4695,lwr_k=10:1344.2517,lwr_k=20:183692.6835,lwr_k=30:426.1901,lwr_k=40:191.3551,lwr_k=50:23.1305,lwr_k=100:9.2067,lwr_k=200:8.806,lwr_k=300:8.7336,lwr_k=400:8.6909,lwr_k=500:8.6395,lwr_k=600:8.7512,lwr_k=700:8.5662,lwr_k=800:9.5787,lwr_k=900:9.489,lwr_k=1000:9.7527'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.7399,lwr_k=10:0.0229,lwr_k=20:11.3517,lwr_k=30:20.4503,lwr_k=40:12.4575,lwr_k=50:9.3786,lwr_k=100:6.3368,lwr_k=200:6.6386,lwr_k=300:6.0165,lwr_k=400:6.06,lwr_k=500:6.1677,lwr_k=600:6.2501,lwr_k=700:6.2587,lwr_k=800:6.2978,lwr_k=900:6.3053,lwr_k=1000:6.321'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.3684,lwr_k=10:2167.6286,lwr_k=20:871576869.9853,lwr_k=30:1048555881.6638,lwr_k=40:284181328.5897,lwr_k=50:37048719.8498,lwr_k=100:142.4016,lwr_k=200:10.9785,lwr_k=300:8.7699,lwr_k=400:8.1782,lwr_k=500:8.1768,lwr_k=600:8.2381,lwr_k=700:8.1359,lwr_k=800:8.1517,lwr_k=900:8.1406,lwr_k=1000:8.1435'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.605,lwr_k=10:0.0291,lwr_k=20:25.8235,lwr_k=30:54.317,lwr_k=40:22.637,lwr_k=50:16.0228,lwr_k=100:9.7433,lwr_k=200:8.713,lwr_k=300:7.4957,lwr_k=400:7.2244,lwr_k=500:7.1107,lwr_k=600:7.1054,lwr_k=700:7.1855,lwr_k=800:7.1854,lwr_k=900:7.2052,lwr_k=1000:7.2152'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.6505,lwr_k=10:810.2556,lwr_k=20:10990437.773,lwr_k=30:21196.4818,lwr_k=40:622.5813,lwr_k=50:566.4069,lwr_k=100:15.3087,lwr_k=200:8.8371,lwr_k=300:9.7606,lwr_k=400:6.6308,lwr_k=500:6.5039,lwr_k=600:6.4813,lwr_k=700:6.4575,lwr_k=800:6.4088,lwr_k=900:6.3914,lwr_k=1000:6.3549'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.147,lwr_k=10:0.1269,lwr_k=20:14.3083,lwr_k=30:12.807,lwr_k=40:8.8559,lwr_k=50:7.6924,lwr_k=100:6.3109,lwr_k=200:5.9268,lwr_k=300:6.3502,lwr_k=400:6.3632,lwr_k=500:6.4544,lwr_k=600:6.7089,lwr_k=700:6.7054,lwr_k=800:6.831,lwr_k=900:6.873,lwr_k=1000:6.9121'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.6814,lwr_k=10:36507.4443,lwr_k=20:304693009.0646,lwr_k=30:305072091.6492,lwr_k=40:16304439.7089,lwr_k=50:24.7608,lwr_k=100:15529333.736,lwr_k=200:9.8607,lwr_k=300:9.3735,lwr_k=400:9.3136,lwr_k=500:9.2371,lwr_k=600:9.6317,lwr_k=700:9.2642,lwr_k=800:9.0722,lwr_k=900:9.1392,lwr_k=1000:9.0354'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.4525,lwr_k=10:0.0035,lwr_k=20:18.9876,lwr_k=30:32.4317,lwr_k=40:16.9753,lwr_k=50:10.3288,lwr_k=100:6.0802,lwr_k=200:5.1344,lwr_k=300:5.1096,lwr_k=400:5.104,lwr_k=500:5.1775,lwr_k=600:5.1968,lwr_k=700:5.1899,lwr_k=800:5.24,lwr_k=900:5.2842,lwr_k=1000:5.3086'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:17.4344,lwr_k=10:639.9986,lwr_k=20:1911.2252,lwr_k=30:254.0194,lwr_k=40:399.0618,lwr_k=50:714.8894,lwr_k=100:73.6912,lwr_k=200:27.0175,lwr_k=300:19.1071,lwr_k=400:18.1505,lwr_k=500:16.3507,lwr_k=600:16.7569,lwr_k=700:17.7988,lwr_k=800:17.488,lwr_k=900:17.4078,lwr_k=1000:17.0412'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:7.3142,lwr_k=10:0.0034,lwr_k=20:12.3331,lwr_k=30:35.8768,lwr_k=40:15.1402,lwr_k=50:9.6844,lwr_k=100:6.4913,lwr_k=200:5.8576,lwr_k=300:5.9239,lwr_k=400:6.0038,lwr_k=500:6.0976,lwr_k=600:6.1951,lwr_k=700:6.1762,lwr_k=800:6.3023,lwr_k=900:6.3207,lwr_k=1000:6.4451'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.7962,lwr_k=10:4586.3045,lwr_k=20:90733.7867,lwr_k=30:298867.4209,lwr_k=40:17389.41,lwr_k=50:783.2634,lwr_k=100:33.6158,lwr_k=200:15.842,lwr_k=300:13.2949,lwr_k=400:10.7775,lwr_k=500:8.8512,lwr_k=600:8.7235,lwr_k=700:8.5023,lwr_k=800:8.2958,lwr_k=900:8.2284,lwr_k=1000:8.305'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_65'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.638,lwr_k=10:3.1252,lwr_k=20:6.4214,lwr_k=30:7.6305,lwr_k=40:8.0681,lwr_k=50:8.357,lwr_k=100:9.1436,lwr_k=200:9.5663,lwr_k=300:9.7446,lwr_k=400:9.8394,lwr_k=500:9.8941,lwr_k=600:9.9731,lwr_k=700:10.05,lwr_k=800:10.0599,lwr_k=900:10.1162,lwr_k=1000:10.134'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:15.2954,lwr_k=10:38.4211,lwr_k=20:17.119,lwr_k=30:15.7617,lwr_k=40:14.8016,lwr_k=50:14.5866,lwr_k=100:13.5574,lwr_k=200:13.5445,lwr_k=300:13.831,lwr_k=400:13.7455,lwr_k=500:13.8931,lwr_k=600:13.9318,lwr_k=700:14.0859,lwr_k=800:14.1325,lwr_k=900:14.2367,lwr_k=1000:14.268'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:3.1622,lwr_k=10:0.9621,lwr_k=20:2.1284,lwr_k=30:2.4889,lwr_k=40:2.629,lwr_k=50:2.6926,lwr_k=100:2.9473,lwr_k=200:3.0702,lwr_k=300:3.0941,lwr_k=400:3.0962,lwr_k=500:3.098,lwr_k=600:3.0987,lwr_k=700:3.1054,lwr_k=800:3.1087,lwr_k=900:3.1149,lwr_k=1000:3.1175'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:6.3374,lwr_k=10:26.7472,lwr_k=20:12.1003,lwr_k=30:8.6489,lwr_k=40:7.0361,lwr_k=50:6.7806,lwr_k=100:6.3379,lwr_k=200:6.2392,lwr_k=300:6.2495,lwr_k=400:6.2531,lwr_k=500:6.2733,lwr_k=600:6.2985,lwr_k=700:6.301,lwr_k=800:6.2996,lwr_k=900:6.3062,lwr_k=1000:6.3133'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.3793,lwr_k=10:1.7552,lwr_k=20:3.7078,lwr_k=30:4.3707,lwr_k=40:4.5718,lwr_k=50:4.7361,lwr_k=100:4.9874,lwr_k=200:5.105,lwr_k=300:5.1749,lwr_k=400:5.2026,lwr_k=500:5.2202,lwr_k=600:5.2254,lwr_k=700:5.2448,lwr_k=800:5.2547,lwr_k=900:5.2635,lwr_k=1000:5.2627'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.6572,lwr_k=10:31.8714,lwr_k=20:11.1246,lwr_k=30:7.9546,lwr_k=40:7.5173,lwr_k=50:7.0059,lwr_k=100:6.4631,lwr_k=200:6.3768,lwr_k=300:6.3676,lwr_k=400:6.4036,lwr_k=500:6.445,lwr_k=600:6.448,lwr_k=700:6.4525,lwr_k=800:6.4689,lwr_k=900:6.4768,lwr_k=1000:6.4802'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.8416,lwr_k=10:1.7499,lwr_k=20:3.739,lwr_k=30:4.357,lwr_k=40:4.6818,lwr_k=50:4.8145,lwr_k=100:5.1532,lwr_k=200:5.4087,lwr_k=300:5.5177,lwr_k=400:5.5689,lwr_k=500:5.6016,lwr_k=600:5.6175,lwr_k=700:5.6402,lwr_k=800:5.649,lwr_k=900:5.6474,lwr_k=1000:5.6623'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.922,lwr_k=10:115.2924,lwr_k=20:45.9801,lwr_k=30:82.8428,lwr_k=40:35.7983,lwr_k=50:11.9876,lwr_k=100:11.3679,lwr_k=200:10.7712,lwr_k=300:10.6514,lwr_k=400:10.6599,lwr_k=500:10.6397,lwr_k=600:10.6769,lwr_k=700:10.6899,lwr_k=800:10.7131,lwr_k=900:10.7037,lwr_k=1000:10.6735'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.5879,lwr_k=10:2.0422,lwr_k=20:4.1374,lwr_k=30:4.856,lwr_k=40:5.1423,lwr_k=50:5.4359,lwr_k=100:6.0188,lwr_k=200:6.3121,lwr_k=300:6.3997,lwr_k=400:6.4963,lwr_k=500:6.5593,lwr_k=600:6.5971,lwr_k=700:6.6382,lwr_k=800:6.6982,lwr_k=900:6.7509,lwr_k=1000:6.7913'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:28.8841,lwr_k=10:42.7694,lwr_k=20:60.799,lwr_k=30:38.969,lwr_k=40:43.6207,lwr_k=50:27.9168,lwr_k=100:23.4875,lwr_k=200:24.1462,lwr_k=300:24.1559,lwr_k=400:24.7455,lwr_k=500:25.0232,lwr_k=600:25.1763,lwr_k=700:25.4276,lwr_k=800:25.8387,lwr_k=900:26.097,lwr_k=1000:26.2893'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:10.6554,lwr_k=10:2.3223,lwr_k=20:4.8747,lwr_k=30:5.6609,lwr_k=40:6.0958,lwr_k=50:6.4131,lwr_k=100:7.2141,lwr_k=200:7.8174,lwr_k=300:8.1542,lwr_k=400:8.363,lwr_k=500:8.5147,lwr_k=600:8.6624,lwr_k=700:8.7497,lwr_k=800:8.8347,lwr_k=900:8.9196,lwr_k=1000:8.9805'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.9539,lwr_k=10:43.4953,lwr_k=20:15.1401,lwr_k=30:12.3751,lwr_k=40:12.0104,lwr_k=50:11.5815,lwr_k=100:10.9333,lwr_k=200:10.9671,lwr_k=300:11.0969,lwr_k=400:11.0831,lwr_k=500:11.0902,lwr_k=600:11.1815,lwr_k=700:11.2893,lwr_k=800:11.3496,lwr_k=900:11.3596,lwr_k=1000:11.4121'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_66'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.1755,lwr_k=10:5.5617,lwr_k=20:5.713,lwr_k=30:5.9352,lwr_k=40:5.9908,lwr_k=50:6.0031,lwr_k=100:6.2108,lwr_k=200:6.4736,lwr_k=300:6.5894,lwr_k=400:6.6333,lwr_k=500:6.6352,lwr_k=600:6.6527,lwr_k=700:6.6469,lwr_k=800:6.6611,lwr_k=900:6.659,lwr_k=1000:6.6748'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.5309,lwr_k=10:7.6911,lwr_k=20:7.2464,lwr_k=30:7.1359,lwr_k=40:7.1511,lwr_k=50:7.0205,lwr_k=100:6.9221,lwr_k=200:7.1297,lwr_k=300:7.2444,lwr_k=400:7.2458,lwr_k=500:7.2445,lwr_k=600:7.2223,lwr_k=700:7.2345,lwr_k=800:7.2427,lwr_k=900:7.2324,lwr_k=1000:7.2558'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.614,lwr_k=10:7.6115,lwr_k=20:8.0865,lwr_k=30:8.2951,lwr_k=40:8.2626,lwr_k=50:8.3261,lwr_k=100:8.6482,lwr_k=200:8.8077,lwr_k=300:8.8721,lwr_k=400:9.0935,lwr_k=500:9.1352,lwr_k=600:9.158,lwr_k=700:9.2461,lwr_k=800:9.2934,lwr_k=900:9.298,lwr_k=1000:9.3288'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:16.2387,lwr_k=10:11.9383,lwr_k=20:10.9393,lwr_k=30:10.6446,lwr_k=40:10.0818,lwr_k=50:10.0391,lwr_k=100:14.591,lwr_k=200:15.9678,lwr_k=300:15.962,lwr_k=400:15.7934,lwr_k=500:15.6858,lwr_k=600:15.673,lwr_k=700:15.5527,lwr_k=800:15.6234,lwr_k=900:15.6281,lwr_k=1000:15.6339'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:21.5042,lwr_k=10:9.0925,lwr_k=20:9.6608,lwr_k=30:9.8852,lwr_k=40:10.3076,lwr_k=50:10.6376,lwr_k=100:11.1729,lwr_k=200:11.8251,lwr_k=300:12.1846,lwr_k=400:12.5332,lwr_k=500:12.7296,lwr_k=600:12.9325,lwr_k=700:13.0853,lwr_k=800:13.3177,lwr_k=900:13.4747,lwr_k=1000:13.6326'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:13.8882,lwr_k=10:9.5237,lwr_k=20:9.0072,lwr_k=30:8.9074,lwr_k=40:8.7527,lwr_k=50:8.6581,lwr_k=100:8.6331,lwr_k=200:8.7431,lwr_k=300:8.8266,lwr_k=400:8.8008,lwr_k=500:8.8213,lwr_k=600:8.8648,lwr_k=700:8.8831,lwr_k=800:8.8887,lwr_k=900:8.929,lwr_k=1000:8.9452'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.066,lwr_k=10:6.9324,lwr_k=20:7.3323,lwr_k=30:7.5587,lwr_k=40:7.6751,lwr_k=50:7.7367,lwr_k=100:7.873,lwr_k=200:8.1813,lwr_k=300:8.2803,lwr_k=400:8.3261,lwr_k=500:8.353,lwr_k=600:8.3817,lwr_k=700:8.3991,lwr_k=800:8.4206,lwr_k=900:8.439,lwr_k=1000:8.444'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:11.8278,lwr_k=10:12.2893,lwr_k=20:11.7273,lwr_k=30:11.7208,lwr_k=40:11.3736,lwr_k=50:11.2792,lwr_k=100:11.3972,lwr_k=200:11.6279,lwr_k=300:11.695,lwr_k=400:11.7122,lwr_k=500:11.7515,lwr_k=600:11.8185,lwr_k=700:11.851,lwr_k=800:11.8878,lwr_k=900:11.9453,lwr_k=1000:11.968'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.5898,lwr_k=10:6.229,lwr_k=20:6.8264,lwr_k=30:6.9118,lwr_k=40:6.9369,lwr_k=50:6.9703,lwr_k=100:7.0448,lwr_k=200:7.115,lwr_k=300:7.1441,lwr_k=400:7.1618,lwr_k=500:7.1533,lwr_k=600:7.1625,lwr_k=700:7.1876,lwr_k=800:7.2048,lwr_k=900:7.2223,lwr_k=1000:7.245'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:21.8787,lwr_k=10:20.9042,lwr_k=20:20.1603,lwr_k=30:19.3759,lwr_k=40:19.9747,lwr_k=50:20.1662,lwr_k=100:21.3052,lwr_k=200:22.1235,lwr_k=300:22.3615,lwr_k=400:22.5264,lwr_k=500:22.5116,lwr_k=600:22.564,lwr_k=700:22.5366,lwr_k=800:22.4484,lwr_k=900:22.3995,lwr_k=1000:22.3334'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:10.572,lwr_k=10:7.4807,lwr_k=20:8.3603,lwr_k=30:8.5425,lwr_k=40:8.5677,lwr_k=50:8.7485,lwr_k=100:9.0882,lwr_k=200:9.4686,lwr_k=300:9.5278,lwr_k=400:9.5644,lwr_k=500:9.5961,lwr_k=600:9.6395,lwr_k=700:9.6575,lwr_k=800:9.6621,lwr_k=900:9.6822,lwr_k=1000:9.6983'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.8822,lwr_k=10:11.4275,lwr_k=20:11.7546,lwr_k=30:11.6598,lwr_k=40:11.1933,lwr_k=50:11.5871,lwr_k=100:11.1265,lwr_k=200:11.2079,lwr_k=300:11.2476,lwr_k=400:11.2259,lwr_k=500:11.2432,lwr_k=600:11.2352,lwr_k=700:11.2426,lwr_k=800:11.2451,lwr_k=900:11.2398,lwr_k=1000:11.2363'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_67'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.9889,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.2904,lwr_k=40:2.1698,lwr_k=50:2.6666,lwr_k=100:3.8036,lwr_k=200:4.4964,lwr_k=300:4.8821,lwr_k=400:4.9997,lwr_k=500:5.1091,lwr_k=600:5.1547,lwr_k=700:5.2047,lwr_k=800:5.2608,lwr_k=900:5.3163,lwr_k=1000:5.3494'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:6.9501,lwr_k=10:60.3673,lwr_k=20:295.1693,lwr_k=30:22.614,lwr_k=40:11.3515,lwr_k=50:8.1673,lwr_k=100:6.7174,lwr_k=200:6.3359,lwr_k=300:6.3878,lwr_k=400:6.5192,lwr_k=500:6.5628,lwr_k=600:6.5738,lwr_k=700:6.6687,lwr_k=800:6.7219,lwr_k=900:6.7088,lwr_k=1000:6.7508'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.8837,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.5038,lwr_k=40:2.5043,lwr_k=50:3.0164,lwr_k=100:4.5393,lwr_k=200:5.4916,lwr_k=300:5.8656,lwr_k=400:6.1809,lwr_k=500:6.3688,lwr_k=600:6.5127,lwr_k=700:6.6164,lwr_k=800:6.6542,lwr_k=900:6.7466,lwr_k=1000:6.7993'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.122,lwr_k=10:41.0255,lwr_k=20:892.7698,lwr_k=30:23.921,lwr_k=40:12.8641,lwr_k=50:10.6365,lwr_k=100:8.1438,lwr_k=200:7.3654,lwr_k=300:7.2374,lwr_k=400:7.0899,lwr_k=500:7.1764,lwr_k=600:7.2063,lwr_k=700:7.188,lwr_k=800:7.0743,lwr_k=900:7.1201,lwr_k=1000:7.1048'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.385,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.7213,lwr_k=40:2.6829,lwr_k=50:3.2049,lwr_k=100:4.7704,lwr_k=200:5.7962,lwr_k=300:6.2293,lwr_k=400:6.4923,lwr_k=500:6.6686,lwr_k=600:6.788,lwr_k=700:6.9063,lwr_k=800:6.9977,lwr_k=900:7.07,lwr_k=1000:7.1687'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.9656,lwr_k=10:32.7294,lwr_k=20:1171.1501,lwr_k=30:16.7798,lwr_k=40:10.3302,lwr_k=50:7.7369,lwr_k=100:5.9755,lwr_k=200:5.6855,lwr_k=300:5.8604,lwr_k=400:5.9538,lwr_k=500:6.0197,lwr_k=600:6.0212,lwr_k=700:6.084,lwr_k=800:6.0867,lwr_k=900:6.1201,lwr_k=1000:6.1764'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.0069,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.6021,lwr_k=40:2.6738,lwr_k=50:3.4461,lwr_k=100:4.6969,lwr_k=200:5.7231,lwr_k=300:6.1459,lwr_k=400:6.3786,lwr_k=500:6.5284,lwr_k=600:6.6528,lwr_k=700:6.7644,lwr_k=800:6.8061,lwr_k=900:6.8424,lwr_k=1000:6.9206'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.0939,lwr_k=10:31.6876,lwr_k=20:222.007,lwr_k=30:16.9101,lwr_k=40:13.2407,lwr_k=50:10.1729,lwr_k=100:8.7716,lwr_k=200:8.6555,lwr_k=300:8.8959,lwr_k=400:9.0582,lwr_k=500:9.1496,lwr_k=600:9.1858,lwr_k=700:9.2072,lwr_k=800:9.2257,lwr_k=900:9.2294,lwr_k=1000:9.2599'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.4927,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.4423,lwr_k=40:2.4225,lwr_k=50:2.8928,lwr_k=100:3.9728,lwr_k=200:4.7356,lwr_k=300:5.0308,lwr_k=400:5.2476,lwr_k=500:5.362,lwr_k=600:5.4653,lwr_k=700:5.5464,lwr_k=800:5.6198,lwr_k=900:5.6764,lwr_k=1000:5.7316'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:24.531,lwr_k=10:38.6925,lwr_k=20:181.3671,lwr_k=30:36.8779,lwr_k=40:23.4611,lwr_k=50:21.349,lwr_k=100:20.0674,lwr_k=200:20.1466,lwr_k=300:20.3609,lwr_k=400:20.9265,lwr_k=500:21.3327,lwr_k=600:21.5702,lwr_k=700:21.6602,lwr_k=800:21.8975,lwr_k=900:22.0408,lwr_k=1000:22.0847'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:7.6999,lwr_k=10:0.0,lwr_k=20:0.0037,lwr_k=30:1.3784,lwr_k=40:2.197,lwr_k=50:2.8696,lwr_k=100:4.1985,lwr_k=200:5.204,lwr_k=300:5.7005,lwr_k=400:6.0789,lwr_k=500:6.2809,lwr_k=600:6.4614,lwr_k=700:6.5531,lwr_k=800:6.6213,lwr_k=900:6.6867,lwr_k=1000:6.7453'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.2531,lwr_k=10:69.5195,lwr_k=20:32642.7297,lwr_k=30:236.723,lwr_k=40:93.1769,lwr_k=50:2121.5647,lwr_k=100:1433.1034,lwr_k=200:8.2445,lwr_k=300:8.3683,lwr_k=400:8.4919,lwr_k=500:8.6523,lwr_k=600:8.7211,lwr_k=700:8.8162,lwr_k=800:8.9683,lwr_k=900:9.0509,lwr_k=1000:9.176'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_68'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:198.6529,lwr_k=10:204.1162,lwr_k=20:199.5629,lwr_k=30:199.1225,lwr_k=40:200.2207,lwr_k=50:201.3046,lwr_k=100:199.0461,lwr_k=200:198.7916,lwr_k=300:198.6531,lwr_k=400:198.9246,lwr_k=500:199.04,lwr_k=600:199.3729,lwr_k=700:198.9219,lwr_k=800:198.6668,lwr_k=900:198.6696,lwr_k=1000:198.66'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:258.1252,lwr_k=10:258.1761,lwr_k=20:256.8263,lwr_k=30:257.008,lwr_k=40:256.7936,lwr_k=50:257.0062,lwr_k=100:257.0664,lwr_k=200:259.1259,lwr_k=300:258.0959,lwr_k=400:259.6039,lwr_k=500:259.9529,lwr_k=600:260.81,lwr_k=700:259.5949,lwr_k=800:258.4118,lwr_k=900:258.4411,lwr_k=1000:257.9379'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:239.6367,lwr_k=10:241.3914,lwr_k=20:240.9123,lwr_k=30:239.7128,lwr_k=40:240.4918,lwr_k=50:240.0855,lwr_k=100:239.8793,lwr_k=200:239.6506,lwr_k=300:239.9838,lwr_k=400:239.6817,lwr_k=500:239.6714,lwr_k=600:239.6373,lwr_k=700:239.6579,lwr_k=800:239.6652,lwr_k=900:239.658,lwr_k=1000:239.6594'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:202.822,lwr_k=10:203.7798,lwr_k=20:203.4181,lwr_k=30:203.064,lwr_k=40:203.1208,lwr_k=50:202.8677,lwr_k=100:202.7683,lwr_k=200:202.7651,lwr_k=300:203.5235,lwr_k=400:202.9945,lwr_k=500:202.9686,lwr_k=600:202.8361,lwr_k=700:202.9308,lwr_k=800:202.952,lwr_k=900:202.9311,lwr_k=1000:202.9354'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:273.725,lwr_k=10:277.091,lwr_k=20:276.4127,lwr_k=30:273.7799,lwr_k=40:275.7834,lwr_k=50:275.1171,lwr_k=100:274.7301,lwr_k=200:274.1188,lwr_k=300:273.7313,lwr_k=400:273.8139,lwr_k=500:273.83,lwr_k=600:273.9627,lwr_k=700:273.8578,lwr_k=800:273.8415,lwr_k=900:273.8576,lwr_k=1000:273.8542'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:155.1861,lwr_k=10:154.056,lwr_k=20:153.8561,lwr_k=30:154.667,lwr_k=40:153.7285,lwr_k=50:153.6867,lwr_k=100:153.7343,lwr_k=200:154.042,lwr_k=300:155.3861,lwr_k=400:154.5445,lwr_k=500:154.4971,lwr_k=600:154.2291,lwr_k=700:154.4258,lwr_k=800:154.4663,lwr_k=900:154.4263,lwr_k=1000:154.4345'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:261.7626,lwr_k=10:264.7799,lwr_k=20:264.1397,lwr_k=30:261.7813,lwr_k=40:263.5504,lwr_k=50:262.9339,lwr_k=100:262.5815,lwr_k=200:262.0434,lwr_k=300:261.7939,lwr_k=400:261.8028,lwr_k=500:261.8138,lwr_k=600:261.9146,lwr_k=700:261.8338,lwr_k=800:261.822,lwr_k=900:261.8337,lwr_k=1000:261.8311'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:239.3147,lwr_k=10:240.8869,lwr_k=20:240.4092,lwr_k=30:239.2198,lwr_k=40:239.9902,lwr_k=50:239.5856,lwr_k=100:239.3808,lwr_k=200:239.1547,lwr_k=300:239.493,lwr_k=400:239.1882,lwr_k=500:239.1777,lwr_k=600:239.1424,lwr_k=700:239.164,lwr_k=800:239.1714,lwr_k=900:239.164,lwr_k=1000:239.1655'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:204.8268,lwr_k=10:209.4469,lwr_k=20:205.7566,lwr_k=30:220.4386,lwr_k=40:211.6878,lwr_k=50:210.0386,lwr_k=100:207.0361,lwr_k=200:206.8144,lwr_k=300:206.1592,lwr_k=400:206.0456,lwr_k=500:205.6606,lwr_k=600:205.6755,lwr_k=700:205.5579,lwr_k=800:205.6872,lwr_k=900:205.6737,lwr_k=1000:206.1653'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:325.691,lwr_k=10:333.6621,lwr_k=20:328.1241,lwr_k=30:335.143,lwr_k=40:328.4684,lwr_k=50:327.3437,lwr_k=100:325.5831,lwr_k=200:325.4807,lwr_k=300:325.2239,lwr_k=400:325.1887,lwr_k=500:325.1013,lwr_k=600:325.1035,lwr_k=700:325.0891,lwr_k=800:325.1053,lwr_k=900:325.1032,lwr_k=1000:325.2259'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:235.0387,lwr_k=10:237.1545,lwr_k=20:236.6246,lwr_k=30:235.06,lwr_k=40:236.151,lwr_k=50:235.6784,lwr_k=100:235.4262,lwr_k=200:235.1,lwr_k=300:235.2496,lwr_k=400:235.0455,lwr_k=500:235.0419,lwr_k=600:235.0503,lwr_k=700:235.039,lwr_k=800:235.0402,lwr_k=900:235.039,lwr_k=1000:235.0392'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:237.5599,lwr_k=10:240.0665,lwr_k=20:239.4842,lwr_k=30:237.5419,lwr_k=40:238.9555,lwr_k=50:238.4145,lwr_k=100:238.1146,lwr_k=200:237.6876,lwr_k=300:237.6473,lwr_k=400:237.5446,lwr_k=500:237.5479,lwr_k=600:237.6003,lwr_k=700:237.5559,lwr_k=800:237.5509,lwr_k=900:237.5558,lwr_k=1000:237.5547'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_69'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.5689,lwr_k=10:0.0,lwr_k=20:0.6159,lwr_k=30:2.1814,lwr_k=40:2.9894,lwr_k=50:3.4197,lwr_k=100:4.7686,lwr_k=200:5.6383,lwr_k=300:6.0446,lwr_k=400:6.2839,lwr_k=500:6.4797,lwr_k=600:6.6486,lwr_k=700:6.7957,lwr_k=800:6.8697,lwr_k=900:6.9835,lwr_k=1000:7.0744'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.7731,lwr_k=10:35.3787,lwr_k=20:35.96,lwr_k=30:13.5065,lwr_k=40:9.6627,lwr_k=50:9.1549,lwr_k=100:7.0242,lwr_k=200:7.102,lwr_k=300:7.2793,lwr_k=400:7.3659,lwr_k=500:7.4765,lwr_k=600:7.5702,lwr_k=700:7.662,lwr_k=800:7.8127,lwr_k=900:7.894,lwr_k=1000:7.9903'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:21.599,lwr_k=10:0.0,lwr_k=20:0.942,lwr_k=30:2.91,lwr_k=40:4.0075,lwr_k=50:4.7385,lwr_k=100:7.5976,lwr_k=200:9.3854,lwr_k=300:10.4199,lwr_k=400:11.2751,lwr_k=500:11.8219,lwr_k=600:12.2976,lwr_k=700:12.7395,lwr_k=800:13.1228,lwr_k=900:13.5363,lwr_k=1000:13.9465'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:17.2788,lwr_k=10:100.6764,lwr_k=20:74.2555,lwr_k=30:18.8709,lwr_k=40:13.8648,lwr_k=50:12.1868,lwr_k=100:9.3298,lwr_k=200:9.4192,lwr_k=300:9.9624,lwr_k=400:10.0917,lwr_k=500:10.3533,lwr_k=600:10.6287,lwr_k=700:10.7894,lwr_k=800:10.9489,lwr_k=900:11.1732,lwr_k=1000:11.397'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.9444,lwr_k=10:0.0,lwr_k=20:0.8508,lwr_k=30:2.6178,lwr_k=40:3.5825,lwr_k=50:4.2952,lwr_k=100:5.9834,lwr_k=200:7.3831,lwr_k=300:8.1477,lwr_k=400:8.5775,lwr_k=500:8.7851,lwr_k=600:8.9573,lwr_k=700:9.0884,lwr_k=800:9.1992,lwr_k=900:9.3343,lwr_k=1000:9.4312'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.1832,lwr_k=10:33.1571,lwr_k=20:54.3189,lwr_k=30:14.7069,lwr_k=40:9.335,lwr_k=50:8.4415,lwr_k=100:6.804,lwr_k=200:6.8526,lwr_k=300:6.9658,lwr_k=400:6.944,lwr_k=500:6.9526,lwr_k=600:7.0135,lwr_k=700:7.0895,lwr_k=800:7.1239,lwr_k=900:7.1595,lwr_k=1000:7.1828'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:27.0091,lwr_k=10:0.0,lwr_k=20:0.9275,lwr_k=30:3.0657,lwr_k=40:4.4095,lwr_k=50:5.2085,lwr_k=100:8.0276,lwr_k=200:10.5655,lwr_k=300:11.8232,lwr_k=400:12.9482,lwr_k=500:13.694,lwr_k=600:14.5211,lwr_k=700:15.1253,lwr_k=800:15.687,lwr_k=900:16.2464,lwr_k=1000:16.8111'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:23.2799,lwr_k=10:78.4029,lwr_k=20:57.0207,lwr_k=30:19.2826,lwr_k=40:14.4967,lwr_k=50:12.8928,lwr_k=100:10.544,lwr_k=200:10.4565,lwr_k=300:11.1413,lwr_k=400:11.8649,lwr_k=500:12.3239,lwr_k=600:12.7182,lwr_k=700:13.1961,lwr_k=800:13.6216,lwr_k=900:13.9794,lwr_k=1000:14.322'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.6781,lwr_k=10:0.0,lwr_k=20:0.7858,lwr_k=30:2.3414,lwr_k=40:3.3171,lwr_k=50:3.944,lwr_k=100:5.1195,lwr_k=200:6.4359,lwr_k=300:6.8674,lwr_k=400:7.1348,lwr_k=500:7.3893,lwr_k=600:7.6464,lwr_k=700:7.8148,lwr_k=800:7.9543,lwr_k=900:8.0605,lwr_k=1000:8.1731'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:34.6979,lwr_k=10:175.0839,lwr_k=20:54.8214,lwr_k=30:16.9951,lwr_k=40:18.4913,lwr_k=50:19.8894,lwr_k=100:20.6732,lwr_k=200:24.0848,lwr_k=300:25.4607,lwr_k=400:26.1425,lwr_k=500:26.8789,lwr_k=600:27.5808,lwr_k=700:28.0181,lwr_k=800:28.4294,lwr_k=900:28.8443,lwr_k=1000:29.2322'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:9.5837,lwr_k=10:0.0,lwr_k=20:0.69,lwr_k=30:2.3188,lwr_k=40:3.1453,lwr_k=50:3.7736,lwr_k=100:5.229,lwr_k=200:6.2566,lwr_k=300:6.719,lwr_k=400:7.0576,lwr_k=500:7.2417,lwr_k=600:7.3685,lwr_k=700:7.4812,lwr_k=800:7.5704,lwr_k=900:7.6565,lwr_k=1000:7.767'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.4096,lwr_k=10:40.7588,lwr_k=20:54.2428,lwr_k=30:16.9656,lwr_k=40:9.9192,lwr_k=50:8.5041,lwr_k=100:8.3417,lwr_k=200:8.4199,lwr_k=300:8.7737,lwr_k=400:9.0525,lwr_k=500:9.2256,lwr_k=600:9.3823,lwr_k=700:9.4379,lwr_k=800:9.5432,lwr_k=900:9.6946,lwr_k=1000:9.785'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_70'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.811,lwr_k=10:0.3643,lwr_k=20:3.5865,lwr_k=30:195805113.3776,lwr_k=40:5.8739,lwr_k=50:6.4353,lwr_k=100:7.518,lwr_k=200:8.3209,lwr_k=300:8.715,lwr_k=400:9.0333,lwr_k=500:9.1847,lwr_k=600:9.3749,lwr_k=700:9.5476,lwr_k=800:9.6753,lwr_k=900:9.7652,lwr_k=1000:9.875'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:13.23,lwr_k=10:4983435605.2529,lwr_k=20:124679482.4509,lwr_k=30:165185816.4902,lwr_k=40:11.1881,lwr_k=50:10.0944,lwr_k=100:9.313,lwr_k=200:9.4152,lwr_k=300:9.744,lwr_k=400:10.0288,lwr_k=500:10.254,lwr_k=600:10.4339,lwr_k=700:10.6413,lwr_k=800:10.8223,lwr_k=900:10.9684,lwr_k=1000:11.1381'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:175.6977,lwr_k=10:198.0252,lwr_k=20:176.6795,lwr_k=30:177.8504,lwr_k=40:174.8883,lwr_k=50:175.3076,lwr_k=100:173.6358,lwr_k=200:173.3506,lwr_k=300:173.7557,lwr_k=400:173.473,lwr_k=500:173.9268,lwr_k=600:175.0499,lwr_k=700:175.2194,lwr_k=800:175.1517,lwr_k=900:175.1239,lwr_k=1000:175.122'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:143.2899,lwr_k=10:165.052,lwr_k=20:145.2727,lwr_k=30:146.1052,lwr_k=40:143.5618,lwr_k=50:143.6781,lwr_k=100:142.5913,lwr_k=200:142.6366,lwr_k=300:142.5017,lwr_k=400:142.4942,lwr_k=500:142.3854,lwr_k=600:142.6705,lwr_k=700:142.7114,lwr_k=800:142.7207,lwr_k=900:142.742,lwr_k=1000:142.7401'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:13.0321,lwr_k=10:0.8009,lwr_k=20:4.2516,lwr_k=30:5.4678,lwr_k=40:6.22,lwr_k=50:6.8259,lwr_k=100:7.9066,lwr_k=200:8.8363,lwr_k=300:9.2643,lwr_k=400:9.6391,lwr_k=500:9.8748,lwr_k=600:10.0298,lwr_k=700:10.1546,lwr_k=800:10.2993,lwr_k=900:10.4119,lwr_k=1000:10.5601'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.0199,lwr_k=10:14826277375.6893,lwr_k=20:64056038.2776,lwr_k=30:222438162.5925,lwr_k=40:75252.0007,lwr_k=50:14.8089,lwr_k=100:11312667.6242,lwr_k=200:7.6985,lwr_k=300:7.843,lwr_k=400:7.9959,lwr_k=500:8.0351,lwr_k=600:8.0524,lwr_k=700:8.0904,lwr_k=800:8.1523,lwr_k=900:8.1955,lwr_k=1000:8.2377'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:17.9448,lwr_k=10:0.3254,lwr_k=20:4.1968,lwr_k=30:5.8697,lwr_k=40:6.9037,lwr_k=50:7.7322,lwr_k=100:9.4269,lwr_k=200:10.991,lwr_k=300:11.9317,lwr_k=400:12.4179,lwr_k=500:12.8192,lwr_k=600:13.2639,lwr_k=700:13.5884,lwr_k=800:13.8922,lwr_k=900:14.1252,lwr_k=1000:14.3347'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:18.3565,lwr_k=10:2274111733.3769,lwr_k=20:808.2497,lwr_k=30:18.6646,lwr_k=40:14.2661,lwr_k=50:13.8676,lwr_k=100:12.5529,lwr_k=200:12.8301,lwr_k=300:13.1385,lwr_k=400:13.4185,lwr_k=500:13.6402,lwr_k=600:13.7449,lwr_k=700:14.071,lwr_k=800:14.2851,lwr_k=900:14.5264,lwr_k=1000:14.7001'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.3939,lwr_k=10:0.6451,lwr_k=20:3.1269,lwr_k=30:4.0031,lwr_k=40:4.5085,lwr_k=50:4.862,lwr_k=100:5.6206,lwr_k=200:6.0468,lwr_k=300:6.3228,lwr_k=400:6.4083,lwr_k=500:6.5213,lwr_k=600:6.5815,lwr_k=700:6.6302,lwr_k=800:6.6872,lwr_k=900:6.7237,lwr_k=1000:6.754'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:23.3495,lwr_k=10:3023932643.5212,lwr_k=20:186685953.4847,lwr_k=30:175357.6075,lwr_k=40:110.0584,lwr_k=50:26.4357,lwr_k=100:19.9019,lwr_k=200:19.8225,lwr_k=300:20.1525,lwr_k=400:20.5718,lwr_k=500:20.9266,lwr_k=600:21.1208,lwr_k=700:21.4296,lwr_k=800:21.5992,lwr_k=900:21.7979,lwr_k=1000:21.8781'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:235.0387,lwr_k=10:237.1545,lwr_k=20:236.6246,lwr_k=30:235.06,lwr_k=40:236.151,lwr_k=50:235.6784,lwr_k=100:235.4262,lwr_k=200:235.1,lwr_k=300:235.2496,lwr_k=400:235.0455,lwr_k=500:235.0419,lwr_k=600:235.0503,lwr_k=700:235.039,lwr_k=800:235.0402,lwr_k=900:235.039,lwr_k=1000:235.0392'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:237.5599,lwr_k=10:240.0665,lwr_k=20:239.4842,lwr_k=30:237.5419,lwr_k=40:238.9555,lwr_k=50:238.4145,lwr_k=100:238.1146,lwr_k=200:237.6876,lwr_k=300:237.6473,lwr_k=400:237.5446,lwr_k=500:237.5479,lwr_k=600:237.6003,lwr_k=700:237.5559,lwr_k=800:237.5509,lwr_k=900:237.5558,lwr_k=1000:237.5547'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_71'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.4878,lwr_k=10:0.0557,lwr_k=20:0.425,lwr_k=30:1.2873,lwr_k=40:1.6051,lwr_k=50:2.0,lwr_k=100:2.6457,lwr_k=200:3.5868,lwr_k=300:4.1563,lwr_k=400:4.4829,lwr_k=500:4.7003,lwr_k=600:4.789,lwr_k=700:4.9112,lwr_k=800:4.9606,lwr_k=900:4.9987,lwr_k=1000:5.0223'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:6.4751,lwr_k=10:157.8465,lwr_k=20:348579.3827,lwr_k=30:253061.9546,lwr_k=40:6227175.1355,lwr_k=50:145632.6077,lwr_k=100:58735155.2247,lwr_k=200:58871.613,lwr_k=300:174565333.0897,lwr_k=400:6277.4286,lwr_k=500:6.9189,lwr_k=600:6.807,lwr_k=700:6.6565,lwr_k=800:6.587,lwr_k=900:6.5457,lwr_k=1000:6.5574'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.5907,lwr_k=10:0.0078,lwr_k=20:0.0361,lwr_k=30:0.3097,lwr_k=40:0.922,lwr_k=50:1.6958,lwr_k=100:3.7058,lwr_k=200:4.8232,lwr_k=300:5.2421,lwr_k=400:5.4753,lwr_k=500:5.6578,lwr_k=600:5.834,lwr_k=700:6.0433,lwr_k=800:6.1441,lwr_k=900:6.2802,lwr_k=1000:6.3356'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.6499,lwr_k=10:57.4025,lwr_k=20:1745.5606,lwr_k=30:28741.7985,lwr_k=40:37.9298,lwr_k=50:24.7674,lwr_k=100:9.0601,lwr_k=200:7.5552,lwr_k=300:7.5826,lwr_k=400:7.5156,lwr_k=500:7.6226,lwr_k=600:7.6354,lwr_k=700:7.6037,lwr_k=800:7.6734,lwr_k=900:7.6361,lwr_k=1000:7.64'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.318,lwr_k=10:0.0012,lwr_k=20:0.0195,lwr_k=30:0.2925,lwr_k=40:0.9525,lwr_k=50:1.6268,lwr_k=100:3.7391,lwr_k=200:4.9743,lwr_k=300:5.4535,lwr_k=400:5.7074,lwr_k=500:5.8819,lwr_k=600:6.0363,lwr_k=700:6.1657,lwr_k=800:6.2626,lwr_k=900:6.3601,lwr_k=1000:6.4335'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.6515,lwr_k=10:39.4021,lwr_k=20:51.0777,lwr_k=30:118401.772,lwr_k=40:10402.2563,lwr_k=50:30.0208,lwr_k=100:8.8551,lwr_k=200:6.28,lwr_k=300:6.0254,lwr_k=400:5.9533,lwr_k=500:5.934,lwr_k=600:5.9027,lwr_k=700:5.9585,lwr_k=800:5.9381,lwr_k=900:5.9386,lwr_k=1000:5.9912'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.0803,lwr_k=10:0.0373,lwr_k=20:0.319,lwr_k=30:1.1683,lwr_k=40:2.1036,lwr_k=50:2.6507,lwr_k=100:3.9645,lwr_k=200:4.7197,lwr_k=300:5.0706,lwr_k=400:5.2901,lwr_k=500:5.4251,lwr_k=600:5.5262,lwr_k=700:5.5679,lwr_k=800:5.6095,lwr_k=900:5.6567,lwr_k=1000:5.6894'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.1861,lwr_k=10:27.6399,lwr_k=20:5091.4401,lwr_k=30:86811.1847,lwr_k=40:654011.2846,lwr_k=50:4901530.4677,lwr_k=100:447981.7907,lwr_k=200:54.1242,lwr_k=300:70.8616,lwr_k=400:24.5619,lwr_k=500:18.8832,lwr_k=600:17.0493,lwr_k=700:14.8455,lwr_k=800:15.1532,lwr_k=900:15.4386,lwr_k=1000:14.9838'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.3089,lwr_k=10:0.0025,lwr_k=20:0.0459,lwr_k=30:0.4319,lwr_k=40:0.5963,lwr_k=50:1.1123,lwr_k=100:3.125,lwr_k=200:4.0063,lwr_k=300:4.3598,lwr_k=400:4.5711,lwr_k=500:4.6783,lwr_k=600:4.7583,lwr_k=700:4.8119,lwr_k=800:4.8545,lwr_k=900:4.882,lwr_k=1000:4.918'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:19.0079,lwr_k=10:180.5092,lwr_k=20:594133.4473,lwr_k=30:275.3324,lwr_k=40:1077354.5092,lwr_k=50:2645476.4908,lwr_k=100:18.549,lwr_k=200:18.1015,lwr_k=300:18.2096,lwr_k=400:18.4854,lwr_k=500:18.4786,lwr_k=600:18.4403,lwr_k=700:18.3059,lwr_k=800:18.4396,lwr_k=900:18.5434,lwr_k=1000:18.5074'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:6.2458,lwr_k=10:0.0098,lwr_k=20:0.0615,lwr_k=30:0.542,lwr_k=40:0.9408,lwr_k=50:1.5312,lwr_k=100:3.5268,lwr_k=200:4.4982,lwr_k=300:4.8627,lwr_k=400:5.0586,lwr_k=500:5.2436,lwr_k=600:5.3552,lwr_k=700:5.4491,lwr_k=800:5.5072,lwr_k=900:5.5836,lwr_k=1000:5.6295'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.6118,lwr_k=10:206.298,lwr_k=20:908.1899,lwr_k=30:6168836.7884,lwr_k=40:96396244.4426,lwr_k=50:466282330.0556,lwr_k=100:9.6186,lwr_k=200:8.2217,lwr_k=300:8.1991,lwr_k=400:8.1656,lwr_k=500:8.1983,lwr_k=600:8.4099,lwr_k=700:8.4288,lwr_k=800:8.4536,lwr_k=900:8.4889,lwr_k=1000:8.6103'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_72'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.9994,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.854,lwr_k=40:1.8941,lwr_k=50:2.6471,lwr_k=100:4.3684,lwr_k=200:5.4775,lwr_k=300:5.8394,lwr_k=400:6.0064,lwr_k=500:6.1443,lwr_k=600:6.2008,lwr_k=700:6.301,lwr_k=800:6.3599,lwr_k=900:6.415,lwr_k=1000:6.4496'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.0165,lwr_k=10:97.7933,lwr_k=20:1592.8055,lwr_k=30:1256959.5781,lwr_k=40:707.4411,lwr_k=50:68.2784,lwr_k=100:10.7987,lwr_k=200:9.0925,lwr_k=300:8.8602,lwr_k=400:8.8782,lwr_k=500:9.007,lwr_k=600:9.1526,lwr_k=700:9.2084,lwr_k=800:9.264,lwr_k=900:9.2585,lwr_k=1000:9.3036'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.0638,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.8946,lwr_k=40:2.484,lwr_k=50:3.6683,lwr_k=100:6.1958,lwr_k=200:7.6984,lwr_k=300:8.472,lwr_k=400:9.0521,lwr_k=500:9.3167,lwr_k=600:9.6123,lwr_k=700:9.7994,lwr_k=800:9.9325,lwr_k=900:10.046,lwr_k=1000:10.1901'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.2081,lwr_k=10:122.1352,lwr_k=20:1026.0053,lwr_k=30:32834.1736,lwr_k=40:977.3632,lwr_k=50:63.6432,lwr_k=100:40.8046,lwr_k=200:9.9436,lwr_k=300:9.521,lwr_k=400:9.4422,lwr_k=500:9.4011,lwr_k=600:9.5034,lwr_k=700:9.6051,lwr_k=800:9.5844,lwr_k=900:9.5992,lwr_k=1000:9.6034'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.9688,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.1144,lwr_k=40:2.6514,lwr_k=50:3.6365,lwr_k=100:5.7745,lwr_k=200:7.3154,lwr_k=300:8.0446,lwr_k=400:8.443,lwr_k=500:8.6815,lwr_k=600:8.9171,lwr_k=700:9.0209,lwr_k=800:9.1536,lwr_k=900:9.2635,lwr_k=1000:9.3471'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.4561,lwr_k=10:44.4488,lwr_k=20:8833.6598,lwr_k=30:20473.4053,lwr_k=40:798.5616,lwr_k=50:51.5077,lwr_k=100:12.8501,lwr_k=200:7.3517,lwr_k=300:7.3499,lwr_k=400:7.3028,lwr_k=500:7.3305,lwr_k=600:7.2865,lwr_k=700:7.2994,lwr_k=800:7.208,lwr_k=900:7.197,lwr_k=1000:7.2093'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.8312,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.9969,lwr_k=40:2.1576,lwr_k=50:3.0305,lwr_k=100:5.3541,lwr_k=200:6.5891,lwr_k=300:6.9989,lwr_k=400:7.3023,lwr_k=500:7.5614,lwr_k=600:7.7093,lwr_k=700:7.8705,lwr_k=800:8.046,lwr_k=900:8.227,lwr_k=1000:8.2991'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:12.9324,lwr_k=10:95.1722,lwr_k=20:768.0494,lwr_k=30:196168.3309,lwr_k=40:7438.4999,lwr_k=50:2517.7153,lwr_k=100:13.1748,lwr_k=200:11.4019,lwr_k=300:11.1315,lwr_k=400:11.0525,lwr_k=500:11.1832,lwr_k=600:11.3373,lwr_k=700:11.2813,lwr_k=800:11.2297,lwr_k=900:11.209,lwr_k=1000:11.2571'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.8859,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.7386,lwr_k=40:2.005,lwr_k=50:2.6785,lwr_k=100:3.9189,lwr_k=200:4.6375,lwr_k=300:4.8355,lwr_k=400:5.0144,lwr_k=500:5.1013,lwr_k=600:5.1949,lwr_k=700:5.2874,lwr_k=800:5.33,lwr_k=900:5.3737,lwr_k=1000:5.4429'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:21.9009,lwr_k=10:54.71,lwr_k=20:42074.5858,lwr_k=30:209.6704,lwr_k=40:201.9582,lwr_k=50:37938.9998,lwr_k=100:577.0845,lwr_k=200:23.6869,lwr_k=300:21.8827,lwr_k=400:21.7519,lwr_k=500:19.8483,lwr_k=600:19.9067,lwr_k=700:20.0043,lwr_k=800:20.9888,lwr_k=900:20.8388,lwr_k=1000:20.333'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:7.3159,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.7659,lwr_k=40:2.0909,lwr_k=50:2.8517,lwr_k=100:4.3023,lwr_k=200:5.2848,lwr_k=300:5.6617,lwr_k=400:5.9347,lwr_k=500:6.1168,lwr_k=600:6.2021,lwr_k=700:6.3219,lwr_k=800:6.4062,lwr_k=900:6.4381,lwr_k=1000:6.4937'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.2839,lwr_k=10:38.6567,lwr_k=20:1420.9298,lwr_k=30:2781.3243,lwr_k=40:2255.3676,lwr_k=50:20.9299,lwr_k=100:11.0654,lwr_k=200:9.1099,lwr_k=300:9.2881,lwr_k=400:9.5824,lwr_k=500:9.7728,lwr_k=600:9.8713,lwr_k=700:9.9699,lwr_k=800:10.062,lwr_k=900:10.0407,lwr_k=1000:10.115'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_73'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.0748,lwr_k=10:0.0,lwr_k=20:0.3539,lwr_k=30:1.7517,lwr_k=40:2.4982,lwr_k=50:2.9952,lwr_k=100:4.0165,lwr_k=200:4.5639,lwr_k=300:4.7777,lwr_k=400:4.9116,lwr_k=500:5.0173,lwr_k=600:5.1431,lwr_k=700:5.2225,lwr_k=800:5.282,lwr_k=900:5.3301,lwr_k=1000:5.3715'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.8924,lwr_k=10:31.8629,lwr_k=20:540.0248,lwr_k=30:17.7561,lwr_k=40:11.4401,lwr_k=50:22.3914,lwr_k=100:7.7339,lwr_k=200:7.2584,lwr_k=300:7.4778,lwr_k=400:7.4795,lwr_k=500:7.452,lwr_k=600:7.4285,lwr_k=700:7.4924,lwr_k=800:7.5486,lwr_k=900:7.5336,lwr_k=1000:7.5096'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:14.3286,lwr_k=10:0.0,lwr_k=20:0.8682,lwr_k=30:2.7736,lwr_k=40:4.0148,lwr_k=50:4.6517,lwr_k=100:6.5851,lwr_k=200:8.2644,lwr_k=300:9.0878,lwr_k=400:9.5242,lwr_k=500:9.764,lwr_k=600:9.9633,lwr_k=700:10.2161,lwr_k=800:10.4324,lwr_k=900:10.5844,lwr_k=1000:10.7489'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.7504,lwr_k=10:55.1828,lwr_k=20:434089158.3272,lwr_k=30:2094.8408,lwr_k=40:15.3677,lwr_k=50:13.1679,lwr_k=100:9.9714,lwr_k=200:8.7762,lwr_k=300:9.1081,lwr_k=400:9.1504,lwr_k=500:9.2559,lwr_k=600:9.3538,lwr_k=700:9.628,lwr_k=800:9.7362,lwr_k=900:9.7628,lwr_k=1000:9.8799'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.8168,lwr_k=10:0.0,lwr_k=20:0.8496,lwr_k=30:2.9446,lwr_k=40:4.148,lwr_k=50:5.061,lwr_k=100:6.6703,lwr_k=200:7.9908,lwr_k=300:8.6439,lwr_k=400:9.1066,lwr_k=500:9.3838,lwr_k=600:9.6243,lwr_k=700:9.7687,lwr_k=800:9.8836,lwr_k=900:10.0038,lwr_k=1000:10.1175'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.1631,lwr_k=10:66.5963,lwr_k=20:2117.3103,lwr_k=30:17.4662,lwr_k=40:12.4736,lwr_k=50:10.011,lwr_k=100:7.6775,lwr_k=200:7.0609,lwr_k=300:7.0131,lwr_k=400:7.0645,lwr_k=500:7.0123,lwr_k=600:7.0659,lwr_k=700:7.1043,lwr_k=800:7.0554,lwr_k=900:7.1113,lwr_k=1000:7.0914'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:13.0465,lwr_k=10:3.3579,lwr_k=20:4.9845,lwr_k=30:5.8406,lwr_k=40:6.5847,lwr_k=50:7.2777,lwr_k=100:12.4498,lwr_k=200:9.4854,lwr_k=300:14.789,lwr_k=400:294.7625,lwr_k=500:14.5197,lwr_k=600:11.6687,lwr_k=700:10.3821,lwr_k=800:10.554,lwr_k=900:10.661,lwr_k=1000:10.7505'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:15.447,lwr_k=10:28363716106.5301,lwr_k=20:1845368422.6496,lwr_k=30:3361931581.3646,lwr_k=40:686352139.521,lwr_k=50:46815404408.3009,lwr_k=100:28808022.2339,lwr_k=200:18529089829.7856,lwr_k=300:1125535470528.502,lwr_k=400:5746045967.8457,lwr_k=500:1484563033158.3635,lwr_k=600:515422221.489,lwr_k=700:12.8289,lwr_k=800:12.9864,lwr_k=900:13.1119,lwr_k=1000:13.2261'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.825,lwr_k=10:0.0001,lwr_k=20:0.3961,lwr_k=30:1.9101,lwr_k=40:2.6913,lwr_k=50:3.2615,lwr_k=100:4.1852,lwr_k=200:4.7898,lwr_k=300:5.0547,lwr_k=400:5.1325,lwr_k=500:5.2156,lwr_k=600:5.259,lwr_k=700:5.2981,lwr_k=800:5.3429,lwr_k=900:5.3802,lwr_k=1000:5.405'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:18.5631,lwr_k=10:66.0336,lwr_k=20:51599299.0774,lwr_k=30:659.1597,lwr_k=40:18.3212,lwr_k=50:18.0645,lwr_k=100:16.0657,lwr_k=200:16.5183,lwr_k=300:16.4732,lwr_k=400:16.2401,lwr_k=500:16.4614,lwr_k=600:16.542,lwr_k=700:16.724,lwr_k=800:16.7972,lwr_k=900:17.0839,lwr_k=1000:17.1485'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:7.9941,lwr_k=10:0.0,lwr_k=20:0.4898,lwr_k=30:1.9166,lwr_k=40:2.8821,lwr_k=50:3.3409,lwr_k=100:4.6254,lwr_k=200:5.5057,lwr_k=300:5.9293,lwr_k=400:6.1279,lwr_k=500:6.3068,lwr_k=600:6.4279,lwr_k=700:6.5189,lwr_k=800:6.5785,lwr_k=900:6.6478,lwr_k=1000:6.7136'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.2361,lwr_k=10:32.6837,lwr_k=20:11670.0839,lwr_k=30:19.7429,lwr_k=40:14.1971,lwr_k=50:10.6283,lwr_k=100:8.7659,lwr_k=200:9.2353,lwr_k=300:9.3329,lwr_k=400:9.3683,lwr_k=500:9.4528,lwr_k=600:9.6719,lwr_k=700:9.6959,lwr_k=800:9.7986,lwr_k=900:9.8334,lwr_k=1000:9.8867'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_74'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.3314,lwr_k=10:0.0,lwr_k=20:1.3435,lwr_k=30:2.3071,lwr_k=40:2.8774,lwr_k=50:3.1946,lwr_k=100:3.9073,lwr_k=200:4.4061,lwr_k=300:4.573,lwr_k=400:4.6725,lwr_k=500:4.7354,lwr_k=600:4.7762,lwr_k=700:4.817,lwr_k=800:4.8749,lwr_k=900:4.918,lwr_k=1000:4.9627'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.3813,lwr_k=10:202.6003,lwr_k=20:42.8126,lwr_k=30:23.5275,lwr_k=40:14.2786,lwr_k=50:10.2341,lwr_k=100:8.5843,lwr_k=200:8.2034,lwr_k=300:8.1327,lwr_k=400:8.114,lwr_k=500:9.4791,lwr_k=600:9.3722,lwr_k=700:10.5847,lwr_k=800:10.7197,lwr_k=900:10.1445,lwr_k=1000:9.903'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.8168,lwr_k=10:0.0,lwr_k=20:1.7164,lwr_k=30:2.7239,lwr_k=40:3.5303,lwr_k=50:3.9166,lwr_k=100:4.7925,lwr_k=200:5.2741,lwr_k=300:5.4071,lwr_k=400:5.5292,lwr_k=500:5.6641,lwr_k=600:5.7198,lwr_k=700:5.7505,lwr_k=800:5.8163,lwr_k=900:5.8405,lwr_k=1000:5.8663'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.5259,lwr_k=10:5153.0049,lwr_k=20:134.2979,lwr_k=30:87.6287,lwr_k=40:43.7829,lwr_k=50:24.6699,lwr_k=100:10.4739,lwr_k=200:7.9085,lwr_k=300:7.6915,lwr_k=400:7.7446,lwr_k=500:7.7754,lwr_k=600:7.7934,lwr_k=700:7.7508,lwr_k=800:7.7294,lwr_k=900:7.8194,lwr_k=1000:7.8679'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.6112,lwr_k=10:0.0,lwr_k=20:1.4376,lwr_k=30:2.9598,lwr_k=40:3.5493,lwr_k=50:3.9322,lwr_k=100:4.9049,lwr_k=200:5.4284,lwr_k=300:5.6386,lwr_k=400:5.771,lwr_k=500:5.855,lwr_k=600:5.9164,lwr_k=700:5.9638,lwr_k=800:6.025,lwr_k=900:6.0235,lwr_k=1000:6.0725'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.1871,lwr_k=10:377.6157,lwr_k=20:134.9338,lwr_k=30:417.3878,lwr_k=40:60.1916,lwr_k=50:120.2333,lwr_k=100:20.2447,lwr_k=200:7.4346,lwr_k=300:7.4889,lwr_k=400:6.9113,lwr_k=500:6.5396,lwr_k=600:6.4482,lwr_k=700:6.3534,lwr_k=800:6.3035,lwr_k=900:6.4288,lwr_k=1000:6.2488'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.5516,lwr_k=10:0.0,lwr_k=20:0.905,lwr_k=30:2.2028,lwr_k=40:2.9941,lwr_k=50:3.6118,lwr_k=100:4.5967,lwr_k=200:5.1329,lwr_k=300:5.3323,lwr_k=400:5.5223,lwr_k=500:5.6452,lwr_k=600:5.7399,lwr_k=700:5.7983,lwr_k=800:5.8513,lwr_k=900:5.9103,lwr_k=1000:5.9788'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.2563,lwr_k=10:330.2757,lwr_k=20:31049.0406,lwr_k=30:37175.0539,lwr_k=40:3242.1774,lwr_k=50:2262.2324,lwr_k=100:25.2966,lwr_k=200:10.7071,lwr_k=300:10.278,lwr_k=400:10.0664,lwr_k=500:9.8516,lwr_k=600:10.0091,lwr_k=700:10.1227,lwr_k=800:10.1953,lwr_k=900:10.2395,lwr_k=1000:10.2769'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.9873,lwr_k=10:0.0,lwr_k=20:0.4549,lwr_k=30:1.843,lwr_k=40:2.7764,lwr_k=50:3.2472,lwr_k=100:4.2914,lwr_k=200:4.8771,lwr_k=300:5.0906,lwr_k=400:5.2263,lwr_k=500:5.3355,lwr_k=600:5.3918,lwr_k=700:5.4346,lwr_k=800:5.4886,lwr_k=900:5.514,lwr_k=1000:5.5538'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:16.6471,lwr_k=10:201.7477,lwr_k=20:136347.9683,lwr_k=30:356.6435,lwr_k=40:87.9317,lwr_k=50:87.6811,lwr_k=100:83.3383,lwr_k=200:74.0836,lwr_k=300:14.8663,lwr_k=400:59.7464,lwr_k=500:4138.3637,lwr_k=600:3768.0781,lwr_k=700:1454.6134,lwr_k=800:1061.0231,lwr_k=900:1139.7964,lwr_k=1000:801.8192'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:6.1942,lwr_k=10:0.0,lwr_k=20:0.3101,lwr_k=30:1.6461,lwr_k=40:2.4825,lwr_k=50:3.1385,lwr_k=100:4.28,lwr_k=200:4.8774,lwr_k=300:5.0383,lwr_k=400:5.24,lwr_k=500:5.2939,lwr_k=600:5.4025,lwr_k=700:5.4466,lwr_k=800:5.4896,lwr_k=900:5.5321,lwr_k=1000:5.574'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.0829,lwr_k=10:152.9362,lwr_k=20:27192.3936,lwr_k=30:4259.7852,lwr_k=40:1228.3567,lwr_k=50:461.5574,lwr_k=100:51.3775,lwr_k=200:26.4576,lwr_k=300:15.846,lwr_k=400:13.3898,lwr_k=500:12.1235,lwr_k=600:13.112,lwr_k=700:12.3922,lwr_k=800:9.8146,lwr_k=900:10.2952,lwr_k=1000:10.2272'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_75'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.5764,lwr_k=10:0.0417,lwr_k=20:3.3031,lwr_k=30:3.9225,lwr_k=40:4.5586,lwr_k=50:4.9984,lwr_k=100:5.9418,lwr_k=200:6.5275,lwr_k=300:6.7984,lwr_k=400:6.9287,lwr_k=500:7.0999,lwr_k=600:7.1992,lwr_k=700:7.2887,lwr_k=800:7.3715,lwr_k=900:7.4124,lwr_k=1000:7.4781'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.4734,lwr_k=10:1185.7057,lwr_k=20:32.6195,lwr_k=30:16.2825,lwr_k=40:11.2262,lwr_k=50:10.4437,lwr_k=100:8.9018,lwr_k=200:8.1212,lwr_k=300:7.9548,lwr_k=400:7.9841,lwr_k=500:8.0002,lwr_k=600:8.0779,lwr_k=700:8.1664,lwr_k=800:8.1563,lwr_k=900:8.1882,lwr_k=1000:8.1985'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.5953,lwr_k=10:0.0,lwr_k=20:1.3121,lwr_k=30:2.9599,lwr_k=40:3.9042,lwr_k=50:4.3969,lwr_k=100:5.6464,lwr_k=200:6.5879,lwr_k=300:7.0217,lwr_k=400:7.299,lwr_k=500:7.498,lwr_k=600:7.6572,lwr_k=700:7.7955,lwr_k=800:7.9372,lwr_k=900:8.0301,lwr_k=1000:8.0811'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.567,lwr_k=10:62.6311,lwr_k=20:46.4124,lwr_k=30:16.3942,lwr_k=40:13.7727,lwr_k=50:11.8904,lwr_k=100:7.7885,lwr_k=200:7.5469,lwr_k=300:7.6135,lwr_k=400:7.6041,lwr_k=500:7.5939,lwr_k=600:7.5946,lwr_k=700:7.6413,lwr_k=800:7.7308,lwr_k=900:7.77,lwr_k=1000:7.8171'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.433,lwr_k=10:0.0,lwr_k=20:1.3461,lwr_k=30:3.0788,lwr_k=40:4.0322,lwr_k=50:4.8461,lwr_k=100:5.973,lwr_k=200:6.7074,lwr_k=300:7.0419,lwr_k=400:7.1719,lwr_k=500:7.2856,lwr_k=600:7.3787,lwr_k=700:7.4847,lwr_k=800:7.5999,lwr_k=900:7.6655,lwr_k=1000:7.7519'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.5139,lwr_k=10:76.7294,lwr_k=20:37.9816,lwr_k=30:13.3969,lwr_k=40:9.5268,lwr_k=50:22.4593,lwr_k=100:7.2931,lwr_k=200:6.6534,lwr_k=300:6.5118,lwr_k=400:6.4884,lwr_k=500:6.4727,lwr_k=600:6.4408,lwr_k=700:6.3816,lwr_k=800:6.4094,lwr_k=900:6.4409,lwr_k=1000:6.4615'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.5484,lwr_k=10:0.0,lwr_k=20:1.2824,lwr_k=30:2.781,lwr_k=40:3.5938,lwr_k=50:4.0659,lwr_k=100:5.322,lwr_k=200:6.3848,lwr_k=300:6.7935,lwr_k=400:6.9425,lwr_k=500:7.0912,lwr_k=600:7.1961,lwr_k=700:7.3129,lwr_k=800:7.4089,lwr_k=900:7.5587,lwr_k=1000:7.6355'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:11.457,lwr_k=10:70.6643,lwr_k=20:82.9711,lwr_k=30:20.7645,lwr_k=40:15.5802,lwr_k=50:12.993,lwr_k=100:9.7239,lwr_k=200:9.865,lwr_k=300:9.7178,lwr_k=400:10.1505,lwr_k=500:10.038,lwr_k=600:10.1086,lwr_k=700:10.1415,lwr_k=800:10.1673,lwr_k=900:10.1906,lwr_k=1000:10.2077'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.3704,lwr_k=10:0.0408,lwr_k=20:1.6896,lwr_k=30:2.7973,lwr_k=40:3.592,lwr_k=50:4.1947,lwr_k=100:4.9448,lwr_k=200:5.538,lwr_k=300:5.8039,lwr_k=400:5.9136,lwr_k=500:5.96,lwr_k=600:6.0003,lwr_k=700:6.0197,lwr_k=800:6.0417,lwr_k=900:6.0547,lwr_k=1000:6.063'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:21.4154,lwr_k=10:338.1745,lwr_k=20:4440.3801,lwr_k=30:20.6168,lwr_k=40:19.2501,lwr_k=50:19.0992,lwr_k=100:18.6902,lwr_k=200:19.8567,lwr_k=300:20.0947,lwr_k=400:20.0329,lwr_k=500:20.0804,lwr_k=600:20.1199,lwr_k=700:20.0826,lwr_k=800:20.129,lwr_k=900:20.1158,lwr_k=1000:20.1202'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:9.0013,lwr_k=10:0.0,lwr_k=20:1.0991,lwr_k=30:2.4397,lwr_k=40:3.2631,lwr_k=50:3.7312,lwr_k=100:5.0319,lwr_k=200:5.9297,lwr_k=300:6.3061,lwr_k=400:6.5517,lwr_k=500:6.6906,lwr_k=600:6.839,lwr_k=700:6.9451,lwr_k=800:7.0565,lwr_k=900:7.1315,lwr_k=1000:7.2531'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.2498,lwr_k=10:83.3689,lwr_k=20:46.8303,lwr_k=30:12.2884,lwr_k=40:10.0595,lwr_k=50:9.1364,lwr_k=100:8.0971,lwr_k=200:8.892,lwr_k=300:9.4694,lwr_k=400:9.7269,lwr_k=500:9.9801,lwr_k=600:10.151,lwr_k=700:10.2815,lwr_k=800:10.3476,lwr_k=900:10.4005,lwr_k=1000:10.4513'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_76'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.4862,lwr_k=10:0.0865,lwr_k=20:1.8757,lwr_k=30:2.7188,lwr_k=40:3.3711,lwr_k=50:3.7513,lwr_k=100:4.8469,lwr_k=200:5.3758,lwr_k=300:5.5754,lwr_k=400:5.7289,lwr_k=500:5.7997,lwr_k=600:5.8684,lwr_k=700:5.902,lwr_k=800:5.9546,lwr_k=900:5.9895,lwr_k=1000:6.0185'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.8725,lwr_k=10:322220.5598,lwr_k=20:732052082.122,lwr_k=30:31279.0964,lwr_k=40:218.9401,lwr_k=50:50.7506,lwr_k=100:8.4347,lwr_k=200:7.711,lwr_k=300:7.7009,lwr_k=400:7.8295,lwr_k=500:7.7639,lwr_k=600:7.7719,lwr_k=700:7.8728,lwr_k=800:7.8927,lwr_k=900:7.7968,lwr_k=1000:7.7994'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.8879,lwr_k=10:0.8362,lwr_k=20:3.635,lwr_k=30:4.2006,lwr_k=40:4.9891,lwr_k=50:5.361,lwr_k=100:6.2596,lwr_k=200:6.8612,lwr_k=300:7.1065,lwr_k=400:7.2144,lwr_k=500:7.2908,lwr_k=600:7.3592,lwr_k=700:7.4128,lwr_k=800:7.4601,lwr_k=900:7.4618,lwr_k=1000:7.4885'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.8026,lwr_k=10:78174.6414,lwr_k=20:2283.8206,lwr_k=30:15.7889,lwr_k=40:12.768,lwr_k=50:11.985,lwr_k=100:10.7548,lwr_k=200:9.9457,lwr_k=300:9.3724,lwr_k=400:9.3669,lwr_k=500:9.381,lwr_k=600:9.4615,lwr_k=700:9.513,lwr_k=800:9.5057,lwr_k=900:9.455,lwr_k=1000:9.462'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.7802,lwr_k=10:0.3803,lwr_k=20:2.1634,lwr_k=30:3.578,lwr_k=40:4.324,lwr_k=50:4.7609,lwr_k=100:5.8954,lwr_k=200:6.7033,lwr_k=300:6.8421,lwr_k=400:7.0575,lwr_k=500:7.1797,lwr_k=600:7.2963,lwr_k=700:7.3543,lwr_k=800:7.4041,lwr_k=900:7.4361,lwr_k=1000:7.4602'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.3856,lwr_k=10:230.3322,lwr_k=20:29547.7155,lwr_k=30:35.4383,lwr_k=40:201466.531,lwr_k=50:217751.1538,lwr_k=100:82718.0366,lwr_k=200:10.1055,lwr_k=300:6.9353,lwr_k=400:6.9151,lwr_k=500:6.8326,lwr_k=600:6.8834,lwr_k=700:6.8914,lwr_k=800:6.9014,lwr_k=900:6.963,lwr_k=1000:6.9647'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.9183,lwr_k=10:0.1453,lwr_k=20:2.8212,lwr_k=30:3.4706,lwr_k=40:3.9216,lwr_k=50:4.2114,lwr_k=100:5.3317,lwr_k=200:5.7742,lwr_k=300:6.0642,lwr_k=400:6.3923,lwr_k=500:6.4867,lwr_k=600:6.5708,lwr_k=700:6.6729,lwr_k=800:6.7911,lwr_k=900:6.8743,lwr_k=1000:6.9625'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.5456,lwr_k=10:1384.4578,lwr_k=20:16693.9336,lwr_k=30:84661.5465,lwr_k=40:194593.0474,lwr_k=50:50663.737,lwr_k=100:9.6253,lwr_k=200:9.362,lwr_k=300:9.3583,lwr_k=400:9.2132,lwr_k=500:9.2828,lwr_k=600:9.3126,lwr_k=700:9.3074,lwr_k=800:9.3382,lwr_k=900:9.41,lwr_k=1000:9.3044'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.925,lwr_k=10:0.097,lwr_k=20:1.6534,lwr_k=30:2.9992,lwr_k=40:3.754,lwr_k=50:4.0943,lwr_k=100:4.9686,lwr_k=200:5.5887,lwr_k=300:5.8575,lwr_k=400:5.9703,lwr_k=500:6.1355,lwr_k=600:6.2293,lwr_k=700:6.3281,lwr_k=800:6.3407,lwr_k=900:6.3646,lwr_k=1000:6.3987'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:22.1527,lwr_k=10:172287098.2029,lwr_k=20:8437150.6556,lwr_k=30:10442.4377,lwr_k=40:22.7818,lwr_k=50:19.398,lwr_k=100:19.5013,lwr_k=200:20.0897,lwr_k=300:19.7543,lwr_k=400:19.8202,lwr_k=500:19.7727,lwr_k=600:19.8754,lwr_k=700:20.0935,lwr_k=800:20.1526,lwr_k=900:20.166,lwr_k=1000:20.4035'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:6.9938,lwr_k=10:0.1112,lwr_k=20:2.1086,lwr_k=30:3.1057,lwr_k=40:3.5435,lwr_k=50:3.9572,lwr_k=100:4.8106,lwr_k=200:5.3062,lwr_k=300:5.5584,lwr_k=400:5.6915,lwr_k=500:5.7422,lwr_k=600:5.789,lwr_k=700:5.825,lwr_k=800:5.8591,lwr_k=900:5.9175,lwr_k=1000:5.9525'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.9579,lwr_k=10:265.0469,lwr_k=20:5083554.4931,lwr_k=30:22081455.8694,lwr_k=40:2776428.3267,lwr_k=50:10862.0312,lwr_k=100:9.1195,lwr_k=200:8.7493,lwr_k=300:8.6381,lwr_k=400:8.6529,lwr_k=500:8.711,lwr_k=600:8.7537,lwr_k=700:8.7322,lwr_k=800:8.7281,lwr_k=900:8.6749,lwr_k=1000:8.7131'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_77'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.713,lwr_k=10:0.0,lwr_k=20:0.9973,lwr_k=30:2.4887,lwr_k=40:3.2934,lwr_k=50:3.8007,lwr_k=100:4.7856,lwr_k=200:5.3557,lwr_k=300:5.6137,lwr_k=400:5.7737,lwr_k=500:5.8669,lwr_k=600:5.9384,lwr_k=700:5.9906,lwr_k=800:6.0437,lwr_k=900:6.1278,lwr_k=1000:6.154'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.4451,lwr_k=10:83.2581,lwr_k=20:222.6576,lwr_k=30:25.6228,lwr_k=40:11.424,lwr_k=50:10.7712,lwr_k=100:7.5283,lwr_k=200:7.4035,lwr_k=300:7.3159,lwr_k=400:7.3929,lwr_k=500:7.4391,lwr_k=600:7.5269,lwr_k=700:7.5702,lwr_k=800:7.611,lwr_k=900:7.6798,lwr_k=1000:7.7152'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.7336,lwr_k=10:0.0,lwr_k=20:1.1701,lwr_k=30:3.1478,lwr_k=40:4.1874,lwr_k=50:4.7349,lwr_k=100:6.4351,lwr_k=200:7.4263,lwr_k=300:8.0191,lwr_k=400:8.3271,lwr_k=500:8.5395,lwr_k=600:8.6905,lwr_k=700:8.7618,lwr_k=800:8.8693,lwr_k=900:8.9315,lwr_k=1000:9.0278'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.3097,lwr_k=10:114.1171,lwr_k=20:43.9746,lwr_k=30:18.4892,lwr_k=40:11.0849,lwr_k=50:10.1532,lwr_k=100:8.548,lwr_k=200:8.2259,lwr_k=300:8.2678,lwr_k=400:8.3847,lwr_k=500:8.4477,lwr_k=600:8.4857,lwr_k=700:8.5736,lwr_k=800:8.5878,lwr_k=900:8.6458,lwr_k=1000:8.6998'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.8509,lwr_k=10:0.0,lwr_k=20:1.1938,lwr_k=30:3.3984,lwr_k=40:4.4813,lwr_k=50:5.2913,lwr_k=100:6.946,lwr_k=200:8.1846,lwr_k=300:8.7025,lwr_k=400:9.144,lwr_k=500:9.3754,lwr_k=600:9.4682,lwr_k=700:9.5873,lwr_k=800:9.6955,lwr_k=900:9.7615,lwr_k=1000:9.8333'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.49,lwr_k=10:109.133,lwr_k=20:68.509,lwr_k=30:15.7437,lwr_k=40:12.6306,lwr_k=50:9.7371,lwr_k=100:8.2452,lwr_k=200:7.4921,lwr_k=300:7.6388,lwr_k=400:7.678,lwr_k=500:7.6963,lwr_k=600:7.7208,lwr_k=700:7.6943,lwr_k=800:7.7096,lwr_k=900:7.7174,lwr_k=1000:7.7548'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.6524,lwr_k=10:0.0,lwr_k=20:1.1813,lwr_k=30:3.0493,lwr_k=40:3.7716,lwr_k=50:4.4257,lwr_k=100:5.6208,lwr_k=200:6.4325,lwr_k=300:6.7697,lwr_k=400:6.9878,lwr_k=500:7.1473,lwr_k=600:7.2219,lwr_k=700:7.3257,lwr_k=800:7.3646,lwr_k=900:7.4001,lwr_k=1000:7.4655'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:11.2339,lwr_k=10:153.3765,lwr_k=20:137.6719,lwr_k=30:37.8709,lwr_k=40:28.3194,lwr_k=50:33.4746,lwr_k=100:10.6563,lwr_k=200:9.8867,lwr_k=300:9.9084,lwr_k=400:9.9716,lwr_k=500:10.1186,lwr_k=600:10.2108,lwr_k=700:10.288,lwr_k=800:10.3004,lwr_k=900:10.3396,lwr_k=1000:10.4115'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.921,lwr_k=10:0.0,lwr_k=20:1.0624,lwr_k=30:2.7548,lwr_k=40:3.4601,lwr_k=50:3.9704,lwr_k=100:4.9257,lwr_k=200:5.5888,lwr_k=300:5.8345,lwr_k=400:5.9481,lwr_k=500:6.0257,lwr_k=600:6.1127,lwr_k=700:6.1559,lwr_k=800:6.1921,lwr_k=900:6.2366,lwr_k=1000:6.2556'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:24.031,lwr_k=10:158.8428,lwr_k=20:84.2221,lwr_k=30:156.2705,lwr_k=40:42.2882,lwr_k=50:35.2911,lwr_k=100:18.0144,lwr_k=200:19.6706,lwr_k=300:20.2476,lwr_k=400:20.6787,lwr_k=500:20.67,lwr_k=600:21.0276,lwr_k=700:21.2712,lwr_k=800:21.4334,lwr_k=900:21.5075,lwr_k=1000:21.6315'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:8.3944,lwr_k=10:0.0,lwr_k=20:1.005,lwr_k=30:2.4814,lwr_k=40:3.4412,lwr_k=50:4.1126,lwr_k=100:5.5175,lwr_k=200:6.3037,lwr_k=300:6.569,lwr_k=400:6.82,lwr_k=500:6.9153,lwr_k=600:7.0237,lwr_k=700:7.1022,lwr_k=800:7.1717,lwr_k=900:7.2187,lwr_k=1000:7.2594'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.5634,lwr_k=10:161.0539,lwr_k=20:15611.9337,lwr_k=30:203.3032,lwr_k=40:45.7343,lwr_k=50:11.1135,lwr_k=100:9.0592,lwr_k=200:9.4519,lwr_k=300:9.6073,lwr_k=400:9.8224,lwr_k=500:9.9978,lwr_k=600:10.0283,lwr_k=700:10.0576,lwr_k=800:10.0904,lwr_k=900:10.1916,lwr_k=1000:10.2497'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_78'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.7092,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0949,lwr_k=40:0.8403,lwr_k=50:2.323,lwr_k=100:3.6068,lwr_k=200:4.8044,lwr_k=300:5.3275,lwr_k=400:5.5823,lwr_k=500:5.7182,lwr_k=600:5.8146,lwr_k=700:5.8818,lwr_k=800:5.9489,lwr_k=900:5.9945,lwr_k=1000:6.048'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.4926,lwr_k=10:23.2624,lwr_k=20:61.2069,lwr_k=30:268.0508,lwr_k=40:1720.9839,lwr_k=50:736.5659,lwr_k=100:10.1252,lwr_k=200:7.4485,lwr_k=300:7.1832,lwr_k=400:7.1046,lwr_k=500:7.134,lwr_k=600:7.0908,lwr_k=700:7.1066,lwr_k=800:7.1293,lwr_k=900:7.0717,lwr_k=1000:7.0457'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.8931,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.1817,lwr_k=40:1.5547,lwr_k=50:2.7159,lwr_k=100:4.6159,lwr_k=200:6.2158,lwr_k=300:6.7909,lwr_k=400:7.1095,lwr_k=500:7.4228,lwr_k=600:7.6637,lwr_k=700:7.8386,lwr_k=800:8.0202,lwr_k=900:8.1123,lwr_k=1000:8.2327'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.6187,lwr_k=10:34.3401,lwr_k=20:50.4542,lwr_k=30:328.2184,lwr_k=40:310.2138,lwr_k=50:24.5327,lwr_k=100:8.5483,lwr_k=200:8.2393,lwr_k=300:8.1422,lwr_k=400:8.1849,lwr_k=500:8.118,lwr_k=600:8.2419,lwr_k=700:8.3531,lwr_k=800:8.4358,lwr_k=900:8.415,lwr_k=1000:8.3845'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.8521,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.2828,lwr_k=40:2.1765,lwr_k=50:2.2666,lwr_k=100:4.2271,lwr_k=200:5.4931,lwr_k=300:5.8992,lwr_k=400:6.0763,lwr_k=500:6.2135,lwr_k=600:6.3551,lwr_k=700:6.4454,lwr_k=800:6.5168,lwr_k=900:6.6085,lwr_k=1000:6.7125'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.3684,lwr_k=10:30.9708,lwr_k=20:243.2667,lwr_k=30:13217.5156,lwr_k=40:1423738.6724,lwr_k=50:288095.6117,lwr_k=100:12133.6588,lwr_k=200:7.0785,lwr_k=300:7.072,lwr_k=400:6.9894,lwr_k=500:6.9154,lwr_k=600:6.9257,lwr_k=700:7.0286,lwr_k=800:6.9861,lwr_k=900:7.006,lwr_k=1000:6.9981'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.2534,lwr_k=10:0.0,lwr_k=20:0.0001,lwr_k=30:0.3181,lwr_k=40:1.7688,lwr_k=50:2.3398,lwr_k=100:3.8219,lwr_k=200:4.609,lwr_k=300:5.0102,lwr_k=400:5.2809,lwr_k=500:5.4063,lwr_k=600:5.4824,lwr_k=700:5.5347,lwr_k=800:5.582,lwr_k=900:5.626,lwr_k=1000:5.6485'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.5717,lwr_k=10:50.5248,lwr_k=20:4364.5572,lwr_k=30:2370496.6008,lwr_k=40:442706.8109,lwr_k=50:51544.754,lwr_k=100:13.7585,lwr_k=200:11.1881,lwr_k=300:10.38,lwr_k=400:10.4716,lwr_k=500:10.2139,lwr_k=600:10.1267,lwr_k=700:10.2593,lwr_k=800:10.1304,lwr_k=900:10.1519,lwr_k=1000:10.1303'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.3065,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.3878,lwr_k=40:1.9929,lwr_k=50:2.3807,lwr_k=100:3.4441,lwr_k=200:4.4879,lwr_k=300:4.801,lwr_k=400:5.0064,lwr_k=500:5.1402,lwr_k=600:5.2621,lwr_k=700:5.3759,lwr_k=800:5.4508,lwr_k=900:5.5343,lwr_k=1000:5.6132'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:21.7629,lwr_k=10:61.0262,lwr_k=20:179.5676,lwr_k=30:252.365,lwr_k=40:3071358.2771,lwr_k=50:90.8141,lwr_k=100:12.8287,lwr_k=200:19.2803,lwr_k=300:18.5311,lwr_k=400:18.6868,lwr_k=500:19.3306,lwr_k=600:19.2455,lwr_k=700:19.3522,lwr_k=800:19.4919,lwr_k=900:19.493,lwr_k=1000:19.5455'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:7.3935,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.127,lwr_k=40:2.2774,lwr_k=50:2.8122,lwr_k=100:3.6367,lwr_k=200:5.0338,lwr_k=300:5.518,lwr_k=400:5.8051,lwr_k=500:5.9789,lwr_k=600:6.0815,lwr_k=700:6.1994,lwr_k=800:6.2839,lwr_k=900:6.3388,lwr_k=1000:6.4224'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.5635,lwr_k=10:24.404,lwr_k=20:172.0964,lwr_k=30:5743.0227,lwr_k=40:2375.0982,lwr_k=50:2883.399,lwr_k=100:8.1507,lwr_k=200:8.4578,lwr_k=300:8.4853,lwr_k=400:8.7046,lwr_k=500:8.7971,lwr_k=600:8.8193,lwr_k=700:8.8424,lwr_k=800:8.9577,lwr_k=900:8.9397,lwr_k=1000:8.9632'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_79'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.7493,lwr_k=10:0.0,lwr_k=20:0.3548,lwr_k=30:2.4023,lwr_k=40:3.6114,lwr_k=50:4.1679,lwr_k=100:5.7478,lwr_k=200:6.8156,lwr_k=300:7.2872,lwr_k=400:7.7039,lwr_k=500:7.9124,lwr_k=600:8.1353,lwr_k=700:8.3066,lwr_k=800:8.4923,lwr_k=900:8.6616,lwr_k=1000:8.7842'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:14.3135,lwr_k=10:60.6287,lwr_k=20:219.4578,lwr_k=30:21.7092,lwr_k=40:15.2877,lwr_k=50:12.8745,lwr_k=100:9.8767,lwr_k=200:9.3818,lwr_k=300:10.1459,lwr_k=400:10.5425,lwr_k=500:10.7752,lwr_k=600:10.9809,lwr_k=700:11.1615,lwr_k=800:11.3494,lwr_k=900:11.5904,lwr_k=1000:11.7344'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.5546,lwr_k=10:0.0,lwr_k=20:0.2157,lwr_k=30:1.7357,lwr_k=40:2.445,lwr_k=50:2.8964,lwr_k=100:4.0846,lwr_k=200:4.4872,lwr_k=300:4.6599,lwr_k=400:4.7819,lwr_k=500:4.8742,lwr_k=600:4.9603,lwr_k=700:5.0044,lwr_k=800:5.0808,lwr_k=900:5.1287,lwr_k=1000:5.1667'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.3391,lwr_k=10:46.113,lwr_k=20:382.8417,lwr_k=30:28.7016,lwr_k=40:15.3996,lwr_k=50:10.6493,lwr_k=100:7.4217,lwr_k=200:6.9502,lwr_k=300:6.9821,lwr_k=400:6.9322,lwr_k=500:6.8219,lwr_k=600:6.8133,lwr_k=700:6.7376,lwr_k=800:6.7585,lwr_k=900:6.7806,lwr_k=1000:6.8104'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.4681,lwr_k=10:0.0,lwr_k=20:0.4635,lwr_k=30:2.8618,lwr_k=40:4.145,lwr_k=50:5.0912,lwr_k=100:6.708,lwr_k=200:8.0497,lwr_k=300:8.5885,lwr_k=400:8.872,lwr_k=500:9.0787,lwr_k=600:9.2821,lwr_k=700:9.46,lwr_k=800:9.5513,lwr_k=900:9.6803,lwr_k=1000:9.7456'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:11.712,lwr_k=10:52.6518,lwr_k=20:950.8237,lwr_k=30:25.2782,lwr_k=40:16.5495,lwr_k=50:13.3886,lwr_k=100:9.5169,lwr_k=200:8.3916,lwr_k=300:8.4484,lwr_k=400:8.5494,lwr_k=500:8.6347,lwr_k=600:8.7184,lwr_k=700:8.8945,lwr_k=800:8.9614,lwr_k=900:9.0798,lwr_k=1000:9.1063'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.2393,lwr_k=10:0.0,lwr_k=20:0.363,lwr_k=30:2.4004,lwr_k=40:3.5191,lwr_k=50:4.2346,lwr_k=100:5.8966,lwr_k=200:7.0202,lwr_k=300:7.6161,lwr_k=400:8.0496,lwr_k=500:8.2507,lwr_k=600:8.4484,lwr_k=700:8.6382,lwr_k=800:8.834,lwr_k=900:9.0412,lwr_k=1000:9.2016'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:16.0848,lwr_k=10:73.4022,lwr_k=20:292.5713,lwr_k=30:24.6082,lwr_k=40:18.7177,lwr_k=50:14.0597,lwr_k=100:10.8637,lwr_k=200:10.7622,lwr_k=300:11.2136,lwr_k=400:11.4476,lwr_k=500:11.5276,lwr_k=600:11.7305,lwr_k=700:11.8423,lwr_k=800:12.0834,lwr_k=900:12.2117,lwr_k=1000:12.3131'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.601,lwr_k=10:0.0,lwr_k=20:0.2529,lwr_k=30:1.9677,lwr_k=40:3.0689,lwr_k=50:3.584,lwr_k=100:5.0017,lwr_k=200:5.9031,lwr_k=300:6.3203,lwr_k=400:6.5714,lwr_k=500:6.7261,lwr_k=600:6.8517,lwr_k=700:6.9301,lwr_k=800:7.0228,lwr_k=900:7.1155,lwr_k=1000:7.2239'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:27.3142,lwr_k=10:84.6222,lwr_k=20:408.4158,lwr_k=30:26.5923,lwr_k=40:21.9943,lwr_k=50:19.4015,lwr_k=100:19.9786,lwr_k=200:21.0621,lwr_k=300:21.9877,lwr_k=400:22.2364,lwr_k=500:22.3452,lwr_k=600:22.9128,lwr_k=700:23.0763,lwr_k=800:23.3361,lwr_k=900:23.5897,lwr_k=1000:23.7364'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:6.2108,lwr_k=10:0.0,lwr_k=20:0.2544,lwr_k=30:1.7859,lwr_k=40:2.6003,lwr_k=50:3.0746,lwr_k=100:4.1384,lwr_k=200:4.6892,lwr_k=300:4.9218,lwr_k=400:5.0669,lwr_k=500:5.1703,lwr_k=600:5.2205,lwr_k=700:5.2831,lwr_k=800:5.334,lwr_k=900:5.3705,lwr_k=1000:5.4188'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.9176,lwr_k=10:77.6522,lwr_k=20:554.1125,lwr_k=30:24.6217,lwr_k=40:28.2485,lwr_k=50:18.6248,lwr_k=100:8.2792,lwr_k=200:8.661,lwr_k=300:9.1123,lwr_k=400:9.0613,lwr_k=500:8.7666,lwr_k=600:8.583,lwr_k=700:8.3794,lwr_k=800:8.3711,lwr_k=900:8.3576,lwr_k=1000:8.1792'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_80'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.7506,lwr_k=10:1.0253,lwr_k=20:3.9596,lwr_k=30:5.0951,lwr_k=40:5.8205,lwr_k=50:6.1782,lwr_k=100:7.042,lwr_k=200:7.459,lwr_k=300:7.6352,lwr_k=400:7.7876,lwr_k=500:7.9529,lwr_k=600:8.0544,lwr_k=700:8.1361,lwr_k=800:8.2464,lwr_k=900:8.2969,lwr_k=1000:8.3437'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.3117,lwr_k=10:257.0405,lwr_k=20:20.5608,lwr_k=30:14.4077,lwr_k=40:12.7343,lwr_k=50:12.0508,lwr_k=100:10.6049,lwr_k=200:9.8315,lwr_k=300:9.9528,lwr_k=400:10.0856,lwr_k=500:10.07,lwr_k=600:10.0669,lwr_k=700:10.1469,lwr_k=800:10.1445,lwr_k=900:10.1753,lwr_k=1000:10.1999'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.4843,lwr_k=10:1.9638,lwr_k=20:4.4257,lwr_k=30:4.979,lwr_k=40:5.31,lwr_k=50:5.4687,lwr_k=100:5.8403,lwr_k=200:6.2665,lwr_k=300:6.3726,lwr_k=400:6.43,lwr_k=500:6.4742,lwr_k=600:6.5017,lwr_k=700:6.5399,lwr_k=800:6.5724,lwr_k=900:6.6056,lwr_k=1000:6.6523'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.3099,lwr_k=10:89.9235,lwr_k=20:10.6904,lwr_k=30:8.7985,lwr_k=40:8.1935,lwr_k=50:7.5843,lwr_k=100:7.2212,lwr_k=200:6.8648,lwr_k=300:6.8167,lwr_k=400:6.8635,lwr_k=500:6.8449,lwr_k=600:6.8483,lwr_k=700:6.8792,lwr_k=800:6.8259,lwr_k=900:6.7984,lwr_k=1000:6.7742'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.9629,lwr_k=10:1.2628,lwr_k=20:4.1465,lwr_k=30:5.11,lwr_k=40:5.5007,lwr_k=50:5.9144,lwr_k=100:6.5602,lwr_k=200:7.0356,lwr_k=300:7.1578,lwr_k=400:7.2827,lwr_k=500:7.3683,lwr_k=600:7.4894,lwr_k=700:7.5681,lwr_k=800:7.6371,lwr_k=900:7.6892,lwr_k=1000:7.7396'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.844,lwr_k=10:137.4258,lwr_k=20:24.5651,lwr_k=30:12.8591,lwr_k=40:10.1883,lwr_k=50:7.6451,lwr_k=100:6.7486,lwr_k=200:6.7511,lwr_k=300:6.6188,lwr_k=400:6.7378,lwr_k=500:6.8517,lwr_k=600:6.9121,lwr_k=700:6.9595,lwr_k=800:7.0602,lwr_k=900:7.1348,lwr_k=1000:7.1953'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.3594,lwr_k=10:1.4809,lwr_k=20:3.8845,lwr_k=30:4.5163,lwr_k=40:4.8638,lwr_k=50:5.0179,lwr_k=100:5.4679,lwr_k=200:5.8424,lwr_k=300:6.0975,lwr_k=400:6.177,lwr_k=500:6.2231,lwr_k=600:6.255,lwr_k=700:6.2798,lwr_k=800:6.3216,lwr_k=900:6.3615,lwr_k=1000:6.665'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.5237,lwr_k=10:85009.6796,lwr_k=20:3091021.4764,lwr_k=30:168.0634,lwr_k=40:4487997.8424,lwr_k=50:9274563.3952,lwr_k=100:9.5327,lwr_k=200:32.4436,lwr_k=300:25.5063,lwr_k=400:9.8565,lwr_k=500:9.8034,lwr_k=600:9.1756,lwr_k=700:9.0842,lwr_k=800:9.1113,lwr_k=900:9.1436,lwr_k=1000:9.126'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.8013,lwr_k=10:1.4534,lwr_k=20:3.5826,lwr_k=30:4.4479,lwr_k=40:4.7948,lwr_k=50:5.012,lwr_k=100:5.4642,lwr_k=200:5.7286,lwr_k=300:5.8265,lwr_k=400:5.8834,lwr_k=500:5.9163,lwr_k=600:5.9789,lwr_k=700:6.037,lwr_k=800:6.0729,lwr_k=900:6.092,lwr_k=1000:6.1179'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:22.3965,lwr_k=10:421953.0891,lwr_k=20:119594.7885,lwr_k=30:504.7767,lwr_k=40:6795.4254,lwr_k=50:20657.8154,lwr_k=100:17.9088,lwr_k=200:16.5926,lwr_k=300:16.7555,lwr_k=400:17.0429,lwr_k=500:17.6623,lwr_k=600:18.411,lwr_k=700:20.0089,lwr_k=800:20.43,lwr_k=900:20.4662,lwr_k=1000:20.6255'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:7.4153,lwr_k=10:2.135,lwr_k=20:4.6002,lwr_k=30:5.4442,lwr_k=40:5.8124,lwr_k=50:6.018,lwr_k=100:6.4546,lwr_k=200:6.7716,lwr_k=300:6.881,lwr_k=400:6.9893,lwr_k=500:7.0179,lwr_k=600:7.0347,lwr_k=700:7.0527,lwr_k=800:7.0545,lwr_k=900:7.0724,lwr_k=1000:7.0957'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.0696,lwr_k=10:124682.2058,lwr_k=20:14.0099,lwr_k=30:11.9969,lwr_k=40:11.6795,lwr_k=50:11.393,lwr_k=100:10.3166,lwr_k=200:9.9149,lwr_k=300:9.8593,lwr_k=400:9.9366,lwr_k=500:9.8629,lwr_k=600:9.8808,lwr_k=700:9.8569,lwr_k=800:9.8313,lwr_k=900:9.8237,lwr_k=1000:9.8652'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_81'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.845,lwr_k=10:0.0005,lwr_k=20:0.1841,lwr_k=30:1.7607,lwr_k=40:13.1756,lwr_k=50:14.3337,lwr_k=100:8.8465,lwr_k=200:8.6071,lwr_k=300:8.7227,lwr_k=400:8.3447,lwr_k=500:9.1732,lwr_k=600:8.9429,lwr_k=700:8.9568,lwr_k=800:10.3961,lwr_k=900:10.9506,lwr_k=1000:11.4158'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.5041,lwr_k=10:1239.9672,lwr_k=20:45305443.6125,lwr_k=30:78446446.1828,lwr_k=40:4854155.3971,lwr_k=50:206313.1458,lwr_k=100:486944.0717,lwr_k=200:177568.2805,lwr_k=300:12.141,lwr_k=400:10.325,lwr_k=500:11.2304,lwr_k=600:10.597,lwr_k=700:11.3376,lwr_k=800:12.8636,lwr_k=900:13.3697,lwr_k=1000:13.7482'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.3824,lwr_k=10:0.0,lwr_k=20:0.0007,lwr_k=30:0.5771,lwr_k=40:3.7399,lwr_k=50:7.0704,lwr_k=100:6.277,lwr_k=200:9.0004,lwr_k=300:9.158,lwr_k=400:9.5285,lwr_k=500:11.3698,lwr_k=600:9.9876,lwr_k=700:9.0333,lwr_k=800:9.2051,lwr_k=900:8.7627,lwr_k=1000:9.0662'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.4735,lwr_k=10:32.5208,lwr_k=20:1842.0046,lwr_k=30:80512.4407,lwr_k=40:2347182.4134,lwr_k=50:241083.0634,lwr_k=100:24.3107,lwr_k=200:14.4869,lwr_k=300:11.9903,lwr_k=400:10.6511,lwr_k=500:12.2763,lwr_k=600:9.9516,lwr_k=700:11.3188,lwr_k=800:9.5855,lwr_k=900:9.5475,lwr_k=1000:9.6588'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:13.1784,lwr_k=10:0.0072,lwr_k=20:0.7014,lwr_k=30:2.4863,lwr_k=40:9.0695,lwr_k=50:9.154,lwr_k=100:12.129,lwr_k=200:12.2552,lwr_k=300:10.4471,lwr_k=400:10.6914,lwr_k=500:10.5335,lwr_k=600:10.6051,lwr_k=700:10.5238,lwr_k=800:10.5713,lwr_k=900:10.5513,lwr_k=1000:10.7242'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.5714,lwr_k=10:8730.6024,lwr_k=20:296003.5687,lwr_k=30:15310.4792,lwr_k=40:5095.7107,lwr_k=50:1084.5449,lwr_k=100:40.0453,lwr_k=200:21.2592,lwr_k=300:11.0835,lwr_k=400:11.879,lwr_k=500:9.8776,lwr_k=600:9.8415,lwr_k=700:10.1597,lwr_k=800:9.7521,lwr_k=900:9.2662,lwr_k=1000:9.2867'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.3446,lwr_k=10:0.0,lwr_k=20:0.1053,lwr_k=30:1.0974,lwr_k=40:8.6094,lwr_k=50:7.3797,lwr_k=100:7.1562,lwr_k=200:9.0224,lwr_k=300:8.7312,lwr_k=400:9.0696,lwr_k=500:9.1968,lwr_k=600:9.2028,lwr_k=700:9.03,lwr_k=800:9.2823,lwr_k=900:9.0452,lwr_k=1000:9.662'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:13.591,lwr_k=10:85.5341,lwr_k=20:6320.3555,lwr_k=30:6592445.345,lwr_k=40:16962.3665,lwr_k=50:699.3812,lwr_k=100:222.8623,lwr_k=200:14.6811,lwr_k=300:12.4977,lwr_k=400:12.0674,lwr_k=500:11.8121,lwr_k=600:11.3285,lwr_k=700:10.6003,lwr_k=800:10.9974,lwr_k=900:10.8469,lwr_k=1000:11.7138'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.4581,lwr_k=10:0.0,lwr_k=20:0.0918,lwr_k=30:0.9016,lwr_k=40:4.9794,lwr_k=50:4.9951,lwr_k=100:5.4636,lwr_k=200:6.4324,lwr_k=300:6.9173,lwr_k=400:8.7856,lwr_k=500:7.279,lwr_k=600:8.5925,lwr_k=700:8.0076,lwr_k=800:7.3756,lwr_k=900:7.2782,lwr_k=1000:7.4219'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:23.6231,lwr_k=10:85.7761,lwr_k=20:1087.6627,lwr_k=30:10062.6089,lwr_k=40:4413.5377,lwr_k=50:134.2987,lwr_k=100:34.4066,lwr_k=200:22.2108,lwr_k=300:22.762,lwr_k=400:24.2551,lwr_k=500:22.9603,lwr_k=600:23.7852,lwr_k=700:23.0193,lwr_k=800:22.0193,lwr_k=900:21.4854,lwr_k=1000:20.848'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:9.2463,lwr_k=10:0.0,lwr_k=20:0.0223,lwr_k=30:0.598,lwr_k=40:3.8057,lwr_k=50:5.5635,lwr_k=100:5.0597,lwr_k=200:8.6575,lwr_k=300:9.2077,lwr_k=400:8.6969,lwr_k=500:7.4533,lwr_k=600:7.8218,lwr_k=700:8.8345,lwr_k=800:7.8658,lwr_k=900:7.9023,lwr_k=1000:8.2831'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.3579,lwr_k=10:35.7953,lwr_k=20:680.1105,lwr_k=30:465.2365,lwr_k=40:1193.4265,lwr_k=50:194.2872,lwr_k=100:21.3693,lwr_k=200:15.129,lwr_k=300:19.7528,lwr_k=400:22.96,lwr_k=500:16.6511,lwr_k=600:13.7162,lwr_k=700:13.1114,lwr_k=800:14.274,lwr_k=900:13.3067,lwr_k=1000:12.3428'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_82'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.5144,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.6233,lwr_k=40:1.8703,lwr_k=50:2.6105,lwr_k=100:3.874,lwr_k=200:4.5089,lwr_k=300:4.7689,lwr_k=400:4.8876,lwr_k=500:4.9748,lwr_k=600:5.0325,lwr_k=700:5.0905,lwr_k=800:5.1427,lwr_k=900:5.1689,lwr_k=1000:5.2036'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.4756,lwr_k=10:51.3889,lwr_k=20:8488.079,lwr_k=30:446.6562,lwr_k=40:26.2485,lwr_k=50:14.201,lwr_k=100:9.0267,lwr_k=200:7.7523,lwr_k=300:7.8309,lwr_k=400:7.877,lwr_k=500:7.7982,lwr_k=600:7.9414,lwr_k=700:7.9653,lwr_k=800:8.0695,lwr_k=900:8.1105,lwr_k=1000:8.1597'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.3006,lwr_k=10:0.0,lwr_k=20:0.0001,lwr_k=30:0.8134,lwr_k=40:2.1599,lwr_k=50:3.2907,lwr_k=100:5.8202,lwr_k=200:7.5997,lwr_k=300:8.438,lwr_k=400:8.7826,lwr_k=500:9.0242,lwr_k=600:9.2299,lwr_k=700:9.3739,lwr_k=800:9.5356,lwr_k=900:9.6803,lwr_k=1000:9.7834'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.6921,lwr_k=10:66.0437,lwr_k=20:196.9076,lwr_k=30:258.3232,lwr_k=40:116.3461,lwr_k=50:35.966,lwr_k=100:11.2398,lwr_k=200:9.6034,lwr_k=300:9.5313,lwr_k=400:9.4528,lwr_k=500:9.4822,lwr_k=600:9.4164,lwr_k=700:9.3801,lwr_k=800:9.5014,lwr_k=900:9.5549,lwr_k=1000:9.5931'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:21.3231,lwr_k=10:0.0,lwr_k=20:0.0001,lwr_k=30:1.2159,lwr_k=40:3.1465,lwr_k=50:4.4756,lwr_k=100:7.6683,lwr_k=200:10.5477,lwr_k=300:11.9651,lwr_k=400:12.7645,lwr_k=500:13.2187,lwr_k=600:13.7426,lwr_k=700:14.1381,lwr_k=800:14.4613,lwr_k=900:14.7866,lwr_k=1000:14.9484'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:15.3075,lwr_k=10:69.3041,lwr_k=20:331.6284,lwr_k=30:7488.9179,lwr_k=40:56.9403,lwr_k=50:39.2022,lwr_k=100:14.321,lwr_k=200:11.3366,lwr_k=300:11.2378,lwr_k=400:9.8991,lwr_k=500:9.9015,lwr_k=600:9.9362,lwr_k=700:9.9935,lwr_k=800:9.9483,lwr_k=900:10.0392,lwr_k=1000:10.0449'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:16.3157,lwr_k=10:0.0,lwr_k=20:0.0005,lwr_k=30:0.9251,lwr_k=40:2.6078,lwr_k=50:3.6808,lwr_k=100:6.9073,lwr_k=200:8.8378,lwr_k=300:10.0611,lwr_k=400:10.617,lwr_k=500:11.0929,lwr_k=600:11.3109,lwr_k=700:11.5801,lwr_k=800:11.7745,lwr_k=900:12.0118,lwr_k=1000:12.1528'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:15.5114,lwr_k=10:205.4565,lwr_k=20:1551.5632,lwr_k=30:1171.4338,lwr_k=40:52.7512,lwr_k=50:27.9115,lwr_k=100:16.6164,lwr_k=200:13.7086,lwr_k=300:12.2272,lwr_k=400:12.1066,lwr_k=500:11.9807,lwr_k=600:11.9068,lwr_k=700:11.7215,lwr_k=800:11.8937,lwr_k=900:11.7478,lwr_k=1000:11.8518'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.5653,lwr_k=10:0.0,lwr_k=20:0.0002,lwr_k=30:0.8043,lwr_k=40:1.7622,lwr_k=50:2.4189,lwr_k=100:3.7775,lwr_k=200:4.5118,lwr_k=300:4.7823,lwr_k=400:4.9235,lwr_k=500:4.9973,lwr_k=600:5.0637,lwr_k=700:5.1087,lwr_k=800:5.1447,lwr_k=900:5.1801,lwr_k=1000:5.2282'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:19.6013,lwr_k=10:71.4545,lwr_k=20:928.3548,lwr_k=30:136.2169,lwr_k=40:47.8322,lwr_k=50:44.0695,lwr_k=100:19.6682,lwr_k=200:25.3699,lwr_k=300:18.554,lwr_k=400:19.0433,lwr_k=500:18.8647,lwr_k=600:18.9375,lwr_k=700:18.8316,lwr_k=800:19.0091,lwr_k=900:18.9485,lwr_k=1000:19.2092'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:8.0184,lwr_k=10:0.0,lwr_k=20:0.0001,lwr_k=30:0.8544,lwr_k=40:2.1191,lwr_k=50:2.8759,lwr_k=100:4.6832,lwr_k=200:5.8181,lwr_k=300:6.3073,lwr_k=400:6.593,lwr_k=500:6.7778,lwr_k=600:6.8914,lwr_k=700:6.9604,lwr_k=800:7.0786,lwr_k=900:7.1558,lwr_k=1000:7.2029'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.8631,lwr_k=10:41.819,lwr_k=20:263.0639,lwr_k=30:169.6641,lwr_k=40:48.02,lwr_k=50:21.5502,lwr_k=100:9.5241,lwr_k=200:8.9642,lwr_k=300:9.1931,lwr_k=400:9.1268,lwr_k=500:9.1078,lwr_k=600:9.1659,lwr_k=700:9.2049,lwr_k=800:9.2732,lwr_k=900:9.2325,lwr_k=1000:9.2161'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_83'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.2521,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.248,lwr_k=40:2.3406,lwr_k=50:2.8096,lwr_k=100:4.0747,lwr_k=200:4.7929,lwr_k=300:5.0982,lwr_k=400:5.2814,lwr_k=500:5.373,lwr_k=600:5.469,lwr_k=700:5.5729,lwr_k=800:5.5951,lwr_k=900:5.6364,lwr_k=1000:5.6854'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.4343,lwr_k=10:26.6464,lwr_k=20:102.9869,lwr_k=30:44.8584,lwr_k=40:13.0254,lwr_k=50:9.1258,lwr_k=100:7.0397,lwr_k=200:6.7803,lwr_k=300:6.7584,lwr_k=400:6.7739,lwr_k=500:6.752,lwr_k=600:6.7637,lwr_k=700:6.8277,lwr_k=800:6.8976,lwr_k=900:6.9373,lwr_k=1000:6.9549'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.0946,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.061,lwr_k=40:2.1242,lwr_k=50:2.7329,lwr_k=100:4.2435,lwr_k=200:5.2867,lwr_k=300:5.6675,lwr_k=400:5.9466,lwr_k=500:6.1501,lwr_k=600:6.3078,lwr_k=700:6.4167,lwr_k=800:6.4849,lwr_k=900:6.5377,lwr_k=1000:6.626'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.592,lwr_k=10:23.0422,lwr_k=20:91.1898,lwr_k=30:25.2338,lwr_k=40:13.4119,lwr_k=50:9.5496,lwr_k=100:7.9974,lwr_k=200:7.6023,lwr_k=300:7.6114,lwr_k=400:7.716,lwr_k=500:7.6775,lwr_k=600:7.63,lwr_k=700:7.6569,lwr_k=800:7.5929,lwr_k=900:7.5903,lwr_k=1000:7.6002'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.0508,lwr_k=10:0.0,lwr_k=20:0.0021,lwr_k=30:1.2491,lwr_k=40:2.6334,lwr_k=50:3.6198,lwr_k=100:5.263,lwr_k=200:6.0471,lwr_k=300:6.5057,lwr_k=400:6.647,lwr_k=500:6.7675,lwr_k=600:6.884,lwr_k=700:6.9714,lwr_k=800:7.0189,lwr_k=900:7.0743,lwr_k=1000:7.1362'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.1208,lwr_k=10:49.8406,lwr_k=20:333.7921,lwr_k=30:40.4188,lwr_k=40:12.3231,lwr_k=50:9.9383,lwr_k=100:6.6389,lwr_k=200:6.4009,lwr_k=300:6.3532,lwr_k=400:6.3641,lwr_k=500:6.3911,lwr_k=600:6.4287,lwr_k=700:6.3876,lwr_k=800:6.3803,lwr_k=900:6.3895,lwr_k=1000:6.3891'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.7257,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.3124,lwr_k=40:2.4124,lwr_k=50:3.1197,lwr_k=100:4.6486,lwr_k=200:5.5821,lwr_k=300:6.0739,lwr_k=400:6.349,lwr_k=500:6.4695,lwr_k=600:6.5788,lwr_k=700:6.6421,lwr_k=800:6.6772,lwr_k=900:6.8054,lwr_k=1000:6.7988'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.1387,lwr_k=10:54.7344,lwr_k=20:152.9207,lwr_k=30:30.4454,lwr_k=40:15.3793,lwr_k=50:13.004,lwr_k=100:9.6133,lwr_k=200:9.1513,lwr_k=300:9.2672,lwr_k=400:9.2181,lwr_k=500:9.2741,lwr_k=600:9.2725,lwr_k=700:9.3215,lwr_k=800:9.2784,lwr_k=900:9.2998,lwr_k=1000:9.3032'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.2409,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.4314,lwr_k=40:2.3234,lwr_k=50:2.8042,lwr_k=100:3.7963,lwr_k=200:4.4025,lwr_k=300:4.6153,lwr_k=400:4.7497,lwr_k=500:4.812,lwr_k=600:4.8629,lwr_k=700:4.9201,lwr_k=800:4.935,lwr_k=900:4.9792,lwr_k=1000:5.0058'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:17.1817,lwr_k=10:47.7068,lwr_k=20:384.7701,lwr_k=30:61.8476,lwr_k=40:27.6818,lwr_k=50:16.8786,lwr_k=100:16.5926,lwr_k=200:16.5787,lwr_k=300:16.8972,lwr_k=400:16.7762,lwr_k=500:16.757,lwr_k=600:16.7936,lwr_k=700:16.7446,lwr_k=800:16.8121,lwr_k=900:16.8078,lwr_k=1000:16.7545'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:7.1789,lwr_k=10:0.0,lwr_k=20:0.0002,lwr_k=30:1.1578,lwr_k=40:2.1147,lwr_k=50:2.7082,lwr_k=100:3.9877,lwr_k=200:4.8289,lwr_k=300:5.2206,lwr_k=400:5.3862,lwr_k=500:5.6026,lwr_k=600:5.7003,lwr_k=700:5.8184,lwr_k=800:5.8893,lwr_k=900:5.9494,lwr_k=1000:5.9982'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.3702,lwr_k=10:69.6108,lwr_k=20:956.828,lwr_k=30:310.0459,lwr_k=40:160.3127,lwr_k=50:126.1662,lwr_k=100:10.6969,lwr_k=200:16.7764,lwr_k=300:13.7008,lwr_k=400:9.5075,lwr_k=500:8.9472,lwr_k=600:8.6729,lwr_k=700:8.5678,lwr_k=800:8.5138,lwr_k=900:8.4609,lwr_k=1000:8.3863'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_84'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:13.0155,lwr_k=10:0.8212,lwr_k=20:3.9635,lwr_k=30:5.3396,lwr_k=40:6.2124,lwr_k=50:6.7517,lwr_k=100:7.9872,lwr_k=200:9.5948,lwr_k=300:10.5068,lwr_k=400:10.9995,lwr_k=500:11.228,lwr_k=600:11.4963,lwr_k=700:11.6314,lwr_k=800:11.7404,lwr_k=900:11.8314,lwr_k=1000:11.8793'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:14.3513,lwr_k=10:52966123.8804,lwr_k=20:1030764.6305,lwr_k=30:130796.7796,lwr_k=40:263.3525,lwr_k=50:140.384,lwr_k=100:9.4508,lwr_k=200:11.0702,lwr_k=300:12.0071,lwr_k=400:12.3789,lwr_k=500:12.4959,lwr_k=600:12.5722,lwr_k=700:12.6831,lwr_k=800:12.7703,lwr_k=900:12.8999,lwr_k=1000:12.9598'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.3638,lwr_k=10:1.5839,lwr_k=20:4.3657,lwr_k=30:5.4259,lwr_k=40:6.2104,lwr_k=50:6.7133,lwr_k=100:7.9163,lwr_k=200:8.4106,lwr_k=300:8.6584,lwr_k=400:8.7706,lwr_k=500:8.8569,lwr_k=600:8.9039,lwr_k=700:8.9774,lwr_k=800:9.0471,lwr_k=900:9.1035,lwr_k=1000:9.1551'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.1971,lwr_k=10:7417926.5824,lwr_k=20:7174648.8838,lwr_k=30:5321523.502,lwr_k=40:10464.5579,lwr_k=50:29714.2823,lwr_k=100:49.9127,lwr_k=200:10.4154,lwr_k=300:9.3134,lwr_k=400:9.3871,lwr_k=500:9.4886,lwr_k=600:9.3851,lwr_k=700:9.3982,lwr_k=800:9.4021,lwr_k=900:9.4293,lwr_k=1000:9.467'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.4498,lwr_k=10:0.5822,lwr_k=20:4.1179,lwr_k=30:5.1932,lwr_k=40:5.8161,lwr_k=50:6.2783,lwr_k=100:7.1386,lwr_k=200:7.9752,lwr_k=300:8.3706,lwr_k=400:8.5635,lwr_k=500:8.6714,lwr_k=600:8.8458,lwr_k=700:8.9398,lwr_k=800:9.1167,lwr_k=900:9.244,lwr_k=1000:9.3827'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.8525,lwr_k=10:783132.8423,lwr_k=20:1378.5612,lwr_k=30:85.2953,lwr_k=40:20.4137,lwr_k=50:9.4791,lwr_k=100:7.3822,lwr_k=200:7.031,lwr_k=300:7.1788,lwr_k=400:7.1152,lwr_k=500:7.1732,lwr_k=600:7.1872,lwr_k=700:7.243,lwr_k=800:7.2854,lwr_k=900:7.3058,lwr_k=1000:7.3723'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.9453,lwr_k=10:0.9278,lwr_k=20:3.744,lwr_k=30:4.5814,lwr_k=40:5.0835,lwr_k=50:5.6363,lwr_k=100:6.5458,lwr_k=200:7.2046,lwr_k=300:7.492,lwr_k=400:7.9706,lwr_k=500:8.0872,lwr_k=600:8.1689,lwr_k=700:8.2438,lwr_k=800:8.3077,lwr_k=900:8.4017,lwr_k=1000:8.4208'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.8595,lwr_k=10:260318059.7973,lwr_k=20:639.9433,lwr_k=30:37.2729,lwr_k=40:13.2387,lwr_k=50:9.7891,lwr_k=100:9.4547,lwr_k=200:9.4896,lwr_k=300:9.5792,lwr_k=400:9.6211,lwr_k=500:9.6696,lwr_k=600:9.6458,lwr_k=700:9.6095,lwr_k=800:9.6459,lwr_k=900:9.6829,lwr_k=1000:9.6879'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.4787,lwr_k=10:0.8223,lwr_k=20:3.4436,lwr_k=30:4.4089,lwr_k=40:4.7612,lwr_k=50:5.046,lwr_k=100:5.7287,lwr_k=200:6.2999,lwr_k=300:6.5198,lwr_k=400:6.6654,lwr_k=500:6.7589,lwr_k=600:6.8342,lwr_k=700:6.8688,lwr_k=800:6.9293,lwr_k=900:6.9821,lwr_k=1000:7.0282'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:23.4356,lwr_k=10:1985839212.9948,lwr_k=20:145143.9424,lwr_k=30:437506.1475,lwr_k=40:75842.0747,lwr_k=50:16997.501,lwr_k=100:18.4027,lwr_k=200:20.3297,lwr_k=300:20.7859,lwr_k=400:21.0805,lwr_k=500:21.1496,lwr_k=600:21.1638,lwr_k=700:21.1997,lwr_k=800:21.2184,lwr_k=900:21.3181,lwr_k=1000:21.548'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:11.239,lwr_k=10:0.6584,lwr_k=20:3.9434,lwr_k=30:5.1153,lwr_k=40:5.7278,lwr_k=50:6.2582,lwr_k=100:7.4661,lwr_k=200:8.4968,lwr_k=300:8.7623,lwr_k=400:9.0992,lwr_k=500:9.27,lwr_k=600:9.4453,lwr_k=700:9.5528,lwr_k=800:9.6444,lwr_k=900:9.7148,lwr_k=1000:9.7667'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.0322,lwr_k=10:2604857.4492,lwr_k=20:20.8386,lwr_k=30:11.6668,lwr_k=40:11.2785,lwr_k=50:10.6103,lwr_k=100:10.6126,lwr_k=200:10.5988,lwr_k=300:10.6151,lwr_k=400:10.5731,lwr_k=500:10.5743,lwr_k=600:10.6394,lwr_k=700:10.662,lwr_k=800:10.7201,lwr_k=900:10.752,lwr_k=1000:10.7934'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_85'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:13.8358,lwr_k=10:5.2956,lwr_k=20:6.9298,lwr_k=30:7.6231,lwr_k=40:7.9626,lwr_k=50:8.255,lwr_k=100:9.0507,lwr_k=200:9.5607,lwr_k=300:9.8457,lwr_k=400:10.1074,lwr_k=500:10.2731,lwr_k=600:10.4199,lwr_k=700:10.5825,lwr_k=800:10.7034,lwr_k=900:10.8395,lwr_k=1000:10.9844'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:17.5102,lwr_k=10:20.5653,lwr_k=20:13.9706,lwr_k=30:11.589,lwr_k=40:11.2403,lwr_k=50:10.8651,lwr_k=100:10.78,lwr_k=200:11.1611,lwr_k=300:11.4187,lwr_k=400:11.7284,lwr_k=500:11.9956,lwr_k=600:12.2836,lwr_k=700:12.4434,lwr_k=800:12.7376,lwr_k=900:13.0489,lwr_k=1000:13.2773'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:11.8433,lwr_k=10:4.9615,lwr_k=20:7.2113,lwr_k=30:7.7303,lwr_k=40:8.0011,lwr_k=50:8.1809,lwr_k=100:8.7305,lwr_k=200:9.3388,lwr_k=300:9.7907,lwr_k=400:10.0508,lwr_k=500:10.1128,lwr_k=600:10.1609,lwr_k=700:10.2236,lwr_k=800:10.2943,lwr_k=900:10.375,lwr_k=1000:10.461'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:13.0902,lwr_k=10:107.5649,lwr_k=20:11.0162,lwr_k=30:10.1417,lwr_k=40:9.7731,lwr_k=50:9.8081,lwr_k=100:9.8729,lwr_k=200:10.2119,lwr_k=300:10.4507,lwr_k=400:10.6583,lwr_k=500:10.723,lwr_k=600:10.8269,lwr_k=700:10.9348,lwr_k=800:10.9478,lwr_k=900:11.1041,lwr_k=1000:11.1996'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:22.4365,lwr_k=10:5.8956,lwr_k=20:8.678,lwr_k=30:9.5297,lwr_k=40:10.1338,lwr_k=50:10.5739,lwr_k=100:11.5396,lwr_k=200:12.5906,lwr_k=300:13.343,lwr_k=400:13.8047,lwr_k=500:14.3369,lwr_k=600:14.7671,lwr_k=700:15.2568,lwr_k=800:15.6341,lwr_k=900:16.1149,lwr_k=1000:16.4925'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:17.8784,lwr_k=10:18.1819,lwr_k=20:13.2465,lwr_k=30:12.5191,lwr_k=40:11.3397,lwr_k=50:10.8745,lwr_k=100:10.4298,lwr_k=200:10.4564,lwr_k=300:10.6946,lwr_k=400:10.7349,lwr_k=500:10.8535,lwr_k=600:11.0087,lwr_k=700:11.1472,lwr_k=800:11.3192,lwr_k=900:11.4875,lwr_k=1000:11.7119'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.0477,lwr_k=10:3.7123,lwr_k=20:5.1065,lwr_k=30:5.5885,lwr_k=40:5.9158,lwr_k=50:6.0459,lwr_k=100:6.3086,lwr_k=200:6.4843,lwr_k=300:6.5557,lwr_k=400:6.6216,lwr_k=500:6.6735,lwr_k=600:6.7061,lwr_k=700:6.726,lwr_k=800:6.7572,lwr_k=900:6.7972,lwr_k=1000:6.81'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.3514,lwr_k=10:42.7141,lwr_k=20:16.3348,lwr_k=30:11.6886,lwr_k=40:11.4935,lwr_k=50:11.139,lwr_k=100:11.042,lwr_k=200:10.826,lwr_k=300:10.796,lwr_k=400:10.335,lwr_k=500:10.3056,lwr_k=600:10.3264,lwr_k=700:10.3154,lwr_k=800:10.2921,lwr_k=900:10.2507,lwr_k=1000:10.263'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.2252,lwr_k=10:3.8468,lwr_k=20:5.2203,lwr_k=30:5.7318,lwr_k=40:5.9282,lwr_k=50:6.0931,lwr_k=100:6.4261,lwr_k=200:6.8753,lwr_k=300:7.0174,lwr_k=400:7.0917,lwr_k=500:7.1449,lwr_k=600:7.1888,lwr_k=700:7.2565,lwr_k=800:7.3222,lwr_k=900:7.4067,lwr_k=1000:7.4621'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:27.356,lwr_k=10:20.1046,lwr_k=20:17.9389,lwr_k=30:18.0029,lwr_k=40:18.063,lwr_k=50:19.6185,lwr_k=100:20.6746,lwr_k=200:22.4437,lwr_k=300:23.0752,lwr_k=400:23.1406,lwr_k=500:23.2535,lwr_k=600:23.4495,lwr_k=700:23.8718,lwr_k=800:24.4361,lwr_k=900:24.9358,lwr_k=1000:25.4399'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:11.7674,lwr_k=10:4.3693,lwr_k=20:6.2934,lwr_k=30:6.9806,lwr_k=40:7.3699,lwr_k=50:7.5054,lwr_k=100:8.2291,lwr_k=200:8.7745,lwr_k=300:9.1632,lwr_k=400:9.3236,lwr_k=500:9.4441,lwr_k=600:9.5503,lwr_k=700:9.6544,lwr_k=800:9.7143,lwr_k=900:9.7499,lwr_k=1000:9.7957'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.9586,lwr_k=10:15.7353,lwr_k=20:12.1984,lwr_k=30:12.0689,lwr_k=40:11.6276,lwr_k=50:11.4262,lwr_k=100:11.1136,lwr_k=200:10.8359,lwr_k=300:10.9718,lwr_k=400:11.1914,lwr_k=500:11.3814,lwr_k=600:11.5959,lwr_k=700:11.6641,lwr_k=800:11.7531,lwr_k=900:11.7653,lwr_k=1000:11.8125'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_86'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.1381,lwr_k=10:4.9303,lwr_k=20:5.426,lwr_k=30:5.6094,lwr_k=40:5.6388,lwr_k=50:5.6634,lwr_k=100:5.6969,lwr_k=200:5.7891,lwr_k=300:5.8435,lwr_k=400:5.8873,lwr_k=500:5.927,lwr_k=600:5.9687,lwr_k=700:6.0019,lwr_k=800:6.0493,lwr_k=900:6.0834,lwr_k=1000:6.1019'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.4141,lwr_k=10:10.3273,lwr_k=20:8.995,lwr_k=30:8.6029,lwr_k=40:8.6229,lwr_k=50:8.4779,lwr_k=100:8.5872,lwr_k=200:8.4975,lwr_k=300:8.4525,lwr_k=400:8.4694,lwr_k=500:8.498,lwr_k=600:8.517,lwr_k=700:8.4796,lwr_k=800:8.5371,lwr_k=900:8.595,lwr_k=1000:8.6165'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.1477,lwr_k=10:6.3981,lwr_k=20:7.1745,lwr_k=30:7.4042,lwr_k=40:7.6016,lwr_k=50:7.7563,lwr_k=100:8.0535,lwr_k=200:8.3261,lwr_k=300:8.4084,lwr_k=400:8.4713,lwr_k=500:8.5391,lwr_k=600:8.6027,lwr_k=700:8.6277,lwr_k=800:8.6832,lwr_k=900:8.7374,lwr_k=1000:8.7566'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.479,lwr_k=10:11.141,lwr_k=20:10.664,lwr_k=30:10.6326,lwr_k=40:10.7391,lwr_k=50:10.6947,lwr_k=100:10.3015,lwr_k=200:10.1996,lwr_k=300:10.1384,lwr_k=400:10.1951,lwr_k=500:10.112,lwr_k=600:10.1227,lwr_k=700:10.1021,lwr_k=800:10.1201,lwr_k=900:10.1249,lwr_k=1000:10.1906'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.405,lwr_k=10:5.7867,lwr_k=20:6.5434,lwr_k=30:6.6824,lwr_k=40:6.8398,lwr_k=50:7.0843,lwr_k=100:7.2949,lwr_k=200:7.4572,lwr_k=300:7.5414,lwr_k=400:7.5603,lwr_k=500:7.6036,lwr_k=600:7.7064,lwr_k=700:7.783,lwr_k=800:7.9014,lwr_k=900:8.0046,lwr_k=1000:8.1382'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.7255,lwr_k=10:9.7598,lwr_k=20:7.5206,lwr_k=30:7.3769,lwr_k=40:7.1411,lwr_k=50:7.026,lwr_k=100:6.9074,lwr_k=200:7.0916,lwr_k=300:7.1528,lwr_k=400:7.2211,lwr_k=500:7.2874,lwr_k=600:7.3759,lwr_k=700:7.4519,lwr_k=800:7.4594,lwr_k=900:7.5736,lwr_k=1000:7.813'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.7144,lwr_k=10:4.701,lwr_k=20:5.5193,lwr_k=30:5.7376,lwr_k=40:5.8008,lwr_k=50:5.8802,lwr_k=100:6.0898,lwr_k=200:6.1558,lwr_k=300:6.1672,lwr_k=400:6.2169,lwr_k=500:6.2412,lwr_k=600:6.2648,lwr_k=700:6.291,lwr_k=800:6.3241,lwr_k=900:6.3694,lwr_k=1000:6.4241'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:11.2285,lwr_k=10:14.2592,lwr_k=20:11.8635,lwr_k=30:11.0913,lwr_k=40:11.0304,lwr_k=50:11.0201,lwr_k=100:11.2174,lwr_k=200:11.3136,lwr_k=300:11.4115,lwr_k=400:11.3845,lwr_k=500:11.5005,lwr_k=600:11.4881,lwr_k=700:11.431,lwr_k=800:11.4646,lwr_k=900:11.5439,lwr_k=1000:11.5891'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.8847,lwr_k=10:4.6717,lwr_k=20:5.2838,lwr_k=30:5.4204,lwr_k=40:5.4921,lwr_k=50:5.5112,lwr_k=100:5.6407,lwr_k=200:5.7268,lwr_k=300:5.7965,lwr_k=400:5.8738,lwr_k=500:5.9169,lwr_k=600:5.9455,lwr_k=700:5.9645,lwr_k=800:5.9817,lwr_k=900:6.0021,lwr_k=1000:6.0222'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:24.2416,lwr_k=10:18.1914,lwr_k=20:17.2355,lwr_k=30:17.5827,lwr_k=40:17.3445,lwr_k=50:18.2427,lwr_k=100:18.9012,lwr_k=200:19.4698,lwr_k=300:19.9013,lwr_k=400:20.4578,lwr_k=500:20.9534,lwr_k=600:21.1578,lwr_k=700:21.2826,lwr_k=800:21.3632,lwr_k=900:21.4364,lwr_k=1000:21.5136'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:6.5521,lwr_k=10:4.8358,lwr_k=20:5.459,lwr_k=30:5.7202,lwr_k=40:5.7472,lwr_k=50:5.7722,lwr_k=100:5.9653,lwr_k=200:6.0025,lwr_k=300:6.0592,lwr_k=400:6.0946,lwr_k=500:6.1058,lwr_k=600:6.1214,lwr_k=700:6.1423,lwr_k=800:6.1588,lwr_k=900:6.181,lwr_k=1000:6.2098'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.635,lwr_k=10:11.5416,lwr_k=20:10.047,lwr_k=30:9.6179,lwr_k=40:9.3976,lwr_k=50:9.1944,lwr_k=100:9.031,lwr_k=200:8.9372,lwr_k=300:9.0335,lwr_k=400:9.0249,lwr_k=500:9.1052,lwr_k=600:9.1188,lwr_k=700:9.1336,lwr_k=800:9.1793,lwr_k=900:9.1696,lwr_k=1000:9.1609'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_87'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.2924,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.7591,lwr_k=40:1.9455,lwr_k=50:2.0472,lwr_k=100:3.8681,lwr_k=200:4.6241,lwr_k=300:5.0559,lwr_k=400:5.2626,lwr_k=500:5.4043,lwr_k=600:5.4901,lwr_k=700:5.5558,lwr_k=800:5.6468,lwr_k=900:5.6912,lwr_k=1000:5.733'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.6873,lwr_k=10:26.5553,lwr_k=20:212.8574,lwr_k=30:4032821.8704,lwr_k=40:430316.1578,lwr_k=50:9893.897,lwr_k=100:18.8062,lwr_k=200:6.7993,lwr_k=300:6.7803,lwr_k=400:6.7829,lwr_k=500:6.7825,lwr_k=600:6.8582,lwr_k=700:6.9049,lwr_k=800:6.9808,lwr_k=900:7.0386,lwr_k=1000:7.0721'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.3003,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.9641,lwr_k=40:1.4546,lwr_k=50:2.4643,lwr_k=100:4.3881,lwr_k=200:5.7383,lwr_k=300:6.18,lwr_k=400:6.5524,lwr_k=500:6.781,lwr_k=600:6.8865,lwr_k=700:6.9979,lwr_k=800:7.0953,lwr_k=900:7.1662,lwr_k=1000:7.2366'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.2062,lwr_k=10:42.4792,lwr_k=20:107.2043,lwr_k=30:86610703.8591,lwr_k=40:72.1623,lwr_k=50:114.6496,lwr_k=100:58.9505,lwr_k=200:10.3149,lwr_k=300:7.8175,lwr_k=400:8.0443,lwr_k=500:8.0755,lwr_k=600:8.0974,lwr_k=700:8.2537,lwr_k=800:8.3104,lwr_k=900:8.3791,lwr_k=1000:8.4579'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.9669,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.4152,lwr_k=40:1.601,lwr_k=50:2.4872,lwr_k=100:4.4897,lwr_k=200:5.551,lwr_k=300:5.9606,lwr_k=400:6.2336,lwr_k=500:6.4299,lwr_k=600:6.5678,lwr_k=700:6.6802,lwr_k=800:6.7556,lwr_k=900:6.8286,lwr_k=1000:6.8913'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.5549,lwr_k=10:19.9454,lwr_k=20:72.0829,lwr_k=30:1673.931,lwr_k=40:47.5546,lwr_k=50:14.4949,lwr_k=100:7.7808,lwr_k=200:7.0306,lwr_k=300:6.8323,lwr_k=400:6.8309,lwr_k=500:6.8454,lwr_k=600:6.8776,lwr_k=700:6.8925,lwr_k=800:6.8925,lwr_k=900:6.8975,lwr_k=1000:6.8773'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.737,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.876,lwr_k=40:1.7401,lwr_k=50:2.1734,lwr_k=100:4.2869,lwr_k=200:5.3325,lwr_k=300:5.7435,lwr_k=400:5.9611,lwr_k=500:6.1739,lwr_k=600:6.3092,lwr_k=700:6.4667,lwr_k=800:6.5683,lwr_k=900:6.6515,lwr_k=1000:6.7124'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:11.6528,lwr_k=10:30.982,lwr_k=20:225.5613,lwr_k=30:8447.1796,lwr_k=40:43.002,lwr_k=50:18.3925,lwr_k=100:10.1499,lwr_k=200:9.7966,lwr_k=300:9.8033,lwr_k=400:9.9181,lwr_k=500:10.0758,lwr_k=600:10.3263,lwr_k=700:10.4366,lwr_k=800:10.4399,lwr_k=900:10.4905,lwr_k=1000:10.6114'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.1136,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.5249,lwr_k=40:1.3438,lwr_k=50:2.0221,lwr_k=100:3.2848,lwr_k=200:3.9737,lwr_k=300:4.2393,lwr_k=400:4.4365,lwr_k=500:4.528,lwr_k=600:4.6077,lwr_k=700:4.6622,lwr_k=800:4.733,lwr_k=900:4.7615,lwr_k=1000:4.7914'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:16.5963,lwr_k=10:50.9519,lwr_k=20:975.8798,lwr_k=30:2854270.1922,lwr_k=40:411.0297,lwr_k=50:226.029,lwr_k=100:24.6556,lwr_k=200:15.3941,lwr_k=300:15.7569,lwr_k=400:16.1647,lwr_k=500:16.137,lwr_k=600:16.0168,lwr_k=700:15.9265,lwr_k=800:16.0546,lwr_k=900:16.0988,lwr_k=1000:16.102'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:7.1291,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:3.7081,lwr_k=40:1.3724,lwr_k=50:2.1918,lwr_k=100:4.0997,lwr_k=200:5.2917,lwr_k=300:5.6437,lwr_k=400:5.8977,lwr_k=500:6.0166,lwr_k=600:6.1188,lwr_k=700:6.2012,lwr_k=800:6.2802,lwr_k=900:6.3339,lwr_k=1000:6.3743'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.8364,lwr_k=10:31.153,lwr_k=20:121.2093,lwr_k=30:4551.5111,lwr_k=40:50.4215,lwr_k=50:23.1221,lwr_k=100:8.7583,lwr_k=200:8.2049,lwr_k=300:8.2394,lwr_k=400:8.285,lwr_k=500:8.469,lwr_k=600:8.6337,lwr_k=700:8.7096,lwr_k=800:8.8524,lwr_k=900:8.9713,lwr_k=1000:8.944'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_88'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.1807,lwr_k=10:0.0005,lwr_k=20:1.449,lwr_k=30:2.6334,lwr_k=40:3.3218,lwr_k=50:3.7504,lwr_k=100:4.7263,lwr_k=200:5.3367,lwr_k=300:5.5938,lwr_k=400:5.7354,lwr_k=500:5.8464,lwr_k=600:5.9174,lwr_k=700:5.9966,lwr_k=800:6.0648,lwr_k=900:6.1648,lwr_k=1000:6.2376'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.3974,lwr_k=10:48352.3036,lwr_k=20:299950617.0872,lwr_k=30:78963818.758,lwr_k=40:353203036.5985,lwr_k=50:473176601.6036,lwr_k=100:118053339.4119,lwr_k=200:7.4382,lwr_k=300:7.5022,lwr_k=400:7.4326,lwr_k=500:7.3126,lwr_k=600:7.4957,lwr_k=700:8496.785,lwr_k=800:7.5836,lwr_k=900:48167.1848,lwr_k=1000:8.0076'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.2221,lwr_k=10:0.0044,lwr_k=20:7.5463,lwr_k=30:5.8348,lwr_k=40:5.8146,lwr_k=50:6.0113,lwr_k=100:6.6311,lwr_k=200:7.2914,lwr_k=300:7.8131,lwr_k=400:8.0336,lwr_k=500:8.2529,lwr_k=600:8.4035,lwr_k=700:8.5045,lwr_k=800:8.5941,lwr_k=900:8.6934,lwr_k=1000:8.7783'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.4803,lwr_k=10:25274.156,lwr_k=20:3759.9275,lwr_k=30:31.7329,lwr_k=40:18.7806,lwr_k=50:7856.741,lwr_k=100:11.6959,lwr_k=200:10.9013,lwr_k=300:10.7608,lwr_k=400:10.8809,lwr_k=500:10.9956,lwr_k=600:11.0829,lwr_k=700:11.0483,lwr_k=800:11.0964,lwr_k=900:11.0558,lwr_k=1000:11.0604'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.255,lwr_k=10:0.0333,lwr_k=20:13.5158,lwr_k=30:5.7071,lwr_k=40:5.1379,lwr_k=50:5.3481,lwr_k=100:6.091,lwr_k=200:6.7122,lwr_k=300:6.978,lwr_k=400:7.0625,lwr_k=500:7.1195,lwr_k=600:7.185,lwr_k=700:7.2492,lwr_k=800:7.3007,lwr_k=900:7.2771,lwr_k=1000:7.31'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.7472,lwr_k=10:7028.5778,lwr_k=20:151998.695,lwr_k=30:77696.092,lwr_k=40:9998.3709,lwr_k=50:3598.1332,lwr_k=100:2036.6678,lwr_k=200:7.6975,lwr_k=300:7.5925,lwr_k=400:7.6023,lwr_k=500:7.6156,lwr_k=600:7.7157,lwr_k=700:7.6995,lwr_k=800:7.6579,lwr_k=900:7.7186,lwr_k=1000:7.7443'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.1506,lwr_k=10:0.0001,lwr_k=20:1.6654,lwr_k=30:2.9245,lwr_k=40:3.5114,lwr_k=50:3.9082,lwr_k=100:4.8173,lwr_k=200:5.3115,lwr_k=300:5.5044,lwr_k=400:5.6067,lwr_k=500:5.6957,lwr_k=600:5.7777,lwr_k=700:5.8031,lwr_k=800:5.8207,lwr_k=900:5.8458,lwr_k=1000:5.8641'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.1477,lwr_k=10:1033.8316,lwr_k=20:10966279584.2646,lwr_k=30:12570090062.3112,lwr_k=40:712533338.5838,lwr_k=50:399495848.3875,lwr_k=100:9881189.2892,lwr_k=200:10.9488,lwr_k=300:738141.9421,lwr_k=400:10.452,lwr_k=500:2476.1545,lwr_k=600:4199.755,lwr_k=700:122838.5122,lwr_k=800:173531.9243,lwr_k=900:57460.6597,lwr_k=1000:152058.7641'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.1128,lwr_k=10:0.0005,lwr_k=20:1.0157,lwr_k=30:2.2513,lwr_k=40:3.0718,lwr_k=50:3.6064,lwr_k=100:4.5406,lwr_k=200:5.1596,lwr_k=300:5.4307,lwr_k=400:5.5272,lwr_k=500:5.6887,lwr_k=600:5.714,lwr_k=700:5.7882,lwr_k=800:5.8707,lwr_k=900:5.9887,lwr_k=1000:6.0876'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:114.9525,lwr_k=10:883.8959,lwr_k=20:764217729.1925,lwr_k=30:541705269.4929,lwr_k=40:435826383.7541,lwr_k=50:643546179.0273,lwr_k=100:82535006.0603,lwr_k=200:209626645.1676,lwr_k=300:112148101.9668,lwr_k=400:1606426.0264,lwr_k=500:1701230.2667,lwr_k=600:1186915.7446,lwr_k=700:696.0726,lwr_k=800:940.1484,lwr_k=900:714.3062,lwr_k=1000:209.4883'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:6.7871,lwr_k=10:0.0033,lwr_k=20:13.4126,lwr_k=30:5.9905,lwr_k=40:5.0844,lwr_k=50:5.1552,lwr_k=100:5.4574,lwr_k=200:5.881,lwr_k=300:6.078,lwr_k=400:6.1724,lwr_k=500:6.226,lwr_k=600:6.2882,lwr_k=700:6.3254,lwr_k=800:6.3656,lwr_k=900:6.3776,lwr_k=1000:6.4215'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.1823,lwr_k=10:504.6491,lwr_k=20:65860.1908,lwr_k=30:3751.8537,lwr_k=40:49.8013,lwr_k=50:165.5147,lwr_k=100:19.062,lwr_k=200:10.6318,lwr_k=300:8.1782,lwr_k=400:8.1521,lwr_k=500:8.1206,lwr_k=600:8.2329,lwr_k=700:8.0861,lwr_k=800:8.0584,lwr_k=900:8.019,lwr_k=1000:8.0547'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_89'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:198.6529,lwr_k=10:204.1162,lwr_k=20:199.5629,lwr_k=30:199.1225,lwr_k=40:200.2207,lwr_k=50:201.3046,lwr_k=100:199.0461,lwr_k=200:198.7916,lwr_k=300:198.6531,lwr_k=400:198.9246,lwr_k=500:199.04,lwr_k=600:199.3729,lwr_k=700:198.9219,lwr_k=800:198.6668,lwr_k=900:198.6696,lwr_k=1000:198.66'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:258.1252,lwr_k=10:258.1761,lwr_k=20:256.8263,lwr_k=30:257.008,lwr_k=40:256.7936,lwr_k=50:257.0062,lwr_k=100:257.0664,lwr_k=200:259.1259,lwr_k=300:258.0959,lwr_k=400:259.6039,lwr_k=500:259.9529,lwr_k=600:260.81,lwr_k=700:259.5949,lwr_k=800:258.4118,lwr_k=900:258.4411,lwr_k=1000:257.9379'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:239.6367,lwr_k=10:241.3914,lwr_k=20:240.9123,lwr_k=30:239.7128,lwr_k=40:240.4918,lwr_k=50:240.0855,lwr_k=100:239.8793,lwr_k=200:239.6506,lwr_k=300:239.9838,lwr_k=400:239.6817,lwr_k=500:239.6714,lwr_k=600:239.6373,lwr_k=700:239.6579,lwr_k=800:239.6652,lwr_k=900:239.658,lwr_k=1000:239.6594'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:202.822,lwr_k=10:203.7798,lwr_k=20:203.4181,lwr_k=30:203.064,lwr_k=40:203.1208,lwr_k=50:202.8677,lwr_k=100:202.7683,lwr_k=200:202.7651,lwr_k=300:203.5235,lwr_k=400:202.9945,lwr_k=500:202.9686,lwr_k=600:202.8361,lwr_k=700:202.9308,lwr_k=800:202.952,lwr_k=900:202.9311,lwr_k=1000:202.9354'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:273.725,lwr_k=10:277.091,lwr_k=20:276.4127,lwr_k=30:273.7799,lwr_k=40:275.7834,lwr_k=50:275.1171,lwr_k=100:274.7301,lwr_k=200:274.1188,lwr_k=300:273.7313,lwr_k=400:273.8139,lwr_k=500:273.83,lwr_k=600:273.9627,lwr_k=700:273.8578,lwr_k=800:273.8415,lwr_k=900:273.8576,lwr_k=1000:273.8542'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:155.1861,lwr_k=10:154.056,lwr_k=20:153.8561,lwr_k=30:154.667,lwr_k=40:153.7285,lwr_k=50:153.6867,lwr_k=100:153.7343,lwr_k=200:154.042,lwr_k=300:155.3861,lwr_k=400:154.5445,lwr_k=500:154.4971,lwr_k=600:154.2291,lwr_k=700:154.4258,lwr_k=800:154.4663,lwr_k=900:154.4263,lwr_k=1000:154.4345'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:281550.3502,lwr_k=10:264.7799,lwr_k=20:264.1396,lwr_k=30:261.7813,lwr_k=40:263.5504,lwr_k=50:599.0897,lwr_k=100:262.5815,lwr_k=200:66808546.9207,lwr_k=300:261.7939,lwr_k=400:261.8028,lwr_k=500:599.0897,lwr_k=600:599.0897,lwr_k=700:599.0897,lwr_k=800:599.0897,lwr_k=900:599.0897,lwr_k=1000:599.0897'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:281086.685,lwr_k=10:240.8869,lwr_k=20:240.4091,lwr_k=30:239.2198,lwr_k=40:239.9902,lwr_k=50:561.3625,lwr_k=100:239.3808,lwr_k=200:66815324.2034,lwr_k=300:239.493,lwr_k=400:239.1882,lwr_k=500:561.3625,lwr_k=600:561.3625,lwr_k=700:561.3625,lwr_k=800:561.3625,lwr_k=900:561.3625,lwr_k=1000:561.3625'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:524.8742,lwr_k=10:209.4469,lwr_k=20:205.7566,lwr_k=30:220.4386,lwr_k=40:211.6878,lwr_k=50:210.0386,lwr_k=100:207.0361,lwr_k=200:524.8742,lwr_k=300:206.1592,lwr_k=400:524.8742,lwr_k=500:205.6606,lwr_k=600:524.8742,lwr_k=700:524.8742,lwr_k=800:1012462.424,lwr_k=900:524.8742,lwr_k=1000:524.8742'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:673.6287,lwr_k=10:333.6621,lwr_k=20:328.1241,lwr_k=30:335.143,lwr_k=40:328.4684,lwr_k=50:327.3437,lwr_k=100:325.5831,lwr_k=200:673.6287,lwr_k=300:325.2239,lwr_k=400:673.6287,lwr_k=500:325.1013,lwr_k=600:673.6287,lwr_k=700:673.6287,lwr_k=800:1011014.7621,lwr_k=900:673.6287,lwr_k=1000:673.6287'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:235.0387,lwr_k=10:237.1545,lwr_k=20:236.6246,lwr_k=30:235.06,lwr_k=40:236.151,lwr_k=50:235.6784,lwr_k=100:235.4262,lwr_k=200:235.1,lwr_k=300:235.2496,lwr_k=400:235.0455,lwr_k=500:235.0419,lwr_k=600:235.0503,lwr_k=700:235.039,lwr_k=800:235.0402,lwr_k=900:235.039,lwr_k=1000:235.0392'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:237.5599,lwr_k=10:240.0665,lwr_k=20:239.4842,lwr_k=30:237.5419,lwr_k=40:238.9555,lwr_k=50:238.4145,lwr_k=100:238.1146,lwr_k=200:237.6876,lwr_k=300:237.6473,lwr_k=400:237.5446,lwr_k=500:237.5479,lwr_k=600:237.6003,lwr_k=700:237.5559,lwr_k=800:237.5509,lwr_k=900:237.5558,lwr_k=1000:237.5547'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_90'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.0458,lwr_k=10:0.0,lwr_k=20:0.5286,lwr_k=30:2.0413,lwr_k=40:2.7794,lwr_k=50:3.2217,lwr_k=100:4.015,lwr_k=200:4.4807,lwr_k=300:4.5901,lwr_k=400:4.6932,lwr_k=500:4.7545,lwr_k=600:4.7899,lwr_k=700:4.8422,lwr_k=800:4.8444,lwr_k=900:4.87,lwr_k=1000:4.8845'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.723,lwr_k=10:41.1857,lwr_k=20:12553.7964,lwr_k=30:16.6812,lwr_k=40:11.6413,lwr_k=50:10.2323,lwr_k=100:8.1217,lwr_k=200:7.5669,lwr_k=300:7.3844,lwr_k=400:7.3586,lwr_k=500:7.4011,lwr_k=600:7.4518,lwr_k=700:7.4076,lwr_k=800:7.4326,lwr_k=900:7.4912,lwr_k=1000:7.5198'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:15.202,lwr_k=10:0.0,lwr_k=20:0.6379,lwr_k=30:3.3423,lwr_k=40:4.9567,lwr_k=50:6.0992,lwr_k=100:8.5548,lwr_k=200:10.3548,lwr_k=300:11.2406,lwr_k=400:11.4982,lwr_k=500:11.7641,lwr_k=600:12.0697,lwr_k=700:12.2426,lwr_k=800:12.326,lwr_k=900:12.4375,lwr_k=1000:12.607'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:13.9928,lwr_k=10:55.5482,lwr_k=20:73045.5375,lwr_k=30:39.354,lwr_k=40:20.9646,lwr_k=50:17.0261,lwr_k=100:13.5673,lwr_k=200:11.2546,lwr_k=300:11.1658,lwr_k=400:11.1836,lwr_k=500:11.1968,lwr_k=600:11.2524,lwr_k=700:11.4342,lwr_k=800:11.4095,lwr_k=900:11.5843,lwr_k=1000:11.5868'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.8552,lwr_k=10:3.8193,lwr_k=20:6.084,lwr_k=30:6.6719,lwr_k=40:6.9225,lwr_k=50:7.2773,lwr_k=100:7.8705,lwr_k=200:8.1781,lwr_k=300:8.3895,lwr_k=400:8.4577,lwr_k=500:8.5084,lwr_k=600:8.5692,lwr_k=700:8.618,lwr_k=800:8.6229,lwr_k=900:8.6499,lwr_k=1000:8.6641'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.4766,lwr_k=10:518962686.9841,lwr_k=20:97079388.0626,lwr_k=30:5761251.8057,lwr_k=40:747994.7565,lwr_k=50:2689788.4192,lwr_k=100:7.4616,lwr_k=200:6.6775,lwr_k=300:6.473,lwr_k=400:6.4203,lwr_k=500:6.3213,lwr_k=600:6.3251,lwr_k=700:6.3418,lwr_k=800:6.3828,lwr_k=900:6.3914,lwr_k=1000:6.3786'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:12.479,lwr_k=10:5.9526,lwr_k=20:7.8494,lwr_k=30:8.2959,lwr_k=40:8.9426,lwr_k=50:9.3324,lwr_k=100:10.2785,lwr_k=200:10.8193,lwr_k=300:11.1478,lwr_k=400:11.3943,lwr_k=500:11.4817,lwr_k=600:11.5859,lwr_k=700:11.6893,lwr_k=800:11.7421,lwr_k=900:11.8209,lwr_k=1000:11.8397'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:13.477,lwr_k=10:376812539.6832,lwr_k=20:1517028.0847,lwr_k=30:18.709,lwr_k=40:2335004.7221,lwr_k=50:14.2702,lwr_k=100:13.7453,lwr_k=200:13.0353,lwr_k=300:13.0812,lwr_k=400:13.026,lwr_k=500:13.0359,lwr_k=600:12.9277,lwr_k=700:12.9763,lwr_k=800:12.9776,lwr_k=900:12.9339,lwr_k=1000:12.9127'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.8624,lwr_k=10:0.0,lwr_k=20:0.5587,lwr_k=30:2.1,lwr_k=40:2.6967,lwr_k=50:3.0635,lwr_k=100:3.9881,lwr_k=200:4.4376,lwr_k=300:4.5167,lwr_k=400:4.5852,lwr_k=500:4.6331,lwr_k=600:4.6476,lwr_k=700:4.6926,lwr_k=800:4.7186,lwr_k=900:4.7304,lwr_k=1000:4.747'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:17.1114,lwr_k=10:66.1684,lwr_k=20:1344.4257,lwr_k=30:52.1165,lwr_k=40:21.9105,lwr_k=50:18.6487,lwr_k=100:16.8211,lwr_k=200:16.8046,lwr_k=300:16.773,lwr_k=400:16.7068,lwr_k=500:16.6577,lwr_k=600:16.7316,lwr_k=700:16.733,lwr_k=800:16.7518,lwr_k=900:16.7617,lwr_k=1000:16.7711'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:8.5615,lwr_k=10:0.0,lwr_k=20:0.8858,lwr_k=30:2.5677,lwr_k=40:3.4992,lwr_k=50:4.2552,lwr_k=100:5.8139,lwr_k=200:6.7228,lwr_k=300:7.0395,lwr_k=400:7.2633,lwr_k=500:7.3607,lwr_k=600:7.4579,lwr_k=700:7.5348,lwr_k=800:7.5971,lwr_k=900:7.6578,lwr_k=1000:7.7281'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:12.2262,lwr_k=10:77.5194,lwr_k=20:408.1172,lwr_k=30:30.7775,lwr_k=40:23.8473,lwr_k=50:19.1858,lwr_k=100:14.215,lwr_k=200:12.5327,lwr_k=300:11.6977,lwr_k=400:11.8391,lwr_k=500:11.6993,lwr_k=600:11.6823,lwr_k=700:11.5342,lwr_k=800:11.5008,lwr_k=900:11.5161,lwr_k=1000:11.5547'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_91'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.8197,lwr_k=10:6.8614,lwr_k=20:4.2516,lwr_k=30:4.3676,lwr_k=40:4.6047,lwr_k=50:4.7953,lwr_k=100:5.1756,lwr_k=200:5.3625,lwr_k=300:5.4245,lwr_k=400:5.4776,lwr_k=500:5.4992,lwr_k=600:5.5223,lwr_k=700:5.5533,lwr_k=800:5.5648,lwr_k=900:5.5878,lwr_k=1000:5.6015'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.0616,lwr_k=10:3334949320.7451,lwr_k=20:3920162.6525,lwr_k=30:48132.6112,lwr_k=40:25169.6658,lwr_k=50:2046937.4487,lwr_k=100:7.9038,lwr_k=200:7.973,lwr_k=300:7.8518,lwr_k=400:7.7711,lwr_k=500:7.7604,lwr_k=600:7.8485,lwr_k=700:7.8015,lwr_k=800:7.8028,lwr_k=900:7.8015,lwr_k=1000:7.8307'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.1327,lwr_k=10:13.0035,lwr_k=20:6.3117,lwr_k=30:5.9439,lwr_k=40:5.9432,lwr_k=50:5.9363,lwr_k=100:6.2036,lwr_k=200:6.3207,lwr_k=300:6.4405,lwr_k=400:6.5023,lwr_k=500:6.5255,lwr_k=600:6.6691,lwr_k=700:6.6983,lwr_k=800:6.7463,lwr_k=900:6.7541,lwr_k=1000:6.7639'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.6602,lwr_k=10:111835756.1348,lwr_k=20:121969720.6967,lwr_k=30:3464.8524,lwr_k=40:672876.7181,lwr_k=50:8.2825,lwr_k=100:6.9972,lwr_k=200:64.0698,lwr_k=300:42.2777,lwr_k=400:11.6351,lwr_k=500:11.9323,lwr_k=600:6.9249,lwr_k=700:7.3746,lwr_k=800:7.3174,lwr_k=900:7.3455,lwr_k=1000:7.3344'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.4641,lwr_k=10:27.1938,lwr_k=20:9.1142,lwr_k=30:7.3574,lwr_k=40:7.5161,lwr_k=50:7.3679,lwr_k=100:7.5901,lwr_k=200:7.6619,lwr_k=300:7.7823,lwr_k=400:7.8162,lwr_k=500:7.8047,lwr_k=600:7.8381,lwr_k=700:7.8563,lwr_k=800:7.8704,lwr_k=900:7.9013,lwr_k=1000:7.9226'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.1872,lwr_k=10:1158537.8416,lwr_k=20:567954.9744,lwr_k=30:5260.0821,lwr_k=40:18.8731,lwr_k=50:7.9001,lwr_k=100:7.0427,lwr_k=200:6.564,lwr_k=300:6.4976,lwr_k=400:6.5077,lwr_k=500:6.5182,lwr_k=600:6.5101,lwr_k=700:6.5014,lwr_k=800:6.5206,lwr_k=900:6.5129,lwr_k=1000:6.5421'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.0531,lwr_k=10:46.1008,lwr_k=20:12.102,lwr_k=30:8.921,lwr_k=40:6.7827,lwr_k=50:6.694,lwr_k=100:6.1606,lwr_k=200:6.2916,lwr_k=300:6.4329,lwr_k=400:6.4042,lwr_k=500:6.4399,lwr_k=600:6.7218,lwr_k=700:6.4943,lwr_k=800:6.5191,lwr_k=900:6.5218,lwr_k=1000:6.5607'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.523,lwr_k=10:14442771608.4075,lwr_k=20:1862310029.9994,lwr_k=30:1457415116.1387,lwr_k=40:853483.2174,lwr_k=50:157233163.0186,lwr_k=100:83.3305,lwr_k=200:798.3022,lwr_k=300:657.3856,lwr_k=400:9.5641,lwr_k=500:9.5395,lwr_k=600:9.5428,lwr_k=700:9.4748,lwr_k=800:9.4093,lwr_k=900:9.4303,lwr_k=1000:9.402'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.0262,lwr_k=10:79.8444,lwr_k=20:14.4768,lwr_k=30:10.1516,lwr_k=40:8.5898,lwr_k=50:7.5963,lwr_k=100:6.7807,lwr_k=200:6.4217,lwr_k=300:6.4722,lwr_k=400:6.6389,lwr_k=500:6.4005,lwr_k=600:6.4502,lwr_k=700:6.4892,lwr_k=800:6.5248,lwr_k=900:6.5479,lwr_k=1000:6.5616'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:18.0958,lwr_k=10:30603435.402,lwr_k=20:1961816.7484,lwr_k=30:376476.1886,lwr_k=40:56765.2078,lwr_k=50:169.1308,lwr_k=100:16.6305,lwr_k=200:44.8614,lwr_k=300:96.5914,lwr_k=400:18.2023,lwr_k=500:35.092,lwr_k=600:16.7515,lwr_k=700:21.1072,lwr_k=800:19.6834,lwr_k=900:17.597,lwr_k=1000:21.8819'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:7.227,lwr_k=10:12.466,lwr_k=20:6.6034,lwr_k=30:6.1869,lwr_k=40:6.0668,lwr_k=50:6.3078,lwr_k=100:6.45,lwr_k=200:6.5347,lwr_k=300:6.5912,lwr_k=400:6.645,lwr_k=500:6.6456,lwr_k=600:6.6725,lwr_k=700:6.7167,lwr_k=800:6.7197,lwr_k=900:6.727,lwr_k=1000:6.7373'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.4872,lwr_k=10:6078819.5315,lwr_k=20:25079410.901,lwr_k=30:13.4997,lwr_k=40:11801.1133,lwr_k=50:7614.6651,lwr_k=100:11.4033,lwr_k=200:16179.7676,lwr_k=300:35.0356,lwr_k=400:20.0072,lwr_k=500:11.09,lwr_k=600:11.0207,lwr_k=700:9.585,lwr_k=800:8.7699,lwr_k=900:8.8225,lwr_k=1000:8.7904'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_92'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.4665,lwr_k=10:0.0459,lwr_k=20:0.3641,lwr_k=30:2.2762,lwr_k=40:3.417,lwr_k=50:4.0402,lwr_k=100:5.2575,lwr_k=200:5.214,lwr_k=300:5.6244,lwr_k=400:5.746,lwr_k=500:5.8756,lwr_k=600:5.9568,lwr_k=700:6.0261,lwr_k=800:6.1055,lwr_k=900:6.1754,lwr_k=1000:6.1954'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.6101,lwr_k=10:86.0189,lwr_k=20:1789.2828,lwr_k=30:345.7441,lwr_k=40:36133.5498,lwr_k=50:113.0021,lwr_k=100:130.814,lwr_k=200:8.7646,lwr_k=300:8.2255,lwr_k=400:8.0489,lwr_k=500:7.8079,lwr_k=600:7.8701,lwr_k=700:7.9147,lwr_k=800:8.0187,lwr_k=900:8.0105,lwr_k=1000:7.9971'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.8275,lwr_k=10:0.043,lwr_k=20:0.3304,lwr_k=30:1.5108,lwr_k=40:2.991,lwr_k=50:3.7826,lwr_k=100:4.5511,lwr_k=200:5.2751,lwr_k=300:5.4889,lwr_k=400:5.6401,lwr_k=500:5.7349,lwr_k=600:5.886,lwr_k=700:5.9568,lwr_k=800:6.0879,lwr_k=900:6.1526,lwr_k=1000:6.2622'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.2382,lwr_k=10:61.3242,lwr_k=20:62136.9472,lwr_k=30:5457.5254,lwr_k=40:326.9635,lwr_k=50:393.9152,lwr_k=100:11.8078,lwr_k=200:7.5453,lwr_k=300:7.2306,lwr_k=400:7.1069,lwr_k=500:7.1327,lwr_k=600:7.1244,lwr_k=700:7.2912,lwr_k=800:7.4149,lwr_k=900:7.4614,lwr_k=1000:7.4551'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.8313,lwr_k=10:0.0537,lwr_k=20:0.8738,lwr_k=30:1.4411,lwr_k=40:2.3691,lwr_k=50:2.8319,lwr_k=100:4.5125,lwr_k=200:5.6479,lwr_k=300:6.0572,lwr_k=400:6.2611,lwr_k=500:6.451,lwr_k=600:6.6354,lwr_k=700:6.7112,lwr_k=800:6.8065,lwr_k=900:6.8633,lwr_k=1000:6.8995'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.4525,lwr_k=10:120.1331,lwr_k=20:866.8705,lwr_k=30:7756.0621,lwr_k=40:14486959.6751,lwr_k=50:13275.6001,lwr_k=100:8.1914,lwr_k=200:6.717,lwr_k=300:6.5944,lwr_k=400:6.5341,lwr_k=500:6.423,lwr_k=600:6.3308,lwr_k=700:6.318,lwr_k=800:6.2668,lwr_k=900:6.2628,lwr_k=1000:6.2448'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.1264,lwr_k=10:0.0908,lwr_k=20:0.4229,lwr_k=30:1.8799,lwr_k=40:2.8484,lwr_k=50:3.2242,lwr_k=100:4.1026,lwr_k=200:4.7147,lwr_k=300:4.965,lwr_k=400:5.1309,lwr_k=500:5.306,lwr_k=600:5.4963,lwr_k=700:5.5951,lwr_k=800:5.6655,lwr_k=900:5.781,lwr_k=1000:5.999'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:8.2971,lwr_k=10:70.891,lwr_k=20:2028.2012,lwr_k=30:1554367007.1783,lwr_k=40:922605328.6193,lwr_k=50:17.2943,lwr_k=100:10.7686,lwr_k=200:9.5846,lwr_k=300:9.2173,lwr_k=400:9.0107,lwr_k=500:8.8033,lwr_k=600:8.6852,lwr_k=700:8.6245,lwr_k=800:8.5279,lwr_k=900:8.4523,lwr_k=1000:8.3239'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.9718,lwr_k=10:0.0342,lwr_k=20:0.2008,lwr_k=30:1.227,lwr_k=40:2.1812,lwr_k=50:2.5893,lwr_k=100:3.4311,lwr_k=200:4.0353,lwr_k=300:4.2049,lwr_k=400:4.2764,lwr_k=500:4.3446,lwr_k=600:4.419,lwr_k=700:4.4513,lwr_k=800:4.4878,lwr_k=900:4.5028,lwr_k=1000:4.5249'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:20.3509,lwr_k=10:359.6398,lwr_k=20:1044740.0388,lwr_k=30:33115685.2483,lwr_k=40:15387567.0918,lwr_k=50:70291.9539,lwr_k=100:20.0444,lwr_k=200:16.8042,lwr_k=300:15.6007,lwr_k=400:15.1747,lwr_k=500:14.4438,lwr_k=600:15.0378,lwr_k=700:14.9952,lwr_k=800:15.1181,lwr_k=900:15.0961,lwr_k=1000:15.1216'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:8.6284,lwr_k=10:0.0204,lwr_k=20:2.4585,lwr_k=30:3.1006,lwr_k=40:3.548,lwr_k=50:4.0265,lwr_k=100:5.4254,lwr_k=200:6.3386,lwr_k=300:6.7041,lwr_k=400:6.8966,lwr_k=500:7.0148,lwr_k=600:7.1343,lwr_k=700:7.233,lwr_k=800:7.2775,lwr_k=900:7.3452,lwr_k=1000:7.4245'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.2631,lwr_k=10:66.3117,lwr_k=20:206791.5694,lwr_k=30:110120629.3887,lwr_k=40:1517961943.8979,lwr_k=50:3174598427.2497,lwr_k=100:307613588.8178,lwr_k=200:10764701.9909,lwr_k=300:9.0358,lwr_k=400:8.9246,lwr_k=500:9.1049,lwr_k=600:9.1101,lwr_k=700:9.0579,lwr_k=800:9.1581,lwr_k=900:9.3103,lwr_k=1000:9.3265'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_93'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:3.9779,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.2276,lwr_k=40:0.9101,lwr_k=50:1.4258,lwr_k=100:2.5726,lwr_k=200:3.2134,lwr_k=300:3.4731,lwr_k=400:3.5711,lwr_k=500:3.6351,lwr_k=600:3.7151,lwr_k=700:3.7402,lwr_k=800:3.7623,lwr_k=900:3.7858,lwr_k=1000:3.8149'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.3637,lwr_k=10:27.357,lwr_k=20:398.5784,lwr_k=30:30780.9882,lwr_k=40:2240.0847,lwr_k=50:5048.6592,lwr_k=100:26.0335,lwr_k=200:8.0829,lwr_k=300:8.0018,lwr_k=400:8.1175,lwr_k=500:8.1428,lwr_k=600:8.1242,lwr_k=700:8.1684,lwr_k=800:8.157,lwr_k=900:8.1901,lwr_k=1000:8.2185'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.7731,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.6115,lwr_k=40:1.6873,lwr_k=50:2.4112,lwr_k=100:4.159,lwr_k=200:5.3004,lwr_k=300:5.7924,lwr_k=400:6.0292,lwr_k=500:6.2657,lwr_k=600:6.4025,lwr_k=700:6.4818,lwr_k=800:6.5841,lwr_k=900:6.6923,lwr_k=1000:6.7722'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.5151,lwr_k=10:55.1961,lwr_k=20:95.0178,lwr_k=30:113768465.836,lwr_k=40:4509.3117,lwr_k=50:6280.5029,lwr_k=100:8.7853,lwr_k=200:7.3817,lwr_k=300:7.3157,lwr_k=400:7.6889,lwr_k=500:7.6648,lwr_k=600:7.7679,lwr_k=700:7.9148,lwr_k=800:8.2832,lwr_k=900:7.9085,lwr_k=1000:8.2895'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.3258,lwr_k=10:0.0,lwr_k=20:0.0001,lwr_k=30:5.7116,lwr_k=40:1.4535,lwr_k=50:2.3145,lwr_k=100:3.0435,lwr_k=200:3.5853,lwr_k=300:3.8099,lwr_k=400:3.9511,lwr_k=500:4.0169,lwr_k=600:4.0463,lwr_k=700:4.0905,lwr_k=800:4.1284,lwr_k=900:4.1517,lwr_k=1000:4.1769'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:5.9688,lwr_k=10:94.4053,lwr_k=20:34832.6893,lwr_k=30:117295.3774,lwr_k=40:254482.276,lwr_k=50:77.6374,lwr_k=100:16508.9112,lwr_k=200:541.3452,lwr_k=300:89.8079,lwr_k=400:5.5541,lwr_k=500:5.5471,lwr_k=600:5.6262,lwr_k=700:5.6405,lwr_k=800:5.7157,lwr_k=900:5.7397,lwr_k=1000:5.7265'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.1647,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:7.1059,lwr_k=40:3.1645,lwr_k=50:2.9505,lwr_k=100:2.6842,lwr_k=200:3.3753,lwr_k=300:3.6044,lwr_k=400:3.7475,lwr_k=500:3.8515,lwr_k=600:3.9115,lwr_k=700:3.9416,lwr_k=800:3.9758,lwr_k=900:4.0032,lwr_k=1000:4.0338'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.7225,lwr_k=10:72.3336,lwr_k=20:2457.4324,lwr_k=30:45223.115,lwr_k=40:23032.9167,lwr_k=50:590.7085,lwr_k=100:72.411,lwr_k=200:19.699,lwr_k=300:10.3337,lwr_k=400:9.5381,lwr_k=500:10.1422,lwr_k=600:10.1071,lwr_k=700:9.8248,lwr_k=800:9.5505,lwr_k=900:9.647,lwr_k=1000:9.7334'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.9811,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:1.7792,lwr_k=40:1.2445,lwr_k=50:2.0212,lwr_k=100:3.8653,lwr_k=200:4.9956,lwr_k=300:5.4194,lwr_k=400:5.6319,lwr_k=500:5.8634,lwr_k=600:6.005,lwr_k=700:6.0971,lwr_k=800:6.2096,lwr_k=900:6.2953,lwr_k=1000:6.3715'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:31.8031,lwr_k=10:85.8712,lwr_k=20:265.3628,lwr_k=30:15251.9417,lwr_k=40:75.5614,lwr_k=50:28.0377,lwr_k=100:20.4705,lwr_k=200:22.5796,lwr_k=300:22.825,lwr_k=400:23.7492,lwr_k=500:24.5863,lwr_k=600:25.2836,lwr_k=700:25.747,lwr_k=800:26.1402,lwr_k=900:26.6131,lwr_k=1000:26.8281'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:6.893,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.4622,lwr_k=40:1.6456,lwr_k=50:2.1979,lwr_k=100:3.5261,lwr_k=200:4.4863,lwr_k=300:4.8082,lwr_k=400:5.0564,lwr_k=500:5.2171,lwr_k=600:5.3964,lwr_k=700:5.4736,lwr_k=800:5.5657,lwr_k=900:5.6406,lwr_k=1000:5.7104'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.5114,lwr_k=10:90.0667,lwr_k=20:1475.343,lwr_k=30:311395.6485,lwr_k=40:308259.5349,lwr_k=50:851.6729,lwr_k=100:55.8244,lwr_k=200:37.3246,lwr_k=300:13.0591,lwr_k=400:10.4347,lwr_k=500:10.2354,lwr_k=600:10.5762,lwr_k=700:9.6986,lwr_k=800:9.4422,lwr_k=900:9.6015,lwr_k=1000:9.4685'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_94'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.6842,lwr_k=10:0.0,lwr_k=20:2.5806,lwr_k=30:3.4905,lwr_k=40:4.0297,lwr_k=50:4.3511,lwr_k=100:5.162,lwr_k=200:5.5697,lwr_k=300:5.7484,lwr_k=400:5.8573,lwr_k=500:5.9374,lwr_k=600:5.9977,lwr_k=700:6.0393,lwr_k=800:6.0814,lwr_k=900:6.0916,lwr_k=1000:6.143'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.2042,lwr_k=10:782.5555,lwr_k=20:14.3782,lwr_k=30:9.3542,lwr_k=40:8.3567,lwr_k=50:8.405,lwr_k=100:7.1976,lwr_k=200:7.341,lwr_k=300:7.3878,lwr_k=400:7.4864,lwr_k=500:7.52,lwr_k=600:7.5436,lwr_k=700:7.582,lwr_k=800:7.5811,lwr_k=900:7.5454,lwr_k=1000:7.5659'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:13.74,lwr_k=10:0.0,lwr_k=20:3.463,lwr_k=30:5.0912,lwr_k=40:6.1418,lwr_k=50:6.6479,lwr_k=100:8.3473,lwr_k=200:9.4397,lwr_k=300:10.0382,lwr_k=400:10.3846,lwr_k=500:10.6869,lwr_k=600:10.893,lwr_k=700:11.0492,lwr_k=800:11.1775,lwr_k=900:11.3104,lwr_k=1000:11.4062'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.8863,lwr_k=10:6175.6782,lwr_k=20:136.0772,lwr_k=30:18.7945,lwr_k=40:23.8249,lwr_k=50:134.9685,lwr_k=100:191.3662,lwr_k=200:77.9179,lwr_k=300:32.7552,lwr_k=400:10.3285,lwr_k=500:10.1566,lwr_k=600:10.2587,lwr_k=700:10.368,lwr_k=800:10.4139,lwr_k=900:10.5343,lwr_k=1000:10.6903'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.704,lwr_k=10:0.0,lwr_k=20:3.0994,lwr_k=30:4.6183,lwr_k=40:5.3208,lwr_k=50:5.7658,lwr_k=100:6.9844,lwr_k=200:7.8444,lwr_k=300:8.1937,lwr_k=400:8.4868,lwr_k=500:8.6369,lwr_k=600:8.7762,lwr_k=700:8.8519,lwr_k=800:8.9777,lwr_k=900:9.0538,lwr_k=1000:9.1427'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.6359,lwr_k=10:845.9309,lwr_k=20:23.3854,lwr_k=30:11.764,lwr_k=40:9.6423,lwr_k=50:8.6637,lwr_k=100:6.9471,lwr_k=200:6.4563,lwr_k=300:6.5327,lwr_k=400:6.465,lwr_k=500:6.4132,lwr_k=600:6.4055,lwr_k=700:6.4413,lwr_k=800:6.477,lwr_k=900:6.4838,lwr_k=1000:6.4695'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:10.0033,lwr_k=10:0.0,lwr_k=20:2.8607,lwr_k=30:4.4058,lwr_k=40:5.1514,lwr_k=50:5.6385,lwr_k=100:6.7918,lwr_k=200:7.5714,lwr_k=300:7.9527,lwr_k=400:8.1752,lwr_k=500:8.2732,lwr_k=600:8.3489,lwr_k=700:8.4754,lwr_k=800:8.5788,lwr_k=900:8.6263,lwr_k=1000:8.7002'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:12.377,lwr_k=10:1018.408,lwr_k=20:22.3104,lwr_k=30:15.2878,lwr_k=40:12.6114,lwr_k=50:11.9462,lwr_k=100:10.9587,lwr_k=200:10.6942,lwr_k=300:10.7872,lwr_k=400:10.8644,lwr_k=500:11.0386,lwr_k=600:11.0464,lwr_k=700:11.0657,lwr_k=800:11.1273,lwr_k=900:11.0921,lwr_k=1000:11.0636'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.4021,lwr_k=10:0.0,lwr_k=20:2.7076,lwr_k=30:3.6277,lwr_k=40:3.9626,lwr_k=50:4.4019,lwr_k=100:5.2529,lwr_k=200:5.7185,lwr_k=300:5.8961,lwr_k=400:6.1414,lwr_k=500:6.2459,lwr_k=600:6.3443,lwr_k=700:6.4214,lwr_k=800:6.5069,lwr_k=900:6.5962,lwr_k=1000:6.6536'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:26.6253,lwr_k=10:425.3464,lwr_k=20:30.1324,lwr_k=30:15.9512,lwr_k=40:14.7303,lwr_k=50:15.7463,lwr_k=100:19.271,lwr_k=200:20.8431,lwr_k=300:21.3445,lwr_k=400:22.1633,lwr_k=500:22.6867,lwr_k=600:23.2789,lwr_k=700:23.6576,lwr_k=800:23.9333,lwr_k=900:24.2123,lwr_k=1000:24.508'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:8.1301,lwr_k=10:0.0001,lwr_k=20:2.7197,lwr_k=30:3.7264,lwr_k=40:4.2915,lwr_k=50:4.7048,lwr_k=100:5.7328,lwr_k=200:6.46,lwr_k=300:6.8004,lwr_k=400:6.9431,lwr_k=500:7.0309,lwr_k=600:7.0621,lwr_k=700:7.099,lwr_k=800:7.1859,lwr_k=900:7.2175,lwr_k=1000:7.2499'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.46,lwr_k=10:854.5728,lwr_k=20:32.5321,lwr_k=30:27.4321,lwr_k=40:16.6105,lwr_k=50:12.6103,lwr_k=100:9.4558,lwr_k=200:9.1023,lwr_k=300:9.418,lwr_k=400:9.4589,lwr_k=500:9.5857,lwr_k=600:9.5347,lwr_k=700:9.6184,lwr_k=800:9.695,lwr_k=900:9.6495,lwr_k=1000:9.692'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_95'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.8236,lwr_k=10:0.0131,lwr_k=20:0.4723,lwr_k=30:2.3094,lwr_k=40:5.1521,lwr_k=50:4.0274,lwr_k=100:4.7081,lwr_k=200:5.6186,lwr_k=300:5.8886,lwr_k=400:6.1097,lwr_k=500:6.2441,lwr_k=600:6.3596,lwr_k=700:6.4604,lwr_k=800:6.5468,lwr_k=900:6.627,lwr_k=1000:6.7246'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.8322,lwr_k=10:105.6498,lwr_k=20:175217.9273,lwr_k=30:6217383.6317,lwr_k=40:10662259.5245,lwr_k=50:271797118.8465,lwr_k=100:32772.9095,lwr_k=200:170517.1983,lwr_k=300:8.197,lwr_k=400:8.094,lwr_k=500:8.0148,lwr_k=600:7.9545,lwr_k=700:7.9738,lwr_k=800:7.9562,lwr_k=900:8.0253,lwr_k=1000:8.0524'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.9499,lwr_k=10:2.1745,lwr_k=20:4.4001,lwr_k=30:5.3197,lwr_k=40:5.7213,lwr_k=50:5.897,lwr_k=100:6.2267,lwr_k=200:6.5123,lwr_k=300:6.6443,lwr_k=400:6.6354,lwr_k=500:6.6518,lwr_k=600:6.7346,lwr_k=700:6.7238,lwr_k=800:6.7094,lwr_k=900:6.7462,lwr_k=1000:6.7833'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.1582,lwr_k=10:279.7177,lwr_k=20:17.8616,lwr_k=30:11.4355,lwr_k=40:10.1002,lwr_k=50:9.9277,lwr_k=100:8.9154,lwr_k=200:8.1903,lwr_k=300:8.0736,lwr_k=400:8.0649,lwr_k=500:8.0848,lwr_k=600:8.0466,lwr_k=700:8.0128,lwr_k=800:8.0607,lwr_k=900:8.2372,lwr_k=1000:8.4802'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.2849,lwr_k=10:0.1226,lwr_k=20:2.6066,lwr_k=30:4.9784,lwr_k=40:4.0274,lwr_k=50:3.6314,lwr_k=100:3.9601,lwr_k=200:3.5703,lwr_k=300:4.6061,lwr_k=400:5.5666,lwr_k=500:5.7452,lwr_k=600:6.2954,lwr_k=700:6.7084,lwr_k=800:6.5045,lwr_k=900:6.3047,lwr_k=1000:6.7822'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.5902,lwr_k=10:2805.3948,lwr_k=20:1590.9516,lwr_k=30:2831.8717,lwr_k=40:105.0411,lwr_k=50:18.9486,lwr_k=100:10.2096,lwr_k=200:9.9016,lwr_k=300:8.5774,lwr_k=400:8.3011,lwr_k=500:7.7246,lwr_k=600:7.7872,lwr_k=700:6.8712,lwr_k=800:7.1826,lwr_k=900:7.4624,lwr_k=1000:6.9461'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:9.4171,lwr_k=10:0.0009,lwr_k=20:0.4422,lwr_k=30:1.2497,lwr_k=40:1.4486,lwr_k=50:1.9484,lwr_k=100:3.4175,lwr_k=200:4.2381,lwr_k=300:5.8818,lwr_k=400:5.9097,lwr_k=500:6.7039,lwr_k=600:7.1095,lwr_k=700:7.2874,lwr_k=800:7.4984,lwr_k=900:7.6989,lwr_k=1000:7.8507'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.5078,lwr_k=10:31.1393,lwr_k=20:1012.991,lwr_k=30:97267.6504,lwr_k=40:29399.4343,lwr_k=50:29.367,lwr_k=100:21.1129,lwr_k=200:12.9465,lwr_k=300:10.8294,lwr_k=400:10.3681,lwr_k=500:8.8846,lwr_k=600:8.9293,lwr_k=700:8.8027,lwr_k=800:8.8769,lwr_k=900:8.6793,lwr_k=1000:8.7699'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.4258,lwr_k=10:0.0111,lwr_k=20:0.2583,lwr_k=30:1.3017,lwr_k=40:1.6343,lwr_k=50:2.0164,lwr_k=100:2.9199,lwr_k=200:3.9743,lwr_k=300:5.0255,lwr_k=400:5.319,lwr_k=500:5.4492,lwr_k=600:5.5546,lwr_k=700:5.68,lwr_k=800:5.7502,lwr_k=900:5.7867,lwr_k=1000:5.8533'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:20.2936,lwr_k=10:177.4866,lwr_k=20:56211.9901,lwr_k=30:108732019.2969,lwr_k=40:81423870.2347,lwr_k=50:61686647.298,lwr_k=100:1434367.5941,lwr_k=200:25.7383,lwr_k=300:18.7645,lwr_k=400:19.4262,lwr_k=500:19.7581,lwr_k=600:19.479,lwr_k=700:19.0894,lwr_k=800:19.1367,lwr_k=900:19.1662,lwr_k=1000:19.2171'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:7.5303,lwr_k=10:2.3039,lwr_k=20:4.118,lwr_k=30:5.0806,lwr_k=40:5.7072,lwr_k=50:6.0271,lwr_k=100:6.699,lwr_k=200:6.9555,lwr_k=300:7.0205,lwr_k=400:7.0763,lwr_k=500:7.1947,lwr_k=600:7.2505,lwr_k=700:7.2938,lwr_k=800:7.2727,lwr_k=900:7.2995,lwr_k=1000:7.3021'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.3012,lwr_k=10:57570.6756,lwr_k=20:556.1301,lwr_k=30:148.4028,lwr_k=40:28244733.29,lwr_k=50:399475588.0858,lwr_k=100:12.6684,lwr_k=200:11.175,lwr_k=300:12.7139,lwr_k=400:12367762084.6704,lwr_k=500:19421.8787,lwr_k=600:168732366.1439,lwr_k=700:52421397.9114,lwr_k=800:12.3532,lwr_k=900:11.6957,lwr_k=1000:2753046.1823'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_96'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.6975,lwr_k=10:2.7912,lwr_k=20:3.2466,lwr_k=30:4.4264,lwr_k=40:4.951,lwr_k=50:5.2893,lwr_k=100:5.8189,lwr_k=200:6.297,lwr_k=300:6.6583,lwr_k=400:6.7221,lwr_k=500:6.8286,lwr_k=600:6.9619,lwr_k=700:7.0919,lwr_k=800:7.1934,lwr_k=900:7.308,lwr_k=1000:7.4173'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:11.7053,lwr_k=10:496253005.1616,lwr_k=20:32389909.4033,lwr_k=30:14.5969,lwr_k=40:13577791.238,lwr_k=50:90393747.0926,lwr_k=100:76045665.9265,lwr_k=200:3550235.2213,lwr_k=300:8.8488,lwr_k=400:9.0694,lwr_k=500:9.0274,lwr_k=600:9.1941,lwr_k=700:9.2345,lwr_k=800:9.2592,lwr_k=900:9.3479,lwr_k=1000:9.6795'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.2878,lwr_k=10:6.5082,lwr_k=20:6.2914,lwr_k=30:5.7539,lwr_k=40:5.8842,lwr_k=50:5.8302,lwr_k=100:6.3225,lwr_k=200:6.5976,lwr_k=300:6.7355,lwr_k=400:6.8342,lwr_k=500:6.8739,lwr_k=600:6.9166,lwr_k=700:6.9313,lwr_k=800:6.9675,lwr_k=900:7.0554,lwr_k=1000:7.0688'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.1077,lwr_k=10:80996988157.328,lwr_k=20:264449799.4094,lwr_k=30:73719.0784,lwr_k=40:2111603.3574,lwr_k=50:1949495.2071,lwr_k=100:8.8995,lwr_k=200:8.451,lwr_k=300:8.1714,lwr_k=400:8.1501,lwr_k=500:8.1827,lwr_k=600:8.1831,lwr_k=700:8.1872,lwr_k=800:8.1627,lwr_k=900:8.059,lwr_k=1000:7.9558'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:8.7872,lwr_k=10:3.409,lwr_k=20:4.7384,lwr_k=30:4.8997,lwr_k=40:5.1054,lwr_k=50:5.3506,lwr_k=100:6.009,lwr_k=200:6.5332,lwr_k=300:6.9582,lwr_k=400:7.0471,lwr_k=500:7.1899,lwr_k=600:7.2349,lwr_k=700:7.3266,lwr_k=800:7.3793,lwr_k=900:7.5853,lwr_k=1000:7.6167'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:7.4821,lwr_k=10:41090886.1581,lwr_k=20:96949.2908,lwr_k=30:125146305.6529,lwr_k=40:57250485.8312,lwr_k=50:840207.4111,lwr_k=100:8.0473,lwr_k=200:7.0901,lwr_k=300:6.581,lwr_k=400:6.4964,lwr_k=500:6.485,lwr_k=600:6.3206,lwr_k=700:6.2763,lwr_k=800:6.2463,lwr_k=900:6.2904,lwr_k=1000:6.3269'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.5009,lwr_k=10:0.2591,lwr_k=20:3.8292,lwr_k=30:4.1291,lwr_k=40:4.5042,lwr_k=50:4.7729,lwr_k=100:5.5494,lwr_k=200:6.0296,lwr_k=300:6.2572,lwr_k=400:6.4203,lwr_k=500:6.5268,lwr_k=600:6.6844,lwr_k=700:6.7361,lwr_k=800:6.7526,lwr_k=900:6.8043,lwr_k=1000:6.8355'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:9.1949,lwr_k=10:10577621.4903,lwr_k=20:13268254712.8629,lwr_k=30:2097974879.3725,lwr_k=40:104296602.0456,lwr_k=50:446149.4522,lwr_k=100:615769.8358,lwr_k=200:414860.7043,lwr_k=300:11.9488,lwr_k=400:30.2467,lwr_k=500:58.2312,lwr_k=600:9.5174,lwr_k=700:65.6552,lwr_k=800:18.4653,lwr_k=900:4359.2148,lwr_k=1000:9.5126'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.7686,lwr_k=10:1.3507,lwr_k=20:2.9736,lwr_k=30:3.6614,lwr_k=40:3.9971,lwr_k=50:4.2921,lwr_k=100:4.7682,lwr_k=200:5.0841,lwr_k=300:5.2148,lwr_k=400:5.2936,lwr_k=500:5.3374,lwr_k=600:5.4003,lwr_k=700:5.4641,lwr_k=800:5.4942,lwr_k=900:5.4918,lwr_k=1000:5.5046'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:21.8712,lwr_k=10:29982476865.2109,lwr_k=20:3525607237.9656,lwr_k=30:6279110.5436,lwr_k=40:11076982.1357,lwr_k=50:729530.2091,lwr_k=100:15.1471,lwr_k=200:281.4888,lwr_k=300:14.1791,lwr_k=400:15.1358,lwr_k=500:5423052.0776,lwr_k=600:15.6597,lwr_k=700:15.6721,lwr_k=800:21.1374,lwr_k=900:21.2862,lwr_k=1000:21.2478'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:7.8359,lwr_k=10:1.3306,lwr_k=20:3.6634,lwr_k=30:4.7532,lwr_k=40:5.1509,lwr_k=50:5.4661,lwr_k=100:6.339,lwr_k=200:6.6167,lwr_k=300:6.7414,lwr_k=400:6.7608,lwr_k=500:6.847,lwr_k=600:6.8889,lwr_k=700:6.9195,lwr_k=800:6.9524,lwr_k=900:6.9794,lwr_k=1000:7.0039'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.9773,lwr_k=10:44196832.9899,lwr_k=20:218.1799,lwr_k=30:45.1182,lwr_k=40:73.6489,lwr_k=50:110.1706,lwr_k=100:18.9801,lwr_k=200:9.6953,lwr_k=300:9.5317,lwr_k=400:9.7195,lwr_k=500:9.8776,lwr_k=600:9.9065,lwr_k=700:9.8798,lwr_k=800:9.9521,lwr_k=900:9.9657,lwr_k=1000:10.0116'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_97'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:198.6529,lwr_k=10:204.6077,lwr_k=20:198.9498,lwr_k=30:198.6535,lwr_k=40:198.6569,lwr_k=50:198.7108,lwr_k=100:199.279,lwr_k=200:199.0935,lwr_k=300:199.0971,lwr_k=400:198.9837,lwr_k=500:198.9108,lwr_k=600:198.7665,lwr_k=700:198.6869,lwr_k=800:198.6691,lwr_k=900:198.6586,lwr_k=1000:198.7115'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:258.1252,lwr_k=10:269.7306,lwr_k=20:259.6836,lwr_k=30:258.0686,lwr_k=40:258.2741,lwr_k=50:258.7401,lwr_k=100:260.5833,lwr_k=200:260.1028,lwr_k=300:260.1125,lwr_k=400:259.7876,lwr_k=500:259.5589,lwr_k=600:259.0192,lwr_k=700:258.5856,lwr_k=800:258.4357,lwr_k=900:258.3056,lwr_k=1000:258.7442'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:239.6367,lwr_k=10:247.3583,lwr_k=20:240.417,lwr_k=30:239.7352,lwr_k=40:239.7977,lwr_k=50:239.9721,lwr_k=100:240.913,lwr_k=200:240.6413,lwr_k=300:240.6466,lwr_k=400:240.4714,lwr_k=500:240.353,lwr_k=600:240.0931,lwr_k=700:239.91,lwr_k=800:239.8536,lwr_k=900:239.8081,lwr_k=1000:239.9738'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:202.822,lwr_k=10:208.8718,lwr_k=20:203.0709,lwr_k=30:202.7317,lwr_k=40:202.7416,lwr_k=50:202.809,lwr_k=100:203.4186,lwr_k=200:203.2236,lwr_k=300:203.2273,lwr_k=400:203.107,lwr_k=500:203.0291,lwr_k=600:202.872,lwr_k=700:202.7808,lwr_k=800:202.7587,lwr_k=900:202.7443,lwr_k=1000:202.8098'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:273.725,lwr_k=10:286.0596,lwr_k=20:279.7698,lwr_k=30:284.7255,lwr_k=40:280.9807,lwr_k=50:276.729,lwr_k=100:279.4766,lwr_k=200:277.9348,lwr_k=300:274.6314,lwr_k=400:274.8378,lwr_k=500:274.0095,lwr_k=600:274.1698,lwr_k=700:274.138,lwr_k=800:273.9476,lwr_k=900:273.8063,lwr_k=1000:273.8585'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:155.1861,lwr_k=10:158.9139,lwr_k=20:155.2057,lwr_k=30:158.0586,lwr_k=40:155.8406,lwr_k=50:153.9426,lwr_k=100:155.0605,lwr_k=200:154.3677,lwr_k=300:153.7594,lwr_k=400:153.7137,lwr_k=500:154.1635,lwr_k=600:153.9965,lwr_k=700:154.0242,lwr_k=800:154.2526,lwr_k=900:154.5687,lwr_k=1000:154.4242'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:261.7626,lwr_k=10:266.5313,lwr_k=20:270.2479,lwr_k=30:262.3543,lwr_k=40:262.3381,lwr_k=50:262.026,lwr_k=100:262.8406,lwr_k=200:262.9699,lwr_k=300:261.776,lwr_k=400:262.0761,lwr_k=500:261.9114,lwr_k=600:262.041,lwr_k=700:262.0314,lwr_k=800:261.8857,lwr_k=900:261.7905,lwr_k=1000:261.7883'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:239.3147,lwr_k=10:242.2668,lwr_k=20:245.3766,lwr_k=30:239.2665,lwr_k=40:239.2591,lwr_k=50:239.1512,lwr_k=100:239.5289,lwr_k=200:239.6079,lwr_k=300:239.4243,lwr_k=400:239.1624,lwr_k=500:239.1426,lwr_k=600:239.1542,lwr_k=700:239.1522,lwr_k=800:239.1459,lwr_k=900:239.2038,lwr_k=1000:239.2071'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:204.8268,lwr_k=10:211.2806,lwr_k=20:205.4066,lwr_k=30:206.4864,lwr_k=40:209.5979,lwr_k=50:206.8734,lwr_k=100:205.0478,lwr_k=200:205.3937,lwr_k=300:204.9822,lwr_k=400:205.3989,lwr_k=500:205.0975,lwr_k=600:204.9847,lwr_k=700:204.8698,lwr_k=800:204.8308,lwr_k=900:204.8269,lwr_k=1000:204.88'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:325.691,lwr_k=10:328.1843,lwr_k=20:327.4579,lwr_k=30:325.3422,lwr_k=40:327.0568,lwr_k=50:325.5073,lwr_k=100:326.6448,lwr_k=200:327.4316,lwr_k=300:326.461,lwr_k=400:327.4422,lwr_k=500:326.7727,lwr_k=600:326.4684,lwr_k=700:326.0573,lwr_k=800:325.7929,lwr_k=900:325.6835,lwr_k=1000:326.1039'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:5.509,lwr_k=10:4.8549,lwr_k=20:5.2157,lwr_k=30:5.1218,lwr_k=40:5.1131,lwr_k=50:5.1296,lwr_k=100:5.1602,lwr_k=200:5.1884,lwr_k=300:5.1952,lwr_k=400:5.2021,lwr_k=500:5.1964,lwr_k=600:5.2006,lwr_k=700:5.2021,lwr_k=800:5.2038,lwr_k=900:5.2092,lwr_k=1000:5.2074'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:10.6294,lwr_k=10:10.6829,lwr_k=20:10.6495,lwr_k=30:10.5083,lwr_k=40:10.3924,lwr_k=50:10.3597,lwr_k=100:10.138,lwr_k=200:10.0369,lwr_k=300:10.072,lwr_k=400:10.0646,lwr_k=500:10.0659,lwr_k=600:10.0835,lwr_k=700:10.0746,lwr_k=800:10.0868,lwr_k=900:10.1011,lwr_k=1000:10.1063'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_98'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.9115,lwr_k=10:1.3532,lwr_k=20:2.9151,lwr_k=30:3.5382,lwr_k=40:3.9208,lwr_k=50:4.1516,lwr_k=100:4.5785,lwr_k=200:4.9068,lwr_k=300:5.0721,lwr_k=400:5.2145,lwr_k=500:5.3075,lwr_k=600:5.4137,lwr_k=700:5.4922,lwr_k=800:5.5104,lwr_k=900:5.5417,lwr_k=1000:5.5603'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:9.0136,lwr_k=10:104.6347,lwr_k=20:14.9234,lwr_k=30:11.3363,lwr_k=40:9.8114,lwr_k=50:8.9919,lwr_k=100:8.0148,lwr_k=200:8.0933,lwr_k=300:8.213,lwr_k=400:8.333,lwr_k=500:8.3807,lwr_k=600:8.481,lwr_k=700:8.505,lwr_k=800:8.5603,lwr_k=900:8.61,lwr_k=1000:8.6766'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.4968,lwr_k=10:1.9431,lwr_k=20:3.598,lwr_k=30:4.2346,lwr_k=40:4.5061,lwr_k=50:4.6511,lwr_k=100:5.246,lwr_k=200:5.6045,lwr_k=300:5.7641,lwr_k=400:5.7899,lwr_k=500:5.811,lwr_k=600:5.8661,lwr_k=700:5.8955,lwr_k=800:5.9276,lwr_k=900:5.9404,lwr_k=1000:5.9497'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.659,lwr_k=10:4214355385.8225,lwr_k=20:1644344.6231,lwr_k=30:11.5332,lwr_k=40:11.1336,lwr_k=50:9.5737,lwr_k=100:7.9698,lwr_k=200:7.1445,lwr_k=300:7.1379,lwr_k=400:7.3164,lwr_k=500:7.0792,lwr_k=600:6.9517,lwr_k=700:7.2289,lwr_k=800:7.2586,lwr_k=900:6.9932,lwr_k=1000:7.2891'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.0827,lwr_k=10:1.9303,lwr_k=20:3.7075,lwr_k=30:4.1977,lwr_k=40:4.5398,lwr_k=50:4.6329,lwr_k=100:5.0557,lwr_k=200:5.286,lwr_k=300:5.382,lwr_k=400:5.4588,lwr_k=500:5.5256,lwr_k=600:5.5888,lwr_k=700:5.6233,lwr_k=800:5.6464,lwr_k=900:5.6716,lwr_k=1000:5.7398'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.1976,lwr_k=10:56.4515,lwr_k=20:9.7101,lwr_k=30:103.5067,lwr_k=40:6.1121,lwr_k=50:6.149,lwr_k=100:6.0708,lwr_k=200:6.5887,lwr_k=300:6.6593,lwr_k=400:6.5048,lwr_k=500:6.4839,lwr_k=600:6.5812,lwr_k=700:6.0289,lwr_k=800:6.8499,lwr_k=900:6.8149,lwr_k=1000:6.5471'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:6.3985,lwr_k=10:2.8745,lwr_k=20:4.1401,lwr_k=30:4.5958,lwr_k=40:4.8121,lwr_k=50:5.0304,lwr_k=100:5.5915,lwr_k=200:5.872,lwr_k=300:5.9889,lwr_k=400:6.0771,lwr_k=500:6.1104,lwr_k=600:6.1304,lwr_k=700:6.1558,lwr_k=800:6.197,lwr_k=900:6.2157,lwr_k=1000:6.213'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:10.8295,lwr_k=10:17.8869,lwr_k=20:12.6957,lwr_k=30:10.6707,lwr_k=40:10.5568,lwr_k=50:10.6727,lwr_k=100:10.4461,lwr_k=200:10.2449,lwr_k=300:10.3078,lwr_k=400:10.3815,lwr_k=500:10.4229,lwr_k=600:10.3607,lwr_k=700:10.3483,lwr_k=800:10.3458,lwr_k=900:10.3576,lwr_k=1000:11.4417'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.6733,lwr_k=10:0.2356,lwr_k=20:2.0435,lwr_k=30:2.91,lwr_k=40:3.3509,lwr_k=50:3.7265,lwr_k=100:4.3377,lwr_k=200:4.7458,lwr_k=300:4.9221,lwr_k=400:5.04,lwr_k=500:5.1157,lwr_k=600:5.1481,lwr_k=700:5.2039,lwr_k=800:5.2356,lwr_k=900:5.2667,lwr_k=1000:5.2878'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:18.582,lwr_k=10:365.345,lwr_k=20:33.6982,lwr_k=30:449.01,lwr_k=40:20.0941,lwr_k=50:159.1132,lwr_k=100:18.3727,lwr_k=200:18.7963,lwr_k=300:18.7586,lwr_k=400:19.1373,lwr_k=500:19.1816,lwr_k=600:19.0692,lwr_k=700:19.1515,lwr_k=800:19.119,lwr_k=900:19.1063,lwr_k=1000:18.8886'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:6.1136,lwr_k=10:1.6281,lwr_k=20:3.3102,lwr_k=30:4.0286,lwr_k=40:4.4147,lwr_k=50:4.7358,lwr_k=100:5.222,lwr_k=200:5.517,lwr_k=300:5.6345,lwr_k=400:5.7024,lwr_k=500:5.7219,lwr_k=600:5.7493,lwr_k=700:5.7863,lwr_k=800:5.8202,lwr_k=900:5.8194,lwr_k=1000:5.8582'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:8.7336,lwr_k=10:242.4506,lwr_k=20:74.1386,lwr_k=30:59.3694,lwr_k=40:49.363,lwr_k=50:23.9534,lwr_k=100:19.1718,lwr_k=200:9.8357,lwr_k=300:8.4695,lwr_k=400:8.6103,lwr_k=500:8.3031,lwr_k=600:8.3173,lwr_k=700:8.2646,lwr_k=800:8.2638,lwr_k=900:8.2147,lwr_k=1000:8.2465'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model random_99'\n",
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.0695,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0005,lwr_k=40:0.4156,lwr_k=50:0.9703,lwr_k=100:2.3519,lwr_k=200:3.0165,lwr_k=300:3.2985,lwr_k=400:3.4302,lwr_k=500:3.5023,lwr_k=600:3.5732,lwr_k=700:3.6505,lwr_k=800:3.6773,lwr_k=900:3.7069,lwr_k=1000:3.732'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:6.7848,lwr_k=10:22.7002,lwr_k=20:65.8104,lwr_k=30:190.3185,lwr_k=40:2301.6954,lwr_k=50:263.8742,lwr_k=100:8.5084,lwr_k=200:6.7393,lwr_k=300:6.5236,lwr_k=400:6.4295,lwr_k=500:6.324,lwr_k=600:6.2124,lwr_k=700:6.2041,lwr_k=800:6.219,lwr_k=900:6.2856,lwr_k=1000:6.3603'\n",
      "-----------------------------------Fold 1 - Train 3663 - Val 1222 - Test 1222-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.3704,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0061,lwr_k=40:0.4614,lwr_k=50:1.2288,lwr_k=100:2.7022,lwr_k=200:3.3095,lwr_k=300:3.547,lwr_k=400:3.7018,lwr_k=500:3.8253,lwr_k=600:3.8955,lwr_k=700:3.954,lwr_k=800:4.0058,lwr_k=900:4.0322,lwr_k=1000:4.0646'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.6367,lwr_k=10:43.5673,lwr_k=20:77.6182,lwr_k=30:21961.6422,lwr_k=40:6623.9843,lwr_k=50:655.8555,lwr_k=100:5376.9183,lwr_k=200:6.9714,lwr_k=300:7.0398,lwr_k=400:7.0199,lwr_k=500:7.034,lwr_k=600:6.9814,lwr_k=700:6.9656,lwr_k=800:6.9846,lwr_k=900:6.9774,lwr_k=1000:7.0219'\n",
      "-----------------------------------Fold 2 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:5.268,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0834,lwr_k=40:3.1885,lwr_k=50:1.5435,lwr_k=100:2.9087,lwr_k=200:3.7698,lwr_k=300:4.0412,lwr_k=400:4.2445,lwr_k=500:4.4153,lwr_k=600:4.5257,lwr_k=700:4.6155,lwr_k=800:4.6701,lwr_k=900:4.7212,lwr_k=1000:4.7778'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:6.0598,lwr_k=10:22.2074,lwr_k=20:122.3026,lwr_k=30:2031.9209,lwr_k=40:12207.9948,lwr_k=50:603.1233,lwr_k=100:128.6811,lwr_k=200:6.1727,lwr_k=300:5.6369,lwr_k=400:5.5106,lwr_k=500:5.5763,lwr_k=600:5.5632,lwr_k=700:5.5989,lwr_k=800:5.5757,lwr_k=900:5.5758,lwr_k=1000:5.6149'\n",
      "-----------------------------------Fold 3 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:7.2392,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0039,lwr_k=40:0.7696,lwr_k=50:1.6488,lwr_k=100:3.6248,lwr_k=200:4.7425,lwr_k=300:5.1806,lwr_k=400:5.4383,lwr_k=500:5.6304,lwr_k=600:5.8061,lwr_k=700:5.9027,lwr_k=800:5.9812,lwr_k=900:6.0715,lwr_k=1000:6.1388'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:11.5972,lwr_k=10:31.0336,lwr_k=20:84.4748,lwr_k=30:700.4182,lwr_k=40:242.7354,lwr_k=50:201.9699,lwr_k=100:11.6585,lwr_k=200:10.3329,lwr_k=300:10.0827,lwr_k=400:9.9173,lwr_k=500:9.8319,lwr_k=600:9.5817,lwr_k=700:9.6212,lwr_k=800:9.6332,lwr_k=900:9.6799,lwr_k=1000:9.7208'\n",
      "-----------------------------------Fold 4 - Train 3665 - Val 1221 - Test 1221-----------------------------------'\n",
      "Finished training DeepLWR with a train loss of lr:4.2421,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0574,lwr_k=40:0.546,lwr_k=50:1.0827,lwr_k=100:2.4209,lwr_k=200:3.0597,lwr_k=300:3.3557,lwr_k=400:3.4521,lwr_k=500:3.5524,lwr_k=600:3.6365,lwr_k=700:3.6617,lwr_k=800:3.7099,lwr_k=900:3.734,lwr_k=1000:3.7526'\n",
      "Tested (test) on 1221 instances with mean losses of: lr:18.9518,lwr_k=10:28.6135,lwr_k=20:189.2448,lwr_k=30:975.5975,lwr_k=40:782603.2499,lwr_k=50:654.8872,lwr_k=100:78.7301,lwr_k=200:25.1533,lwr_k=300:18.3403,lwr_k=400:18.0063,lwr_k=500:17.3252,lwr_k=600:17.4079,lwr_k=700:17.9648,lwr_k=800:18.0712,lwr_k=900:18.4994,lwr_k=1000:18.6551'\n",
      "Building final model - Train 4885 - Test 1222'\n",
      "Finished training DeepLWR with a train loss of lr:4.0714,lwr_k=10:0.0,lwr_k=20:0.0,lwr_k=30:0.0158,lwr_k=40:0.5474,lwr_k=50:0.9964,lwr_k=100:2.2848,lwr_k=200:3.1767,lwr_k=300:3.4051,lwr_k=400:3.5179,lwr_k=500:3.5882,lwr_k=600:3.6421,lwr_k=700:3.6911,lwr_k=800:3.7167,lwr_k=900:3.7351,lwr_k=1000:3.7448'\n",
      "Tested (test) on 1222 instances with mean losses of: lr:7.6235,lwr_k=10:16.7361,lwr_k=20:92.0803,lwr_k=30:612.8863,lwr_k=40:6183.2996,lwr_k=50:938.1831,lwr_k=100:10.1846,lwr_k=200:7.2051,lwr_k=300:7.2737,lwr_k=400:7.2076,lwr_k=500:7.2619,lwr_k=600:7.2443,lwr_k=700:7.3174,lwr_k=800:7.3645,lwr_k=900:7.3336,lwr_k=1000:7.373'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4068: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    }
   ],
   "source": [
    "for deep_name,deep_model in tqdm(deep_models.items()):\n",
    "    #if int(deep_name.replace(\"random_\",\"\"))>=85:\n",
    "        logging.getLogger().info(f\"Running model {deep_name}\")\n",
    "        temp_dict = {deep_name:deep_model}\n",
    "\n",
    "        lwr_scheme = DeepLWRScheme_1_to_n(lwr_models = setup_pls_models_exh(nrow),n_neighbours=500,loss_fun_sk = mean_squared_error)\n",
    "        lwr_scores, lwr_preds, _ , _, _,_= eval.evaluate(temp_dict,dataset,lwr_scheme,logger_name=\"log\",load_fun=load_fun_cv,load_fun_pp = load_fun_pp_cv)\n",
    "        lwr_scores_final, lwr_preds_final, _ , _, _,_= eval.build(temp_dict,dataset,lwr_scheme,logger_name=\"test_log\",load_fun=load_fun_build,load_fun_pp = load_fun_pp_build)\n",
    "\n",
    "        #scores\n",
    "        for k,v in ut.flip_dicts(lwr_scores).items():\n",
    "            dict1 = {'model_num':deep_name,\"predictor\":k}\n",
    "            all_scores.append({**dict1,**v})\n",
    "\n",
    "        for k,v in ut.flip_dicts(lwr_scores_final).items():\n",
    "            dict1 = {'model_num':deep_name,\"predictor\":k}\n",
    "            all_scores_final.append({**dict1,**v})\n",
    "\n",
    "        lwr_preds['deep'] = deep_preds[deep_name]\n",
    "        lwr_preds_final['deep'] = deep_preds_final[deep_name]\n",
    "\n",
    "        lwr_preds.to_csv(log_dir/deep_name/ f\"predictions.csv\",index=False)\n",
    "        lwr_preds_final.to_csv(log_dir/deep_name/ f\"predictions_test.csv\",index=False)\n",
    "\n",
    "        #preds\n",
    "        # todo save predictions - appending solns\n",
    "        plot_preds_and_res(lwr_preds,name_lambda=lambda x:f\"{deep_name} with {x} predictor\",save_lambda= lambda x:f\"deep_lwr{x}\",save_loc=log_dir/deep_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% save scores\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank - model_num - predictor - fold_0 - fold_1 - fold_2 - fold_3 - fold_4 - MSE - R2'\n",
      "0 - random_25 - lwr_k=200 - 8.034434276443106 - 7.3714001091352355 - 6.944130803078013 - 10.125055890576716 - 9.705445335740432 - 8.435853172927498 - 0.964229049715442'\n",
      "1 - random_5 - lwr_k=500 - 6.93741866252285 - 6.542271424521091 - 5.263570256127725 - 10.455479236160528 - 15.270178895310115 - 8.893078295009875 - 0.9622902562377001'\n",
      "2 - random_5 - lwr_k=400 - 7.040964533696965 - 6.628886657106437 - 5.385501369459019 - 10.415794965862464 - 15.006131386439584 - 8.894780973216314 - 0.9622830362901472'\n",
      "3 - random_5 - lwr_k=700 - 6.908637187007973 - 6.423653461201897 - 5.197672798623087 - 10.507479669200112 - 15.466510590450852 - 8.900058910474058 - 0.9622606560012326'\n",
      "4 - random_5 - lwr_k=800 - 6.97531783384612 - 6.464079579307727 - 5.157656794916131 - 10.60983272821156 - 15.402992325494266 - 8.921254621916066 - 0.9621707788157605'\n",
      "5 - random_92 - lwr_k=500 - 7.807864200451539 - 7.132659401873883 - 6.422989138158542 - 8.80327518040168 - 14.443819372619048 - 8.921645984766915 - 0.9621691193012106'\n",
      "6 - random_5 - lwr_k=900 - 6.966278756438642 - 6.452911374250747 - 5.27782011142013 - 10.563395606930678 - 15.432133191301233 - 8.937777854656554 - 0.9621007145644295'\n",
      "7 - random_5 - lwr_k=300 - 7.229023178272314 - 6.511048130318322 - 5.405599481762138 - 10.504649082869056 - 15.076973868364277 - 8.94477906169727 - 0.9620710270125183'\n",
      "8 - random_5 - lwr_k=1000 - 7.016450479043726 - 6.435879367396077 - 5.32666754885877 - 10.551108482214742 - 15.430904147805878 - 8.951472993411032 - 0.9620426424148217'\n",
      "9 - random_5 - lwr_k=600 - 6.916769597836045 - 6.53474610595849 - 5.201883812843292 - 10.515119807955395 - 15.624878370535281 - 8.957948272712576 - 0.9620151849793702'\n",
      "10 - random_92 - lwr_k=600 - 7.870125058403479 - 7.124429278979767 - 6.330762035858578 - 8.685175029311958 - 15.037835687989903 - 9.009170121482107 - 0.9617979865326621'\n",
      "11 - random_92 - lwr_k=1000 - 7.99709915560702 - 7.4550555961514755 - 6.244832115263215 - 8.323888972752757 - 15.121570435748213 - 9.02806272427787 - 0.9617178753285551'\n",
      "12 - random_92 - lwr_k=700 - 7.914673899150158 - 7.291177040394502 - 6.318025457961705 - 8.624540064327288 - 14.995168052668966 - 9.02824996612858 - 0.961717081358011'\n",
      "13 - random_92 - lwr_k=900 - 8.010531470224906 - 7.4613586172640165 - 6.262771500190483 - 8.452314476633441 - 15.096069087221808 - 9.056176522042133 - 0.9615986630519162'\n",
      "14 - random_92 - lwr_k=800 - 8.018741690726053 - 7.414912257595965 - 6.266795680983404 - 8.527924175494995 - 15.118131903529838 - 9.068858215788367 - 0.9615448882614805'\n",
      "15 - random_25 - lwr_k=300 - 7.9604697387257595 - 7.472341856189868 - 6.736691493164804 - 10.126738102717155 - 13.420413970300405 - 9.14286372413551 - 0.9612310791771351'\n",
      "16 - random_99 - lwr_k=600 - 6.212399233501028 - 6.981372692000912 - 5.563244261321232 - 9.581705997055787 - 17.407862097600738 - 9.148480952941249 - 0.961207260174098'\n",
      "17 - random_92 - lwr_k=400 - 8.048868105949763 - 7.106894344549763 - 6.5341418479819 - 9.010682145261724 - 15.174700380558006 - 9.174534300789619 - 0.961096784921444'\n",
      "18 - random_99 - lwr_k=500 - 6.323964033103509 - 7.0339843794041 - 5.576301146790095 - 9.83190095206729 - 17.32519871870621 - 9.217438244363755 - 0.9609148572845896'\n",
      "19 - random_99 - lwr_k=700 - 6.204114385900475 - 6.965638337256475 - 5.598893053287881 - 9.621224947749788 - 17.964846985131075 - 9.270063873556547 - 0.9606917063208447'\n",
      "20 - random_5 - lwr_k=200 - 7.510883425583596 - 6.606974251668049 - 5.566445309195536 - 10.504385082671869 - 16.191629705073073 - 9.275337458648409 - 0.9606693444866264'\n",
      "21 - random_5 - lr - 6.956346198565897 - 6.533402250474058 - 5.9423368602254465 - 11.029988578435853 - 15.941270357630284 - 9.27983839397394 - 0.960650258956039'\n",
      "22 - random_99 - lwr_k=800 - 6.2189616834935855 - 6.98456287695516 - 5.575702773104916 - 9.633152265107771 - 18.071229865311835 - 9.295839312276373 - 0.9605824094995145'\n",
      "23 - random_83 - lwr_k=200 - 6.780267223543273 - 7.602339976899287 - 6.40090515446192 - 9.151319873619867 - 16.578699073864517 - 9.30201479080027 - 0.9605562233235897'\n",
      "24 - random_60 - lr - 7.827681489147881 - 8.709524793199174 - 7.36825633952156 - 10.345832985960051 - 12.509346727890016 - 9.35177362013998 - 0.9603452285879013'\n",
      "25 - random_83 - lwr_k=400 - 6.773867439377832 - 7.715960916110621 - 6.364109397179595 - 9.218132906470618 - 16.776225896130633 - 9.368963471809286 - 0.9602723376405534'\n",
      "26 - random_83 - lwr_k=500 - 6.751982820151262 - 7.677495246149421 - 6.391119977377674 - 9.274117499867065 - 16.75703604057113 - 9.369644369129297 - 0.960269450399194'\n",
      "27 - random_92 - lwr_k=300 - 8.22550345319699 - 7.2306400396468495 - 6.5944374970555355 - 9.217338750743052 - 15.600737644120864 - 9.37319253484343 - 0.9602544049430013'\n",
      "28 - random_99 - lwr_k=400 - 6.429522805360048 - 7.019946574112867 - 5.510584130009898 - 9.917331367551862 - 18.006268053388798 - 9.375862075884482 - 0.9602430851608874'\n",
      "29 - random_83 - lwr_k=300 - 6.758410811231434 - 7.611427323971593 - 6.353205943650892 - 9.267164937966163 - 16.89718783738527 - 9.376761322600284 - 0.9602392720421778'\n",
      "30 - random_83 - lwr_k=600 - 6.763686013953376 - 7.630033862794317 - 6.42872628328996 - 9.272502669789722 - 16.793642586423605 - 9.377004067319337 - 0.9602382427201758'\n",
      "31 - random_83 - lwr_k=700 - 6.827652851405745 - 7.656859286076873 - 6.387629038047896 - 9.32151519726592 - 16.744645144573948 - 9.386957698517557 - 0.9601960358633892'\n",
      "32 - random_6 - lwr_k=400 - 8.347089867382012 - 5.869592261431251 - 6.562206998849558 - 7.949275011911375 - 18.212345351606597 - 9.387355292428452 - 0.9601943499269785'\n",
      "33 - random_83 - lwr_k=800 - 6.89763044032266 - 7.592881258110555 - 6.380337547796269 - 9.27835033914494 - 16.812139778940278 - 9.391564741366741 - 0.9601765004000118'\n",
      "34 - random_87 - lwr_k=300 - 6.780334546058304 - 7.817513775434225 - 6.832336683108992 - 9.803288474388118 - 15.756869864487694 - 9.397381213477537 - 0.9601518365360917'\n",
      "35 - random_83 - lwr_k=1000 - 6.954869726305881 - 7.60016553025362 - 6.389077370278365 - 9.30322326897664 - 16.754529968327045 - 9.399677952410904 - 0.9601420975645238'\n",
      "36 - random_99 - lwr_k=900 - 6.285574375317336 - 6.977424759212849 - 5.575784600904116 - 9.67993876469455 - 18.49941219976724 - 9.40271908755652 - 0.9601292020942173'\n",
      "37 - random_83 - lwr_k=900 - 6.937255023723221 - 7.590303781453236 - 6.3894823500660145 - 9.299844121777005 - 16.80778750370444 - 9.404233342732777 - 0.96012278112582'\n",
      "38 - random_59 - lwr_k=200 - 6.9734289909705165 - 7.124286108215117 - 5.958053929736128 - 8.711021395992482 - 18.319581716248763 - 9.416498788187198 - 0.9600707713728555'\n",
      "39 - random_6 - lwr_k=1000 - 8.338697719131035 - 6.044593313666762 - 6.699105357380591 - 8.00704493378616 - 18.11070606551831 - 9.439293147796194 - 0.9599741153633651'\n",
      "40 - random_6 - lwr_k=300 - 8.413979217388457 - 5.886711301318397 - 6.8096419869004725 - 8.0928099008036 - 18.070865084568563 - 9.454046804814912 - 0.9599115547283098'\n",
      "41 - random_25 - lwr_k=400 - 7.880511549468645 - 7.564193267511107 - 6.771456883121285 - 10.046023341915278 - 15.048144573324418 - 9.461496178999186 - 0.959879966791899'\n",
      "42 - random_6 - lwr_k=900 - 8.321518387495688 - 6.046498261794485 - 6.780271379284399 - 8.08661549368259 - 18.100174496695004 - 9.466267934794478 - 0.9598597328883656'\n",
      "43 - random_6 - lwr_k=500 - 8.342411877823272 - 5.915739115483297 - 7.122813586908634 - 7.905606902213434 - 18.080553327150263 - 9.472657203780768 - 0.9598326401665533'\n",
      "44 - random_99 - lwr_k=1000 - 6.36034451553814 - 7.021915360371328 - 5.614921346635627 - 9.720777991867628 - 18.655081886241014 - 9.473696650364191 - 0.9598282325516628'\n",
      "45 - random_59 - lwr_k=300 - 7.039039405474571 - 6.920314715096617 - 5.912612025730292 - 8.662206543541767 - 18.839775770352166 - 9.473972560016241 - 0.9598270625988147'\n",
      "46 - random_6 - lwr_k=800 - 8.352309844415451 - 6.043159763706984 - 6.813911648238059 - 8.064707943823073 - 18.101832816556847 - 9.474438554453606 - 0.9598250866203913'\n",
      "47 - random_40 - lwr_k=1000 - 7.930037350395597 - 6.772710736553519 - 6.799748706068009 - 9.443287314588854 - 16.47331847607773 - 9.483122155356927 - 0.9597882651335973'\n",
      "48 - random_91 - lwr_k=600 - 7.848485561859704 - 6.924947163193283 - 6.510129029922155 - 9.542827517936995 - 16.751539611069152 - 9.514888586878373 - 0.9596535644199435'\n",
      "49 - random_63 - lwr_k=400 - 6.12781773475136 - 6.943634927074631 - 5.668226553383242 - 8.36731949627039 - 20.483655270403997 - 9.517154079669513 - 0.9596439579428818'\n",
      "50 - random_59 - lwr_k=400 - 6.908684866308599 - 6.83529552190345 - 5.8520586605633955 - 8.703002101814342 - 19.29722747377124 - 9.518386764488788 - 0.9596387309306895'\n",
      "51 - random_6 - lwr_k=700 - 8.341893227995651 - 6.042232619266471 - 7.006871860118833 - 7.955404899604406 - 18.257551098170726 - 9.520028098877164 - 0.9596317711022522'\n",
      "52 - random_99 - lwr_k=300 - 6.5236296650896675 - 7.039834919022975 - 5.6368716009099185 - 10.08271805165548 - 18.340274957915888 - 9.523767547270886 - 0.959615914540996'\n",
      "53 - random_63 - lwr_k=300 - 6.130550012458766 - 7.018348331657451 - 5.758593131080083 - 8.349124070052277 - 20.39725332202186 - 9.529805597720866 - 0.9595903111078834'\n",
      "54 - random_87 - lwr_k=400 - 6.782926202885466 - 8.044335333946282 - 6.830896278891415 - 9.918087373007676 - 16.164738302410555 - 9.547497642741195 - 0.9595152906861333'\n",
      "55 - random_63 - lwr_k=200 - 6.198039949387699 - 7.701734548237338 - 5.943477801136486 - 8.205252932899628 - 19.70100761906158 - 9.549051083220908 - 0.9595087035584261'\n",
      "56 - random_6 - lwr_k=600 - 8.38740128465835 - 5.974662871902424 - 7.080263088891981 - 8.102494594137493 - 18.28493373127595 - 9.565170069786179 - 0.9594403534514192'\n",
      "57 - random_83 - lwr_k=100 - 7.039663790517869 - 7.9973507423114985 - 6.6389175939636775 - 9.613312918345214 - 16.59261042901766 - 9.575697158748518 - 0.9593957149343428'\n",
      "58 - random_25 - lwr_k=500 - 7.633099928988557 - 7.589263431714278 - 6.731711478196346 - 9.889496071865365 - 16.040432136829494 - 9.576156882998696 - 0.9593937655436932'\n",
      "59 - random_63 - lwr_k=500 - 6.156787254003959 - 6.883074603186485 - 5.7248003945400825 - 8.464272033192088 - 20.667618990301428 - 9.578308729475316 - 0.9593846409665155'\n",
      "60 - random_87 - lwr_k=500 - 6.7824638405785835 - 8.075500839240522 - 6.845389151922046 - 10.075791981295493 - 16.136978762882993 - 9.582519415732389 - 0.9593667861928871'\n",
      "61 - random_49 - lwr_k=200 - 7.6267933623180655 - 7.301162295391706 - 5.755055701270539 - 10.359727201592364 - 16.90862299726519 - 9.58957596492185 - 0.9593368639710249'\n",
      "62 - random_59 - lwr_k=500 - 6.836591358758581 - 6.8602291963644895 - 5.847954860602507 - 8.779642117092886 - 19.659370803493463 - 9.595857602618633 - 0.9593102276432997'\n",
      "63 - random_59 - lwr_k=600 - 6.868520505795957 - 6.839582524424334 - 5.856885109203934 - 8.815257407572114 - 19.705242727820206 - 9.616192776581174 - 0.9592239994359173'\n",
      "64 - random_63 - lwr_k=600 - 6.177216497136529 - 6.818068266120317 - 5.68911998708552 - 8.63359070266817 - 20.776696180094593 - 9.617916123920848 - 0.9592166918440536'\n",
      "65 - random_87 - lwr_k=600 - 6.8582417102709945 - 8.097391536460556 - 6.877590601497264 - 10.326272101308279 - 16.016800715926813 - 9.63455278561948 - 0.9591461466145048'\n",
      "66 - random_67 - lwr_k=200 - 6.335909064156107 - 7.365368272849987 - 5.685469678307395 - 8.655485593433156 - 20.146575648890504 - 9.63684888814671 - 0.9591364103415203'\n",
      "67 - random_16 - lwr_k=500 - 6.565352172490468 - 7.934321681673581 - 6.0335080257813685 - 9.39598501249677 - 18.29505981419721 - 9.64406099273962 - 0.9591058285106667'\n",
      "68 - random_49 - lwr_k=400 - 7.531689611924118 - 7.5298260126969065 - 5.971043641557022 - 9.838765708534893 - 17.36296660029804 - 9.64616530681399 - 0.9590969054873999'\n",
      "69 - random_59 - lwr_k=700 - 6.685062415666942 - 6.808677659514862 - 5.841073888566763 - 8.807177316981571 - 20.0971176960743 - 9.6468717538486 - 0.9590939099063679'\n",
      "70 - random_16 - lwr_k=400 - 6.517411965199771 - 7.926520641948016 - 5.977087814917002 - 9.309068380935882 - 18.600481173735528 - 9.665313553987563 - 0.959015710262253'\n",
      "71 - random_49 - lwr_k=500 - 7.517802886303103 - 7.524441293317698 - 6.059919301660424 - 9.679808283443288 - 17.56727110581562 - 9.669144881137896 - 0.9589994641031261'\n",
      "72 - random_40 - lwr_k=900 - 7.916747497677426 - 6.750930350549914 - 6.5793602796014214 - 9.73622694044408 - 17.391857322879623 - 9.674257756254361 - 0.9589777837350764'\n",
      "73 - random_87 - lwr_k=700 - 6.904889882176484 - 8.253702928621355 - 6.892456601314224 - 10.436628134839843 - 15.926466421469394 - 9.682139901466055 - 0.958944360699051'\n",
      "74 - random_63 - lwr_k=700 - 6.226405809015215 - 6.7098728070982725 - 5.715579131441693 - 8.726914000764067 - 21.06967067446657 - 9.688633449621353 - 0.9589168257973082'\n",
      "75 - random_74 - lwr_k=300 - 8.132706868735239 - 7.691511731735008 - 7.488944176110206 - 10.278022225579821 - 14.866317147188504 - 9.690917691657987 - 0.9589071398169271'\n",
      "76 - random_25 - lwr_k=800 - 7.482400043508005 - 7.829930718312903 - 6.668310677655801 - 9.8892772719529 - 16.631407932104093 - 9.69959590019919 - 0.9588703411956232'\n",
      "77 - random_0 - lwr_k=400 - 7.5276180058106 - 7.583813976571105 - 6.686163272604474 - 8.741260878058663 - 17.989084488396276 - 9.704884056126083 - 0.9588479175759987'\n",
      "78 - random_49 - lwr_k=300 - 7.609266576494412 - 7.500669762087005 - 5.911437164222981 - 10.052327066621205 - 17.46281149769481 - 9.70659753893771 - 0.9588406518131634'\n",
      "79 - random_49 - lwr_k=600 - 7.491557306037344 - 7.671693130092914 - 6.114695751886598 - 9.604662314212241 - 17.678086522722555 - 9.711441276551842 - 0.9588201126816926'\n",
      "80 - random_40 - lr - 8.290819619663356 - 7.01902742212943 - 7.444204837608634 - 9.724066942138665 - 16.096968366768383 - 9.714342771240615 - 0.9588078093354708'\n",
      "81 - random_16 - lwr_k=600 - 6.602493952007094 - 7.9591635170626525 - 6.100229330559457 - 9.284264560445846 - 18.639963859770564 - 9.716425141788203 - 0.9587989793603865'\n",
      "82 - random_0 - lwr_k=300 - 7.350636240832214 - 7.568982360620483 - 6.659341446902051 - 8.953296818912028 - 18.058144619532882 - 9.717340729324352 - 0.9587950969509176'\n",
      "83 - random_87 - lwr_k=800 - 6.980761045982269 - 8.310381037013233 - 6.8925043728752025 - 10.439866511596492 - 16.05455856915335 - 9.734929832699637 - 0.9587205130375316'\n",
      "84 - random_91 - lwr_k=900 - 7.801480636525363 - 7.345468878856006 - 6.512886416183112 - 9.430344120982447 - 17.597028470072832 - 9.736733020408401 - 0.9587128668947453'\n",
      "85 - random_8 - lwr_k=800 - 7.871390729233597 - 7.524250896350178 - 6.651198190313552 - 9.339040729624246 - 17.303219089400574 - 9.73715184147135 - 0.9587110909478299'\n",
      "86 - random_59 - lwr_k=800 - 6.797289545039302 - 6.777714518695082 - 5.835122290480617 - 8.82744086200024 - 20.461167837739076 - 9.73878017110561 - 0.9587041862640713'\n",
      "87 - random_40 - lwr_k=800 - 8.036537301857905 - 6.69767239100209 - 6.431395495109077 - 10.156802849876998 - 17.397269602073667 - 9.743157132476643 - 0.958685626426152'\n",
      "88 - random_67 - lwr_k=300 - 6.387795623530938 - 7.23740557416335 - 5.860438367005365 - 8.895868263378457 - 20.360894063334264 - 9.747518898086163 - 0.9586671310235444'\n",
      "89 - random_8 - lwr_k=600 - 7.903556811734509 - 7.4167528345128515 - 6.648001834179682 - 9.457090534612929 - 17.373300719309572 - 9.759052947156874 - 0.9586182226455262'\n",
      "90 - random_8 - lwr_k=900 - 7.881068287774786 - 7.518286192517816 - 6.663954456520422 - 9.302076866064743 - 17.43522440934153 - 9.759447261110209 - 0.9586165506172758'\n",
      "91 - random_0 - lwr_k=500 - 7.635425907479836 - 7.577209548274963 - 6.619078457979995 - 8.840864344844261 - 18.15477026484186 - 9.764762597437807 - 0.9585940117433027'\n",
      "92 - random_96 - lwr_k=600 - 9.194140637984502 - 8.18307766604565 - 6.320582660108654 - 9.51735171190743 - 15.659673426203895 - 9.774609446397813 - 0.9585522576803304'\n",
      "93 - random_87 - lwr_k=900 - 7.038555222740322 - 8.37907248901866 - 6.897530144493483 - 10.490473835261598 - 16.09878687068117 - 9.780205123981027 - 0.9585285300619693'\n",
      "94 - random_64 - lwr_k=500 - 8.63946881430455 - 8.17675663129589 - 6.503907267768253 - 9.237067944543838 - 16.35072559436423 - 9.781135447760851 - 0.9585245851657039'\n",
      "95 - random_49 - lwr_k=800 - 7.554397750426824 - 7.74384136067071 - 6.236004145984396 - 9.317293291084177 - 18.066375642154807 - 9.78288341632437 - 0.9585171731712911'\n",
      "96 - random_59 - lwr_k=900 - 6.771895628120749 - 6.770673429204237 - 5.809972794264178 - 8.853427303569957 - 20.748891197845605 - 9.789983143945651 - 0.9584870678578665'\n",
      "97 - random_5 - deep - 6.5993118223699145 - 6.699335931756102 - 5.808911881224236 - 12.437064430149338 - 17.411863611332222 - 9.790268489036634 - 0.958485857983749'\n",
      "98 - random_49 - lwr_k=700 - 7.6045003308258785 - 7.718743029897339 - 6.199691870468538 - 9.52532450171246 - 17.940429637168204 - 9.79703831082678 - 0.9584571514974723'\n",
      "99 - random_8 - lwr_k=1000 - 7.892660955681868 - 7.5509987153099996 - 6.743169804066998 - 9.33800571236122 - 17.48885774549001 - 9.802057103447556 - 0.9584358700719133'\n",
      "100 - random_16 - lwr_k=300 - 6.5273920348302985 - 8.079123053193467 - 6.0298426209974005 - 9.297859392086451 - 19.102484361096344 - 9.806520222767162 - 0.9584169448943382'\n",
      "101 - random_49 - lwr_k=900 - 7.584955499237644 - 7.806450952274735 - 6.294688115108857 - 9.243540517428718 - 18.14953931204125 - 9.815140550856212 - 0.9583803917062742'\n",
      "102 - random_87 - lwr_k=1000 - 7.072112709944753 - 8.457932970680758 - 6.877255987164108 - 10.611359036993678 - 16.10202428943216 - 9.823462653286015 - 0.9583451030986874'\n",
      "103 - random_8 - lwr_k=700 - 7.872745200737295 - 7.481326307862104 - 6.626496501498206 - 9.822776805984475 - 17.330174131245656 - 9.825999788144058 - 0.9583343447648222'\n",
      "104 - random_25 - lwr_k=600 - 7.567143625895118 - 7.6927756210834675 - 6.700450685139001 - 9.860906990847408 - 17.31747745500842 - 9.827031114254545 - 0.95832997158356'\n",
      "105 - random_25 - lwr_k=700 - 7.498789927817126 - 7.7691907964682345 - 6.672103332769081 - 9.880254666423328 - 17.340004863948163 - 9.83134886212074 - 0.9583116628314876'\n",
      "106 - random_8 - lwr_k=500 - 7.945154000434291 - 7.4477588640455 - 6.81862805399065 - 9.462421761084638 - 17.49119892493073 - 9.832332607162543 - 0.958307491410499'\n",
      "107 - random_63 - lwr_k=800 - 6.3034127678569645 - 6.804072544226074 - 5.768314752805412 - 8.855480252578948 - 21.44081397057469 - 9.833344458983376 - 0.9583032008069955'\n",
      "108 - random_14 - lwr_k=100 - 7.33545344062687 - 8.471875832285216 - 6.916340734565279 - 8.593139747058817 - 17.857830459837043 - 9.834295567549145 - 0.9582991677760117'\n",
      "109 - random_59 - lwr_k=1000 - 6.7767283854747 - 6.732648997227628 - 5.802729621564206 - 8.878864022757975 - 20.98855663646772 - 9.834896455625792 - 0.9582966198016604'\n",
      "110 - random_0 - lwr_k=700 - 7.716741535956948 - 7.671429205836708 - 6.562539982520397 - 8.924853610128922 - 18.326140779026193 - 9.839638138904151 - 0.9582765134160538'\n",
      "111 - random_0 - lwr_k=600 - 7.693019709749623 - 7.62791221411486 - 6.589330014501765 - 8.989742317018413 - 18.339885744513815 - 9.847261605010628 - 0.9582441872693677'\n",
      "112 - random_63 - lwr_k=900 - 6.3301150192467555 - 6.812253569996327 - 5.822963624303009 - 8.791507291939654 - 21.541983005777652 - 9.858687515120659 - 0.9581957374381385'\n",
      "113 - random_0 - lwr_k=800 - 7.752116748263301 - 7.705312590031583 - 6.597745857980129 - 8.977595713062994 - 18.287517132679493 - 9.86335829845123 - 0.9581759316929598'\n",
      "114 - random_8 - lwr_k=400 - 7.8891596572953775 - 7.499188643203109 - 6.773740267457787 - 9.55407849788393 - 17.617485870906144 - 9.866019090235433 - 0.9581646490107368'\n",
      "115 - random_87 - lwr_k=200 - 6.799279252490672 - 10.314892525609896 - 7.030620661430916 - 9.796587734548568 - 15.394098275608064 - 9.866666670844488 - 0.9581619030438148'\n",
      "116 - random_92 - lwr_k=200 - 8.764564133140992 - 7.545336707645299 - 6.716952590064785 - 9.584638034785328 - 16.804200731700448 - 9.882572469917976 - 0.9580944569258905'\n",
      "117 - random_16 - lwr_k=700 - 6.701623722474377 - 7.896136410247519 - 6.1153670924719155 - 9.314078205188078 - 19.391250646876315 - 9.882844707788603 - 0.958093302542672'\n",
      "118 - random_25 - lwr_k=900 - 7.472546936586546 - 7.8724124514490486 - 6.617634523006236 - 9.916265638669653 - 17.56622522551003 - 9.888291054516703 - 0.9580702081390534'\n",
      "119 - random_0 - lwr_k=900 - 7.796808277563157 - 7.750684068899333 - 6.575015508910003 - 9.014206268643662 - 18.35314736429382 - 9.897276628389216 - 0.9580321061818852'\n",
      "120 - random_1 - lwr_k=800 - 8.350335166878855 - 9.056136291322973 - 5.598147432524862 - 9.843964484430629 - 16.65480152433581 - 9.900284826222254 - 0.958019350377235'\n",
      "121 - random_49 - lwr_k=1000 - 7.68684129289083 - 7.817693550056576 - 6.3508326594588365 - 9.23165750922488 - 18.417918964947095 - 9.90028510401915 - 0.9580193491992796'\n",
      "122 - random_14 - lwr_k=200 - 7.278511863155684 - 7.7195298948134585 - 6.2228034432569075 - 8.502813605931427 - 19.820878728991946 - 9.908118286130083 - 0.9579861337838241'\n",
      "123 - random_67 - lwr_k=400 - 6.519171457214701 - 7.089920360045605 - 5.953752163824306 - 9.05824716964435 - 20.92651403034702 - 9.90850417846914 - 0.9579844974661461'\n",
      "124 - random_1 - lwr_k=700 - 8.379288840949872 - 9.074857324427315 - 5.6083967334723805 - 9.94339519571516 - 16.61578186877233 - 9.923951894776474 - 0.9579189937784085'\n",
      "125 - random_63 - lwr_k=1000 - 6.359786230510666 - 6.7153188383764695 - 5.8632891337862265 - 8.897772068197389 - 21.802509824595692 - 9.926624957857117 - 0.9579076590616223'\n",
      "126 - random_67 - lwr_k=100 - 6.717372670679007 - 8.143796329992538 - 5.975520094166538 - 8.771589282271082 - 20.067351619733856 - 9.934305779457345 - 0.9578750897077023'\n",
      "127 - random_96 - lwr_k=300 - 8.848789726521208 - 8.17137834111204 - 6.581043870888499 - 11.94877991355813 - 14.179142252011355 - 9.945356625047165 - 0.9578282302804348'\n",
      "128 - random_0 - lwr_k=1000 - 7.8072993814672795 - 7.818048225906953 - 6.600949046786478 - 9.039328836863099 - 18.472163151911495 - 9.946858568958799 - 0.9578218615160797'\n",
      "129 - random_1 - lwr_k=900 - 8.250252563352543 - 9.261483079556067 - 5.582218807254552 - 9.934022443672374 - 16.753334435372153 - 9.95586914501482 - 0.9577836535409611'\n",
      "130 - random_64 - lwr_k=600 - 8.75124054660661 - 8.23807031571983 - 6.4812581864919485 - 9.631661022624462 - 16.756945316167542 - 9.971351311467926 - 0.9577180037726284'\n",
      "131 - random_8 - lwr_k=300 - 7.857290527611102 - 7.804337015577175 - 6.812951029065178 - 9.709181806759009 - 17.817541311605485 - 9.999549859470722 - 0.9575984321255138'\n",
      "132 - random_16 - lwr_k=800 - 6.746589778971102 - 7.849466984428876 - 6.141452283544491 - 9.267133898252647 - 20.029508757610248 - 10.005943226771112 - 0.9575713220254248'\n",
      "133 - random_6 - deep - 7.657056827201781 - 6.564094499753462 - 9.682900840390618 - 8.210204705949888 - 17.981783471665942 - 10.018255385589088 - 0.9575191142818861'\n",
      "134 - random_60 - lwr_k=600 - 7.814577303405706 - 7.22853898572126 - 6.532201435950183 - 9.829405391138177 - 18.714779476455252 - 10.02308101882101 - 0.9574986518289625'\n",
      "135 - random_40 - lwr_k=700 - 8.03328966805684 - 6.738866530068546 - 6.473718210627666 - 10.436014061627093 - 18.510026314750547 - 10.037514345643146 - 0.9574374495053075'\n",
      "136 - random_64 - lwr_k=700 - 8.56620435122351 - 8.135858332465382 - 6.457485135939012 - 9.264188549603219 - 17.79882375345066 - 10.043957421398094 - 0.9574101286240896'\n",
      "137 - random_67 - lwr_k=500 - 6.56280758870122 - 7.176362413056105 - 6.019688987329472 - 9.14958964919228 - 21.332708599890097 - 10.047190463043446 - 0.9573964194035053'\n",
      "138 - random_25 - lwr_k=1000 - 7.482767411623137 - 7.90718008627588 - 6.62573496408806 - 9.926592326116245 - 18.298719460733448 - 10.047428184923465 - 0.957395411380087'\n",
      "139 - random_7 - lwr_k=500 - 7.486103550314779 - 8.328437159100844 - 6.152181646141242 - 9.885743879439914 - 18.39498160648355 - 10.048788006411804 - 0.9573896452642172'\n",
      "140 - random_7 - lwr_k=400 - 7.511416537535306 - 8.183547651974507 - 6.166324185389175 - 9.895766691738125 - 18.55276012954313 - 10.061237812137806 - 0.9573368537596062'\n",
      "141 - random_64 - lwr_k=1000 - 9.752676101870444 - 8.14345319785477 - 6.354947053269185 - 9.035409327966171 - 17.041188296164204 - 10.065168831729011 - 0.9573201848698706'\n",
      "142 - random_6 - lwr_k=200 - 8.633419212030528 - 6.129672109476137 - 6.809827302741744 - 10.882426213669229 - 17.956138484549015 - 10.081412187334745 - 0.9572513073948937'\n",
      "143 - random_16 - lwr_k=900 - 6.846687191633728 - 7.801609883236435 - 6.153079003788041 - 9.187408515588247 - 20.435813102101033 - 10.08401540597923 - 0.9572402688427991'\n",
      "144 - random_7 - lwr_k=800 - 7.517182150060823 - 8.573193347768274 - 6.135696693543543 - 9.827983439209216 - 18.396664544444103 - 10.089474326052812 - 0.9572171211238244'\n",
      "145 - random_7 - lwr_k=600 - 7.483541660962192 - 8.417215081442004 - 6.123370982507278 - 9.882670193304468 - 18.551693439185115 - 10.090997004114802 - 0.9572106644394631'\n",
      "146 - random_83 - lr - 7.434309816168181 - 8.592028438361753 - 7.120785906957851 - 10.138725016211731 - 17.181747088642695 - 10.092837953079805 - 0.9572028581758223'\n",
      "147 - random_7 - lwr_k=900 - 7.523147128027787 - 8.616629232330745 - 6.150252893307 - 9.824528653981684 - 18.354198859130648 - 10.093088552250881 - 0.9572017955482129'\n",
      "148 - random_1 - lwr_k=1000 - 8.710312518550957 - 9.347385675908647 - 5.570961341491166 - 9.8918758116352 - 16.986922851461713 - 10.101140356841604 - 0.9571676530974363'\n",
      "149 - random_91 - lr - 8.061621311706253 - 7.660249805503667 - 7.187195653296038 - 9.52297272717844 - 18.095827994249714 - 10.104838395082488 - 0.9571519721296251'\n",
      "150 - random_7 - lwr_k=700 - 7.495564786906891 - 8.498785112053707 - 6.148156482867966 - 9.851413225174317 - 18.542320949896414 - 10.106557077103817 - 0.9571446843203331'\n",
      "151 - random_67 - lwr_k=600 - 6.573755770538766 - 7.20633582433353 - 6.021162341726758 - 9.185799283832502 - 21.570231267990426 - 10.110401907967598 - 0.9571283809007664'\n",
      "152 - random_7 - lwr_k=300 - 7.67320111800857 - 8.078898301015082 - 6.3703338937298275 - 9.992632171503274 - 18.447852181371694 - 10.111851083863003 - 0.9571222358911455'\n",
      "153 - random_7 - lwr_k=1000 - 7.512219261156479 - 8.629019013937725 - 6.137920999730094 - 9.865107980577687 - 18.421468441362023 - 10.112478225646626 - 0.9571195765919491'\n",
      "154 - random_64 - lwr_k=900 - 9.489026969666464 - 8.140607129132633 - 6.391385933491567 - 9.13923237990057 - 17.407765221845814 - 10.113178183274607 - 0.9571166085282573'\n",
      "155 - random_29 - lwr_k=50 - 7.210862412860763 - 8.361501811545924 - 7.610989471673314 - 10.310153970476016 - 17.081011405519213 - 10.114141174263674 - 0.9571125251116677'\n",
      "156 - random_80 - lwr_k=400 - 10.085643201447603 - 6.8635256269000715 - 6.737818151885422 - 9.856460522387737 - 17.042866863175334 - 10.116724907395689 - 0.9571015691849203'\n",
      "157 - random_7 - lwr_k=200 - 7.805364786826882 - 7.933835204550811 - 6.550916838906471 - 10.074947012509377 - 18.235963698349 - 10.119468450585627 - 0.9570899356079674'\n",
      "158 - random_64 - lwr_k=800 - 9.578742259073952 - 8.151666547029288 - 6.408762681501123 - 9.072212621511072 - 17.48803243069622 - 10.139465859491752 - 0.9570051396418526'\n",
      "159 - random_91 - lwr_k=800 - 7.802784448904785 - 7.317351766459723 - 6.520594361158582 - 9.409327129162632 - 19.68344542832424 - 10.145853522652944 - 0.956978053729297'\n",
      "160 - random_95 - lwr_k=700 - 7.973753272472075 - 8.0127759871162 - 6.8711621063202974 - 8.802737760993681 - 19.089350119862285 - 10.149249547987406 - 0.9569636534012077'\n",
      "161 - random_67 - lwr_k=700 - 6.668742721815432 - 7.187987903053442 - 6.083958819514872 - 9.207151913680322 - 21.660154731857347 - 10.160540356381025 - 0.956915776448226'\n",
      "162 - random_16 - lwr_k=1000 - 6.941635009417598 - 7.900617010844014 - 6.171833077265536 - 9.247821067451339 - 20.5774988301613 - 10.166981455884383 - 0.9568884639470018'\n",
      "163 - random_98 - lwr_k=200 - 8.093285431893952 - 7.144517702872657 - 6.588700486960374 - 10.244850313152538 - 18.79625150663953 - 10.172684468188887 - 0.9568642811921062'\n",
      "164 - random_98 - lwr_k=100 - 8.014789215353746 - 7.969814141705773 - 6.07083549602513 - 10.446105278005 - 18.37270104765731 - 10.174134266678744 - 0.9568581335424683'\n",
      "165 - random_6 - lr - 8.108591561913835 - 6.1325527440181915 - 9.718458659531704 - 7.955769258617302 - 18.990763371876156 - 10.18022477598979 - 0.9568323076655467'\n",
      "166 - random_21 - lwr_k=50 - 8.525522707660764 - 8.178656074587545 - 8.208254679547375 - 9.765757910786979 - 16.25402675803675 - 10.185842887877607 - 0.9568084849179345'\n",
      "167 - random_64 - lwr_k=400 - 8.690900201644684 - 8.1782327944664 - 6.630755486940226 - 9.313552677739798 - 18.150487915217628 - 10.192210010611229 - 0.956781486103933'\n",
      "168 - random_71 - lr - 6.475126528835364 - 8.649896458394418 - 6.651466000866624 - 10.18611326567887 - 19.007934013239645 - 10.193245424105527 - 0.9567770955907425'\n",
      "169 - random_74 - lr - 9.381265982506797 - 8.52587543662752 - 6.1870736296041615 - 10.256308631985725 - 16.647095597875104 - 10.199115814735368 - 0.9567522030935562'\n",
      "170 - random_67 - lwr_k=800 - 6.721873393456466 - 7.074348801783996 - 6.086711732876268 - 9.225728347467655 - 21.89754432899052 - 10.200159568754087 - 0.9567477772136426'\n",
      "171 - random_99 - lr - 6.7847840038084515 - 7.636656252524416 - 6.059782131914478 - 11.597231844184524 - 18.951770022859186 - 10.205063903056608 - 0.9567269811311458'\n",
      "172 - random_98 - lwr_k=300 - 8.213026839262032 - 7.137861886262495 - 6.6593459738473895 - 10.307797870867303 - 18.75857087128652 - 10.214488896484108 - 0.9566870158822948'\n",
      "173 - random_95 - lwr_k=800 - 7.956170300490946 - 8.060687110562574 - 7.182644003022981 - 8.876872107383294 - 19.136672058556854 - 10.241877437467068 - 0.9565708789465533'\n",
      "174 - random_67 - lwr_k=900 - 6.708809605044074 - 7.120147930805097 - 6.120062031715835 - 9.229427603174564 - 22.040820558004913 - 10.242763198679036 - 0.9565671230110685'\n",
      "175 - random_80 - lwr_k=500 - 10.070039522768882 - 6.8448608347342965 - 6.851673028434228 - 9.803368888875873 - 17.662256961298674 - 10.245853965555954 - 0.956554017084966'\n",
      "176 - random_98 - lwr_k=700 - 8.505035070034763 - 7.228853761597309 - 6.028930250614628 - 10.34832264575576 - 19.15154218110671 - 10.251755516923845 - 0.956528992455417'\n",
      "177 - random_39 - lwr_k=1000 - 7.122199809739635 - 7.785651144936299 - 6.739626479918257 - 10.023784355370925 - 19.617732995633524 - 10.256880708067891 - 0.9565072598631286'\n",
      "178 - random_0 - lwr_k=200 - 7.241403071090999 - 7.535846190122231 - 6.786686664408165 - 11.864523692472554 - 17.87368162652396 - 10.259487753224118 - 0.9564962050852939'\n",
      "179 - random_78 - lwr_k=300 - 7.183158204969085 - 8.142225506629567 - 7.071971787858255 - 10.379977680335749 - 18.53111219525274 - 10.260837921506798 - 0.9564904799023721'\n",
      "180 - random_1 - lr - 8.209703462669896 - 9.179626929393768 - 5.890968832657984 - 10.512703375236951 - 17.536163027013938 - 10.265318578831756 - 0.9564714803575569'\n",
      "181 - random_67 - lwr_k=1000 - 6.7508476561470445 - 7.104841488504529 - 6.176367179261278 - 9.259910689422735 - 22.084716621941688 - 10.274240446670738 - 0.9564336485361191'\n",
      "182 - random_80 - lwr_k=600 - 10.06693748173554 - 6.84826197903038 - 6.9121200154482745 - 9.175582743754646 - 18.41104717008137 - 10.282192140938607 - 0.9563999305879161'\n",
      "183 - random_78 - lwr_k=400 - 7.104550264683881 - 8.184914589943054 - 6.989354953024347 - 10.471649975782336 - 18.686784884974426 - 10.28658546176205 - 0.9563813013802309'\n",
      "184 - random_98 - lwr_k=600 - 8.4810404772334 - 6.951733913908081 - 6.581162420191751 - 10.360729816046632 - 19.06924109124804 - 10.287939102478434 - 0.9563755614729831'\n",
      "185 - random_8 - lwr_k=200 - 7.896555963876245 - 7.8609934809570055 - 7.207844623557044 - 10.63753362551114 - 17.85987431608386 - 10.291769903985077 - 0.9563593175427683'\n",
      "186 - random_49 - lwr_k=100 - 8.576847358838998 - 7.819249600843254 - 6.226480283029271 - 11.974360694015083 - 16.865867163937637 - 10.291875081852092 - 0.9563588715520457'\n",
      "187 - random_95 - lwr_k=1000 - 8.052385666582158 - 8.480163390577408 - 6.946091972626949 - 8.7698983333599 - 19.21707869647954 - 10.292459832955698 - 0.9563563920040702'\n",
      "188 - random_14 - lwr_k=300 - 7.352072747832848 - 7.876409035718008 - 6.613285725900999 - 9.00296665617526 - 20.63971394226727 - 10.29601107278903 - 0.9563413335125434'\n",
      "189 - random_34 - lwr_k=300 - 8.448129288086715 - 7.591217871985221 - 5.9603416498180355 - 11.395755360606236 - 18.117147294627006 - 10.301770677267974 - 0.9563169107871531'\n",
      "190 - random_41 - lwr_k=200 - 8.877631703968973 - 8.014117735510686 - 6.333340853411453 - 9.219026302384831 - 19.087676972260578 - 10.305749417946876 - 0.9563000395434149'\n",
      "191 - random_98 - lwr_k=500 - 8.380682941249294 - 7.079165961397335 - 6.483864716743906 - 10.422938333646275 - 19.181618601877062 - 10.30880926747216 - 0.9562870647176321'\n",
      "192 - random_95 - lwr_k=900 - 8.025317457895998 - 8.237162325196984 - 7.462426585276171 - 8.679334937548113 - 19.166202798033158 - 10.313373952956699 - 0.9562677088642069'\n",
      "193 - random_41 - lwr_k=300 - 8.911852282629434 - 8.141441568763444 - 6.279059564205852 - 9.241100824671763 - 19.05555138308714 - 10.325211914192085 - 0.9562175117929516'\n",
      "194 - random_78 - lwr_k=600 - 7.090829538143163 - 8.241922328095422 - 6.925705575368777 - 10.12669950282926 - 19.24552843702255 - 10.325266023027329 - 0.9562172823526784'\n",
      "195 - random_41 - lwr_k=100 - 8.92294312259923 - 8.150188209753885 - 6.401854438448204 - 9.093269222689205 - 19.091112313547413 - 10.331285510598603 - 0.9561917576326251'\n",
      "196 - random_98 - lwr_k=400 - 8.333045679675108 - 7.316443487896839 - 6.504830396355702 - 10.381459068264972 - 19.13726006513051 - 10.333785776747849 - 0.956181155634903'\n",
      "197 - random_78 - lwr_k=500 - 7.133968155943468 - 8.11798627233133 - 6.915369082023521 - 10.213941119617559 - 19.330615308852188 - 10.34148638605929 - 0.956148502374208'\n",
      "198 - random_34 - lwr_k=400 - 8.314173960401913 - 7.578583232921677 - 6.02722875799801 - 11.57091674228536 - 18.265132349743663 - 10.35041944410094 - 0.9561106231024205'\n",
      "199 - random_25 - lr - 7.5106595662692985 - 8.484090197913362 - 6.749471781957879 - 9.790092349976925 - 19.24937885024402 - 10.355965874090643 - 0.9560871043109774'\n",
      "200 - random_29 - lwr_k=40 - 7.795506505664878 - 8.564768073526238 - 7.655747277844108 - 10.583368440780571 - 17.189734438808095 - 10.357111769771894 - 0.9560822453149037'\n",
      "201 - random_40 - lwr_k=600 - 7.952571971825682 - 6.941229280859369 - 6.462099931100603 - 11.032088328204296 - 19.426166558645228 - 10.361876267152159 - 0.9560620421895735'\n",
      "202 - random_39 - lwr_k=900 - 7.129552381798353 - 8.450843553870355 - 6.77849947536201 - 9.945283948270887 - 19.514743920649494 - 10.362941824260021 - 0.9560575238569817'\n",
      "203 - random_21 - lwr_k=100 - 7.9498419788357975 - 7.284782073350909 - 7.971343036906729 - 9.447116281930631 - 19.16666910421824 - 10.36305099006436 - 0.9560570609560184'\n",
      "204 - random_34 - lwr_k=500 - 8.335701761267066 - 7.551374282233331 - 5.783508839398306 - 11.798590899883507 - 18.380253950084928 - 10.369091334513556 - 0.9560314477955515'\n",
      "205 - random_98 - lwr_k=900 - 8.610022581183756 - 6.993167382248942 - 6.814909388488683 - 10.357621571986968 - 19.1062990714101 - 10.37556076702919 - 0.956004015152505'\n",
      "206 - random_75 - lwr_k=300 - 7.9548118761767315 - 7.613504117012281 - 6.5118495292734435 - 9.717823096109896 - 20.09466616297282 - 10.377681317219558 - 0.9559950232824617'\n",
      "207 - random_80 - lwr_k=100 - 10.604927409404961 - 7.221245555534578 - 6.748569254268402 - 9.532708716926361 - 17.9087688715758 - 10.402755945337969 - 0.9558886981417271'\n",
      "208 - random_75 - lwr_k=200 - 8.121188919121295 - 7.5469241725565706 - 6.653396943257218 - 9.864984870836778 - 19.856662225960452 - 10.40778827099015 - 0.95586735933141'\n",
      "209 - random_78 - lwr_k=700 - 7.106637394922796 - 8.353054298490642 - 7.028608380342703 - 10.25928691415744 - 19.352183449043125 - 10.419073095663592 - 0.9558195077514822'\n",
      "210 - random_78 - lwr_k=1000 - 7.045705929821018 - 8.38452140115649 - 6.998088633681482 - 10.130301543933628 - 19.54551105191096 - 10.419939610235113 - 0.9558158334284441'\n",
      "211 - random_33 - lwr_k=500 - 8.582854371941185 - 8.872296857340622 - 6.194041255667129 - 12.250947348601187 - 16.213443039053317 - 10.422161427653197 - 0.9558064121501485'\n",
      "212 - random_3 - lwr_k=200 - 8.097314123401715 - 7.215082009606246 - 7.199572329419855 - 9.888800187609363 - 19.71586651225391 - 10.422420816978947 - 0.9558053122492273'\n",
      "213 - random_98 - lwr_k=800 - 8.560250914940326 - 7.258634833057922 - 6.849880451060373 - 10.345761372201558 - 19.118992517387625 - 10.42587963222318 - 0.9557906456700908'\n",
      "214 - random_78 - lwr_k=900 - 7.071702725511032 - 8.41496447157794 - 7.005989926581285 - 10.15191237864765 - 19.492976387547518 - 10.426630129150611 - 0.9557874633021987'\n",
      "215 - random_33 - lwr_k=400 - 8.658413819329258 - 8.918762386725511 - 6.165275951676735 - 12.251549498813338 - 16.145638759679926 - 10.427391210809972 - 0.9557842360513633'\n",
      "216 - random_78 - lwr_k=800 - 7.129319968198439 - 8.435798276010035 - 6.986100911254971 - 10.13038663417874 - 19.491921103236617 - 10.433836819133653 - 0.9557569044311734'\n",
      "217 - random_33 - lwr_k=600 - 8.565657495267988 - 8.891596914923301 - 6.237337936196745 - 12.23234499962244 - 16.25471514730006 - 10.43577123771776 - 0.9557487018238516'\n",
      "218 - random_75 - lwr_k=500 - 8.000232383147626 - 7.59385162137378 - 6.472738029932182 - 10.037964743360794 - 20.080398656826887 - 10.43617250691035 - 0.9557470003029688'\n",
      "219 - random_95 - lwr_k=600 - 7.954544821582628 - 8.046643547571232 - 7.787224754930143 - 8.929253405450014 - 19.479006788895884 - 10.43853599315082 - 0.9557369782996131'\n",
      "220 - random_3 - lwr_k=300 - 8.202567500221553 - 7.27535333900535 - 7.165520661149272 - 9.436411441021928 - 20.12489229347329 - 10.440064164502573 - 0.9557304983217936'\n",
      "221 - random_33 - lwr_k=200 - 8.356435015695427 - 8.864405881607684 - 6.122143740258726 - 12.479147643905023 - 16.41965301787661 - 10.447755148419139 - 0.9556978858761193'\n",
      "222 - random_41 - lwr_k=400 - 8.968540925678676 - 8.237768756755399 - 6.297959829696033 - 9.256080437678749 - 19.490801000663332 - 10.449625285771898 - 0.9556899558435665'\n",
      "223 - random_91 - lwr_k=700 - 7.801536529593276 - 7.374590691592902 - 6.501443233344475 - 9.474772195150601 - 21.107154967544208 - 10.450961637113718 - 0.9556842892492777'\n",
      "224 - random_75 - lwr_k=400 - 7.984050761583198 - 7.60406921154099 - 6.4884221755590685 - 10.150465020266635 - 20.032879160059625 - 10.45110681639 - 0.9556836736386729'\n",
      "225 - random_34 - lwr_k=600 - 8.42328607788782 - 7.5700360432161515 - 5.833025336408417 - 11.959335449163017 - 18.476338538277187 - 10.45160005017328 - 0.9556815821559528'\n",
      "226 - random_98 - lr - 9.013581697263058 - 7.6590023103302265 - 6.197642250619275 - 10.82951080296731 - 18.58195334529005 - 10.455643780957486 - 0.9556644353315735'\n",
      "227 - random_38 - lwr_k=100 - 9.207734767366366 - 9.466796607720935 - 7.113962551823935 - 11.19004737949181 - 15.32872095002558 - 10.461084287944072 - 0.9556413656904891'\n",
      "228 - random_59 - lr - 6.739355920898477 - 6.730246828897732 - 5.977194233047188 - 8.94040980659319 - 23.93832503979609 - 10.463884716922038 - 0.9556294908980179'\n",
      "229 - random_75 - lwr_k=600 - 8.077866093662342 - 7.594631140064406 - 6.440794283071373 - 10.108618824493417 - 20.119870982358734 - 10.467494267792812 - 0.9556141851474197'\n",
      "230 - random_34 - lwr_k=700 - 8.364985294473712 - 7.60727324609217 - 5.847223289972797 - 12.01638653877161 - 18.508445191073577 - 10.468049634152491 - 0.9556118302009745'\n",
      "231 - random_33 - lwr_k=700 - 8.563710431927833 - 8.995780623535442 - 6.277040008158742 - 12.267289376827936 - 16.250976613518496 - 10.47040555001653 - 0.9556018402986469'\n",
      "232 - random_1 - lwr_k=600 - 8.289727116852209 - 8.968393010175879 - 5.701506462553245 - 9.758835382339191 - 19.666569277197137 - 10.476401060307792 - 0.9555764172505968'\n",
      "233 - random_75 - lwr_k=100 - 8.90176820791871 - 7.788524180318047 - 7.293051719858448 - 9.723904217134873 - 18.690193563541598 - 10.478789395542568 - 0.9555662898788663'\n",
      "234 - random_75 - lwr_k=700 - 8.166420696684723 - 7.641276652995129 - 6.381600124630647 - 10.141533542150755 - 20.082629841428354 - 10.481847618279497 - 0.9555533219512318'\n",
      "235 - random_33 - lwr_k=300 - 8.89979284980925 - 8.988731256645783 - 6.131784211353594 - 12.265845175085184 - 16.165048185525837 - 10.48973403855524 - 0.9555198807874486'\n",
      "236 - random_9 - lwr_k=100 - 7.487807306194717 - 9.35530441241687 - 7.995023023683745 - 9.435961003756303 - 18.19070008791022 - 10.492280796624534 - 0.9555090816478221'\n",
      "237 - random_95 - lwr_k=500 - 8.01480339267912 - 8.084761394189352 - 7.72463741809882 - 8.884573048181627 - 19.758131882063303 - 10.492581165364498 - 0.9555078079799376'\n",
      "238 - random_41 - lwr_k=500 - 9.01661948700442 - 8.333794081564001 - 6.272685462278364 - 9.284041310121479 - 19.6199924804503 - 10.504827180042678 - 0.9554558806202198'\n",
      "239 - random_33 - lwr_k=900 - 8.574000140058148 - 9.018893207639982 - 6.3971587009166075 - 12.3102293194629 - 16.22934540357221 - 10.505365511958763 - 0.9554535979057377'\n",
      "240 - random_33 - lwr_k=800 - 8.58692019914013 - 9.012926792885036 - 6.337589031831831 - 12.273849628988426 - 16.318747433480194 - 10.505447886915933 - 0.9554532486072806'\n",
      "241 - random_34 - lwr_k=900 - 8.38152034906533 - 7.718232802251504 - 5.810804782846625 - 11.977597666862488 - 18.67162814024651 - 10.51115043413214 - 0.955429067824521'\n",
      "242 - random_34 - lwr_k=800 - 8.373320551351831 - 7.62370964712479 - 5.838183200378672 - 12.085561685327077 - 18.65936319256007 - 10.515203187506303 - 0.9554118827412235'\n",
      "243 - random_75 - lwr_k=800 - 8.156293726118735 - 7.730841578860133 - 6.409398964280708 - 10.167275080244115 - 20.12899774242567 - 10.51771812584751 - 0.9554012185283065'\n",
      "244 - random_77 - lwr_k=200 - 7.4034647459079075 - 8.22585464721581 - 7.492084044702999 - 9.886703068410815 - 19.670631551620698 - 10.534856474216149 - 0.9553285459728593'\n",
      "245 - random_3 - lwr_k=400 - 8.34359019586917 - 7.316546971578291 - 7.227120996901826 - 9.457667224398584 - 20.345594885979665 - 10.537217192174518 - 0.9553185357079823'\n",
      "246 - random_87 - lr - 7.687267949931637 - 9.206218405050056 - 7.554942186286931 - 11.65283456543166 - 16.596337060217152 - 10.538834663527291 - 0.9553116770671121'\n",
      "247 - random_75 - lwr_k=900 - 8.188211594514389 - 7.7699594337155595 - 6.440918342209067 - 10.190550238711616 - 20.115830221131972 - 10.540254926118108 - 0.9553056546599548'\n",
      "248 - random_21 - lwr_k=40 - 8.593107785944177 - 9.179254989073902 - 8.330974450250084 - 9.924698060560132 - 16.685074920631504 - 10.542079568505393 - 0.9552979175418725'\n",
      "249 - random_33 - lwr_k=1000 - 8.572434595066094 - 9.110865545186948 - 6.459818751217719 - 12.32719375215639 - 16.243687768164655 - 10.542242967568043 - 0.9552972246730392'\n",
      "250 - random_34 - lwr_k=1000 - 8.424628379080788 - 7.732756611459592 - 5.810349575795202 - 12.014680657060078 - 18.765044226650687 - 10.548682720402347 - 0.95526991788216'\n",
      "251 - random_3 - lwr_k=100 - 8.059276025779422 - 7.7293456512592424 - 7.559447104375131 - 10.26808652664994 - 19.178977246765854 - 10.558153835127552 - 0.955229757062972'\n",
      "252 - random_75 - lwr_k=1000 - 8.198474257243408 - 7.8170518642188105 - 6.461518778665096 - 10.207663857596458 - 20.1202251254316 - 10.56015061363258 - 0.9552212900279053'\n",
      "253 - random_99 - deep - 8.878893956810284 - 7.774931539295153 - 6.359399620574776 - 10.877428736284461 - 18.93252708988049 - 10.563903451673058 - 0.9552053767851393'\n",
      "254 - random_98 - lwr_k=1000 - 8.676573648607826 - 7.289112462365584 - 6.5471376806358315 - 11.441657107520362 - 18.888644961885202 - 10.567778346554782 - 0.955188945788437'\n",
      "255 - random_92 - lr - 9.61011901891039 - 8.238155382690852 - 6.452535971748191 - 8.297099748906179 - 20.35085358146636 - 10.589207263246461 - 0.9550980797316296'\n",
      "256 - random_29 - lwr_k=100 - 6.833585886944236 - 8.02775921523469 - 6.8064150608865255 - 10.081524125172566 - 21.20334609699858 - 10.589491247007706 - 0.9550968755417515'\n",
      "257 - random_91 - lwr_k=1000 - 7.830707805865365 - 7.334366875272437 - 6.5420893254374395 - 9.40203816438563 - 21.881939040596208 - 10.597240624527974 - 0.9550640154868961'\n",
      "258 - random_77 - lwr_k=100 - 7.528321788079996 - 8.547966822358338 - 8.245151348476455 - 10.656328980912527 - 18.014429495632935 - 10.597601208172163 - 0.9550624864868833'\n",
      "259 - random_21 - lwr_k=200 - 8.050998975978922 - 7.3012213721212325 - 6.759181993658636 - 9.535682736159004 - 21.37931510959234 - 10.604320754756506 - 0.9550339932732301'\n",
      "260 - random_76 - lwr_k=500 - 7.763860394322729 - 9.380979976188193 - 6.8326030364846355 - 9.28279084232851 - 19.772669656171775 - 10.605914607660216 - 0.9550272347827952'\n",
      "261 - random_80 - lwr_k=700 - 10.146859194438514 - 6.879238034794641 - 6.959492408828038 - 9.084208819292268 - 20.0089355255766 - 10.61505817756457 - 0.9549884628675227'\n",
      "262 - random_1 - lwr_k=500 - 8.343828332193812 - 11.465196219049197 - 5.801923086468532 - 9.867982430910635 - 17.6015064406164 - 10.615854265978639 - 0.9549850871758768'\n",
      "263 - random_14 - lwr_k=400 - 7.425797160625629 - 8.070293111814204 - 6.643332173745289 - 9.39356874202048 - 21.56137263303746 - 10.617932588179174 - 0.9549762743671915'\n",
      "264 - random_76 - lwr_k=300 - 7.700858987565662 - 9.372375736937608 - 6.935281921423677 - 9.358251154486522 - 19.75428261489796 - 10.623526410974252 - 0.9549525546137769'\n",
      "265 - random_76 - lwr_k=400 - 7.829450285107944 - 9.366898567540701 - 6.915139134519666 - 9.213241024545917 - 19.820179747260365 - 10.628316676528675 - 0.9549322422224296'\n",
      "266 - random_3 - lwr_k=500 - 8.424322875278959 - 7.396304492028671 - 7.310139756675753 - 9.491491050998057 - 20.545353637752363 - 10.63263053065585 - 0.9549139499811642'\n",
      "267 - random_3 - lwr_k=600 - 8.31751062629655 - 7.421749203561818 - 7.362629041623379 - 9.436658899354788 - 20.643128075893113 - 10.635429092629236 - 0.9549020831054416'\n",
      "268 - random_39 - lwr_k=700 - 7.138204339315523 - 9.741549555068854 - 6.774479987021738 - 9.97370651076458 - 19.586544210131677 - 10.642175446750544 - 0.9548734762185116'\n",
      "269 - random_78 - lwr_k=200 - 7.4485064352466654 - 8.239278571666485 - 7.0785180359235085 - 11.188143858597924 - 19.280312353847123 - 10.646033868627129 - 0.9548571151683256'\n",
      "270 - random_76 - lwr_k=600 - 7.771855147317117 - 9.461530591359864 - 6.883412563061424 - 9.312562871420889 - 19.875422940556863 - 10.66028734052499 - 0.9547966754920784'\n",
      "271 - random_41 - lwr_k=600 - 9.026765132769354 - 8.39375682162585 - 6.289172699746151 - 9.343267904422774 - 20.277061962640186 - 10.665364411715222 - 0.9547751469451236'\n",
      "272 - random_3 - lwr_k=700 - 8.267989877119495 - 7.451065600728697 - 7.46716574736468 - 9.335056014720108 - 20.817974572202317 - 10.666930656311617 - 0.9547685055235101'\n",
      "273 - random_71 - lwr_k=700 - 6.65645889658158 - 7.603740669199427 - 5.958512046879642 - 14.84551802352915 - 18.3058544213811 - 10.672856203350227 - 0.9547433791439748'\n",
      "274 - random_77 - lwr_k=300 - 7.3158742694784795 - 8.267813646252232 - 7.6387628453754175 - 9.908437973305043 - 20.247561394351187 - 10.674745586234579 - 0.9547353675036773'\n",
      "275 - random_95 - lr - 8.832227635079528 - 8.15823857075421 - 6.59022949430433 - 9.507788078970965 - 20.293630319462356 - 10.67570849526142 - 0.9547312844346358'\n",
      "276 - random_39 - lr - 7.099489818046665 - 8.691369828801871 - 7.074162605640329 - 10.182785110788714 - 20.419010129178126 - 10.69244719468401 - 0.9546603065296629'\n",
      "277 - random_3 - lwr_k=800 - 8.304255896791886 - 7.475718041039639 - 7.523485654626964 - 9.357314970736867 - 20.891244846600838 - 10.709480215048139 - 0.9545880806015735'\n",
      "278 - random_80 - lwr_k=800 - 10.14446944294806 - 6.825936843195668 - 7.0602036235762435 - 9.111262182547032 - 20.429952998959642 - 10.713634983244454 - 0.9545704629399633'\n",
      "279 - random_22 - lwr_k=800 - 8.029694400275734 - 9.221454147642632 - 6.062190447645401 - 9.736683575338462 - 20.52706954708584 - 10.714734014182307 - 0.9545658026666947'\n",
      "280 - random_64 - lr - 11.469463768534501 - 8.368392595727174 - 6.650519189147448 - 9.681424121721586 - 17.434430024633357 - 10.720583317467854 - 0.9545409995871795'\n",
      "281 - random_22 - lwr_k=700 - 8.017473401581823 - 9.255561044629852 - 6.070363027535842 - 9.851585314067913 - 20.440864897349687 - 10.726484862949535 - 0.95451597498259'\n",
      "282 - random_22 - lwr_k=900 - 8.059375097513731 - 9.189005395593679 - 6.098686804303464 - 9.634496433378336 - 20.65960555070749 - 10.727544796659664 - 0.9545114804951598'\n",
      "283 - random_71 - deep - 8.978512915378701 - 9.810193436430636 - 7.536236177986513 - 9.99243597140769 - 17.324588967775536 - 10.727956589197204 - 0.9545097344483622'\n",
      "284 - random_71 - lwr_k=1000 - 6.5573753771946555 - 7.6399563564924104 - 5.991174002786566 - 14.9838481433984 - 18.50740910560232 - 10.734761411003417 - 0.9544808795413947'\n",
      "285 - random_76 - lwr_k=700 - 7.872818527039941 - 9.512959425033975 - 6.891359105523771 - 9.307416414263743 - 20.093455835020325 - 10.734932887122106 - 0.9544801524230356'\n",
      "286 - random_91 - lwr_k=400 - 7.77113910494278 - 11.635121470901336 - 6.507675360399438 - 9.56409494729704 - 18.202259370132204 - 10.735719774080618 - 0.9544768157487603'\n",
      "287 - random_3 - lwr_k=900 - 8.316174239052863 - 7.481867589004259 - 7.591528729525051 - 9.346597836785325 - 20.965829854873057 - 10.73946911808593 - 0.9544609172266708'\n",
      "288 - random_22 - lwr_k=600 - 7.968438374534668 - 9.28546982423373 - 6.045764170567915 - 9.988883321855695 - 20.42384520910855 - 10.741787359991116 - 0.9544510870750266'\n",
      "289 - random_22 - lwr_k=1000 - 8.09093274622437 - 9.221758270899453 - 6.0594544057680695 - 9.601143256890046 - 20.73925429817056 - 10.741825391697578 - 0.954450925807369'\n",
      "290 - random_80 - lwr_k=900 - 10.17529594104852 - 6.798424395041588 - 7.134824125643328 - 9.143615760779317 - 20.46618753607628 - 10.742930462350303 - 0.9544462399236096'\n",
      "291 - random_22 - lwr_k=500 - 7.935678727327869 - 9.118597697798526 - 6.095074114998735 - 10.502651157945419 - 20.100750990744668 - 10.749822385699769 - 0.9544170157725433'\n",
      "292 - random_73 - lwr_k=700 - 7.492380837441159 - 9.627969011966018 - 7.104339599171302 - 12.828926641208806 - 16.723982727444298 - 10.75480080328953 - 0.9543959055511527'\n",
      "293 - random_3 - lwr_k=1000 - 8.343382430027635 - 7.470465577615681 - 7.609291270018299 - 9.362209728746942 - 21.004012830474878 - 10.756938701628021 - 0.95438684011893'\n",
      "294 - random_71 - lwr_k=800 - 6.587019568923713 - 7.673359287885919 - 5.938063964396184 - 15.153221820936539 - 18.439553759117153 - 10.757055517732875 - 0.9543863447780329'\n",
      "295 - random_76 - lwr_k=900 - 7.7967720219754115 - 9.454981584219796 - 6.9629767636935815 - 9.409981977321952 - 20.165971613498666 - 10.757438491834352 - 0.9543847208346967'\n",
      "296 - random_76 - lwr_k=800 - 7.892675960027763 - 9.505747750364135 - 6.901426511739621 - 9.3382402286373 - 20.152557165160577 - 10.757455241978088 - 0.954384649808264'\n",
      "297 - random_21 - lwr_k=300 - 8.11117601653867 - 7.485752031752813 - 6.605713508817459 - 9.561384059042508 - 22.030608527154534 - 10.757957297695253 - 0.9543825209174771'\n",
      "298 - random_44 - lwr_k=700 - 8.763808599393974 - 8.415887451974902 - 5.870057592328432 - 9.076116725590754 - 21.67769336752997 - 10.760001804274795 - 0.9543738514987812'\n",
      "299 - random_21 - lwr_k=400 - 8.186959097799928 - 7.39250434901259 - 6.629448840946243 - 9.544683589777406 - 22.083667809784668 - 10.7664775545547 - 0.9543463920662177'\n",
      "300 - random_41 - lwr_k=700 - 9.084529690684022 - 8.493403962593009 - 6.318059225561962 - 9.423459213909668 - 20.533956906012467 - 10.770032801889736 - 0.9543313165814903'\n",
      "301 - random_1 - deep - 8.820022142070014 - 9.606977017928653 - 6.009644924666821 - 11.45034731278939 - 17.968321699475187 - 10.770552584248263 - 0.9543291126206918'\n",
      "302 - random_80 - lwr_k=1000 - 10.199859917252956 - 6.774176848177433 - 7.195254160264724 - 9.125952000852015 - 20.62550783987057 - 10.783397858614942 - 0.954274644094444'\n",
      "303 - random_29 - lwr_k=30 - 8.232187594395118 - 8.728247206670432 - 8.357686193703316 - 11.40344766501551 - 17.205434065089925 - 10.784645613537348 - 0.9542693531797751'\n",
      "304 - random_76 - lwr_k=1000 - 7.799385427831433 - 9.462005894635876 - 6.964728748934884 - 9.3044117781463 - 20.403543114738415 - 10.786108878781201 - 0.9542631484264185'\n",
      "305 - random_44 - lwr_k=800 - 8.788810062216038 - 8.437454049827958 - 5.851691094782758 - 9.085814376160759 - 21.77913771742162 - 10.787869015541444 - 0.9542556848346212'\n",
      "306 - random_1 - lwr_k=400 - 8.547524956838595 - 11.28264977012999 - 5.920273980413629 - 9.833561588665955 - 18.402422454228294 - 10.796997635961114 - 0.9542169763103612'\n",
      "307 - random_44 - lwr_k=600 - 8.780159417246375 - 8.385982064788003 - 6.16150911770697 - 9.192279673227896 - 21.53614610890116 - 10.810485574544915 - 0.9541597827615129'\n",
      "308 - random_71 - lwr_k=900 - 6.545703232898299 - 7.636139117471859 - 5.938601491304054 - 15.438623640529617 - 18.54336521249673 - 10.819265132238375 - 0.9541225543845681'\n",
      "309 - random_77 - lwr_k=400 - 7.392892566378124 - 8.38474859370425 - 7.678006471879942 - 9.971557754820797 - 20.678677325451854 - 10.820216216276238 - 0.9541185214575911'\n",
      "310 - random_73 - lwr_k=800 - 7.548594533682236 - 9.736175704076281 - 7.055393880304939 - 12.986369780396448 - 16.797206197145297 - 10.824033310458772 - 0.954102335651196'\n",
      "311 - random_44 - lwr_k=900 - 8.865859820393748 - 8.486056393583937 - 5.8495412470224855 - 9.128671627119166 - 21.845467492997592 - 10.834412205953456 - 0.9540583255259438'\n",
      "312 - random_0 - lr - 8.439567910052006 - 8.742310107405759 - 7.619149810425681 - 9.439214035051025 - 19.939988198597035 - 10.835310755386837 - 0.9540545153639541'\n",
      "313 - random_44 - lwr_k=1000 - 8.896362074980313 - 8.486267443290455 - 5.850647178357841 - 9.023647558166031 - 21.974085563648178 - 10.845496253125424 - 0.954011325312425'\n",
      "314 - random_59 - deep - 6.5454984639943525 - 6.758544790764292 - 6.103265019540998 - 9.500744471093068 - 25.332817496284903 - 10.84680016087757 - 0.954005796388034'\n",
      "315 - random_38 - lwr_k=500 - 9.384438612672433 - 8.71345914958162 - 6.093807314307334 - 10.843705967119067 - 19.214683672916763 - 10.849429105408133 - 0.9539946486514405'\n",
      "316 - random_95 - lwr_k=400 - 8.094004274697205 - 8.064863635235584 - 8.301118640777556 - 10.368080345642854 - 19.426175729637087 - 10.84994090621321 - 0.9539924784380929'\n",
      "317 - random_38 - lwr_k=400 - 9.30596790716876 - 8.6957415174997 - 6.193472610070988 - 10.748741702848738 - 19.32326508291428 - 10.852831056043588 - 0.9539802231980148'\n",
      "318 - random_63 - lr - 6.896410591257935 - 7.808761605645141 - 6.5526760477406985 - 9.566846783072082 - 23.47665699160137 - 10.859121661664194 - 0.9539535488431746'\n",
      "319 - random_22 - lwr_k=400 - 7.891213519214792 - 9.559381279620338 - 6.0976274723806565 - 10.76800873477203 - 19.98709046496979 - 10.859964976393984 - 0.9539499728955313'\n",
      "320 - random_38 - lwr_k=600 - 9.431180893986529 - 8.79921123680805 - 6.019910373953175 - 10.951401997923671 - 19.10283695431287 - 10.860336582791652 - 0.9539483971552115'\n",
      "321 - random_41 - lwr_k=800 - 9.154348045852382 - 8.671666545725847 - 6.35701789061447 - 9.462315876168036 - 20.661735319196712 - 10.860778644979407 - 0.9539465226578469'\n",
      "322 - random_33 - lr - 9.005482840683307 - 9.65446961251157 - 6.640676556850703 - 12.298174382041626 - 16.728249427643377 - 10.864907719886071 - 0.9539290139447173'\n",
      "323 - random_21 - lwr_k=500 - 8.266125695864215 - 7.486119742454094 - 6.652480402751741 - 9.602422274137425 - 22.335379050301917 - 10.867525448587733 - 0.9539179138649357'\n",
      "324 - random_38 - lwr_k=700 - 9.498201949992003 - 8.895286997984028 - 6.079489441224303 - 10.978347523259908 - 18.891246427461024 - 10.867966974945446 - 0.9539160416396768'\n",
      "325 - random_77 - lwr_k=500 - 7.439127778168856 - 8.447707641929203 - 7.69628890061133 - 10.118573401329854 - 20.669960553236137 - 10.873371801134981 - 0.9538931232975768'\n",
      "326 - random_69 - lwr_k=100 - 7.024248940114332 - 9.32983627746049 - 6.804041234748149 - 10.54397623942241 - 20.673204804120218 - 10.874177916762644 - 0.9538897050870592'\n",
      "327 - random_9 - lwr_k=200 - 7.7276923946850635 - 9.27473942291228 - 8.030026674061563 - 9.943984647538556 - 19.407280017319025 - 10.87596666217852 - 0.9538821201846158'\n",
      "328 - random_29 - lwr_k=200 - 6.900083040860686 - 7.788275042298008 - 6.502867604625713 - 9.991643142328922 - 23.222216267694737 - 10.879858729737483 - 0.9538656164650392'\n",
      "329 - random_49 - lr - 8.183327537341002 - 9.43346892011095 - 7.488419689461123 - 9.130638957217935 - 20.19622048328607 - 10.885734581413738 - 0.9538407008111175'\n",
      "330 - random_95 - lwr_k=300 - 8.196965100150683 - 8.073574129870652 - 8.577351905064933 - 10.829399217065511 - 18.764512180257313 - 10.887458888374246 - 0.9538333891501277'\n",
      "331 - random_7 - lwr_k=100 - 8.198685643721088 - 8.864672017695106 - 7.195620544115914 - 10.859767732943828 - 19.407561164237052 - 10.904484088762727 - 0.9537611963814537'\n",
      "332 - random_9 - lwr_k=300 - 7.776116992482882 - 9.29894771588829 - 8.138847438006987 - 9.806633305907052 - 19.51565517083565 - 10.906464061820689 - 0.9537528006073253'\n",
      "333 - random_83 - deep - 9.132638854793168 - 9.88492751940963 - 7.650007874334961 - 10.757322543473833 - 17.116600231886107 - 10.907841219023979 - 0.9537469610803455'\n",
      "334 - random_22 - lwr_k=300 - 7.839031796367671 - 9.473576280582536 - 6.110938419954213 - 11.401309111155017 - 19.734641930078777 - 10.911160815808696 - 0.95373288473753'\n",
      "335 - random_73 - lwr_k=900 - 7.533554685277896 - 9.762839679794492 - 7.111272688230418 - 13.11186674039392 - 17.083923913727403 - 10.919947315397074 - 0.9536956269245372'\n",
      "336 - random_90 - lwr_k=500 - 7.401092694117711 - 11.196809450692882 - 6.3213021619283065 - 13.03587243810891 - 16.657705981499053 - 10.92202482577605 - 0.9536868175582576'\n",
      "337 - random_93 - lwr_k=400 - 8.11752004256008 - 7.688904653592741 - 5.554120931298491 - 9.538086282454483 - 23.74924015177831 - 10.928583299821916 - 0.9536590073481664'\n",
      "338 - random_44 - lwr_k=400 - 8.583859400431157 - 8.397105945177794 - 7.072355318159357 - 9.289659718532967 - 21.31249282033337 - 10.930295357092273 - 0.9536517476301217'\n",
      "339 - random_38 - lwr_k=200 - 9.065059163475883 - 8.428540913525959 - 6.324483149679145 - 10.48372497093364 - 20.366810456832955 - 10.933007528599559 - 0.9536402470800099'\n",
      "340 - random_38 - lwr_k=800 - 9.526602051267309 - 8.99640185627422 - 6.097849682739037 - 11.053660817359178 - 18.999805302665752 - 10.934315927655243 - 0.9536346990131324'\n",
      "341 - random_39 - deep - 6.86467381507012 - 8.577284154252023 - 7.003635017526237 - 10.458490877635747 - 21.782771595572957 - 10.936317691033313 - 0.9536262109390761'\n",
      "342 - random_90 - lwr_k=600 - 7.451754689562292 - 11.252415152162323 - 6.325064713416354 - 12.92770362886046 - 16.73163218270468 - 10.937194790828539 - 0.9536224916324043'\n",
      "343 - random_90 - lwr_k=400 - 7.3585843662098664 - 11.183596739924853 - 6.42033409177239 - 13.026012804685577 - 16.706849501339537 - 10.938529247323952 - 0.953616833072923'\n",
      "344 - random_14 - lwr_k=500 - 7.56937999417706 - 8.393653750747358 - 6.639217735787223 - 9.70633335890956 - 22.395756955294374 - 10.939899191966079 - 0.9536110240313618'\n",
      "345 - random_96 - lwr_k=1000 - 9.679495089085142 - 7.955818728532173 - 6.32690727098064 - 9.512623216413624 - 21.247808840442204 - 10.943834092748762 - 0.953594338684025'\n",
      "346 - random_34 - lr - 8.615930919053488 - 7.884474901850729 - 6.444253668411688 - 12.621350937196224 - 19.15932682163464 - 10.944184900246947 - 0.95359285113827'\n",
      "347 - random_21 - lwr_k=600 - 8.333547368547125 - 7.546788550486175 - 6.692443217275986 - 9.650489279398954 - 22.514437974145775 - 10.946556384138 - 0.9535827952220955'\n",
      "348 - random_41 - lwr_k=40 - 9.682345100287447 - 7.771881419115851 - 8.172292762148965 - 9.43397327755365 - 19.691864954110393 - 10.949743368291553 - 0.9535692813012889'\n",
      "349 - random_9 - lwr_k=400 - 7.866934653736529 - 9.235255062227376 - 8.195467801366577 - 9.988574442689963 - 19.503766147876963 - 10.9572113770196 - 0.9535376143479342'\n",
      "350 - random_21 - lwr_k=30 - 9.198389736695948 - 9.843077578288412 - 9.338540551379582 - 9.862099384807674 - 16.548742287935056 - 10.957699157452797 - 0.9535355459893143'\n",
      "351 - random_9 - lwr_k=500 - 7.938989664475316 - 9.085156953255876 - 8.304461792630715 - 10.027388021275607 - 19.468532401018077 - 10.964102480971052 - 0.9535083936714003'\n",
      "352 - random_4 - lwr_k=200 - 8.534816041900175 - 8.759326360041413 - 7.571582832947411 - 10.576363206506455 - 19.38555832220756 - 10.964770073810582 - 0.9535055628456636'\n",
      "353 - random_57 - lr - 10.624893221916913 - 7.559057548647684 - 8.520096528386858 - 10.022188067357094 - 18.121199573305855 - 10.968872115939048 - 0.9534881687609119'\n",
      "354 - random_73 - lwr_k=1000 - 7.509586409935858 - 9.879899953288094 - 7.091437512606185 - 13.226083240033605 - 17.148480046840437 - 10.970351942365324 - 0.953481893782384'\n",
      "355 - random_90 - lwr_k=300 - 7.384404035698312 - 11.165843737818504 - 6.473038008264846 - 13.08120372484449 - 16.77300822767109 - 10.974942685664065 - 0.9534624274347673'\n",
      "356 - random_86 - lwr_k=40 - 8.622850323658222 - 10.739139031038267 - 7.141116205402131 - 11.030419453357233 - 17.34445047104084 - 10.975171124271354 - 0.9534614587756528'\n",
      "357 - random_38 - lwr_k=900 - 9.51800167198213 - 9.112856536857706 - 6.119475176129996 - 11.072116917195512 - 19.069667074420295 - 10.977878856233815 - 0.9534499770507546'\n",
      "358 - random_90 - lwr_k=700 - 7.4075774497393025 - 11.43421187341438 - 6.341820320026054 - 12.976251438472259 - 16.733045015938384 - 10.978071088010646 - 0.953449161921189'\n",
      "359 - random_90 - lwr_k=800 - 7.432634329775796 - 11.409455922927043 - 6.382767404535059 - 12.977614536331867 - 16.751830730090983 - 10.990346481094234 - 0.9533971099868185'\n",
      "360 - random_77 - lwr_k=600 - 7.526855660711766 - 8.485728507264044 - 7.720779739158929 - 10.210841850318648 - 21.02760172921021 - 10.993382925394503 - 0.953384234407327'\n",
      "361 - random_38 - lwr_k=300 - 9.13579119156108 - 8.530831350776577 - 6.237903380814689 - 10.65769225138816 - 20.426341705392403 - 10.99700314981831 - 0.9533688833971531'\n",
      "362 - random_60 - lwr_k=800 - 7.828845646610252 - 7.167998709011342 - 6.628692069865213 - 9.851256087665416 - 23.51907081900763 - 10.998026195009464 - 0.953364545329868'\n",
      "363 - random_21 - lwr_k=700 - 8.361812279453263 - 7.599314075098679 - 6.710761333846927 - 9.627270194261031 - 22.714011739966512 - 11.001644217386836 - 0.9533492036571366'\n",
      "364 - random_41 - lwr_k=50 - 9.705737725869295 - 7.846960253093499 - 7.796125534804382 - 9.554130370081603 - 20.12878926647052 - 11.005618320700641 - 0.9533323520774386'\n",
      "365 - random_55 - lwr_k=500 - 9.091009757942542 - 8.20725289252276 - 5.826578637966053 - 12.33433232399714 - 19.57841728096486 - 11.006745823396718 - 0.9533275710740167'\n",
      "366 - random_9 - lwr_k=600 - 7.9661595426036484 - 9.18401796803528 - 8.36647212942359 - 10.020587641859445 - 19.50691497127087 - 11.00803341717031 - 0.9533221112287683'\n",
      "367 - random_38 - lwr_k=1000 - 9.585681243241586 - 9.152789192384896 - 6.1012093447219815 - 11.11711935354422 - 19.086812093043466 - 11.008185324795262 - 0.9533214670876259'\n",
      "368 - random_41 - lwr_k=900 - 9.248307130502772 - 8.738643780255773 - 6.380949056196552 - 9.534433277513466 - 21.218798487279116 - 11.0235612895652 - 0.9532562676513522'\n",
      "369 - random_9 - lwr_k=700 - 8.012203972553957 - 9.213141981808624 - 8.449973883910305 - 9.92085418932027 - 19.53961613332145 - 11.026367305130359 - 0.953244369169809'\n",
      "370 - random_90 - lwr_k=900 - 7.491170726800883 - 11.58427418127225 - 6.391408889858489 - 12.933911177429307 - 16.76172014838606 - 11.032007496479963 - 0.9532204527976026'\n",
      "371 - random_90 - lwr_k=1000 - 7.519797744100279 - 11.586791241030463 - 6.378563612295217 - 12.912713981539211 - 16.771095256494448 - 11.033307514341137 - 0.9532149402699035'\n",
      "372 - random_29 - lwr_k=300 - 6.852495039298242 - 7.646352430236344 - 6.419592534619386 - 10.029310864155166 - 24.27352179149694 - 11.043011751284222 - 0.9531737909314638'\n",
      "373 - random_21 - lwr_k=800 - 8.394893616099225 - 7.627936959844363 - 6.744928432473739 - 9.65886997749377 - 22.809611378114298 - 11.046253858695096 - 0.9531600432688819'\n",
      "374 - random_12 - lwr_k=1000 - 7.546502097176145 - 9.630266054157627 - 7.196289915654472 - 10.538524378923556 - 20.35020747950013 - 11.051551050774407 - 0.9531375813328297'\n",
      "375 - random_86 - lwr_k=30 - 8.602872160110431 - 10.63259111396591 - 7.376884329462787 - 11.09130302576729 - 17.58270307749897 - 11.056799302322492 - 0.9531153269216446'\n",
      "376 - random_12 - lwr_k=700 - 7.468428629498544 - 9.512758342582012 - 7.18688800428199 - 10.701814821408567 - 20.424293251873486 - 11.057995528178832 - 0.9531102545081306'\n",
      "377 - random_87 - deep - 9.591051292887686 - 9.928160022791786 - 7.964778551793704 - 12.002320319877894 - 15.844098512116853 - 11.065653805796035 - 0.9530777808483076'\n",
      "378 - random_9 - lwr_k=800 - 8.06024512143192 - 9.235551328091207 - 8.546742422339786 - 9.931318012573003 - 19.56542614260596 - 11.067064085879752 - 0.9530718006706796'\n",
      "379 - random_90 - lwr_k=200 - 7.566872549807339 - 11.25460422587211 - 6.6774831335999005 - 13.035264755516463 - 16.804564741375835 - 11.067215218881818 - 0.9530711598142075'\n",
      "380 - random_55 - lwr_k=600 - 9.174489659309488 - 8.195171711874899 - 5.666550744754291 - 12.66481398753117 - 19.651766747779963 - 11.069777260970561 - 0.9530602958650248'\n",
      "381 - random_99 - lwr_k=200 - 6.739299365244117 - 6.9713881733250105 - 6.172666181146451 - 10.332914056392145 - 25.153340335739525 - 11.072540067480606 - 0.9530485806048975'\n",
      "382 - random_44 - lwr_k=500 - 8.68570954040784 - 8.377584856829493 - 7.555626161245398 - 9.196322416783568 - 21.556596527863316 - 11.073535177291387 - 0.9530443609933372'\n",
      "383 - random_12 - lwr_k=900 - 7.526412536717196 - 9.59197442001943 - 7.240272649820702 - 10.605054801612976 - 20.428000707245882 - 11.077518019138676 - 0.9530274723592193'\n",
      "384 - random_77 - lwr_k=700 - 7.5701786464373 - 8.57363781060755 - 7.694287336556398 - 10.288032784694884 - 21.271192842479902 - 11.078480929953264 - 0.9530233892825972'\n",
      "385 - random_9 - lwr_k=40 - 8.367257800606906 - 9.876391083122634 - 8.447647218474263 - 9.849941567691724 - 18.865883856846633 - 11.080782550030442 - 0.9530136296132826'\n",
      "386 - random_12 - lwr_k=800 - 7.482874320162869 - 9.553794548903312 - 7.222169898932829 - 10.686123995542223 - 20.46584660790141 - 11.081322238644267 - 0.9530113411459304'\n",
      "387 - random_39 - lwr_k=800 - 7.1068869028855355 - 11.958241718307722 - 6.768157920858036 - 10.027010848779401 - 19.57484553027247 - 11.086519507988891 - 0.9529893029170126'\n",
      "388 - random_12 - lwr_k=500 - 7.454522207583476 - 9.381982453057459 - 7.292326829906325 - 10.62210362876521 - 20.69026322211796 - 11.087365266013949 - 0.9529857166089478'\n",
      "389 - random_86 - lwr_k=50 - 8.477914385215822 - 10.694711626654632 - 7.025950675022676 - 11.020082782185279 - 18.24273485988187 - 11.09178567245331 - 0.9529669725488346'\n",
      "390 - random_41 - lwr_k=1000 - 9.312691107916153 - 8.7669279232023 - 6.388852195784546 - 9.512611591098178 - 21.508126916802247 - 11.097167955661716 - 0.952944149796812'\n",
      "391 - random_9 - lwr_k=900 - 8.080121603058652 - 9.238958525795015 - 8.647456603170095 - 9.966248746432495 - 19.571317439130034 - 11.100021081136683 - 0.9529320515528729'\n",
      "392 - random_21 - lwr_k=900 - 8.422487245947696 - 7.702282052448346 - 6.756669150903754 - 9.665321075859312 - 22.976565002007334 - 11.10366857982129 - 0.9529165848903509'\n",
      "393 - random_12 - lwr_k=600 - 7.5070721565786105 - 9.45758310189554 - 7.304346498144708 - 10.655587895121217 - 20.602089420191955 - 11.10447679745988 - 0.9529131577665761'\n",
      "394 - random_77 - lwr_k=800 - 7.610966610041902 - 8.587785994476137 - 7.709622977488656 - 10.300385617674273 - 21.433399485016665 - 11.127440142233556 - 0.9528157852012378'\n",
      "395 - random_9 - lwr_k=1000 - 8.144505758866696 - 9.212597866720333 - 8.6865658692251 - 10.008739211408699 - 19.59711811109127 - 11.1291025622341 - 0.9528087359624741'\n",
      "396 - random_55 - lwr_k=700 - 9.276709154140685 - 8.113532851163393 - 5.643687564609286 - 12.7655202979348 - 19.86014894693792 - 11.131121728321451 - 0.952800173996275'\n",
      "397 - random_4 - lwr_k=300 - 8.246652237746098 - 8.531168282875816 - 7.343923934785035 - 10.608101913837354 - 21.011519084644906 - 11.147369418653897 - 0.9527312781405498'\n",
      "398 - random_64 - lwr_k=300 - 8.733640488790346 - 8.769904343940896 - 9.760554444088859 - 9.37354705915913 - 19.107078267050337 - 11.148159863293243 - 0.9527279263804709'\n",
      "399 - random_21 - lwr_k=1000 - 8.472154382742255 - 7.789234470620919 - 6.758679220155213 - 9.689586526754923 - 23.096770433662588 - 11.160292509462764 - 0.9526764797426438'\n",
      "400 - random_71 - lwr_k=600 - 6.807020361991884 - 7.6353931712877365 - 5.902718273189085 - 17.049303549364055 - 18.44031258784955 - 11.165657385421545 - 0.9526537308034114'\n",
      "401 - random_55 - lwr_k=400 - 9.359731186700037 - 8.748079792931925 - 6.1028482328582685 - 12.362545698429843 - 19.2829831249431 - 11.170544195639197 - 0.952633009029127'\n",
      "402 - random_77 - lwr_k=900 - 7.679828879175896 - 8.645766113365257 - 7.717443548308012 - 10.339611948656374 - 21.507460459407575 - 11.177034724679132 - 0.9526054869294829'\n",
      "403 - random_86 - lwr_k=100 - 8.587193407862243 - 10.301549670120666 - 6.907394702442051 - 11.21736881455451 - 18.901231949601552 - 11.1823783373197 - 0.9525828281541122'\n",
      "404 - random_29 - lwr_k=400 - 6.971073005908893 - 7.611382266913555 - 6.581246854484485 - 10.005675084997536 - 24.80498234256379 - 11.193593494640078 - 0.9525352719879822'\n",
      "405 - random_55 - lwr_k=800 - 9.294094453735186 - 8.16813749888486 - 5.627310106605671 - 12.891885171495293 - 20.03460650030472 - 11.202397153681847 - 0.9524979414129413'\n",
      "406 - random_14 - lwr_k=600 - 7.78411724875963 - 8.541862749405412 - 6.733838387659462 - 10.004743377302592 - 23.013849046192703 - 11.214682426425028 - 0.9524458475853793'\n",
      "407 - random_93 - lwr_k=500 - 8.142839802570562 - 7.664799788225616 - 5.547060160024548 - 10.142201396580097 - 24.58625199769158 - 11.215545706525074 - 0.9524421869776248'\n",
      "408 - random_7 - lr - 7.969841373303209 - 10.52369176577926 - 7.085781030243055 - 11.161491249682738 - 19.373272690479553 - 11.22216847958469 - 0.952414104117361'\n",
      "409 - random_55 - lwr_k=900 - 9.27389826062482 - 8.150920429211629 - 5.651157446762724 - 12.930759962080296 - 20.109747005346176 - 11.222474322696112 - 0.9524128072362383'\n",
      "410 - random_77 - lwr_k=1000 - 7.715155764182996 - 8.699846871564436 - 7.754768777272474 - 10.41150441384489 - 21.631481495329734 - 11.24155750663751 - 0.9523318879018159'\n",
      "411 - random_55 - lwr_k=1000 - 9.265394813730964 - 8.148557019666317 - 5.639707130356669 - 12.997315105669172 - 20.209074124804815 - 11.25117615801305 - 0.9522911014759375'\n",
      "412 - random_86 - lwr_k=20 - 8.99499336937966 - 10.663966856976597 - 7.520554729277979 - 11.863465003190026 - 17.235457556806253 - 11.255220430131196 - 0.952273952356114'\n",
      "413 - random_29 - lwr_k=500 - 6.9655044346987305 - 7.6161484694934005 - 6.592281524139169 - 10.113195563041245 - 25.017633495636463 - 11.259652508685475 - 0.9522551588021756'\n",
      "414 - random_31 - lwr_k=200 - 8.004334341324187 - 6.8101127080053425 - 6.884352265629204 - 17.861028541352407 - 16.96423658580976 - 11.30353645503176 - 0.9520690756128568'\n",
      "415 - random_79 - lwr_k=200 - 9.381788867490513 - 6.950211466111751 - 8.391642586047581 - 10.762183403487379 - 21.062086490383614 - 11.308553061346414 - 0.9520478034579959'\n",
      "416 - random_86 - lwr_k=200 - 8.49754964323842 - 10.199581062644723 - 7.091569205890322 - 11.313579891178433 - 19.469828479922267 - 11.313777852316235 - 0.952025648527819'\n",
      "417 - random_12 - lwr_k=400 - 7.445181322705789 - 9.274536647071045 - 7.326989672992623 - 10.495921962916283 - 22.081180365783087 - 11.32379100919162 - 0.9519831892614664'\n",
      "418 - random_7 - deep - 8.345489717349288 - 10.644410606296871 - 7.061051931857672 - 11.681684226310225 - 18.90712838090901 - 11.327352664624346 - 0.9519680867063077'\n",
      "419 - random_8 - lr - 8.384155722768275 - 8.765670029293446 - 8.58018195865655 - 9.282113180516289 - 21.646076725341462 - 11.330736714522903 - 0.9519537370560992'\n",
      "420 - random_67 - lr - 6.950058852070278 - 8.12199462458967 - 6.965599968956311 - 10.093869959517589 - 24.531049324965927 - 11.331271222683556 - 0.9519514705556695'\n",
      "421 - random_4 - lwr_k=400 - 8.233187260134518 - 8.808627495133631 - 7.39966338819417 - 10.436088428851004 - 21.8030539681996 - 11.335202144215089 - 0.9519348020817411'\n",
      "422 - random_3 - lwr_k=50 - 8.554209511164316 - 8.623465953748289 - 8.405542951321106 - 10.911860631818506 - 20.24297073640076 - 11.346706478186015 - 0.9518860197060768'\n",
      "423 - random_29 - lwr_k=600 - 7.026953495869552 - 7.675088655343694 - 6.653552547204718 - 10.258009187552771 - 25.1459072139582 - 11.350591959254047 - 0.9518695439155115'\n",
      "424 - random_76 - lr - 7.872506522248756 - 9.802573021556356 - 7.385600939917709 - 9.545551392660254 - 22.152707809107845 - 11.350964538321305 - 0.9518679640507345'\n",
      "425 - random_72 - lwr_k=500 - 9.00703307352364 - 9.401110924449869 - 7.330524816092959 - 11.183202664909615 - 19.848304627684943 - 11.353331123338968 - 0.9518579289074863'\n",
      "426 - random_78 - lr - 7.492635679552266 - 9.618674448288017 - 7.36844383340395 - 10.571749458526527 - 21.76287550677607 - 11.361956440059197 - 0.9518213545658918'\n",
      "427 - random_63 - lwr_k=100 - 7.060273136177887 - 12.277252940579846 - 6.9137211851036104 - 9.047318734518997 - 21.561399640451473 - 11.371435331365385 - 0.9517811607712967'\n",
      "428 - random_93 - lwr_k=600 - 8.124237320097492 - 7.76789659946304 - 5.626198380549364 - 10.107068129785334 - 25.2835861600791 - 11.38067214021258 - 0.9517419934904909'\n",
      "429 - random_31 - lwr_k=300 - 7.809361771210429 - 6.674125009053434 - 6.823763528523759 - 18.764744380182755 - 16.90953569025498 - 11.394945485478047 - 0.9516814695442558'\n",
      "430 - random_29 - lwr_k=700 - 7.0378963056144235 - 7.722610441247038 - 6.7078532366967805 - 10.355640917837851 - 25.18872855639948 - 11.401228618751475 - 0.9516548268747655'\n",
      "431 - random_55 - lwr_k=300 - 9.559620555475966 - 9.441963550544601 - 6.364800220462852 - 12.603540724244995 - 19.047221438730567 - 11.402806197434984 - 0.9516481373926822'\n",
      "432 - random_86 - lwr_k=300 - 8.452524189132733 - 10.138388119498908 - 7.152754734794145 - 11.411500712506006 - 19.90126294080354 - 11.410593219751512 - 0.9516151177108031'\n",
      "433 - random_14 - lwr_k=700 - 7.903353215045932 - 8.739970092175968 - 6.798972699113729 - 10.132223371360766 - 23.490838222733398 - 11.41205910486851 - 0.9516089018482734'\n",
      "434 - random_12 - lwr_k=300 - 7.45039998094592 - 9.079444502698879 - 7.389720031187084 - 10.380479926745485 - 22.820186601493884 - 11.423011617500187 - 0.9515624593869352'\n",
      "435 - random_66 - lwr_k=50 - 7.020546898442283 - 10.039110789538311 - 8.658097118941704 - 11.279208013390992 - 20.16621312049655 - 11.431684539285538 - 0.9515256831833141'\n",
      "436 - random_72 - lwr_k=600 - 9.152553510609248 - 9.503398231883974 - 7.286522721285581 - 11.337325858680266 - 19.906685110112164 - 11.436606298495494 - 0.9515048132131518'\n",
      "437 - random_29 - lwr_k=800 - 7.036532393527732 - 7.73165949782839 - 6.75698558480969 - 10.436357608762332 - 25.231211822615922 - 11.43722157622444 - 0.951502204221672'\n",
      "438 - random_76 - lwr_k=200 - 7.710993433088397 - 9.9457223623658 - 10.105485034811359 - 9.362046015369492 - 20.089667303276276 - 11.441926623811396 - 0.9514822531841338'\n",
      "439 - random_38 - lr - 10.024950222910192 - 10.045279574471447 - 6.263171126949856 - 11.726518188143118 - 19.216433682300234 - 11.45480546785942 - 0.9514276424078767'\n",
      "440 - random_22 - lr - 10.729419093042145 - 10.079366541905795 - 6.57205222393897 - 10.019521177326663 - 19.876759806303493 - 11.455079563181313 - 0.9514264801484192'\n",
      "441 - random_93 - lwr_k=700 - 8.168443564413234 - 7.9147784468441 - 5.640534138073139 - 9.824796868214358 - 25.746994779999405 - 11.457990352515601 - 0.951414137389669'\n",
      "442 - random_9 - lwr_k=50 - 10.584380170227803 - 10.040294589457702 - 8.212056107867857 - 9.855301664160233 - 18.59991576064188 - 11.458014334325483 - 0.9514140356984575'\n",
      "443 - random_31 - lwr_k=100 - 8.637802054432505 - 6.998347391335066 - 7.494945962276332 - 17.494397416404553 - 16.677437385200083 - 11.459393144821783 - 0.9514081890625923'\n",
      "444 - random_94 - lwr_k=400 - 7.48641884974848 - 10.328520749800257 - 6.465008318624892 - 10.864417044768658 - 22.16326877840454 - 11.460690312392995 - 0.9514026886211135'\n",
      "445 - random_66 - lwr_k=40 - 7.151142324595021 - 10.081783860111226 - 8.752725452391509 - 11.373595742399647 - 19.97474615920707 - 11.46586524266299 - 0.9513807451176386'\n",
      "446 - random_80 - lr - 10.311711502736422 - 7.3098959496478475 - 7.843969544193749 - 9.523694949474075 - 22.396545548791856 - 11.476290284699008 - 0.9513365393150078'\n",
      "447 - random_72 - lwr_k=700 - 9.20837890388456 - 9.60509994001132 - 7.299390431689081 - 11.281258783217737 - 20.004250602325577 - 11.478996860010072 - 0.9513250624947137'\n",
      "448 - random_16 - lr - 8.707304412473789 - 8.370337371229107 - 7.242702125554311 - 10.169410513927659 - 22.92399008711023 - 11.48178478612314 - 0.9513132407187364'\n",
      "449 - random_29 - lwr_k=900 - 7.049704437516478 - 7.773132308689641 - 6.847884103591843 - 10.514992542660282 - 25.252621997404933 - 11.486332134889333 - 0.9512939583790243'\n",
      "450 - random_12 - lwr_k=200 - 7.311061281317684 - 9.041733850580853 - 7.512090117366051 - 10.351978930230624 - 23.374173790508728 - 11.517113174473014 - 0.9511634360697706'\n",
      "451 - random_79 - lwr_k=100 - 9.876650611703766 - 7.421701177325876 - 9.516887924274975 - 10.863742082847088 - 19.978626601422015 - 11.530577731329057 - 0.9511063416675771'\n",
      "452 - random_29 - lwr_k=1000 - 7.068531724261485 - 7.792330956443784 - 6.92191926392988 - 10.626188716123373 - 25.27485930147148 - 11.535421196424354 - 0.9510858036916809'\n",
      "453 - random_4 - lwr_k=500 - 8.289504154646867 - 8.935536902887634 - 7.468258161696752 - 10.602111793064484 - 22.41029761269273 - 11.540182621936868 - 0.9510656135921379'\n",
      "454 - random_72 - lwr_k=1000 - 9.30355896700941 - 9.603364590336506 - 7.209271957170091 - 11.257079185680375 - 20.33300302860016 - 11.540571807830016 - 0.9510639633086455'\n",
      "455 - random_98 - lwr_k=40 - 9.811405250905363 - 11.133606023963909 - 6.112141110940643 - 10.556812451747803 - 20.094084681606233 - 11.541259779625682 - 0.9510610460690498'\n",
      "456 - random_86 - lwr_k=400 - 8.469367056845618 - 10.19505265427336 - 7.221116033767262 - 11.384524118766029 - 20.457818418570707 - 11.54485079454812 - 0.9510458189173168'\n",
      "457 - random_66 - lwr_k=30 - 7.135916062399534 - 10.64461869809283 - 8.907444082121783 - 11.720840059287221 - 19.37592503131324 - 11.55607546704037 - 0.9509982224035499'\n",
      "458 - random_3 - lwr_k=40 - 9.913815008423335 - 8.543685269795732 - 9.001505935840843 - 10.952414102477489 - 19.39050548971904 - 11.559621567007913 - 0.9509831857068409'\n",
      "459 - random_94 - lwr_k=500 - 7.520023143635602 - 10.156636198240031 - 6.413220123445609 - 11.038609644473654 - 22.686724526865106 - 11.56215040276793 - 0.9509724625640366'\n",
      "460 - random_71 - lwr_k=500 - 6.918853218205616 - 7.62261452589055 - 5.934014401973582 - 18.883212140362343 - 18.478567068977128 - 11.566045125707998 - 0.9509559475847209'\n",
      "461 - random_93 - lwr_k=800 - 8.156999600250892 - 8.283166585512856 - 5.715682950578633 - 9.55046362523063 - 26.140158314925614 - 11.568197371965056 - 0.9509468213123352'\n",
      "462 - random_44 - lr - 9.606868133714478 - 9.54264175543194 - 6.125670538694331 - 9.295198319961793 - 23.301631932246448 - 11.573747265470603 - 0.9509232878343808'\n",
      "463 - random_82 - lwr_k=700 - 7.9652689194139645 - 9.380113027768216 - 9.993506921410555 - 11.721478172084458 - 18.831572969882377 - 11.577436406556478 - 0.9509076445936001'\n",
      "464 - random_31 - lwr_k=400 - 7.944498660585967 - 6.5376569601413665 - 6.830877578842041 - 19.53610504209782 - 17.045654654866198 - 11.577537953543146 - 0.9509072139990725'\n",
      "465 - random_69 - lwr_k=200 - 7.101950524392847 - 9.419201340933382 - 6.852563375440909 - 10.456516813060755 - 24.08482888106936 - 11.581924112227751 - 0.950888615161346'\n",
      "466 - random_82 - lwr_k=500 - 7.798153248896143 - 9.482209994120863 - 9.90147229771535 - 11.980660724882052 - 18.86471943669404 - 11.604472037950162 - 0.9507930041172107'\n",
      "467 - random_67 - lwr_k=50 - 8.167305662220054 - 10.636484162168335 - 7.73691950201039 - 10.172869878541729 - 21.34895687762142 - 11.611783256366875 - 0.9507620020092861'\n",
      "468 - random_93 - lwr_k=900 - 8.190139512044086 - 7.908531186035778 - 5.739721991223473 - 9.647007312553773 - 26.61308355638141 - 11.618527443148405 - 0.9507334043126303'\n",
      "469 - random_72 - lwr_k=900 - 9.258545809922502 - 9.599171326373769 - 7.1969747495444905 - 11.209001802036994 - 20.838814004743583 - 11.619783790703611 - 0.9507280769622111'\n",
      "470 - random_9 - lr - 8.607237161566088 - 9.538960856014883 - 9.298036498092788 - 10.48729409142754 - 20.204279410592797 - 11.62632516581334 - 0.9507003392575538'\n",
      "471 - random_82 - lwr_k=600 - 7.941366854717039 - 9.416438588197618 - 9.93617220731696 - 11.906779605798116 - 18.937459696269816 - 11.626677698411767 - 0.950698844396774'\n",
      "472 - random_55 - lwr_k=200 - 9.541737110248887 - 10.233446108650632 - 6.525993970412054 - 12.722056488527327 - 19.167556703203573 - 11.63758477787322 - 0.950652594587873'\n",
      "473 - random_28 - lwr_k=200 - 8.472656589680966 - 8.619053081148738 - 6.842186191159319 - 11.739502746625668 - 22.53944293340692 - 11.641554156292651 - 0.9506357630433698'\n",
      "474 - random_72 - lwr_k=800 - 9.264025480616183 - 9.584401978655423 - 7.20795214452586 - 11.229675066154924 - 20.988781484745243 - 11.654236674692168 - 0.9505819847561174'\n",
      "475 - random_14 - lwr_k=800 - 7.9713181155254595 - 8.928322197713303 - 6.893253736621813 - 10.423465761430228 - 24.11395894841831 - 11.665010454546925 - 0.9505363001838921'\n",
      "476 - random_98 - deep - 10.827458913899873 - 7.994082135576663 - 7.116821232045116 - 14.383903774836336 - 18.01002699971492 - 11.66571975795087 - 0.9505332925960427'\n",
      "477 - random_84 - lwr_k=200 - 11.070171180722427 - 10.415433087692023 - 7.030962308384474 - 9.48964593964075 - 20.329740857032828 - 11.666887943880521 - 0.9505283389763847'\n",
      "478 - random_86 - lwr_k=500 - 8.498004224120079 - 10.111954981318235 - 7.28742535446148 - 11.500462088221004 - 20.95335835074862 - 11.66946639312674 - 0.9505174054551502'\n",
      "479 - random_96 - lr - 11.705306332101152 - 8.10768060528372 - 7.482109784329101 - 9.194946558147217 - 21.871229973675913 - 11.671676376208644 - 0.9505080343585542'\n",
      "480 - random_4 - lwr_k=600 - 8.410662671018258 - 9.109153746918349 - 7.531060317786822 - 10.831366653536602 - 22.492296452216266 - 11.673953326160225 - 0.950498379299149'\n",
      "481 - random_82 - lwr_k=400 - 7.877043738108918 - 9.452756414006066 - 9.899057303079875 - 12.106621722375028 - 19.043302939263317 - 11.674770388866131 - 0.9504949146692138'\n",
      "482 - random_60 - lwr_k=700 - 7.881666385049985 - 7.230022992905608 - 6.60113834483655 - 9.83682204119663 - 26.832694771767926 - 11.675119431396247 - 0.950493434607537'\n",
      "483 - random_82 - lwr_k=900 - 8.110523720565858 - 9.55488113168655 - 10.039170940647331 - 11.747833310002317 - 18.948462732198397 - 11.679241839775651 - 0.9504759541628007'\n",
      "484 - random_82 - lwr_k=800 - 8.06945946060515 - 9.501418313483605 - 9.948257912015377 - 11.89366482724138 - 19.009147029044534 - 11.683440122323944 - 0.9504581519852098'\n",
      "485 - random_24 - lwr_k=400 - 7.542589155464172 - 8.620079149527518 - 8.639331051268384 - 11.02214120527177 - 22.59963886112049 - 11.68357578899026 - 0.9504575767113784'\n",
      "486 - random_72 - lwr_k=400 - 8.878165857357512 - 9.442189702687708 - 7.302782865899923 - 11.052537256181468 - 21.75190336713584 - 11.684688779222121 - 0.9504528572458502'\n",
      "487 - random_31 - lwr_k=500 - 7.9675881350288345 - 6.547578108981406 - 6.800096061284082 - 20.119337828370494 - 17.001410014856575 - 11.685751360338735 - 0.9504483515324958'\n",
      "488 - random_83 - lwr_k=50 - 9.125790010241694 - 9.549604615550349 - 9.938328463558943 - 13.004029909594516 - 16.87860695475424 - 11.69849859146769 - 0.9503942988407725'\n",
      "489 - random_60 - lwr_k=900 - 7.798892095084745 - 7.200011566941338 - 6.655587771656484 - 9.878440533171801 - 26.989353869143596 - 11.703080057215576 - 0.9503748718331968'\n",
      "490 - random_94 - lwr_k=600 - 7.543579431156414 - 10.258743454912894 - 6.405539745436352 - 11.046371483391326 - 23.278851361746103 - 11.705698328126294 - 0.9503637694542603'\n",
      "491 - random_35 - deep - 11.110560755097534 - 9.984226474980481 - 7.603945034630078 - 13.315264213875999 - 16.599770405368666 - 11.722368415456629 - 0.9502930825941048'\n",
      "492 - random_24 - lwr_k=500 - 7.286950867665014 - 8.726331530699222 - 8.482214978048708 - 11.005157143741217 - 23.119088502595442 - 11.72273121224425 - 0.950291544103902'\n",
      "493 - random_86 - lwr_k=600 - 8.517039630990839 - 10.122653908000393 - 7.375913515079017 - 11.488143823282043 - 21.157811161950967 - 11.731522342152983 - 0.9502542666567412'\n",
      "494 - random_28 - lwr_k=300 - 8.45966257257744 - 8.588180104617622 - 6.491276464221444 - 11.737695476546419 - 23.39762305837224 - 11.733835966183465 - 0.9502444560864912'\n",
      "495 - random_4 - lwr_k=100 - 8.510912736638344 - 10.788038725374637 - 8.46914649183315 - 12.676970760774152 - 18.26053333915615 - 11.7404354116789 - 0.9502164721432096'\n",
      "496 - random_72 - lwr_k=300 - 8.860190450265334 - 9.520951624852135 - 7.349874060016926 - 11.131512569306224 - 21.882668578632746 - 11.748201576012397 - 0.9501835409064301'\n",
      "497 - random_86 - lwr_k=700 - 8.479576119184918 - 10.102130232681874 - 7.451914359873288 - 11.431022325242722 - 21.28264327719618 - 11.74865208710732 - 0.9501816305827617'\n",
      "498 - random_79 - lwr_k=300 - 10.145877496782235 - 6.982078989059013 - 8.44841533818062 - 11.213603901509474 - 21.987723419233788 - 11.7544946147444 - 0.9501568562343516'\n",
      "499 - random_90 - lr - 7.722956853982271 - 13.992832321366677 - 6.476559205824611 - 13.476957009301893 - 17.111431899632905 - 11.75585328645778 - 0.9501510949939275'\n",
      "500 - random_93 - lwr_k=1000 - 8.21854190450588 - 8.289547612395454 - 5.726520644354799 - 9.733384663856711 - 26.828056061857676 - 11.758062260112643 - 0.9501417281776531'\n",
      "501 - random_82 - lwr_k=1000 - 8.159745799766362 - 9.593133332323324 - 10.044852566478212 - 11.851816803601027 - 19.209176304553914 - 11.770796768976696 - 0.950087729432756'\n",
      "502 - random_84 - lwr_k=300 - 12.007090563952302 - 9.313449320123038 - 7.178780501105991 - 9.579212338709338 - 20.78587495681537 - 11.772517163587056 - 0.950080434361491'\n",
      "503 - random_19 - lwr_k=40 - 10.010031637054208 - 10.003389325904001 - 8.893569684567835 - 13.742906750902769 - 16.259969600255566 - 11.781392013485604 - 0.9500428019124671'\n",
      "504 - random_4 - lwr_k=700 - 8.458564534232716 - 9.175221154830304 - 7.573611750624916 - 10.966555191524066 - 22.759264289343143 - 11.785670811476484 - 0.9500246583214074'\n",
      "505 - random_24 - lwr_k=700 - 7.21989203371874 - 8.663347279172353 - 8.207411677940556 - 11.143569784131152 - 23.703714391124844 - 11.786327505666595 - 0.9500218737097332'\n",
      "506 - random_86 - lwr_k=800 - 8.53705418019727 - 10.120074388372698 - 7.459392358816478 - 11.464579168315934 - 21.363232933040315 - 11.788060873860063 - 0.9500145236259655'\n",
      "507 - random_12 - lr - 8.767710349345323 - 11.314160308951342 - 7.704315616623635 - 10.64752048389717 - 20.559118393350282 - 11.797989418938124 - 0.94997242314305'\n",
      "508 - random_24 - lwr_k=600 - 7.352472326441472 - 8.79134827622217 - 8.451852234214213 - 11.098123608476772 - 23.306119834104063 - 11.798762337943625 - 0.9499691456977515'\n",
      "509 - random_24 - lwr_k=900 - 7.265395257147383 - 8.688877002608294 - 7.829211990728642 - 11.154565946054039 - 24.078080165883588 - 11.801973054654814 - 0.9499555311426496'\n",
      "510 - random_35 - lwr_k=30 - 9.664564511089285 - 7.886903828404644 - 8.65412419542872 - 13.601870424169038 - 19.210132067821444 - 11.802527426919998 - 0.9499531804115108'\n",
      "511 - random_38 - deep - 10.113525965014768 - 10.159051621994294 - 6.33290873815738 - 12.240612707794153 - 20.170954792544453 - 11.802864964336807 - 0.9499517492419325'\n",
      "512 - random_16 - lwr_k=200 - 6.885437962499164 - 8.493132836671146 - 6.31406940589997 - 9.159734009216894 - 28.176475343377163 - 11.804421791502755 - 0.9499451476470799'\n",
      "513 - random_28 - lwr_k=400 - 8.389775464002724 - 8.532426680056528 - 6.415183077665365 - 11.805672009034145 - 23.92635838112834 - 11.812785109588702 - 0.9499096842707819'\n",
      "514 - random_66 - lwr_k=20 - 7.246357333850878 - 10.939303056859057 - 9.007163200095917 - 11.727338196624283 - 20.160270167987274 - 11.81519454363114 - 0.9498994674327718'\n",
      "515 - random_37 - lwr_k=100 - 9.569537181333216 - 9.925348137333662 - 7.511051497103493 - 9.904216073271865 - 22.177825380021716 - 11.816917693113762 - 0.9498921606798907'\n",
      "516 - random_3 - lr - 8.847719851224433 - 7.8792887501106375 - 9.59542528624833 - 10.111013873838774 - 22.660108120237464 - 11.817579620125718 - 0.9498893538792326'\n",
      "517 - random_31 - lwr_k=600 - 8.047015727502304 - 6.561865890378064 - 6.861303378450305 - 20.65071322889704 - 16.9832550923102 - 11.819351577260795 - 0.9498818401649365'\n",
      "518 - random_24 - lwr_k=800 - 7.164826945960545 - 8.568852019537218 - 8.065449253690442 - 11.198407009669411 - 24.110929364304443 - 11.820397731654882 - 0.9498774041065967'\n",
      "519 - random_94 - lwr_k=700 - 7.582045245686121 - 10.368042506663096 - 6.44125007771213 - 11.065654805656592 - 23.657606992508384 - 11.821987266058079 - 0.9498706639281054'\n",
      "520 - random_86 - lwr_k=900 - 8.594990204648953 - 10.124944262093301 - 7.573601314458315 - 11.543901987577595 - 21.43644751122956 - 11.85396002314658 - 0.9497350883223157'\n",
      "521 - random_82 - lwr_k=300 - 7.830936694398551 - 9.531255558680757 - 11.237759957720549 - 12.22716757590785 - 18.55396629671455 - 11.875170836763003 - 0.9496451470983737'\n",
      "522 - random_75 - lr - 9.473406442900336 - 9.567025857808412 - 7.513918815446663 - 11.457007549940624 - 21.41535816100863 - 11.884568802717457 - 0.9496052964554047'\n",
      "523 - random_14 - lwr_k=900 - 8.148403457479125 - 9.289525766425156 - 6.910243489875389 - 10.592531854518763 - 24.523453000365592 - 11.891792094410132 - 0.9495746671873588'\n",
      "524 - random_12 - lwr_k=50 - 8.083583486181947 - 9.39043227084376 - 8.692577431396055 - 12.046242466382981 - 21.261161459100347 - 11.89376526802606 - 0.9495663002452285'\n",
      "525 - random_4 - lwr_k=800 - 8.485091276917654 - 9.23680742478305 - 7.584598231613527 - 11.142834365245264 - 23.067549779114614 - 11.902379841909578 - 0.9495297714569965'\n",
      "526 - random_94 - lwr_k=800 - 7.581054051334168 - 10.4138763094375 - 6.4770049676038 - 11.12725785683362 - 23.933316168472683 - 11.905549181464911 - 0.9495163323554208'\n",
      "527 - random_84 - lwr_k=400 - 12.378911803743428 - 9.387131703729406 - 7.115187959483897 - 9.621054396139519 - 21.08047587562116 - 11.91621387366652 - 0.9494711103527704'\n",
      "528 - random_9 - lwr_k=30 - 11.360735470532564 - 11.222957045371373 - 9.370393237170912 - 10.084347081725594 - 17.5477460724322 - 11.917030970729813 - 0.9494676455771479'\n",
      "529 - random_74 - deep - 9.23080498028894 - 10.394778142389026 - 6.994467568924738 - 9.894535338849343 - 23.175846472419156 - 11.937390534733838 - 0.9493813140551413'\n",
      "530 - random_79 - lwr_k=400 - 10.542506365176047 - 6.932150719742817 - 8.549358717638876 - 11.447554678657996 - 22.23635220700504 - 11.940535166104876 - 0.9493679796172261'\n",
      "531 - random_90 - lwr_k=100 - 8.121673936390595 - 13.567288821698074 - 7.4615888626872025 - 13.745325020921557 - 16.821105855688177 - 11.943036612422778 - 0.9493573726151786'\n",
      "532 - random_86 - lwr_k=1000 - 8.616531023381826 - 10.190605271586136 - 7.812956563748655 - 11.589132887871752 - 21.513568337204713 - 11.94372665999776 - 0.9493544465735568'\n",
      "533 - random_41 - lr - 9.975256759327216 - 10.292200861715656 - 6.878070655852888 - 9.749579971479365 - 22.837207345020495 - 11.945869460808431 - 0.9493453603531478'\n",
      "534 - random_37 - lwr_k=200 - 9.73291383329265 - 9.851132734078842 - 7.687067585373223 - 9.839385332822687 - 22.64013353531373 - 11.949419840357496 - 0.9493303055095252'\n",
      "535 - random_37 - lwr_k=40 - 10.09271289467829 - 10.374984676360679 - 7.7639034450435895 - 10.564814840907509 - 21.012474250634302 - 11.961212136725134 - 0.949280302073185'\n",
      "536 - random_34 - deep - 9.46002191958763 - 11.774634365169193 - 7.078968640334483 - 14.021272275020214 - 17.484042000516723 - 11.963347006061326 - 0.9492712495928504'\n",
      "537 - random_94 - lwr_k=900 - 7.545376950137272 - 10.534256309953538 - 6.483790409025744 - 11.092053249308629 - 24.212269900998702 - 11.972588586830955 - 0.9492320619695692'\n",
      "538 - random_31 - lwr_k=700 - 8.103837914660124 - 6.546535198848787 - 7.048220177319161 - 21.21534365401625 - 16.982554448334856 - 11.97777408943594 - 0.9492100736357183'\n",
      "539 - random_84 - lwr_k=600 - 12.57222002755286 - 9.385134581765586 - 7.187244204660135 - 9.64580039912561 - 21.16381284505144 - 11.990510934602481 - 0.9491560649840864'\n",
      "540 - random_84 - lwr_k=500 - 12.495862443996138 - 9.488572477700002 - 7.17316745071835 - 9.669611551825934 - 21.14956424401393 - 11.995027112880344 - 0.9491369148180816'\n",
      "541 - random_28 - lwr_k=100 - 8.551344652648996 - 9.716475031352758 - 7.478366186662098 - 11.90405011308613 - 22.36277661803795 - 12.001663043469309 - 0.949108776165319'\n",
      "542 - random_37 - lwr_k=300 - 9.712539470886496 - 9.81635377648736 - 7.809009301391342 - 9.855960237634891 - 22.825369315353345 - 12.003113032501771 - 0.9491026277077143'\n",
      "543 - random_19 - lwr_k=100 - 9.398843150687556 - 9.407574925696457 - 8.193086096191601 - 12.556712544494902 - 20.494774650836924 - 12.009344502749528 - 0.9490762041074141'\n",
      "544 - random_37 - lwr_k=50 - 9.891739090120845 - 10.102801380557453 - 7.632766428088158 - 10.403324722867819 - 22.057102626445005 - 12.016885223084245 - 0.9490442288315679'\n",
      "545 - random_79 - lwr_k=500 - 10.77520668831292 - 6.821893386885289 - 8.63473055711474 - 11.527610331288383 - 22.345245405655326 - 12.019881964456784 - 0.9490315216062931'\n",
      "546 - random_28 - lwr_k=500 - 8.322513875086637 - 8.68258138135603 - 6.443484149851673 - 12.097893401311689 - 24.559421377947405 - 12.020026509876729 - 0.9490309086834595'\n",
      "547 - random_84 - lwr_k=700 - 12.683058763935055 - 9.39815644699672 - 7.243030069890425 - 9.609508271255603 - 21.199683547405407 - 12.02636448561977 - 0.9490040334628314'\n",
      "548 - random_44 - lwr_k=300 - 8.764474514696998 - 8.574637880776425 - 7.827867767090799 - 9.133911395625708 - 25.85162026791561 - 12.029401678740845 - 0.9489911547080794'\n",
      "549 - random_4 - lwr_k=900 - 8.531016262533413 - 9.368719081256401 - 7.728905782305501 - 11.241462056349812 - 23.298511208997063 - 12.032712937169592 - 0.9489771138211405'\n",
      "550 - random_14 - lwr_k=1000 - 8.285532293068497 - 9.411910186985123 - 6.949804199184138 - 10.795559993923064 - 24.75460633859404 - 12.038437650210046 - 0.9489528390475799'\n",
      "551 - random_90 - deep - 7.595128773472313 - 14.622320732391408 - 6.502217566937721 - 14.118738551299948 - 17.361157728159263 - 12.039607784390217 - 0.9489478773803967'\n",
      "552 - random_94 - lwr_k=1000 - 7.565890026107937 - 10.690296804881857 - 6.469505465126644 - 11.06361189929907 - 24.507991057291402 - 12.058499048695944 - 0.9488677717434'\n",
      "553 - random_19 - lwr_k=50 - 9.520131931888034 - 9.687339990205963 - 8.479483736388744 - 13.41719182630745 - 19.20740642784195 - 12.061505616382172 - 0.948855022851139'\n",
      "554 - random_84 - lwr_k=800 - 12.770339394105742 - 9.402120228329586 - 7.285381847117911 - 9.645851875408852 - 21.21841275305072 - 12.064100868723774 - 0.9488440180789383'\n",
      "555 - random_31 - lwr_k=800 - 8.156917451494621 - 6.540001214383535 - 7.064277469499153 - 21.61861302173119 - 16.9535317613838 - 12.065123003134923 - 0.948839683873677'\n",
      "556 - random_4 - lwr_k=1000 - 8.563245472540242 - 9.436304651238407 - 7.718320430160026 - 11.250695546116745 - 23.515026686758947 - 12.095704329920439 - 0.9487100084161203'\n",
      "557 - random_31 - lwr_k=900 - 8.18274150970755 - 6.538118464740873 - 7.0890920222414495 - 21.791957252597484 - 16.94440693761059 - 12.107708027538372 - 0.9486591085649825'\n",
      "558 - random_21 - lr - 9.088486539017214 - 8.691405486170426 - 7.714871659657014 - 10.830450865081556 - 24.253270752271202 - 12.11464064942005 - 0.9486297118379903'\n",
      "559 - random_84 - lwr_k=900 - 12.899913857567535 - 9.429320868473564 - 7.305820976143603 - 9.682924599245624 - 21.31809568155683 - 12.126899952505873 - 0.9485777281308042'\n",
      "560 - random_44 - deep - 9.740283725694821 - 10.072496225322718 - 6.500976538287139 - 10.218064598623208 - 24.121856076320988 - 12.130007074525196 - 0.9485645529625797'\n",
      "561 - random_69 - lwr_k=300 - 7.279264847765463 - 9.962446983263094 - 6.965756512869046 - 11.141291669558054 - 25.460720370286104 - 12.160736410738453 - 0.9484342497842242'\n",
      "562 - random_24 - lwr_k=300 - 8.564162298983852 - 8.5477058133285 - 9.230189417813849 - 12.890275429957052 - 21.618820681145024 - 12.169047071238259 - 0.9483990096943985'\n",
      "563 - random_31 - lwr_k=1000 - 8.17862062930024 - 6.530584614948947 - 7.101996276061495 - 22.128676973189197 - 17.006187047421104 - 12.187629806895167 - 0.9483202124346894'\n",
      "564 - random_22 - lwr_k=100 - 7.979235365426846 - 11.214380568542328 - 11.205736748102343 - 12.498526500032185 - 18.064773242427158 - 12.191680403845776 - 0.9483030364953775'\n",
      "565 - random_37 - lwr_k=30 - 10.262354959157816 - 10.672685478265176 - 7.797687748192598 - 11.001166412012914 - 21.28848938329052 - 12.203907954992559 - 0.948251187427452'\n",
      "566 - random_84 - lwr_k=1000 - 12.9597550734836 - 9.467011706910439 - 7.372329083047419 - 9.687942388871082 - 21.548016409548083 - 12.206685526266941 - 0.9482394095595906'\n",
      "567 - random_33 - lwr_k=100 - 8.09881667529904 - 17.160392491880955 - 5.982865120996542 - 12.677034537925037 - 17.135249574352066 - 12.211008812202566 - 0.9482210773241784'\n",
      "568 - random_65 - lwr_k=200 - 13.544501593644952 - 6.239176833935682 - 6.376839748221696 - 10.771193691223583 - 24.146229898822334 - 12.214827341498 - 0.9482048854324057'\n",
      "569 - random_37 - lwr_k=20 - 11.328391411079135 - 10.980932285542844 - 7.972094185175892 - 11.225629054836254 - 19.644081623088393 - 12.229873472264257 - 0.9481410846069785'\n",
      "570 - random_79 - lwr_k=600 - 10.980944951313043 - 6.8132814167507405 - 8.71836857786672 - 11.730466471219849 - 22.912828026574076 - 12.230086005756608 - 0.948140183391081'\n",
      "571 - random_55 - lr - 9.062228729508474 - 8.756953567366276 - 6.013257202239465 - 15.676216505911379 - 21.648516422743473 - 12.23034660497837 - 0.9481390783597801'\n",
      "572 - random_28 - lwr_k=600 - 8.364892941866888 - 8.795485653216607 - 6.436184558886384 - 12.359949751902109 - 25.242448263820005 - 12.238593739444738 - 0.9481041076424218'\n",
      "573 - random_65 - lwr_k=100 - 13.557406379894353 - 6.3379341945384695 - 6.463118119276048 - 11.367933836575789 - 23.487525308694146 - 12.242031934168587 - 0.9480895285014607'\n",
      "574 - random_64 - deep - 17.1164923260528 - 10.829608022873998 - 7.9538056664931585 - 10.457449399087391 - 14.88143995177248 - 12.248324163670285 - 0.9480628473712347'\n",
      "575 - random_65 - lwr_k=300 - 13.831026757399604 - 6.249506008059991 - 6.367579972050322 - 10.6513561860084 - 24.15590164261449 - 12.250350089080335 - 0.9480542566327228'\n",
      "576 - random_72 - lwr_k=200 - 9.092488105733015 - 9.943637011165313 - 7.351660823394663 - 11.401863759170778 - 23.686860780015362 - 12.294392569274754 - 0.9478675012047716'\n",
      "577 - random_79 - lwr_k=700 - 11.1615414592248 - 6.737600166469111 - 8.894475303172705 - 11.842283922292735 - 23.076268998049652 - 12.341322830769588 - 0.947668500580133'\n",
      "578 - random_2 - lwr_k=700 - 11.915708416222886 - 13.79053600586016 - 10.14764709911191 - 8.849554626129322 - 17.060015676446543 - 12.35285625204622 - 0.9476195948641798'\n",
      "579 - random_65 - lwr_k=400 - 13.745456572716401 - 6.253080312943933 - 6.403574701791529 - 10.659908316961957 - 24.745542901445607 - 12.360738942662524 - 0.9475861695154494'\n",
      "580 - random_28 - lwr_k=700 - 8.41459752829124 - 8.867626441194991 - 6.447852836760533 - 12.400752618569697 - 25.704559366129466 - 12.365857530328222 - 0.9475644649242049'\n",
      "581 - random_21 - lwr_k=20 - 11.695205514814914 - 10.853521046542005 - 10.125240176320078 - 10.799326334866615 - 18.38331191968145 - 12.37096175242642 - 0.947542821247961'\n",
      "582 - random_60 - deep - 9.537977507383655 - 9.611352725427006 - 8.562594127889348 - 12.290543086991557 - 21.85820404571358 - 12.371218274960283 - 0.9475417336147767'\n",
      "583 - random_37 - lwr_k=400 - 9.92444070100098 - 10.075349809364655 - 7.844031164027977 - 9.874137725827177 - 24.163476817734782 - 12.375508991752762 - 0.9475235393723118'\n",
      "584 - random_85 - lwr_k=40 - 11.240284813945468 - 9.773076222826301 - 11.339723902152008 - 11.493492329938842 - 18.06301546882968 - 12.381304420134118 - 0.9474989647411207'\n",
      "585 - random_67 - deep - 9.084528671739145 - 9.259823190405013 - 7.346201403037531 - 11.170672286640514 - 25.062296899394067 - 12.383652479862825 - 0.947489008263193'\n",
      "586 - random_19 - lwr_k=30 - 10.483713376455707 - 10.663387778556345 - 8.899153319025777 - 14.746018500592063 - 17.147728677013827 - 12.387406110661862 - 0.947473091451953'\n",
      "587 - random_29 - deep - 7.651454427628587 - 9.439089905415736 - 7.995207907624366 - 11.918936876270442 - 25.169781284972746 - 12.433620086847574 - 0.9472771282381559'\n",
      "588 - random_29 - lr - 7.385103882247256 - 8.961206972811167 - 7.500636312411586 - 11.662805887671356 - 26.669853322958524 - 12.434525249470285 - 0.947273289921839'\n",
      "589 - random_65 - lwr_k=500 - 13.893067366268541 - 6.273278244587577 - 6.4450083467797885 - 10.639703021931604 - 25.023180409036197 - 12.454070770770588 - 0.9471904101162774'\n",
      "590 - random_85 - lwr_k=50 - 10.865146586543927 - 9.808110743970262 - 10.87454698923906 - 11.13895527587699 - 19.618532875490395 - 12.460362758239532 - 0.9471637299019198'\n",
      "591 - random_12 - lwr_k=100 - 7.381333354288268 - 10.575772344399118 - 7.689338055331342 - 10.696609537658608 - 25.96526166130759 - 12.460522296265479 - 0.9471630534052244'\n",
      "592 - random_28 - lwr_k=800 - 8.361835357519755 - 8.906070285803883 - 6.371720569516479 - 12.491842785565451 - 26.207868635403404 - 12.466611946376958 - 0.9471372311716076'\n",
      "593 - random_66 - lwr_k=10 - 7.6910549858537784 - 11.938316565487607 - 9.523692197926541 - 12.289250971051445 - 20.904245783803475 - 12.468442729177232 - 0.9471294680160396'\n",
      "594 - random_69 - lwr_k=400 - 7.365897299048516 - 10.091714391586592 - 6.943983941960808 - 11.864880641519257 - 26.142521841095675 - 12.480570543697901 - 0.9470780418660835'\n",
      "595 - random_37 - lwr_k=500 - 10.196022110556164 - 10.042532248487216 - 8.0909222272134 - 9.873618846233601 - 24.250155415988804 - 12.489873561544478 - 0.9470385938361018'\n",
      "596 - random_79 - lwr_k=800 - 11.349383195119838 - 6.758468844449876 - 8.961371366075145 - 12.08341924142053 - 23.336069981421392 - 12.496614699757972 - 0.9470100090664344'\n",
      "597 - random_65 - lwr_k=600 - 13.931791461560401 - 6.298456970846898 - 6.447985497270643 - 10.676910789463296 - 25.176292192569317 - 12.505504293045849 - 0.9469723141003139'\n",
      "598 - random_2 - lwr_k=600 - 11.674287472963218 - 13.482727085947701 - 10.129130202245504 - 8.800566037304025 - 18.46078720129253 - 12.509522199485392 - 0.9469552767801341'\n",
      "599 - random_69 - lwr_k=50 - 9.15485505680528 - 12.186810666038694 - 8.441507152000563 - 12.892802938039019 - 19.889433741113642 - 12.512478587056398 - 0.9469427406690074'\n",
      "600 - random_78 - deep - 8.194250665592874 - 12.04101844153896 - 9.582719403734762 - 12.731468619527044 - 20.07083730517988 - 12.523270901041109 - 0.9468969776185411'\n",
      "601 - random_9 - deep - 9.764407516891554 - 11.916917937476974 - 9.85930973384148 - 11.47092040497001 - 19.703502986979817 - 12.542454445737413 - 0.9468156327197991'\n",
      "602 - random_28 - lwr_k=900 - 8.414051834591282 - 8.890835531866788 - 6.37304027507413 - 12.545397642297596 - 26.558148769426932 - 12.5550163265749 - 0.9467623658646643'\n",
      "603 - random_85 - lwr_k=100 - 10.78000109071074 - 9.872867560722078 - 10.429801759860426 - 11.042029659925833 - 20.674606318135538 - 12.559129846087016 - 0.9467449231118158'\n",
      "604 - random_63 - deep - 8.316906090844087 - 10.470420238039107 - 9.909878086399388 - 10.602487590252426 - 23.511038040548538 - 12.561108260208657 - 0.946736534061374'\n",
      "605 - random_66 - lwr_k=100 - 6.922133647427076 - 14.59102244884301 - 8.633060308466966 - 11.39720510198384 - 21.305234325050062 - 12.569137371326116 - 0.9467024877255589'\n",
      "606 - random_37 - lwr_k=600 - 10.326087246082764 - 10.06550457572485 - 8.295662124684302 - 9.917013342006399 - 24.299224736744335 - 12.579917366078776 - 0.9466567768000057'\n",
      "607 - random_35 - lwr_k=20 - 9.859971773103926 - 8.873421646134673 - 9.251926582967494 - 14.4986673559023 - 20.42744405409837 - 12.581233199234271 - 0.9466511972099604'\n",
      "608 - random_2 - lwr_k=500 - 11.375644256798218 - 13.270787305537889 - 9.920073447921025 - 8.678465757223622 - 19.673841947116873 - 12.583677215668255 - 0.9466408337305869'\n",
      "609 - random_65 - lwr_k=700 - 14.085888813536048 - 6.301024586963336 - 6.45254921780244 - 10.689936046137081 - 25.427603073186074 - 12.590615037668348 - 0.9466114149532847'\n",
      "610 - random_2 - lwr_k=800 - 12.028957236395808 - 14.055358498232268 - 10.269607686395604 - 8.882432306014698 - 17.72254444061138 - 12.591927529136234 - 0.9466058495331566'\n",
      "611 - random_1 - lwr_k=300 - 9.30472347980088 - 12.89444990955149 - 6.033145782071178 - 10.299409434169174 - 24.45465188880645 - 12.59678561602272 - 0.9465852495557824'\n",
      "612 - random_19 - lwr_k=200 - 9.538279973048379 - 9.670910920160129 - 8.090517081998296 - 12.504123702908878 - 23.282823347491608 - 12.616344355192354 - 0.9465023137018594'\n",
      "613 - random_46 - lr - 9.94105070953343 - 10.34802367633976 - 9.952186519867034 - 10.334880323619824 - 22.527160225848586 - 12.619849377959 - 0.9464874511867633'\n",
      "614 - random_15 - lwr_k=300 - 7.25510160494685 - 9.969637612231425 - 7.326132445808408 - 11.046233057697656 - 27.55324715608794 - 12.628754606313432 - 0.9464496899225249'\n",
      "615 - random_28 - lwr_k=1000 - 8.425090646091595 - 8.902640657828718 - 6.380137618430765 - 12.667120393524108 - 26.851297508630317 - 12.643953486822522 - 0.9463852413850827'\n",
      "616 - random_79 - lwr_k=900 - 11.590372623765848 - 6.780634296578667 - 9.079841665903686 - 12.211689615257274 - 23.58967713047728 - 12.649308322788814 - 0.946362535018909'\n",
      "617 - random_15 - lwr_k=100 - 7.18700235956866 - 10.214448117159142 - 7.003099931288417 - 11.246567390168087 - 27.60194259260654 - 12.649318517706295 - 0.9463624917889145'\n",
      "618 - random_96 - lwr_k=800 - 9.259247674969725 - 8.162745093464306 - 6.246283335219426 - 18.465335740351502 - 21.137429051904064 - 12.65291680476066 - 0.9463472338008139'\n",
      "619 - random_15 - lwr_k=200 - 7.231561484883255 - 9.99640772626675 - 7.370288117273569 - 11.080865255604785 - 27.603626618042735 - 12.65522592843294 - 0.9463374423136472'\n",
      "620 - random_37 - lwr_k=700 - 10.409792156166205 - 10.146828441978037 - 8.435652153079904 - 9.959514017080767 - 24.387778380645603 - 12.66713045152558 - 0.946286962996976'\n",
      "621 - random_15 - lwr_k=400 - 7.300213365476768 - 9.949080165380495 - 7.382333468438541 - 11.082820242485289 - 27.647095536334767 - 12.67098297455548 - 0.9462706269599508'\n",
      "622 - random_65 - lwr_k=800 - 14.132540682345624 - 6.299576537052454 - 6.4688742195047055 - 10.713054296597951 - 25.838726107411514 - 12.689743988441991 - 0.9461910737385676'\n",
      "623 - random_77 - lr - 8.445067137530737 - 10.309676039423927 - 9.489951409565258 - 11.233929616604925 - 24.03098889992047 - 12.700833853282347 - 0.9461440488561123'\n",
      "624 - random_17 - deep - 13.617373124667353 - 10.3417157190326 - 6.836537090117184 - 11.973941703588626 - 20.766931151094052 - 12.707061358352181 - 0.9461176421841158'\n",
      "625 - random_37 - lwr_k=800 - 10.483990651949785 - 10.186569007561209 - 8.485001376344064 - 9.998518428394851 - 24.431224114889215 - 12.716280699162992 - 0.9460785488592864'\n",
      "626 - random_34 - lwr_k=100 - 8.950594127221054 - 20.311542903618655 - 6.224831268463911 - 10.227410793935485 - 17.873404588549388 - 12.718183398148778 - 0.9460704807540893'\n",
      "627 - random_15 - lwr_k=500 - 7.464472279798283 - 9.95851895348701 - 7.375971967075311 - 11.1450959637952 - 27.706358583205233 - 12.728767489890473 - 0.9460256005254107'\n",
      "628 - random_2 - lwr_k=900 - 12.133634503491058 - 14.374805797002894 - 10.466976892047816 - 8.951189615770987 - 17.738903917724514 - 12.733272807727525 - 0.9460064964114507'\n",
      "629 - random_86 - lwr_k=10 - 10.327261274172585 - 11.141044982002823 - 9.759761591195247 - 14.259196856522573 - 18.19142794124349 - 12.735083023738024 - 0.9459988204662217'\n",
      "630 - random_15 - lwr_k=600 - 7.461683336145485 - 9.980402511395193 - 7.36915983930468 - 11.196779452898715 - 27.6748505320727 - 12.735260075509386 - 0.9459980697051569'\n",
      "631 - random_79 - lwr_k=1000 - 11.734420538690442 - 6.810419864541142 - 9.106271730758788 - 12.313136405443549 - 23.73639491982521 - 12.738993041453844 - 0.9459822406317394'\n",
      "632 - random_73 - lr - 7.892408679640977 - 12.750434048940953 - 9.163084433417376 - 15.446978096814897 - 18.56311553077085 - 12.762404491045022 - 0.9458829679461854'\n",
      "633 - random_65 - lwr_k=900 - 14.236658969379405 - 6.3061634413869445 - 6.4767522620270865 - 10.703668565647563 - 26.09704305302211 - 12.763240934057183 - 0.9458794211370138'\n",
      "634 - random_37 - lwr_k=900 - 10.522223996203232 - 10.206887599121439 - 8.486823885216813 - 10.080533115645183 - 24.53208753880455 - 12.764924865305323 - 0.9458722806830817'\n",
      "635 - random_85 - lwr_k=30 - 11.588958284274149 - 10.141667290787947 - 12.519129329092463 - 11.688567905690087 - 18.002911246692506 - 12.78761706379678 - 0.9457760578722473'\n",
      "636 - random_37 - lwr_k=1000 - 10.550083223181256 - 10.242763315816571 - 8.450446881020767 - 10.154086136845816 - 24.54534571623343 - 12.78776165138226 - 0.9457754447706171'\n",
      "637 - random_69 - lwr_k=500 - 7.476450371754022 - 10.353331462648924 - 6.952635123125582 - 12.323894580411073 - 26.87888766721737 - 12.795768464275596 - 0.9457414931002793'\n",
      "638 - random_65 - lwr_k=1000 - 14.267968109747379 - 6.313338405928162 - 6.480172476373007 - 10.673499015694803 - 26.289314478766137 - 12.804035112582897 - 0.9457064396374513'\n",
      "639 - random_60 - lwr_k=1000 - 7.769450337384074 - 7.175634643683418 - 6.721372086122323 - 9.912638131068546 - 32.6330620482419 - 12.840672848036332 - 0.9455510829015553'\n",
      "640 - random_93 - lr - 8.363693558819415 - 8.515105938832091 - 5.968796084520467 - 9.722535372897315 - 31.80313510244984 - 12.873200696860735 - 0.9454131535138206'\n",
      "641 - random_2 - lwr_k=1000 - 12.288364612496801 - 14.473035899420806 - 10.636471129305168 - 8.94290209261069 - 18.094974249081048 - 12.887311230989116 - 0.9453533199434102'\n",
      "642 - random_72 - lr - 10.016501254279818 - 11.20813682411418 - 8.456070961964675 - 12.932353572087045 - 21.90085267956122 - 12.902032947374956 - 0.9452908947477427'\n",
      "643 - random_81 - lwr_k=900 - 13.369692986289461 - 9.547479596923155 - 9.266217570660578 - 10.846918798887353 - 21.48537865314916 - 12.902664440707213 - 0.9452882169964674'\n",
      "644 - random_25 - deep - 10.562950493270948 - 10.314274984568893 - 7.960588215414761 - 10.695593128626118 - 25.053874358693466 - 12.916644365202666 - 0.9452289373727689'\n",
      "645 - random_50 - lwr_k=40 - 8.772230294758554 - 12.735051816702933 - 12.375587810497647 - 12.070379716083888 - 18.698341625348064 - 12.929605406044207 - 0.9451739779370703'\n",
      "646 - random_91 - deep - 10.775702972849144 - 11.854144468635273 - 7.904855595759736 - 11.360214189659075 - 22.932287254458465 - 12.964900416554617 - 0.9450243149386435'\n",
      "647 - random_45 - lwr_k=50 - 14.12997522852435 - 10.590878783889458 - 7.938232078573873 - 11.81047876371044 - 20.515316207237465 - 12.996767746951983 - 0.9448891862617816'\n",
      "648 - random_19 - lwr_k=300 - 9.832829135414396 - 9.796245728349367 - 8.09427838160885 - 12.839241955647587 - 24.448850016250148 - 13.001245076985567 - 0.9448702008258386'\n",
      "649 - random_23 - lwr_k=50 - 8.98199671833766 - 10.931090653805517 - 14.099131566933869 - 12.650603721379758 - 18.35190031177183 - 13.001946919348832 - 0.944867224770214'\n",
      "650 - random_85 - lwr_k=200 - 11.16107648414582 - 10.211936138894663 - 10.456379432866264 - 10.826013437279364 - 22.44374548933171 - 13.019066049169364 - 0.9447946337080924'\n",
      "651 - random_23 - lwr_k=100 - 8.90871852568815 - 10.693513054329477 - 13.753455463275158 - 12.35191265812374 - 19.397530324552545 - 13.019971507006424 - 0.9447907942520694'\n",
      "652 - random_31 - lwr_k=50 - 11.672873189703957 - 8.35831572439178 - 9.21790825096134 - 19.51943833814435 - 16.38727280350958 - 13.030174084068811 - 0.9447475317782695'\n",
      "653 - random_77 - deep - 8.376413025364354 - 10.53947632441544 - 9.786798172559434 - 11.412733754596195 - 25.100094699156664 - 13.041929150781968 - 0.9446976863261977'\n",
      "654 - random_81 - lwr_k=800 - 12.86356195758868 - 9.585545108462474 - 9.752081974768073 - 10.99737441004068 - 22.01931955890842 - 13.042980884557588 - 0.9446932264917505'\n",
      "655 - random_81 - lwr_k=1000 - 13.748248134004474 - 9.658819198009548 - 9.286659480015926 - 11.713814228188996 - 20.847972168703425 - 13.050661322322945 - 0.9446606587654237'\n",
      "656 - random_45 - lwr_k=30 - 14.534237268085064 - 10.91462140872225 - 7.717946451380988 - 12.127019172997144 - 20.018779972526392 - 13.062410132123624 - 0.9446108397272461'\n",
      "657 - random_50 - lwr_k=30 - 8.857868001104368 - 12.6990732458312 - 12.868633389906403 - 12.594492340199377 - 18.305609286234866 - 13.064386385902603 - 0.9446024597241536'\n",
      "658 - random_45 - lwr_k=40 - 14.167887954945545 - 10.734227174417168 - 7.729633324966391 - 12.421272000268669 - 20.349504797457204 - 13.080298910739886 - 0.9445349850866525'\n",
      "659 - random_64 - lwr_k=200 - 8.806028678068307 - 10.978450938824837 - 8.8370930115139 - 9.860679931309654 - 27.017471469631342 - 13.0988943048061 - 0.944456134150887'\n",
      "660 - random_81 - lwr_k=600 - 10.596997518473879 - 9.95158053961884 - 9.84147768115258 - 11.328515327483041 - 23.785232152382505 - 13.099834994038387 - 0.9444521453014997'\n",
      "661 - random_69 - lwr_k=600 - 7.570166322385407 - 10.62871425494384 - 7.01346824412427 - 12.71818259189608 - 27.580849782566176 - 13.100965338266004 - 0.944447352248996'\n",
      "662 - random_23 - lwr_k=40 - 9.09201324397606 - 10.968674345469157 - 14.396393846999896 - 12.675522979978531 - 18.412556169959476 - 13.108023868276279 - 0.944417421627765'\n",
      "663 - random_66 - lwr_k=200 - 7.129650296187671 - 15.96780277005 - 8.74307345035038 - 11.6278524017374 - 22.12352203769763 - 13.117866140555169 - 0.9443756869715118'\n",
      "664 - random_80 - lwr_k=300 - 9.952828295510928 - 6.816701480914339 - 6.618842505366952 - 25.50625318480201 - 16.75548201243463 - 13.128467457290103 - 0.9443307337028733'\n",
      "665 - random_23 - lwr_k=30 - 9.200839746220877 - 10.994833387811916 - 14.830582347226633 - 12.78260912829749 - 17.940031437720243 - 13.148779719479196 - 0.944244602649376'\n",
      "666 - random_69 - lwr_k=40 - 9.662679550249488 - 13.864811011540132 - 9.335020008035753 - 14.496664914897933 - 18.491341605148456 - 13.169642845471627 - 0.9441561357418378'\n",
      "667 - random_50 - lwr_k=20 - 9.110190280400214 - 12.993712054666819 - 13.005763731028255 - 13.046482560777873 - 17.805059976281324 - 13.191540790369526 - 0.9440632808423725'\n",
      "668 - random_66 - lwr_k=500 - 7.244499605954703 - 15.68577376877298 - 8.821302394884743 - 11.751532349835188 - 22.51164004161241 - 13.202380510566401 - 0.9440173166601715'\n",
      "669 - random_45 - lwr_k=20 - 14.838797063479921 - 10.909682582733302 - 7.856953435898664 - 12.622382279983906 - 19.810078251093277 - 13.207469556467998 - 0.9439957373362774'\n",
      "670 - random_66 - lwr_k=700 - 7.234482305210496 - 15.552701789541212 - 8.883051190999247 - 11.851011726179225 - 22.53658939274601 - 13.21097190669795 - 0.9439808861536659'\n",
      "671 - random_66 - lwr_k=400 - 7.245775429811175 - 15.793443314278223 - 8.800766632607735 - 11.71218574009891 - 22.52636468848058 - 13.215151700832951 - 0.9439631623733751'\n",
      "672 - random_86 - lr - 10.414101892791061 - 11.47902473470679 - 8.725477306735854 - 11.228450120669992 - 24.241612386498428 - 13.216989495919352 - 0.943955369483276'\n",
      "673 - random_66 - lwr_k=300 - 7.244381745120617 - 15.962035006079793 - 8.826570575914484 - 11.694976315757476 - 22.36152404775015 - 13.217368738660767 - 0.9439537613622038'\n",
      "674 - random_66 - lwr_k=800 - 7.24265528766657 - 15.623393229471173 - 8.888676054757454 - 11.887790550828619 - 22.44836343927754 - 13.217591087722417 - 0.9439528185248799'\n",
      "675 - random_66 - lwr_k=900 - 7.2324153133130915 - 15.628149836188038 - 8.928991129622013 - 11.945321989653962 - 22.39954206063515 - 13.226295691397059 - 0.9439159079790336'\n",
      "676 - random_66 - lwr_k=1000 - 7.255831379749156 - 15.633942105850354 - 8.945161196944982 - 11.968036827239423 - 22.333396930587412 - 13.226689968753725 - 0.9439142361059658'\n",
      "677 - random_2 - lwr_k=300 - 11.21439541254137 - 12.109031737740223 - 9.703471950555063 - 8.853611112983264 - 24.260470004434207 - 13.227683031543895 - 0.9439100251744846'\n",
      "678 - random_66 - lwr_k=600 - 7.222335750627547 - 15.672950749532228 - 8.864828267589042 - 11.818521259202978 - 22.56402527496202 - 13.227949031625501 - 0.9439088972416603'\n",
      "679 - random_73 - deep - 7.9652308902646825 - 13.003598683009171 - 9.483937673545293 - 15.766737568876374 - 19.96698984525713 - 13.236397538178737 - 0.9438730727559874'\n",
      "680 - random_57 - deep - 13.81493567483905 - 10.748424102358655 - 7.141529981471006 - 13.18816987536756 - 21.527939323906427 - 13.283871435637309 - 0.9436717669262886'\n",
      "681 - random_85 - lwr_k=300 - 11.418714232012853 - 10.450714662086453 - 10.69457852271728 - 10.795964863595145 - 23.075183320304898 - 13.286260752803193 - 0.9436616352710574'\n",
      "682 - random_81 - lwr_k=700 - 11.337588285690513 - 11.318832613927428 - 10.15973991158601 - 10.600288696051589 - 23.0192545782572 - 13.2864992810399 - 0.9436606238284064'\n",
      "683 - random_49 - lwr_k=50 - 13.072063385182414 - 10.664943342921784 - 8.426594991380666 - 13.561480356828373 - 20.810595111150484 - 13.306664295500461 - 0.9435751171564706'\n",
      "684 - random_15 - lwr_k=700 - 7.476424722247368 - 9.981626042080801 - 7.320133887695016 - 11.248881049017688 - 30.521458419082965 - 13.308204683385545 - 0.9435685853763025'\n",
      "685 - random_19 - lwr_k=20 - 11.410645800099447 - 11.81800123291145 - 9.934125879465212 - 15.96354539273992 - 17.4532852059261 - 13.315363440991902 - 0.9435382297552186'\n",
      "686 - random_93 - deep - 8.212067184198508 - 8.78585536819433 - 5.852539317125575 - 9.974807312017967 - 33.7628589305206 - 13.316047811423505 - 0.9435353279067201'\n",
      "687 - random_23 - lwr_k=20 - 9.59166412502929 - 11.571789042547936 - 15.462007107578653 - 13.3700384599363 - 16.59609972289527 - 13.317423476399254 - 0.943529494488894'\n",
      "688 - random_85 - lwr_k=400 - 11.728438590478659 - 10.658293263467879 - 10.734872820030594 - 10.334991067179944 - 23.140564854931092 - 13.318735847335452 - 0.9435239295798622'\n",
      "689 - random_50 - lwr_k=50 - 8.826257640788233 - 12.669240095781662 - 12.045224678435838 - 12.119654971222753 - 20.940282168253304 - 13.3192894733517 - 0.9435215820130782'\n",
      "690 - random_84 - lr - 14.351277838677715 - 10.19709704234415 - 8.852499390259906 - 9.859504838092878 - 23.435612158715426 - 13.338849469988375 - 0.9434386408270578'\n",
      "691 - random_18 - lwr_k=50 - 14.934388474284784 - 12.685895799035595 - 8.514738535974365 - 10.784080178125935 - 19.797462365227776 - 13.343465953800075 - 0.9434190653307171'\n",
      "692 - random_94 - lr - 8.204159196882742 - 11.886336904633021 - 7.635928221875999 - 12.377005682095108 - 26.625290635944374 - 13.34466323878976 - 0.9434139884261095'\n",
      "693 - random_69 - lwr_k=700 - 7.6619524132453325 - 10.789422352099555 - 7.08945232898741 - 13.196140955554625 - 28.018147192624635 - 13.3496720297809 - 0.9433927494109373'\n",
      "694 - random_19 - lwr_k=400 - 10.167311717016005 - 9.975768407665356 - 8.247092444678362 - 13.068971869351774 - 25.373172918621076 - 13.365384406980148 - 0.9433261234690052'\n",
      "695 - random_55 - deep - 11.913200824038292 - 9.534548206142045 - 6.238147298202077 - 17.441172372113478 - 21.712506045092333 - 13.367049252273334 - 0.9433190640666789'\n",
      "696 - random_24 - lr - 7.527457534068296 - 9.765996397702335 - 8.681436657403292 - 12.486952035174147 - 28.431123300605023 - 13.377043531733992 - 0.9432766847266064'\n",
      "697 - random_18 - lwr_k=40 - 14.939916060117097 - 12.662460668227771 - 8.600621941093948 - 10.795131310863498 - 20.121991390788935 - 13.42414779283294 - 0.9430769462831531'\n",
      "698 - random_85 - lwr_k=500 - 11.995576118280967 - 10.723027941136557 - 10.85352050881508 - 10.30564775494773 - 23.253532244263706 - 13.425583998838809 - 0.9430708562703729'\n",
      "699 - random_18 - lwr_k=100 - 14.77487722446705 - 12.52774597049639 - 8.366969320482095 - 10.814301743787594 - 20.64959184833968 - 13.42677078115955 - 0.9430658239007331'\n",
      "700 - random_61 - lwr_k=300 - 9.862674307030481 - 13.423781846308604 - 9.168206741454844 - 13.778129517526057 - 20.995128917276542 - 13.444994006810335 - 0.9429885510884383'\n",
      "701 - random_72 - deep - 10.425143336313251 - 11.89836512532835 - 9.157874342459914 - 13.392998635231912 - 22.397916156478246 - 13.453708693395594 - 0.9429515979088481'\n",
      "702 - random_45 - lwr_k=100 - 14.077322202257387 - 10.496679568820845 - 8.305379000518762 - 11.487761866660419 - 22.92494275433607 - 13.458033447867649 - 0.9429332593250281'\n",
      "703 - random_23 - lwr_k=200 - 9.187437284562904 - 10.527006317117582 - 13.85133928960216 - 12.404240012377116 - 21.342309525768197 - 13.461285793340956 - 0.9429194682495022'\n",
      "704 - random_2 - lwr_k=400 - 11.28685886407387 - 12.655559328813107 - 9.835441242187624 - 8.663201790128703 - 24.92838053904265 - 13.473396235850206 - 0.9428681157629141'\n",
      "705 - random_50 - lwr_k=100 - 8.548973958939033 - 12.617518669142145 - 11.71370899073422 - 12.26652902407026 - 22.30125782264026 - 13.488645883254499 - 0.9428034519561865'\n",
      "706 - random_65 - deep - 15.17693110027407 - 6.344937910620008 - 6.691614983610986 - 11.243705050853507 - 28.003481286163705 - 13.491239822392176 - 0.942792452874068'\n",
      "707 - random_61 - lwr_k=200 - 9.861391471863294 - 13.48330622045528 - 9.608636422805574 - 13.810313026753285 - 20.783777605816535 - 13.508883300249984 - 0.942717638272331'\n",
      "708 - random_34 - lwr_k=50 - 8.918953677960468 - 15.20906690566024 - 7.613560695920708 - 16.10132057761597 - 19.77736673920374 - 13.523575565210507 - 0.9426553379609461'\n",
      "709 - random_18 - lwr_k=20 - 15.312885021540348 - 13.00935630473737 - 8.883494756974324 - 11.414701131962405 - 19.038534685638286 - 13.532000480148595 - 0.94261961339258'\n",
      "710 - random_82 - lwr_k=200 - 7.752256316103614 - 9.60344864189602 - 11.33655118606618 - 13.70856065965393 - 25.36993028371075 - 13.552552464174004 - 0.9425324658351544'\n",
      "711 - random_61 - lwr_k=500 - 9.848495802352797 - 13.719053075386064 - 8.976719443281365 - 13.959870416112206 - 21.323487943286363 - 13.564941825064531 - 0.9424799306376601'\n",
      "712 - random_85 - lwr_k=600 - 12.283551986532455 - 10.826870165160154 - 11.008734828194607 - 10.326365048680481 - 23.449461009169116 - 13.578333831869713 - 0.9424231438729052'\n",
      "713 - random_61 - lwr_k=700 - 9.858090283951492 - 13.762456692126023 - 8.952289824866261 - 13.851755528158957 - 21.507176846524423 - 13.585772181171908 - 0.9423916026858274'\n",
      "714 - random_69 - lwr_k=800 - 7.812680950713578 - 10.948917544276478 - 7.123919222600127 - 13.621588210434918 - 28.429398801079767 - 13.585923345788773 - 0.9423909616952976'\n",
      "715 - random_19 - lwr_k=500 - 10.340965377293367 - 10.122385392066805 - 8.244762775167045 - 13.225332448156264 - 26.00988534598092 - 13.587566876580743 - 0.9423839925533453'\n",
      "716 - random_18 - lwr_k=200 - 14.789419322943115 - 12.871406244614539 - 8.435574462799119 - 10.914009343705406 - 20.945275389373677 - 13.59121531385668 - 0.9423685219108698'\n",
      "717 - random_18 - lwr_k=30 - 15.202501654600473 - 12.953017468051298 - 8.61687010261267 - 10.94463500047254 - 20.282663912446075 - 13.600094110994624 - 0.9423308727241785'\n",
      "718 - random_61 - lwr_k=400 - 9.902623200333716 - 13.742329774324036 - 9.11522050281014 - 13.996805610774636 - 21.285056231713554 - 13.607822184156054 - 0.9422981030072114'\n",
      "719 - random_65 - lr - 15.29535714779166 - 6.3373787694721235 - 6.657176480662926 - 10.922004838000737 - 28.884090533847495 - 13.618283645458076 - 0.9422537427742311'\n",
      "720 - random_61 - lwr_k=600 - 9.835405952366562 - 13.763599611447685 - 9.002358921671366 - 14.088104992091575 - 21.423980403588832 - 13.622092895115037 - 0.9422375902313517'\n",
      "721 - random_81 - lwr_k=500 - 11.230429592139044 - 12.276271207862555 - 9.877609779631275 - 11.812061244316507 - 22.960264477859948 - 13.630712236009362 - 0.9422010412293366'\n",
      "722 - random_65 - lwr_k=50 - 14.586620947285963 - 6.780642647981692 - 7.0059409126080805 - 11.987588831979005 - 27.91680428507997 - 13.654546252438275 - 0.9420999767134799'\n",
      "723 - random_61 - lwr_k=800 - 9.936900492005247 - 14.026970335479264 - 8.951017845162148 - 13.726943559284642 - 21.642080913879784 - 13.656234128354809 - 0.9420928195327823'\n",
      "724 - random_31 - lr - 8.493892652234177 - 6.811287739067989 - 7.925600059530961 - 26.914406437383327 - 18.261198756462342 - 13.679302775920112 - 0.9419950004469975'\n",
      "725 - random_61 - lwr_k=900 - 9.974686859967012 - 14.126662309144631 - 8.861419740209723 - 13.735140929895495 - 21.793402059537364 - 13.697722806213854 - 0.9419168931145949'\n",
      "726 - random_61 - lwr_k=1000 - 9.958967824495554 - 14.337985623843657 - 8.870649267777951 - 13.57808269672758 - 21.80122988491539 - 13.708871873589892 - 0.9418696171928058'\n",
      "727 - random_94 - deep - 8.312731803731715 - 12.097069780877296 - 7.440156336791392 - 12.823409493588503 - 27.886320496855166 - 13.710788955566503 - 0.9418614882223477'\n",
      "728 - random_48 - lwr_k=200 - 11.707128672440279 - 11.449168337417497 - 7.278469687750762 - 13.418378431703433 - 24.828172770496575 - 13.735556812293451 - 0.9417564637753454'\n",
      "729 - random_23 - lwr_k=300 - 9.420327046520768 - 10.555689081712108 - 13.74839176209068 - 12.562723153871662 - 22.41757962341548 - 13.73971307373193 - 0.9417388397818692'\n",
      "730 - random_85 - lwr_k=700 - 12.443384388047587 - 10.934814144825289 - 11.147211159545073 - 10.315402161418294 - 23.871786002388387 - 13.74184709038532 - 0.9417297908093408'\n",
      "731 - random_81 - lr - 10.504145221243133 - 11.473548424357624 - 9.571391263625577 - 13.590955550348086 - 23.623071380270964 - 13.751717250711776 - 0.9416879378980703'\n",
      "732 - random_19 - lwr_k=600 - 10.587766208787798 - 10.227154548227428 - 8.296537584671778 - 13.337408371426973 - 26.53348272336997 - 13.795360010267549 - 0.9415028774245949'\n",
      "733 - random_10 - lwr_k=100 - 7.92003476838335 - 16.524059014839665 - 6.630047233870264 - 20.190231940978606 - 17.759278367821818 - 13.804211947388149 - 0.9414653421337154'\n",
      "734 - random_69 - lwr_k=900 - 7.8939654939851085 - 11.17315011302092 - 7.159490787859972 - 13.979401855181012 - 28.844320049724463 - 13.808665133392497 - 0.9414464590768503'\n",
      "735 - random_96 - lwr_k=400 - 9.069428102643313 - 8.150073421078893 - 6.496412419819541 - 30.246678515089638 - 15.135809747852141 - 13.817974225583775 - 0.9414069852895354'\n",
      "736 - random_94 - lwr_k=40 - 8.356745945842233 - 23.824879603145337 - 9.642274739995628 - 12.611403489544767 - 14.73027275527333 - 13.833854686969113 - 0.9413396466122177'\n",
      "737 - random_81 - lwr_k=400 - 10.325012638795782 - 10.65112784540837 - 11.878988657879448 - 12.067384099642057 - 24.255133809296577 - 13.834433140622622 - 0.9413371937676202'\n",
      "738 - random_37 - lr - 10.830309883739075 - 11.542655398017905 - 10.26045195570641 - 10.940180635062982 - 25.66183935696223 - 13.846216116339829 - 0.9412872298540782'\n",
      "739 - random_45 - lwr_k=10 - 15.688894297441328 - 11.463210479060542 - 8.24922670392606 - 13.746564682003344 - 20.123445885937226 - 13.854177295862854 - 0.9412534716850955'\n",
      "740 - random_18 - lwr_k=300 - 14.938346461130154 - 13.33221058376189 - 8.544432177953238 - 10.87061839969454 - 21.611321467754863 - 13.859476170999624 - 0.9412310026122945'\n",
      "741 - random_75 - lwr_k=40 - 11.226219970329016 - 13.772743671437809 - 9.52676166738345 - 15.580227444288889 - 19.25011503669675 - 13.870764325429462 - 0.9411831369130419'\n",
      "742 - random_56 - lwr_k=300 - 7.902016979838652 - 13.086406665632072 - 6.795371137248732 - 13.587831529893505 - 28.131584689759457 - 13.899526618231896 - 0.9410611747919837'\n",
      "743 - random_23 - lwr_k=400 - 9.659015342734216 - 10.626487150604701 - 13.768384730540062 - 12.657985374848266 - 22.953839094889474 - 13.931901012018889 - 0.9409238961069583'\n",
      "744 - random_85 - lwr_k=800 - 12.737565747051267 - 10.94783953691846 - 11.319246488227938 - 10.292072762974687 - 24.436051743379828 - 13.945866258620628 - 0.9408646786061718'\n",
      "745 - random_24 - deep - 8.213547367901342 - 10.079688495192707 - 8.843384348301493 - 13.541834973293088 - 29.180876710783938 - 13.970286214924116 - 0.9407611296274978'\n",
      "746 - random_18 - lwr_k=400 - 14.508160237939913 - 13.62255899189817 - 8.590779038425538 - 10.928076881025932 - 22.22570635198679 - 13.975085873962382 - 0.9407407772785529'\n",
      "747 - random_19 - lwr_k=700 - 10.814352891482747 - 10.363679511119509 - 8.312780291337214 - 13.46287278960401 - 26.945101919281353 - 13.978647036392948 - 0.940725676711764'\n",
      "748 - random_12 - deep - 14.035210795176205 - 13.461898888776814 - 10.60095466032661 - 12.892193795226813 - 18.926347332641203 - 13.98324422720537 - 0.9407061831505248'\n",
      "749 - random_20 - lwr_k=100 - 27.4284317196568 - 7.313833894898295 - 7.489877288767809 - 10.849349252735093 - 17.0156949065852 - 14.020535069445211 - 0.9405480568887165'\n",
      "750 - random_0 - deep - 10.707637811838703 - 14.454446158900783 - 8.183836109027034 - 13.639974670269565 - 23.124657241171448 - 14.021638737964489 - 0.9405433770769137'\n",
      "751 - random_69 - lwr_k=1000 - 7.9902917841970735 - 11.397023235029602 - 7.18278371503301 - 14.321982310552881 - 29.23221106265499 - 14.023439983336681 - 0.9405357390438865'\n",
      "752 - random_50 - lwr_k=200 - 8.913134461840285 - 12.678940929585135 - 11.443830241843083 - 12.17492959190715 - 24.94551683718353 - 14.030210896271816 - 0.9405070280190477'\n",
      "753 - random_27 - lwr_k=20 - 9.823230240231679 - 15.558370223523447 - 9.320241918719695 - 13.002930945262635 - 22.452018676521778 - 14.030919377380757 - 0.9405040238128334'\n",
      "754 - random_48 - lwr_k=500 - 11.179155712881595 - 11.588371325007104 - 7.225258437767713 - 13.58919325171341 - 26.640512975989807 - 14.04362696853447 - 0.9404501392083865'\n",
      "755 - random_48 - lwr_k=600 - 10.669220356824873 - 11.76833245698722 - 7.258352508074451 - 13.626523804114385 - 26.90707213968946 - 14.044974389753035 - 0.9404444256739717'\n",
      "756 - random_8 - deep - 10.041625729949502 - 10.88432570799283 - 9.162887204191316 - 11.690284599543203 - 28.5557421825637 - 14.065792701911594 - 0.9403561489194905'\n",
      "757 - random_79 - lwr_k=50 - 12.87450309849794 - 10.649270154119248 - 13.388568200733658 - 14.05968008096221 - 19.40154045917837 - 14.07395496435331 - 0.9403215379622022'\n",
      "758 - random_31 - deep - 8.74330190355766 - 6.616497465904723 - 7.975137802345844 - 28.445882393433166 - 18.65024067318703 - 14.084114149406338 - 0.9402784596125944'\n",
      "759 - random_81 - lwr_k=300 - 12.14096555005491 - 11.99028053063024 - 11.083516583935092 - 12.49773191510438 - 22.762017923409832 - 14.094237925725864 - 0.9402355311543538'\n",
      "760 - random_48 - lwr_k=700 - 10.679945870691192 - 11.679819918650123 - 7.307754825114566 - 13.615104133411833 - 27.201565762515443 - 14.09588281954596 - 0.9402285562326884'\n",
      "761 - random_23 - lwr_k=500 - 9.844210885674013 - 10.713547377832231 - 13.840483487439327 - 12.741438792314398 - 23.350642788505585 - 14.09681390966911 - 0.9402246080868609'\n",
      "762 - random_20 - lwr_k=500 - 27.406710852210928 - 7.230341345637613 - 7.4526370751832784 - 11.044591236850254 - 17.41381442690538 - 14.110669882105016 - 0.9401658539465282'\n",
      "763 - random_82 - lr - 8.475580023661058 - 11.692064267003353 - 15.307467290146835 - 15.511395679908835 - 19.60134814641618 - 14.116250056722455 - 0.9401421920661317'\n",
      "764 - random_24 - lwr_k=1000 - 17.57302064208674 - 8.67231065639843 - 8.733222211256912 - 11.248768300400291 - 24.395337517447484 - 14.124203761304354 - 0.9401084655934981'\n",
      "765 - random_33 - deep - 14.280725569654798 - 14.57955609679027 - 7.901298767030483 - 14.635686588521672 - 19.227592967359087 - 14.125072120712169 - 0.9401047835750688'\n",
      "766 - random_20 - lwr_k=600 - 27.452696162877874 - 7.263508801149449 - 7.472784273435822 - 11.078435015823333 - 17.37859105963734 - 14.13026050467398 - 0.9400827828959073'\n",
      "767 - random_20 - lwr_k=40 - 27.59090797608094 - 7.517800625437904 - 7.613130170975725 - 10.938361485540018 - 16.992231335277506 - 14.13160761159908 - 0.9400770706942061'\n",
      "768 - random_20 - lwr_k=400 - 27.508519567351076 - 7.243193499809053 - 7.52593358770766 - 10.985760958115895 - 17.393146515719042 - 14.13237339188033 - 0.9400738235195804'\n",
      "769 - random_19 - lwr_k=800 - 10.966847478716153 - 10.446427417934322 - 8.358006255786234 - 13.505050619783594 - 27.44531913239473 - 14.143204360476764 - 0.9400278964472079'\n",
      "770 - random_20 - lwr_k=300 - 27.626696405904084 - 7.2098870623674784 - 7.499578346481618 - 10.956995927802962 - 17.495151122626414 - 14.158729606657305 - 0.9399620640058537'\n",
      "771 - random_20 - lwr_k=200 - 27.64478983421363 - 7.226361213623306 - 7.457482963947469 - 10.970308692376092 - 17.494169035282862 - 14.15969552724876 - 0.9399579681667314'\n",
      "772 - random_10 - lwr_k=500 - 8.191094699296293 - 17.054921990464052 - 6.3601598687927225 - 19.674676523204113 - 19.521451237537537 - 14.15995735885846 - 0.9399568579096771'\n",
      "773 - random_29 - lwr_k=20 - 10.505791944532788 - 10.02737445382892 - 12.044922380249504 - 13.533955182441986 - 24.704418003816095 - 14.162016247830977 - 0.9399481275046349'\n",
      "774 - random_20 - lwr_k=700 - 27.467829082982277 - 7.302280017503927 - 7.507835761509235 - 11.11002924870578 - 17.42056097170314 - 14.16276264036617 - 0.9399449625407927'\n",
      "775 - random_45 - lwr_k=200 - 13.866599760619305 - 10.438827216020215 - 8.894045375620717 - 11.662620768186002 - 25.959032348320918 - 14.163566337701216 - 0.9399415545846765'\n",
      "776 - random_85 - lwr_k=900 - 13.048916878189873 - 11.104082593559733 - 11.487523152529329 - 10.25072723663021 - 24.935771655773955 - 14.164720201627938 - 0.9399366618004719'\n",
      "777 - random_91 - lwr_k=500 - 7.760433501707437 - 11.932283501418997 - 6.518195383652659 - 9.539518558964355 - 35.09204617362679 - 14.167079954110722 - 0.9399266556295469'\n",
      "778 - random_40 - deep - 15.763469237157445 - 9.505746225678589 - 7.892708075427306 - 11.310814607153166 - 26.37290060119879 - 14.168625286156745 - 0.9399201030121661'\n",
      "779 - random_82 - lwr_k=100 - 9.02671416302024 - 11.239752333412802 - 14.321010987037168 - 16.616403170288564 - 19.66820008909189 - 14.173092689293778 - 0.9399011595419672'\n",
      "780 - random_40 - lwr_k=300 - 9.83939684333714 - 7.148098693672823 - 9.178007289697225 - 23.253304785711624 - 21.461908606436047 - 14.174282298797902 - 0.9398961151840869'\n",
      "781 - random_48 - lwr_k=800 - 10.764034021096968 - 11.810835416963576 - 7.327169007815671 - 13.614177547295355 - 27.40180053210415 - 14.182654829998318 - 0.9398606127409804'\n",
      "782 - random_88 - deep - 11.06975191431623 - 16.335735538001927 - 9.697478103403377 - 13.215554891308246 - 20.6559527582267 - 14.194733308907097 - 0.9398093959191548'\n",
      "783 - random_20 - lwr_k=800 - 27.564448473851538 - 7.30591872714775 - 7.521064104941824 - 11.12730007511028 - 17.4507277666284 - 14.194953330034062 - 0.9398084628250993'\n",
      "784 - random_18 - lwr_k=500 - 14.537046335906833 - 13.693955086253473 - 8.629335415839325 - 11.152629026196308 - 23.002940522148 - 14.20315256249355 - 0.939773695228904'\n",
      "785 - random_23 - lwr_k=600 - 9.938683233720822 - 10.74096803500409 - 13.83339186266158 - 12.852678185530662 - 23.673817244721192 - 14.206640942365999 - 0.9397589032854653'\n",
      "786 - random_20 - lwr_k=900 - 27.61842983873806 - 7.296895190200026 - 7.52634345203259 - 11.142897534470519 - 17.491965555993808 - 14.216368163378268 - 0.9397176564865816'\n",
      "787 - random_20 - lwr_k=50 - 27.37611698003408 - 8.371464141347436 - 7.533428676669904 - 10.919907261549566 - 16.896565815513394 - 14.22069333087212 - 0.9396993162727083'\n",
      "788 - random_94 - lwr_k=30 - 9.354228280897829 - 18.79450352340568 - 11.76400498050584 - 15.287813423594718 - 15.951249745605647 - 14.230308903823508 - 0.9396585429004185'\n",
      "789 - random_20 - lwr_k=1000 - 27.720434172026167 - 7.286565526662689 - 7.534135394668913 - 11.14640019474399 - 17.506489724178593 - 14.239874167298671 - 0.9396179828577986'\n",
      "790 - random_27 - lwr_k=30 - 9.695272707500356 - 15.090199467170947 - 9.470007425823887 - 12.95057634035561 - 24.011040364473644 - 14.24281317520482 - 0.9396055204424932'\n",
      "791 - random_27 - lwr_k=40 - 9.47260845362992 - 14.720112968324871 - 9.458950521141553 - 12.691507555239966 - 24.87851531990274 - 14.243635515718328 - 0.9396020334328188'\n",
      "792 - random_10 - lwr_k=600 - 8.20439844027993 - 17.194874288287842 - 6.358755556415807 - 19.920755564267413 - 19.562957190980015 - 14.247841015695208 - 0.9395842006508214'\n",
      "793 - random_67 - lwr_k=40 - 11.35146220846562 - 12.864101060020324 - 10.330238635183969 - 13.240747548239124 - 23.461123327926018 - 14.248833146790089 - 0.9395799936700522'\n",
      "794 - random_66 - lr - 7.530877537423047 - 16.238730456022168 - 13.888237159958704 - 11.827761087862108 - 21.87870175921264 - 14.272079527849662 - 0.9394814209324603'\n",
      "795 - random_23 - lwr_k=700 - 10.061896150611023 - 10.82350054329265 - 13.838359081948667 - 12.861804146186273 - 23.824236227891696 - 14.280701898765157 - 0.9394448590821021'\n",
      "796 - random_48 - lwr_k=900 - 10.860688848266113 - 11.890375400699126 - 7.376998087905881 - 13.580284146630698 - 27.72450737692138 - 14.285617427186128 - 0.9394240155326513'\n",
      "797 - random_20 - lwr_k=30 - 28.1097796727263 - 7.668059130777823 - 7.721806180938657 - 10.97285704268406 - 16.952271957492826 - 14.286135070251207 - 0.9394218205460902'\n",
      "798 - random_19 - lwr_k=900 - 11.134050729462059 - 10.572909287648368 - 8.391717662705776 - 13.610600723680141 - 27.73149625982097 - 14.287030100528503 - 0.9394180253065458'\n",
      "799 - random_48 - lwr_k=1000 - 10.96172414265859 - 11.888281132063872 - 7.382152032101576 - 13.631657330650064 - 27.742012637872246 - 14.320216981901526 - 0.9392773013916841'\n",
      "800 - random_10 - lwr_k=700 - 8.199165655860503 - 17.224860689616822 - 6.3600200306097925 - 20.184995188541535 - 19.645548866754435 - 14.322390526128087 - 0.9392680848085032'\n",
      "801 - random_21 - deep - 11.076382857102613 - 14.041427347742008 - 9.081903980170772 - 11.820969692514531 - 25.675538370786974 - 14.338661524813547 - 0.9391990902465395'\n",
      "802 - random_23 - lwr_k=800 - 10.134424070396898 - 10.885885743813102 - 13.885214453201835 - 12.926784114616455 - 24.056704126228354 - 14.376535873979954 - 0.9390384897092944'\n",
      "803 - random_85 - lwr_k=1000 - 13.277347453718638 - 11.199573222946015 - 11.711854493057437 - 10.263013756593953 - 25.43985775490278 - 14.377628544038782 - 0.9390338564083643'\n",
      "804 - random_66 - deep - 7.838620371592221 - 18.411629753300094 - 14.036441267171323 - 12.029402654352586 - 19.600798533452913 - 14.382966583521727 - 0.9390112213948008'\n",
      "805 - random_50 - lwr_k=300 - 9.169655674631477 - 13.069451690568986 - 11.267503751024247 - 12.411631228303708 - 26.06701991412474 - 14.3959790936926 - 0.9389560436980309'\n",
      "806 - random_18 - lwr_k=600 - 14.567702698373367 - 13.72613014884201 - 8.684069491556185 - 11.301430313235066 - 23.73764398528335 - 14.403311332401877 - 0.9389249524567578'\n",
      "807 - random_20 - lwr_k=20 - 28.8043367259945 - 7.630975147015432 - 7.86297988122148 - 11.167371733116868 - 16.676526277877468 - 14.429678895564704 - 0.9388131448219296'\n",
      "808 - random_10 - lwr_k=800 - 8.221298894031742 - 17.314777985721648 - 6.364390490207206 - 20.491212748996574 - 19.768246670116916 - 14.431440426778975 - 0.9388056753171624'\n",
      "809 - random_19 - lwr_k=1000 - 11.286994469458053 - 10.668970726967759 - 8.429974805014142 - 13.681125313351925 - 28.17062198429383 - 14.446401204745387 - 0.9387422364172806'\n",
      "810 - random_76 - deep - 14.123920518715916 - 15.398745425984433 - 7.884998857340395 - 12.524327021949512 - 22.322185216430363 - 14.450937148710258 - 0.9387230025681852'\n",
      "811 - random_18 - lwr_k=10 - 16.482404282086062 - 14.123563339178721 - 9.627076928705568 - 12.027527319265015 - 19.993981846267758 - 14.45118979110047 - 0.9387219311463223'\n",
      "812 - random_7 - lwr_k=50 - 10.936142216376853 - 14.118720320218253 - 10.672342425990474 - 14.024109334572367 - 22.514880672705402 - 14.452608305344642 - 0.9387159161527625'\n",
      "813 - random_37 - lwr_k=10 - 16.222316758652692 - 12.704129146435477 - 8.50232107859232 - 12.510295979106616 - 22.335874178780664 - 14.454990125390427 - 0.9387058164076985'\n",
      "814 - random_23 - lwr_k=900 - 10.203463791393565 - 10.917600955361916 - 13.916674028264953 - 13.033685277139126 - 24.222361558554784 - 14.45748048066628 - 0.9386952564355253'\n",
      "815 - random_20 - lr - 28.175255689208125 - 7.3147861104222125 - 7.636237161358113 - 11.04903025527226 - 18.167501795038383 - 14.469635219681646 - 0.938643716116351'\n",
      "816 - random_45 - lwr_k=300 - 13.958361138197558 - 10.620390092255294 - 8.960641830532158 - 11.76888739564058 - 27.046671425611656 - 14.470275912795502 - 0.9386409993548032'\n",
      "817 - random_80 - lwr_k=200 - 9.831458503561022 - 6.864820780910996 - 6.751130460214788 - 32.44363481523343 - 16.5926267872338 - 14.494720647479868 - 0.9385373451812193'\n",
      "818 - random_85 - lwr_k=20 - 13.970587778941612 - 11.016196207170616 - 13.246489289445538 - 16.33481287527588 - 17.93885690654068 - 14.500731006556439 - 0.9385118591691571'\n",
      "819 - random_49 - deep - 12.358104440467448 - 10.970255800237828 - 7.192526347123629 - 11.924549981475755 - 30.118010431113152 - 14.51175635822424 - 0.9384651079758247'\n",
      "820 - random_27 - lwr_k=10 - 11.218035729819077 - 16.222729698627422 - 9.913022909701315 - 13.910782231270371 - 21.338555822640828 - 14.5203632045411 - 0.9384286118243196'\n",
      "821 - random_23 - lwr_k=1000 - 10.269590751797878 - 10.964790397544483 - 13.98576966773716 - 13.030926797106789 - 24.37638320199264 - 14.524212221664293 - 0.9384122906535576'\n",
      "822 - random_10 - lwr_k=900 - 8.229662319377296 - 17.44085955981272 - 6.366712333148237 - 20.886909079779638 - 19.798178451243565 - 14.54390459645055 - 0.93832878813816'\n",
      "823 - random_61 - lwr_k=100 - 10.39753108087964 - 15.71948190089488 - 12.76075232934323 - 13.368414610809415 - 20.607620153555047 - 14.570264762591377 - 0.9382170118692769'\n",
      "824 - random_45 - lwr_k=400 - 14.01793453146845 - 10.694730170125402 - 8.842070867565948 - 11.767642024232366 - 27.560012564640495 - 14.575750949346563 - 0.9381937485300924'\n",
      "825 - random_18 - lwr_k=700 - 14.684715197582003 - 13.71574476906032 - 8.715076779813465 - 11.456701416513036 - 24.323309166946864 - 14.578985385589275 - 0.9381800333959306'\n",
      "826 - random_56 - lwr_k=800 - 7.7162286883560585 - 13.237219906129024 - 6.753868798539222 - 13.484633258309366 - 31.837396851973285 - 14.60451723434293 - 0.938071769480744'\n",
      "827 - random_33 - lwr_k=50 - 8.652016636523815 - 27.53016215637428 - 6.386649854819446 - 12.805414848159996 - 17.653382348035933 - 14.606666666810423 - 0.9380626551398044'\n",
      "828 - random_45 - lwr_k=500 - 14.08034376624102 - 10.719716495795133 - 8.849154668300438 - 11.809873867730373 - 27.58348916946062 - 14.607792329885905 - 0.9380578819370133'\n",
      "829 - random_27 - lwr_k=50 - 9.478412628324666 - 14.77281486990657 - 9.289321718082286 - 12.732305035270842 - 26.942839685240514 - 14.642314315434978 - 0.9379114967162792'\n",
      "830 - random_10 - lwr_k=1000 - 8.25714166396479 - 17.562576472980123 - 6.349176232046617 - 21.229844548081306 - 19.826730493516436 - 14.644525604792278 - 0.9379021200806213'\n",
      "831 - random_45 - lwr_k=600 - 14.302346900426024 - 10.736621941504497 - 8.909684173377977 - 11.840759250518047 - 27.556942679992844 - 14.668566949023644 - 0.9378001764228021'\n",
      "832 - random_15 - lwr_k=50 - 7.332482043395689 - 10.557547748116662 - 7.165296245868163 - 11.847326306614429 - 36.66573836850785 - 14.711788945432687 - 0.9376169001313526'\n",
      "833 - random_31 - lwr_k=40 - 11.432763883661654 - 9.750249217833609 - 10.510911494499084 - 23.00861567057266 - 18.866616026335933 - 14.712481225926965 - 0.9376139646213768'\n",
      "834 - random_95 - deep - 13.887281623877792 - 12.686402309155504 - 8.705236733301462 - 10.955745172539663 - 27.346525450010557 - 14.715770223104796 - 0.937600018262563'\n",
      "835 - random_50 - lwr_k=400 - 9.433936246735025 - 13.335466541653053 - 11.53272843082534 - 12.529954360779742 - 26.83762763956667 - 14.732845790710813 - 0.937527611786694'\n",
      "836 - random_45 - lwr_k=700 - 14.609387194955415 - 10.754582838619726 - 8.914282005451575 - 11.888755643091562 - 27.54998229945059 - 14.74272289790882 - 0.9374857294182729'\n",
      "837 - random_10 - lwr_k=50 - 8.856856817606761 - 17.581028502175094 - 7.47953628217612 - 21.09996061831225 - 18.72153631002254 - 14.747283021255484 - 0.9374663929098983'\n",
      "838 - random_15 - lwr_k=800 - 7.46666737741616 - 10.135358579311788 - 7.306785930691148 - 11.265724597451907 - 37.65937344813583 - 14.764828238768311 - 0.9373919950878266'\n",
      "839 - random_45 - lwr_k=800 - 14.73881873202259 - 10.760822696974289 - 8.91633061712725 - 11.914915793795808 - 27.51085065078523 - 14.767686644601277 - 0.9373798744533337'\n",
      "840 - random_18 - lwr_k=800 - 14.862417935948926 - 13.767807197504036 - 8.732435513350298 - 11.553239016340475 - 24.990294237807646 - 14.781086127099247 - 0.9373230559890423'\n",
      "841 - random_45 - lwr_k=900 - 14.788217066481765 - 10.772751494595225 - 8.915352000896963 - 11.97328859252615 - 27.483033751466802 - 14.785871615645435 - 0.9373027638538707'\n",
      "842 - random_23 - lwr_k=10 - 11.031959415216617 - 12.272840866536937 - 17.466174254558478 - 14.99152328095804 - 18.1773643750933 - 14.786945560366371 - 0.9372982099548834'\n",
      "843 - random_56 - lwr_k=900 - 7.702522077035282 - 13.33187718396483 - 6.720966539907556 - 13.532087221739788 - 32.65955533482313 - 14.788002555056917 - 0.9372937279299178'\n",
      "844 - random_40 - lwr_k=500 - 7.997951974644008 - 7.019702277593196 - 7.191924512860656 - 32.336205149817864 - 19.542184842644218 - 14.815200181305539 - 0.937178400539024'\n",
      "845 - random_45 - lwr_k=1000 - 14.842232299792048 - 10.792758363700308 - 9.039984098826293 - 12.025900489330867 - 27.433630792139343 - 14.826243142316962 - 0.9371315745451101'\n",
      "846 - random_4 - lr - 8.924427738178636 - 12.63443050299255 - 10.668427346934518 - 14.185556945902427 - 27.90508352321862 - 14.862247678748163 - 0.9369789027932081'\n",
      "847 - random_27 - lwr_k=100 - 9.224642922081234 - 14.680686660892794 - 9.20572560980292 - 12.618836035059228 - 28.704815682421778 - 14.885980426888478 - 0.9368782676901023'\n",
      "848 - random_50 - lwr_k=500 - 9.599345913658441 - 13.341697987574008 - 11.547679170198952 - 12.608741683382608 - 27.47532321614251 - 14.91342969648586 - 0.9367618732439227'\n",
      "849 - random_18 - lwr_k=900 - 15.022979684263765 - 13.820640335743704 - 8.739994696783512 - 11.57146802384831 - 25.4456742795246 - 14.919988200694304 - 0.9367340629059318'\n",
      "850 - random_33 - lwr_k=40 - 8.715120886283039 - 26.0861200726584 - 7.853661607174796 - 13.345063217048603 - 18.64303668432349 - 14.929410062839725 - 0.9366941109348053'\n",
      "851 - random_15 - lwr_k=900 - 7.465352046876013 - 10.14337827734161 - 7.31874528971574 - 11.259716550234169 - 38.675388000160176 - 14.970496006380932 - 0.9365198922501412'\n",
      "852 - random_18 - lwr_k=1000 - 15.096272965182148 - 13.874032410820222 - 8.747574784288098 - 11.594620179856296 - 25.7095285028229 - 15.004235716823638 - 0.9363768241478259'\n",
      "853 - random_50 - lwr_k=600 - 9.758189293933837 - 13.330147396379354 - 11.544006664441326 - 12.677764575225147 - 27.864220293008128 - 15.03372246552455 - 0.9362517900818919'\n",
      "854 - random_15 - lwr_k=1000 - 7.470708204526015 - 10.343670578414969 - 7.330269572753818 - 11.337897247346007 - 38.82209578423723 - 15.058912970795713 - 0.9361449735817434'\n",
      "855 - random_81 - deep - 10.839223532356725 - 11.438874458525262 - 9.923302851184092 - 14.3276319013669 - 28.793481295661394 - 15.063217302114161 - 0.9361267218563124'\n",
      "856 - random_56 - lwr_k=1000 - 7.663210158057209 - 12.82285047121338 - 6.7049585702680226 - 13.656946452554209 - 34.562821546641096 - 15.080572659289896 - 0.9360531289722627'\n",
      "857 - random_46 - lwr_k=800 - 8.94933556580259 - 9.07019989900023 - 8.784760021268243 - 10.855906771872547 - 37.87473497745754 - 15.104990650055608 - 0.935949588202192'\n",
      "858 - random_50 - lwr_k=700 - 9.910316325000414 - 13.39751531392044 - 11.4793563094934 - 12.784769414271034 - 28.147724479375572 - 15.142793410970238 - 0.9357892913533046'\n",
      "859 - random_50 - lwr_k=10 - 11.324284319330207 - 15.969189179487808 - 15.39748877882895 - 14.933426308345647 - 18.107191464406544 - 15.145824908307876 - 0.9357764367506546'\n",
      "860 - random_86 - deep - 14.618445510364788 - 12.008941018249711 - 10.839531906309135 - 12.146646476006723 - 26.38024088635394 - 15.198143793148613 - 0.935554586626637'\n",
      "861 - random_28 - lr - 9.236445336770018 - 9.40355427945216 - 6.960654263304977 - 17.79669708785487 - 32.91985489036097 - 15.261494735863835 - 0.9352859564677347'\n",
      "862 - random_50 - lwr_k=800 - 10.016927644649922 - 13.52687659301087 - 11.479267570053977 - 12.861805543287677 - 28.526667441920033 - 15.281159324774196 - 0.935202571774111'\n",
      "863 - random_46 - lwr_k=1000 - 9.04086023673092 - 9.144759368318773 - 8.825889866205937 - 10.879526863403699 - 38.67709929845315 - 15.311589852240933 - 0.9350735357580933'\n",
      "864 - random_79 - lr - 14.313456569129814 - 7.339086150240868 - 11.7120089181538 - 16.084789726844857 - 27.314154888652514 - 15.351216876970534 - 0.934905503396402'\n",
      "865 - random_75 - lwr_k=50 - 10.443662696782166 - 11.890369680018477 - 22.459263225750256 - 12.992979864956505 - 19.099176610972496 - 15.375711645685403 - 0.934801637061134'\n",
      "866 - random_50 - lwr_k=900 - 10.123623351952917 - 13.59793089341608 - 11.535578877098747 - 12.916699804140428 - 28.789908754198624 - 15.391591640168736 - 0.9347343003636431'\n",
      "867 - random_14 - lwr_k=50 - 12.266287873922911 - 14.283019890373842 - 13.677604580053961 - 14.702754623228504 - 22.067763786448285 - 15.398790283024704 - 0.9347037756152343'\n",
      "868 - random_45 - lr - 15.039823075452805 - 11.027893717566284 - 10.462825961699263 - 12.853808217930744 - 28.004259199950912 - 15.476921686185955 - 0.9343724713024575'\n",
      "869 - random_80 - deep - 11.848765964789008 - 14.037151750292988 - 12.375476183410944 - 11.848667316803866 - 27.40291676376805 - 15.501757385813377 - 0.9342671594413597'\n",
      "870 - random_50 - lwr_k=1000 - 10.212867184661482 - 13.7009925184167 - 11.561563220576668 - 13.012298504162572 - 29.067175473054363 - 15.509815453687164 - 0.9342329902923169'\n",
      "871 - random_2 - lr - 14.60046690635387 - 17.182669440302544 - 16.514097658234448 - 9.288678162234161 - 20.0888880598986 - 15.535076832084902 - 0.9341258732654718'\n",
      "872 - random_82 - deep - 8.342797227850133 - 12.141466226593367 - 19.603783872262266 - 17.173684197307903 - 20.464274763377546 - 15.543464267651935 - 0.9340903077628139'\n",
      "873 - random_4 - deep - 8.556454881708673 - 13.020580602355558 - 12.08877030168763 - 14.700136087840937 - 29.458151466625818 - 15.563254508173994 - 0.9340063902628506'\n",
      "874 - random_48 - lr - 11.202068353859458 - 13.277229148982927 - 8.251838426076041 - 15.084524933182884 - 30.039446541335973 - 15.5699304793024 - 0.9339780816839571'\n",
      "875 - random_15 - lr - 7.311187316170873 - 10.64944221079438 - 7.831739232066123 - 11.838939850372862 - 40.542123717654604 - 15.63250720498867 - 0.9337127346114551'\n",
      "876 - random_27 - lwr_k=200 - 9.712382393442343 - 15.371087917510108 - 9.135544448695565 - 12.644428972883022 - 31.369373813861955 - 15.645546699599935 - 0.9336574426209576'\n",
      "877 - random_62 - lwr_k=100 - 12.17995007268059 - 16.11101907850577 - 11.640344418068562 - 12.591212293664771 - 25.828199142143788 - 15.66964568533383 - 0.9335552545431278'\n",
      "878 - random_61 - lr - 10.978055354966056 - 18.90250490720721 - 10.249318122825764 - 14.747092427380474 - 23.632556028332676 - 15.701655941177625 - 0.9334195199295787'\n",
      "879 - random_94 - lwr_k=300 - 7.387799296918681 - 32.75515186945425 - 6.532656638679152 - 10.787186464908489 - 21.344529901192022 - 15.762876332756461 - 0.933159924185239'\n",
      "880 - random_28 - deep - 9.021792635004571 - 9.498723345380368 - 7.059886680090652 - 20.28872149621337 - 33.140115017777674 - 15.799705734014557 - 0.933003754862151'\n",
      "881 - random_84 - deep - 17.77190507648424 - 12.215879825835533 - 9.365415257571858 - 13.661853450522083 - 26.022885376463943 - 15.807321158882795 - 0.9329714628131793'\n",
      "882 - random_59 - lwr_k=100 - 8.172639516982903 - 9.062898541543534 - 31.59654731901494 - 11.8060629378201 - 18.529182485208747 - 15.831103069519367 - 0.9328706191014738'\n",
      "883 - random_3 - deep - 9.571443411020132 - 12.394262685323113 - 10.314060813378937 - 14.209947725478788 - 32.69996268470008 - 15.836345373595204 - 0.932848390052424'\n",
      "884 - random_27 - lwr_k=300 - 10.072720036116623 - 15.438677729767909 - 9.133871450525929 - 13.054207615738981 - 32.51682772315308 - 16.042184257375187 - 0.9319755614799131'\n",
      "885 - random_37 - deep - 12.193490716712565 - 13.668665629947244 - 11.233125335168488 - 11.0598222238914 - 32.18282595079127 - 16.066558964368948 - 0.9318722043929445'\n",
      "886 - random_22 - deep - 16.736614255390073 - 12.309537467706809 - 8.061898582202309 - 15.120774216577717 - 28.269031366103015 - 16.099054904974818 - 0.9317344103074428'\n",
      "887 - random_48 - deep - 12.170441250559156 - 13.53926678921314 - 8.47447781871324 - 15.752754500808528 - 30.88238018638867 - 16.162780426012915 - 0.9314641919438301'\n",
      "888 - random_14 - lr - 11.327905888662361 - 13.472124448823978 - 9.821685236218679 - 14.606690730058993 - 31.766802003527875 - 16.1977975067203 - 0.9313157072017701'\n",
      "889 - random_18 - lr - 15.460743187736085 - 16.913044426655528 - 9.666402910531408 - 11.970542390609992 - 27.149175178950667 - 16.23196685295218 - 0.9311708172943404'\n",
      "890 - random_83 - lwr_k=40 - 13.025433316350183 - 13.411861395458917 - 12.323089961299617 - 15.379298747781231 - 27.6817770923132 - 16.363261925877328 - 0.9306140805387405'\n",
      "891 - random_33 - lwr_k=20 - 10.877695526638503 - 11.457955947787335 - 7.985776638330835 - 17.09245150667239 - 34.453300984180466 - 16.37173131954833 - 0.9305781673650833'\n",
      "892 - random_27 - lwr_k=400 - 10.33756935851878 - 15.699993101374968 - 9.15875639333425 - 13.259176203905207 - 33.41314282698497 - 16.37262885524287 - 0.9305743615016988'\n",
      "893 - random_75 - deep - 10.883400013325844 - 12.273388917239325 - 8.142829727091621 - 15.596326254509592 - 35.22638329514512 - 16.422878688801067 - 0.9303612848854936'\n",
      "894 - random_23 - lr - 11.131920992506183 - 12.465983772006231 - 17.578825218527584 - 14.9238944559818 - 26.38418044503466 - 16.49542241169108 - 0.9300536741316211'\n",
      "895 - random_25 - lwr_k=100 - 8.80283184765031 - 45.13808000018791 - 7.242823431602813 - 10.44534474107202 - 11.294020352982258 - 16.58802136366229 - 0.9296610223820632'\n",
      "896 - random_56 - lwr_k=600 - 7.811110836791998 - 24.31045482197161 - 6.651444136879301 - 13.17030679133967 - 31.099845849891434 - 16.60845307042442 - 0.9295743848420497'\n",
      "897 - random_14 - deep - 11.789443225205043 - 14.185837790930915 - 9.935165691141414 - 14.95420842362075 - 32.3177764292724 - 16.635291122080933 - 0.9294605823224258'\n",
      "898 - random_27 - lwr_k=500 - 10.600010065170405 - 15.940522517832713 - 9.188003528939197 - 13.308534973377649 - 34.215635732349156 - 16.649434346971834 - 0.9294006099818282'\n",
      "899 - random_7 - lwr_k=40 - 15.235834705819974 - 17.265281013631057 - 13.788426192922723 - 16.637371124556207 - 20.346185662324427 - 16.654487412397096 - 0.9293791832276628'\n",
      "900 - random_69 - lwr_k=30 - 13.506507367838502 - 18.870883378006322 - 14.70691483933146 - 19.282591393393997 - 16.995070161635347 - 16.672235020294238 - 0.9293039271999979'\n",
      "901 - random_20 - deep - 39.16207127797428 - 7.318969784313645 - 7.831019791494521 - 11.125705932028268 - 18.427248875885883 - 16.77512115481405 - 0.9288676543712139'\n",
      "902 - random_16 - lwr_k=100 - 8.947507503358118 - 10.03310657620521 - 7.354173737400432 - 10.543311784565335 - 47.43320155574515 - 16.859845968087672 - 0.9285083915560283'\n",
      "903 - random_49 - lwr_k=40 - 18.040827603571422 - 13.33498349806613 - 9.252410013021453 - 19.168450220470337 - 24.51367995848257 - 16.861685727951823 - 0.9285005903345884'\n",
      "904 - random_27 - lwr_k=600 - 10.836680465421942 - 16.19273822546024 - 9.17553466876641 - 13.279779857456472 - 34.867410916940685 - 16.869329852032216 - 0.9284681765909109'\n",
      "905 - random_41 - deep - 11.182869110482024 - 17.755222688915296 - 8.1350486702259 - 18.33685722515097 - 28.978743638297168 - 16.876959291107994 - 0.9284358252689261'\n",
      "906 - random_48 - lwr_k=400 - 26.019817584500377 - 11.563557853037185 - 7.176477068229277 - 13.468628420931209 - 26.15831083926847 - 16.87798528273828 - 0.9284314745555469'\n",
      "907 - random_15 - lwr_k=40 - 7.370198815354752 - 10.7253968141237 - 7.205362379812996 - 12.006361859713376 - 47.55979877939773 - 16.970828142349596 - 0.9280377885527973'\n",
      "908 - random_19 - lr - 14.21581530718983 - 12.563450424820125 - 10.125863581422381 - 16.385277385181787 - 31.665675107909923 - 16.990036867774336 - 0.9279563368788394'\n",
      "909 - random_56 - lwr_k=700 - 7.725899725595211 - 26.274297777375786 - 6.724208126197756 - 13.185605984010644 - 31.107367899476113 - 17.00347479653764 - 0.9278993553890166'\n",
      "910 - random_23 - deep - 11.350582731531022 - 12.786688141268716 - 18.882350515479636 - 15.077111395727309 - 27.136569926818797 - 17.045030146758197 - 0.92772314641052'\n",
      "911 - random_27 - lwr_k=700 - 11.080117936380146 - 16.419800580271847 - 9.217633880707256 - 13.388138741536196 - 35.38247520731338 - 17.096536929300417 - 0.9275047395918716'\n",
      "912 - random_9 - lwr_k=20 - 17.648029288324317 - 28.119696439615833 - 9.650955117316222 - 11.102892061112492 - 19.27925512893515 - 17.162040078079933 - 0.9272269834680461'\n",
      "913 - random_62 - lwr_k=200 - 12.791221631951538 - 16.11663982152627 - 12.808559811323668 - 13.130945241687103 - 31.063380017137266 - 17.18125583212717 - 0.9271455019902852'\n",
      "914 - random_85 - lr - 17.51019986994006 - 13.0901848324752 - 17.87841523950887 - 10.35139427986486 - 27.355988024287594 - 17.23660208075355 - 0.926910814654284'\n",
      "915 - random_19 - deep - 14.644790571371974 - 12.446621086320393 - 9.88012947594895 - 16.844071190831702 - 32.486593880680715 - 17.259224780931948 - 0.9268148866960734'\n",
      "916 - random_92 - deep - 17.49085430388755 - 13.79361206810135 - 8.634275291905258 - 15.668314896285974 - 30.828361815062827 - 17.28254548881785 - 0.9267159987871076'\n",
      "917 - random_27 - lwr_k=800 - 11.236768017775209 - 16.65585349544845 - 9.244392060476633 - 13.466991083317733 - 35.861172319633255 - 17.291939366301694 - 0.9266761653248508'\n",
      "918 - random_33 - lwr_k=30 - 9.312833122008684 - 28.640200648619253 - 7.857167076609291 - 13.996918085296594 - 26.869533362273742 - 17.335867936094754 - 0.9264898929166038'\n",
      "919 - random_61 - deep - 11.111676331627779 - 22.76433100130671 - 11.895224247088109 - 16.161543610250238 - 24.759658569004767 - 17.338355643954106 - 0.9264793443267247'\n",
      "920 - random_4 - lwr_k=50 - 11.501603383162973 - 18.448003616368734 - 13.552579224003118 - 21.835845027597138 - 21.752471956582045 - 17.41730047873233 - 0.9261445906247643'\n",
      "921 - random_56 - lr - 7.83433837927476 - 16.470445756882206 - 6.85258829448458 - 15.675974743323431 - 40.42518968275743 - 17.4499718824618 - 0.9260060525143234'\n",
      "922 - random_75 - lwr_k=30 - 16.282545046858765 - 16.394164163415322 - 13.396948108350433 - 20.76450986257081 - 20.616750120799292 - 17.490605982470232 - 0.9258337498033274'\n",
      "923 - random_16 - deep - 14.38338790743714 - 16.897919384040147 - 9.222873071487765 - 13.51767848711537 - 33.46603755341701 - 17.49697104110285 - 0.9258067598995897'\n",
      "924 - random_27 - lwr_k=900 - 11.434088152584762 - 16.88141185986839 - 9.256723881303445 - 13.63512848468183 - 36.28913226776054 - 17.49820259587709 - 0.9258015375214234'\n",
      "925 - random_79 - lwr_k=40 - 15.287718896551565 - 15.399646414927183 - 16.54952320397022 - 18.717729877099597 - 21.99425191511097 - 17.58903848222757 - 0.9254163617830494'\n",
      "926 - random_27 - lwr_k=1000 - 11.605178517691984 - 17.06466331460924 - 9.294883344480905 - 13.597462999152155 - 36.66746293879537 - 17.644845890440017 - 0.9251797189700753'\n",
      "927 - random_96 - deep - 33.82618497982743 - 13.470626386995207 - 7.724528352907698 - 13.340178411188523 - 20.486696780655446 - 17.771568224691908 - 0.9246423723764545'\n",
      "928 - random_54 - lwr_k=800 - 8.54135103319622 - 15.205380552470587 - 7.926854217712073 - 10.920909353600823 - 46.545360763748704 - 17.82602109231948 - 0.9244114731262497'\n",
      "929 - random_54 - lwr_k=700 - 8.468571572975234 - 15.060942675175298 - 9.026840343751644 - 10.890718562509544 - 45.766262770426124 - 17.840676711720185 - 0.9243493282047788'\n",
      "930 - random_54 - lwr_k=600 - 8.603835377331938 - 15.160035244126005 - 10.389898572596383 - 10.969581952867538 - 44.41976008853603 - 17.906648549061 - 0.9240695846785077'\n",
      "931 - random_62 - lwr_k=300 - 13.17716590369423 - 16.28179675278353 - 13.206212323315796 - 13.871529722228907 - 33.07987605901684 - 17.92227019352963 - 0.924003343474898'\n",
      "932 - random_50 - lr - 12.208977543685565 - 16.201029366168612 - 14.347872404833788 - 15.26211013676939 - 31.89584094703415 - 17.98192875773178 - 0.9237503704216243'\n",
      "933 - random_54 - lwr_k=900 - 8.525314342396063 - 15.346593606336375 - 7.939564698908319 - 10.912781143201462 - 47.29665728173439 - 18.00219491199685 - 0.9236644348817549'\n",
      "934 - random_60 - lwr_k=300 - 8.129425807653009 - 6.905940429970236 - 6.708810496440292 - 9.912021286838645 - 58.546239988009546 - 18.03704145654476 - 0.9235166734180286'\n",
      "935 - random_2 - lwr_k=200 - 11.333198660221601 - 12.283678444140323 - 9.60861754078574 - 9.13998722765835 - 48.76293553971171 - 18.22358187944793 - 0.9227256771717752'\n",
      "936 - random_46 - deep - 19.16498477798437 - 15.584358277765702 - 10.399757078297307 - 17.48997126164542 - 28.524801799452373 - 18.232493529023824 - 0.9226878888437151'\n",
      "937 - random_54 - lwr_k=1000 - 8.477210667510011 - 15.609963478538416 - 7.9253333067447755 - 10.936733247927597 - 48.28065882467035 - 18.243948664589592 - 0.9226393149220035'\n",
      "938 - random_60 - lwr_k=400 - 7.8610364525308265 - 7.054263928527813 - 6.690351184666843 - 9.922377601811682 - 59.81296124073875 - 18.2646577024539 - 0.9225515014016992'\n",
      "939 - random_60 - lwr_k=500 - 7.840905697741013 - 7.19833458137576 - 6.5908431834905326 - 9.872537172540207 - 60.04563339919325 - 18.306117146914882 - 0.9223756989432829'\n",
      "940 - random_50 - deep - 12.605820535637939 - 17.939984433974846 - 14.012108086563348 - 16.017238188922455 - 31.554113951986878 - 18.424820005043824 - 0.9218723577223857'\n",
      "941 - random_74 - lwr_k=400 - 8.11403557355956 - 7.744592504962335 - 6.911273262671218 - 10.066401735709054 - 59.74637226236177 - 18.513067826644797 - 0.9214981561121995'\n",
      "942 - random_62 - lwr_k=400 - 13.739515723928223 - 16.598165686176642 - 13.644250045741963 - 14.377415019846493 - 35.01923574813581 - 18.674567967366215 - 0.9208133393679685'\n",
      "943 - random_15 - deep - 9.540106974725052 - 16.03993334590706 - 11.546460853845344 - 14.34311277372343 - 42.14972473225762 - 18.721924620818292 - 0.920612530861993'\n",
      "944 - random_10 - lr - 8.664596452745103 - 23.661174882881745 - 7.35972957155264 - 33.639254746521615 - 20.315996142991963 - 18.7273102530044 - 0.9205896937403382'\n",
      "945 - random_48 - lwr_k=300 - 36.14336780700806 - 11.262103736391614 - 7.208532597284026 - 13.400003284991575 - 25.654372549276914 - 18.73530332748649 - 0.9205558003256403'\n",
      "946 - random_69 - lr - 9.773102148494564 - 17.27881754220861 - 9.18324790267029 - 23.279925891147723 - 34.69788016878626 - 18.84085356973622 - 0.9201082306026211'\n",
      "947 - random_84 - lwr_k=100 - 9.450815891434068 - 49.91266411036 - 7.382168462425127 - 9.454682886577293 - 18.402727245690937 - 18.92413591392057 - 0.9197550845091162'\n",
      "948 - random_2 - deep - 18.52165936409159 - 19.136198137473748 - 22.287460430248363 - 11.309546325560186 - 24.64734511410575 - 19.180326942365646 - 0.9186687456533845'\n",
      "949 - random_1 - lwr_k=200 - 8.878889737947807 - 28.228746361827447 - 23.34376350236374 - 11.076820277766341 - 25.015124802241672 - 19.30842172801724 - 0.9181255790555093'\n",
      "950 - random_5 - lwr_k=100 - 9.033427479177055 - 8.489027618083258 - 7.478603236856441 - 11.304654760221988 - 60.39515602066118 - 19.336709291716417 - 0.9180056299508945'\n",
      "951 - random_45 - deep - 17.232204743180844 - 20.97682205549824 - 9.98956318042202 - 22.3814328628715 - 27.89457114810928 - 19.694725516187283 - 0.9164875168678306'\n",
      "952 - random_56 - deep - 11.113954569040851 - 16.70346545392675 - 6.77581204653372 - 17.079380891442202 - 46.83968665695503 - 19.70056254677312 - 0.9164627658288815'\n",
      "953 - random_8 - lwr_k=100 - 7.8775110357724705 - 9.36744880805035 - 56.1626682471023 - 8.452624279250273 - 17.516818765083922 - 19.87172896937865 - 0.9157369605112302'\n",
      "954 - random_77 - lwr_k=50 - 10.771215904964436 - 10.153179997062287 - 9.737116633763472 - 33.47462722191993 - 35.29113561853723 - 19.88236902411987 - 0.9156918429799774'\n",
      "955 - random_79 - deep - 26.206360743596004 - 7.524766065094943 - 13.22555059375185 - 16.263619559598105 - 37.022056963871385 - 20.04742841589449 - 0.9149919339546158'\n",
      "956 - random_85 - deep - 25.525780847927162 - 13.213472968817928 - 23.830927468439675 - 11.133268950124142 - 27.343019333947982 - 20.209019155059014 - 0.9143067330429452'\n",
      "957 - random_62 - lwr_k=500 - 14.221710966923338 - 16.790907088207327 - 16.803657555529504 - 16.4372722421004 - 37.23291797523016 - 20.295724151634484 - 0.9139390735311629'\n",
      "958 - random_2 - lwr_k=100 - 51.172372799658284 - 12.781312372403228 - 10.260030882761452 - 11.142194899955669 - 18.355412154442536 - 20.745943867943527 - 0.9120299854094182'\n",
      "959 - random_69 - deep - 9.947689936711239 - 19.96999430773496 - 8.831297386288155 - 28.942375776711103 - 36.05063212845386 - 20.74650121826745 - 0.9120276222373315'\n",
      "960 - random_27 - lr - 13.87035060000105 - 21.432459062585643 - 11.623774412617815 - 16.246767379809118 - 40.734441813471484 - 20.780533550103634 - 0.9118833131315176'\n",
      "961 - random_63 - lwr_k=50 - 11.996374882161518 - 17.159255108831204 - 11.313053685956175 - 15.542216281965006 - 48.16229747015269 - 20.8325904193354 - 0.9116625739077441'\n",
      "962 - random_96 - lwr_k=700 - 9.234545230803993 - 8.187187198293898 - 6.276347860358947 - 65.6552171436066 - 15.672124324617652 - 21.001058080654616 - 0.9109482124538315'\n",
      "963 - random_62 - lwr_k=600 - 14.529113191015394 - 17.007977527067077 - 16.98306459680273 - 18.131351423972585 - 38.77635589979319 - 21.08383123833485 - 0.9105972254881293'\n",
      "964 - random_27 - deep - 13.957714427317418 - 22.632999891540226 - 10.951942775017116 - 16.64726336933764 - 41.500669299726304 - 21.137187327804668 - 0.9103709773094353'\n",
      "965 - random_77 - lwr_k=40 - 11.424022601488245 - 11.08493051307234 - 12.630562288511294 - 28.31944835142449 - 42.288151447648254 - 21.146182514316873 - 0.9103328344007536'\n",
      "966 - random_34 - lwr_k=200 - 8.442230437757926 - 63.82477605179873 - 5.906170927889239 - 10.960281963398703 - 17.934362365477135 - 21.41838502681323 - 0.9091786011131133'\n",
      "967 - random_31 - lwr_k=30 - 19.51270356427841 - 14.175163516414507 - 18.783628039255547 - 33.62336710469591 - 21.284854012302834 - 21.474426296445593 - 0.908940966646414'\n",
      "968 - random_62 - lwr_k=700 - 14.943889505704691 - 17.434965531551292 - 17.073634501626778 - 18.246136346247784 - 40.15728826476039 - 21.56942034256658 - 0.9085381588649761'\n",
      "969 - random_74 - lwr_k=200 - 8.20338308925517 - 7.908541278796391 - 7.434642378282555 - 10.707139736459556 - 74.08362511786326 - 21.663008647252365 - 0.9081413119159473'\n",
      "970 - random_26 - lwr_k=400 - 38.73026776564176 - 13.733451883603054 - 13.075407205463797 - 14.432174822378185 - 28.435517711488725 - 21.68285413346978 - 0.9080571601548504'\n",
      "971 - random_26 - lwr_k=500 - 38.87856119817887 - 13.772312240341956 - 12.961008112719421 - 14.336003891950677 - 28.539100839226183 - 21.6989129071999 - 0.9079890653711984'\n",
      "972 - random_65 - lwr_k=40 - 14.801637676513293 - 7.036069039969662 - 7.517310924061777 - 35.798270909514386 - 43.6207194401639 - 21.751252900405895 - 0.9077671256033425'\n",
      "973 - random_26 - lwr_k=300 - 38.882822699779055 - 13.755374713950488 - 13.155361169152442 - 14.466340049812604 - 28.58111965035332 - 21.769694043067826 - 0.9076889287471588'\n",
      "974 - random_26 - lwr_k=600 - 38.969678070880164 - 13.830055782700734 - 13.195692835262816 - 14.493623131667432 - 28.679648140857264 - 21.83523496719902 - 0.907411012442709'\n",
      "975 - random_26 - lwr_k=700 - 39.189926440843024 - 13.852665010952585 - 13.243356273806363 - 14.544866987579525 - 28.67839101258974 - 21.903353984381603 - 0.9071221641274146'\n",
      "976 - random_26 - lwr_k=800 - 39.17834492448286 - 13.857697841877702 - 13.203180440382866 - 14.57154386496566 - 28.980341512141045 - 21.959715019428096 - 0.9068831737441888'\n",
      "977 - random_26 - lwr_k=900 - 39.22543836717461 - 13.85627936819149 - 13.187727809422013 - 14.595996123139294 - 29.017412085310124 - 21.978065523241913 - 0.9068053612282329'\n",
      "978 - random_54 - lr - 9.446442418474206 - 22.00039183393907 - 8.826575417730663 - 11.962539550118787 - 57.792918547368096 - 22.00371612551656 - 0.9066965937568311'\n",
      "979 - random_26 - lwr_k=1000 - 39.22653738721141 - 13.887567620125125 - 13.220316270738902 - 14.603609120793125 - 29.18030927532179 - 22.025152586735512 - 0.9066056957086052'\n",
      "980 - random_62 - lwr_k=800 - 15.224983633977551 - 17.85713755624086 - 17.073256633130207 - 18.755432877082942 - 41.29145231338791 - 22.038651590152906 - 0.9065484552410157'\n",
      "981 - random_15 - lwr_k=30 - 7.45350536942383 - 10.691157486796481 - 7.5263380932305415 - 12.695834514798609 - 72.61919882950824 - 22.19290855338699 - 0.9058943520875135'\n",
      "982 - random_26 - lwr_k=200 - 38.47407892885651 - 13.72064893273069 - 13.347764863147127 - 15.246681573955195 - 30.26648590724888 - 22.212404755086357 - 0.9058116814142064'\n",
      "983 - random_62 - lwr_k=900 - 15.508659044083192 - 18.379509529701867 - 17.10738966721673 - 18.80046538071056 - 42.195327513128596 - 22.396484019029728 - 0.9050311213375958'\n",
      "984 - random_25 - lwr_k=50 - 9.69527769694806 - 7.84642112679291 - 65.82375371698406 - 11.426825472982738 - 18.12017695853601 - 22.577967777948807 - 0.9042615671046483'\n",
      "985 - random_42 - lwr_k=300 - 39.05656316886378 - 13.395967441083942 - 16.53529658210874 - 10.742520566489498 - 33.19106450200147 - 22.585475176148417 - 0.9042297331262398'\n",
      "986 - random_26 - lr - 40.44425502346608 - 14.413429351658376 - 13.687297659635158 - 14.701949565451791 - 30.825575367806426 - 22.81601255810134 - 0.9032521744775147'\n",
      "987 - random_62 - lwr_k=1000 - 15.875900807083779 - 19.154960809714982 - 17.131352892628787 - 18.83496909445671 - 43.17233086023422 - 22.832161130228503 - 0.9031836989174397'\n",
      "988 - random_35 - lwr_k=500 - 8.227515666842786 - 7.660844968283428 - 6.947835361167875 - 13.10587471775792 - 78.54900560552001 - 22.893317921537996 - 0.9029243728603533'\n",
      "989 - random_42 - lwr_k=100 - 43.62250775999648 - 14.07986743690593 - 16.9033342688272 - 11.85508977766459 - 28.39207109009527 - 22.972499926705023 - 0.9025886136298167'\n",
      "990 - random_10 - deep - 9.425579701624994 - 30.077315219685605 - 7.729342117746964 - 45.80480305783384 - 22.19570557003037 - 23.0454701729803 - 0.9022791946243629'\n",
      "991 - random_42 - lwr_k=400 - 39.80050756725557 - 13.86956486588325 - 16.8016459360905 - 10.74987734172598 - 34.824045851312476 - 23.210315772058635 - 0.9015801917658226'\n",
      "992 - random_67 - lwr_k=30 - 22.613980937903538 - 23.920998212885127 - 16.779842149205574 - 16.910113910839147 - 36.87792726375251 - 23.42052236132637 - 0.9006888427463406'\n",
      "993 - random_42 - lwr_k=500 - 39.670176820183514 - 13.819245912570764 - 16.759610922795694 - 10.72024680027433 - 36.13907234070804 - 23.422758832044988 - 0.9006793593329563'\n",
      "994 - random_42 - lwr_k=200 - 44.458861521520035 - 14.051782968269544 - 16.49583321301422 - 10.85392526414576 - 31.275184870675503 - 23.42902626399705 - 0.9006527832425274'\n",
      "995 - random_26 - deep - 41.06370960241839 - 14.78580060450028 - 15.380237275903875 - 15.269655804425161 - 31.954520563723904 - 23.69217131155899 - 0.8995369569687673'\n",
      "996 - random_42 - lwr_k=600 - 40.77140576149931 - 13.577649814183301 - 17.096054940130383 - 10.85401261259189 - 37.298322906096345 - 23.920555209382954 - 0.8985685296278193'\n",
      "997 - random_10 - lwr_k=200 - 8.041383861164062 - 66.39540739189384 - 6.462097484430874 - 19.89579165573166 - 18.861033877280803 - 23.935494336914203 - 0.8985051825333097'\n",
      "998 - random_87 - lwr_k=100 - 18.80615272071017 - 58.95047105895835 - 7.7807726361527205 - 10.149944748475276 - 24.655630639751593 - 24.073444440236194 - 0.8979202261351215'\n",
      "999 - random_91 - lwr_k=100 - 7.903752754616804 - 6.997210053624786 - 7.0427357738484355 - 83.33047274823468 - 16.63046026648605 - 24.37538171647523 - 0.8966399071115445'\n",
      "1000 - random_56 - lwr_k=500 - 7.843955642660324 - 65.05469733521491 - 6.684552806358445 - 13.005822755197189 - 29.652314246707785 - 24.45219882018228 - 0.8963141758853862'\n",
      "1001 - random_42 - lwr_k=700 - 41.8397254976475 - 14.695171560987651 - 17.135213989678398 - 10.848994243923523 - 38.10529323847075 - 24.526105371385768 - 0.8960007864137336'\n",
      "1002 - random_94 - lwr_k=200 - 7.340951940269005 - 77.91793662100777 - 6.456343087680569 - 10.69420459042791 - 20.843094557536226 - 24.656394136499497 - 0.8954483167531098'\n",
      "1003 - random_42 - lwr_k=800 - 41.89867091533816 - 14.818348968060665 - 17.192064412672956 - 10.839830556231608 - 39.195387187453484 - 24.790029443331672 - 0.894881656592132'\n",
      "1004 - random_42 - lwr_k=900 - 42.3073574176653 - 14.410085253676652 - 17.790083126467817 - 10.840504105958367 - 40.1239223137007 - 25.095459489143735 - 0.8935865270112597'\n",
      "1005 - random_42 - lwr_k=1000 - 42.247042273643956 - 14.317529804714436 - 17.390404477432572 - 10.807470799664461 - 40.788378017171325 - 25.111203922065766 - 0.8935197651419186'\n",
      "1006 - random_79 - lwr_k=30 - 21.709210289986647 - 28.70157929705605 - 25.27817622287226 - 24.60815217050961 - 26.592271660287892 - 25.377821441116254 - 0.8923892141681825'\n",
      "1007 - random_4 - lwr_k=40 - 17.16927291667841 - 31.13816335878264 - 21.63736317972731 - 33.63985089396475 - 26.63739849110475 - 26.043790579700396 - 0.8895652734879684'\n",
      "1008 - random_54 - deep - 9.752599347827868 - 29.575953610009726 - 9.320080328339149 - 13.28857665429049 - 69.48165791704457 - 26.281605162365786 - 0.8885568572151743'\n",
      "1009 - random_55 - lwr_k=100 - 11.44124740138692 - 14.994750150475566 - 7.4090414295673215 - 13.496723346448997 - 84.73282204767884 - 26.410594976274393 - 0.8880098953222941'\n",
      "1010 - random_10 - lwr_k=400 - 8.150298474336072 - 78.78151647462612 - 6.358829705928762 - 19.331735078694674 - 19.406989939176707 - 26.411460976812144 - 0.888006223178938'\n",
      "1011 - random_15 - lwr_k=20 - 7.958601960896685 - 10.92028537543196 - 7.229504871030015 - 12.994614229731084 - 95.97217612042375 - 27.009280627159047 - 0.8854712600218837'\n",
      "1012 - random_93 - lwr_k=300 - 8.001795481261162 - 7.315685747711806 - 89.80790335679656 - 10.333741175462595 - 22.82496956690682 - 27.65026983410621 - 0.8827532429364005'\n",
      "1013 - random_14 - lwr_k=40 - 20.24077681378257 - 28.48498335935211 - 20.807032453251708 - 25.200276811964393 - 44.19575567575422 - 27.784644051825016 - 0.8821834495363716'\n",
      "1014 - random_18 - deep - 18.921754627883338 - 60.605638357309196 - 11.142309173221573 - 22.546304057212662 - 28.144479325909188 - 28.275860438998787 - 0.8801005215320916'\n",
      "1015 - random_63 - lwr_k=10 - 24.281671287000744 - 34.27125036980808 - 17.847103366944552 - 24.0455274380535 - 41.06490944080064 - 28.302411479407336 - 0.8799879356350098'\n",
      "1016 - random_5 - lwr_k=10 - 23.211222670393074 - 30.723715127062526 - 27.948176914716782 - 26.212373032139624 - 33.66088173855585 - 28.35082071006699 - 0.8797826636669298'\n",
      "1017 - random_10 - lwr_k=300 - 8.145050489595874 - 91.54508546929054 - 6.31873532894561 - 19.372236792982985 - 19.283625829600663 - 28.939795356243067 - 0.8772852064026376'\n",
      "1018 - random_12 - lwr_k=40 - 8.6132183440186 - 11.79285008768402 - 9.570773038091314 - 13.15107706637848 - 102.7426036573404 - 29.167891545228557 - 0.8763179992608061'\n",
      "1019 - random_65 - lwr_k=20 - 17.11903058569281 - 12.100273077315421 - 11.12462612645274 - 45.98009139437893 - 60.79899289606283 - 29.41975102263374 - 0.8752500275144666'\n",
      "1020 - random_74 - lwr_k=100 - 8.58429120206677 - 10.47391971150089 - 20.244718872672273 - 25.296648973633317 - 83.33825167352808 - 29.58099708041536 - 0.8745662881702355'\n",
      "1021 - random_42 - lwr_k=50 - 58.53065383273131 - 14.310451383613644 - 19.935484105164097 - 26.732263508736622 - 28.473432113941207 - 29.598691832662276 - 0.8744912562689036'\n",
      "1022 - random_99 - lwr_k=10 - 22.700209933214285 - 43.56729959520532 - 22.20737348956818 - 31.033587664060637 - 28.61352386968064 - 29.625548199965547 - 0.874377375934581'\n",
      "1023 - random_60 - lwr_k=200 - 8.311220720900716 - 6.837139816212096 - 6.678083986554707 - 10.036935251719392 - 118.27652517275239 - 30.020627526152804 - 0.8727021022372086'\n",
      "1024 - random_14 - lwr_k=10 - 24.80885200137372 - 34.48172752307351 - 20.909835090265364 - 25.144478614722672 - 45.121077658546426 - 30.093047492017376 - 0.8723950164042231'\n",
      "1025 - random_65 - lwr_k=30 - 15.761706193384018 - 8.648871035245152 - 7.954587676823503 - 82.84280872589841 - 38.96896926095285 - 30.829287350373622 - 0.8692731034416683'\n",
      "1026 - random_88 - lr - 8.39739840532911 - 12.480287456025792 - 8.747162422904598 - 10.147747113763259 - 114.9524845783043 - 30.938300366293873 - 0.868810850354409'\n",
      "1027 - random_42 - lr - 56.07092899682419 - 19.68356242765663 - 24.41675387507529 - 10.995176402854991 - 45.382660299506455 - 31.311967187778915 - 0.8672263731213028'\n",
      "1028 - random_49 - lwr_k=30 - 33.719827481834734 - 24.414015225242043 - 16.017118932515995 - 35.8785855533463 - 47.46062625487893 - 31.497238516864726 - 0.8664407582740516'\n",
      "1029 - random_19 - lwr_k=10 - 23.42757467500546 - 35.18587735487182 - 35.05863300556889 - 28.522830017905566 - 38.927800932730236 - 32.22358763239495 - 0.8633607854997214'\n",
      "1030 - random_82 - lwr_k=50 - 14.20095494172167 - 35.96601399016452 - 39.20216514500762 - 27.9115045195815 - 44.069472647921216 - 32.267668707768145 - 0.8631738664147941'\n",
      "1031 - random_35 - lwr_k=10 - 27.006636847738235 - 15.522726309880534 - 20.366461462753914 - 23.034227804816968 - 78.12177165443238 - 32.80658368501328 - 0.860888679550648'\n",
      "1032 - random_7 - lwr_k=30 - 24.166493538510903 - 31.152630405952806 - 26.69542118749111 - 37.53578906269815 - 44.94416205902976 - 32.89718340423958 - 0.8605045052429869'\n",
      "1033 - random_4 - lwr_k=10 - 28.34227357306233 - 36.06541872051595 - 34.05624108582623 - 32.22236384809022 - 35.98987798297337 - 33.334864520104 - 0.8586485851463309'\n",
      "1034 - random_87 - lwr_k=10 - 26.555279347087705 - 42.479198332415876 - 19.945427686285388 - 30.98196557888729 - 50.95192093634469 - 34.182867916228275 - 0.8550527559276283'\n",
      "1035 - random_63 - lwr_k=40 - 22.904262397817426 - 39.14164323688446 - 22.261015553912223 - 24.39650009043014 - 64.50511744506525 - 34.640522627766885 - 0.8531121408412399'\n",
      "1036 - random_52 - lwr_k=100 - 11.43146467772834 - 17.32284135643768 - 32.5404771296491 - 40.77342304976227 - 76.39785385535795 - 35.68623115280895 - 0.8486779731412303'\n",
      "1037 - random_52 - lwr_k=50 - 12.982565464297323 - 21.43667711248245 - 29.783358375264307 - 42.437170513904285 - 72.49053235638236 - 35.81996401009376 - 0.8481108993324187'\n",
      "1038 - random_94 - lwr_k=50 - 8.405033042400738 - 134.96850116845292 - 8.663711028930992 - 11.946170314699133 - 15.746309957962351 - 35.95764997296669 - 0.8475270629815663'\n",
      "1039 - random_92 - lwr_k=100 - 130.81395913212913 - 11.807845224990322 - 8.191419965350288 - 10.768588913313984 - 20.044425578583386 - 36.33670532122146 - 0.8459197365215668'\n",
      "1040 - random_59 - lwr_k=10 - 24.534888107503562 - 34.041361458633816 - 25.59649490770638 - 51.02110526247671 - 52.1444390563023 - 37.464979018398324 - 0.8411354637868764'\n",
      "1041 - random_15 - lwr_k=10 - 9.48928930014203 - 12.666731890460762 - 7.536783780125863 - 14.009150607487998 - 147.82834504110875 - 38.297143127130646 - 0.8376067986534449'\n",
      "1042 - random_62 - lr - 21.419899187304797 - 29.298203828229237 - 64.67196566089804 - 23.305721190904034 - 53.13993982137791 - 38.36288587734327 - 0.8373280265362424'\n",
      "1043 - random_70 - lwr_k=200 - 9.415153434146061 - 142.636565635517 - 7.69849603014742 - 12.830120072776266 - 19.82246978696329 - 38.49285681589656 - 0.8367769045707313'\n",
      "1044 - random_52 - lwr_k=40 - 15.089438024103416 - 25.0474374470306 - 34.68914563582943 - 44.54816023250658 - 73.71706579864689 - 38.61217449338658 - 0.8362709561358698'\n",
      "1045 - random_70 - lwr_k=300 - 9.744033587688063 - 142.50166003393355 - 7.842997195060356 - 13.138532638553873 - 20.152469134825665 - 38.68820211987665 - 0.8359485725675833'\n",
      "1046 - random_98 - lwr_k=50 - 8.991886949292658 - 9.573733411436777 - 6.149029332771943 - 10.672715805872686 - 159.1131879688443 - 38.89041123383909 - 0.8350911356238195'\n",
      "1047 - random_70 - lwr_k=400 - 10.028828981442665 - 142.4941579264002 - 7.995948282384914 - 13.418478230285613 - 20.571813970618557 - 38.91408050294415 - 0.8349907696938924'\n",
      "1048 - random_70 - lwr_k=500 - 10.254048307692148 - 142.38540550690706 - 8.035126067783315 - 13.64017246708934 - 20.92658421686463 - 39.060473456903786 - 0.8343700126737364'\n",
      "1049 - random_46 - lwr_k=900 - 8.965991055351404 - 9.078464319930239 - 8.805583945266815 - 10.888723499526934 - 158.02219910981515 - 39.142325032220675 - 0.8340229335376546'\n",
      "1050 - random_70 - lwr_k=600 - 10.433862515505146 - 142.67049429640068 - 8.05242345662673 - 13.744895053106857 - 21.120766262773756 - 39.216719425475745 - 0.8337074754460988'\n",
      "1051 - random_70 - lwr_k=700 - 10.641275306776036 - 142.71139368857462 - 8.090422249332931 - 14.071026635495123 - 21.429621667426556 - 39.40095933463589 - 0.8329262341779187'\n",
      "1052 - random_70 - lwr_k=800 - 10.822279395510133 - 142.7207387674013 - 8.15232811821265 - 14.285120514581848 - 21.59917346074684 - 39.52812899480788 - 0.8323869906066442'\n",
      "1053 - random_70 - lwr_k=900 - 10.968427878196245 - 142.7420184323088 - 8.195524792906642 - 14.52644777651616 - 21.797926123057117 - 39.65825473963569 - 0.8318352122087088'\n",
      "1054 - random_70 - lwr_k=1000 - 11.138072672865665 - 142.74012485748696 - 8.237708369526569 - 14.70010062422369 - 21.87807403647617 - 39.75099894579947 - 0.8314419445308747'\n",
      "1055 - random_78 - lwr_k=10 - 23.26243187414149 - 34.340100685995154 - 30.97077989211851 - 50.52476980502272 - 61.026191894332726 - 40.021179183179406 - 0.8302962863927977'\n",
      "1056 - random_83 - lwr_k=10 - 26.64640262497785 - 23.042194121915074 - 49.84059898113187 - 54.734372243408814 - 47.70680883187076 - 40.38898291691199 - 0.8287366706901388'\n",
      "1057 - random_83 - lwr_k=30 - 44.85836647774382 - 25.23378146976991 - 40.41878283984574 - 30.445358323447568 - 61.847586571509 - 40.55896911021048 - 0.8280158701326972'\n",
      "1058 - random_67 - lwr_k=10 - 60.367250710292765 - 41.02547666777966 - 32.7294482247162 - 31.68759368917151 - 38.69254849565179 - 40.903671646643566 - 0.8265542115380123'\n",
      "1059 - random_70 - lr - 13.230026903102635 - 143.28987554682132 - 10.019859481847838 - 18.35645673628402 - 23.34953215383621 - 41.66113994694287 - 0.823342282601282'\n",
      "1060 - random_52 - lwr_k=200 - 11.271868911915881 - 17.74017022928372 - 39.46749550129061 - 48.84851880086471 - 91.05435560562931 - 41.66758367249893 - 0.8233149589646813'\n",
      "1061 - random_70 - lwr_k=50 - 10.094378893953863 - 143.67811594331076 - 14.808880826452489 - 13.86758657341458 - 26.43570777533399 - 41.788432058303236 - 0.822802519794397'\n",
      "1062 - random_85 - lwr_k=10 - 20.5652543352462 - 107.5649355277379 - 18.181851819714343 - 42.714115602706016 - 20.104561324810376 - 41.83342682379815 - 0.8226117263456978'\n",
      "1063 - random_39 - lwr_k=600 - 7.142644967920483 - 168.6327548566947 - 6.838788797240206 - 9.87599399893639 - 19.446904746857545 - 42.40231849938177 - 0.820199427858817'\n",
      "1064 - random_70 - deep - 13.335169500297884 - 148.690354436197 - 9.853681084760186 - 19.398673318820737 - 25.453612503314194 - 43.358635043103206 - 0.8161443136287975'\n",
      "1065 - random_48 - lwr_k=100 - 12.074940610679887 - 167.94157944086268 - 7.902926417606582 - 14.222192896642593 - 23.908190731770674 - 45.22463714903803 - 0.8082318155694012'\n",
      "1066 - random_94 - lwr_k=20 - 14.378165595883281 - 136.07719751925936 - 23.385420447701698 - 22.310434105064203 - 30.132350606866957 - 45.266528938368424 - 0.8080541798184284'\n",
      "1067 - random_52 - lwr_k=30 - 20.14097730827728 - 34.378044261304055 - 38.31698524407957 - 54.982484954413785 - 78.64691760415118 - 45.28717600541302 - 0.8079666290759446'\n",
      "1068 - random_52 - lwr_k=300 - 11.554972760097359 - 18.53246364367361 - 45.51533212600485 - 54.28733133607546 - 100.73732854457214 - 46.11530661954 - 0.8044550673619353'\n",
      "1069 - random_94 - lwr_k=100 - 7.197620804429448 - 191.36620869296794 - 6.947056949931864 - 10.958705968150609 - 19.27098547194408 - 47.16518903390242 - 0.8000032009201018'\n",
      "1070 - random_39 - lwr_k=200 - 7.341021371223676 - 203.9448636479426 - 6.975474112917275 - 9.113606444993357 - 23.311931747965758 - 50.155557150629804 - 0.7873230004657759'\n",
      "1071 - random_52 - lwr_k=400 - 12.039645401663721 - 19.28149526070871 - 49.268395463583474 - 60.34527114506177 - 110.14919526548964 - 50.20548358240344 - 0.7871112950374984'\n",
      "1072 - random_77 - lwr_k=30 - 25.622805735363055 - 18.489240519444664 - 15.743659812693773 - 37.87090286185361 - 156.27049020552266 - 50.79000656458826 - 0.7846327143762067'\n",
      "1073 - random_65 - lwr_k=10 - 38.42109980015682 - 26.74721529744803 - 31.871402505284962 - 115.29235278611729 - 42.76942005640545 - 51.01426038135685 - 0.7836818002284243'\n",
      "1074 - random_31 - lwr_k=10 - 55.4032641074735 - 39.4494509239597 - 33.18306811602717 - 93.29518029231596 - 37.9920827262381 - 51.86315573665222 - 0.7800821887927354'\n",
      "1075 - random_42 - deep - 137.3897751279041 - 22.320417714001895 - 32.1298212056078 - 13.08043334173629 - 56.24546939559299 - 52.24222943500965 - 0.7784747849929526'\n",
      "1076 - random_58 - lwr_k=50 - 134.74271931010193 - 9.538302792344062 - 89.48468184941989 - 9.30674510183264 - 19.145146292661146 - 52.44996969707018 - 0.7775938935872475'\n",
      "1077 - random_16 - lwr_k=10 - 31.425865240069996 - 61.65434534740413 - 15.953182846991153 - 48.05991375802117 - 106.48122372375101 - 52.71288397892801 - 0.7764790456266857'\n",
      "1078 - random_49 - lwr_k=10 - 49.375622319028054 - 39.5396549352808 - 47.35718897189569 - 74.6522652702298 - 53.39116086094486 - 52.86042571567288 - 0.7758534173681314'\n",
      "1079 - random_58 - lwr_k=100 - 140.86488675890976 - 9.101100878875444 - 88.71604207770343 - 9.426304484169307 - 18.46794212356288 - 53.32235129822996 - 0.7738946923038021'\n",
      "1080 - random_52 - lwr_k=500 - 12.333285394099459 - 19.82350896487614 - 52.602748807411615 - 66.41082022268138 - 118.5663165836076 - 53.93493418061186 - 0.7712971279102946'\n",
      "1081 - random_39 - lwr_k=100 - 8.215226989229906 - 227.60180149492305 - 7.743587437135987 - 10.162853982895296 - 21.580312510931645 - 55.08133868631744 - 0.766435974244037'\n",
      "1082 - random_38 - lwr_k=10 - 61.669218035208395 - 64.15177530868826 - 48.34259131909653 - 48.94261997295345 - 52.707330800577644 - 55.165244434475206 - 0.7660801847009505'\n",
      "1083 - random_69 - lwr_k=20 - 35.96002995110837 - 74.25549682726985 - 54.31891381057014 - 57.02070016448964 - 54.82140383008726 - 55.27525404671077 - 0.7656137056988348'\n",
      "1084 - random_58 - lwr_k=200 - 146.19135788625914 - 9.23139967711436 - 92.45888618691507 - 9.700467951032397 - 18.75937240000192 - 55.27564677342044 - 0.7656120404010494'\n",
      "1085 - random_58 - lwr_k=300 - 147.59666510371497 - 9.274282251002564 - 94.37365065635467 - 9.657612849246005 - 19.03001201122862 - 55.993796475112724 - 0.7625668359920222'\n",
      "1086 - random_58 - lwr_k=400 - 149.87123691429568 - 9.430173102607872 - 95.91411681761907 - 9.724341287529477 - 19.317989046131455 - 56.859037991218116 - 0.7588979111515486'\n",
      "1087 - random_52 - lwr_k=600 - 12.694482864830603 - 20.41109136771947 - 55.67274307857905 - 71.16972710645587 - 125.90375637858496 - 57.15705818672934 - 0.7576342019120804'\n",
      "1088 - random_5 - lwr_k=50 - 15.136307803855964 - 21.314946473133784 - 18.74363412407439 - 17.248568950786048 - 213.97077377347827 - 57.270055257622886 - 0.757155055046627'\n",
      "1089 - random_58 - lwr_k=500 - 152.053306277746 - 9.500122425475531 - 96.46658279232412 - 9.72079504632592 - 19.492829939991985 - 57.454367704774626 - 0.7563735061921437'\n",
      "1090 - random_58 - lwr_k=600 - 153.94133005532535 - 9.54724746602592 - 97.51355423312415 - 9.748325478699186 - 19.60594485108605 - 58.07903316233889 - 0.7537247074792024'\n",
      "1091 - random_58 - lwr_k=700 - 155.05252164850847 - 9.64011408570703 - 98.12436498882157 - 9.760931237782184 - 19.644090050641534 - 58.45223211261303 - 0.7521422141826918'\n",
      "1092 - random_58 - lwr_k=40 - 128.863952169283 - 9.783407317669855 - 126.27027820699249 - 9.403493476148295 - 18.488117356329404 - 58.56537412972809 - 0.7516624526948668'\n",
      "1093 - random_58 - lwr_k=30 - 130.81749735877207 - 10.746068227307227 - 124.84061288362751 - 9.690268000824318 - 17.553165379811063 - 58.73346940148254 - 0.7509496703021747'\n",
      "1094 - random_58 - lwr_k=800 - 156.49523418362705 - 9.639526256980687 - 98.62797150625941 - 9.774922255486066 - 19.755293274272507 - 58.866517705813095 - 0.7503854992358828'\n",
      "1095 - random_58 - lwr_k=900 - 157.20116540669886 - 9.709022772462484 - 99.55638739283089 - 9.789440788055177 - 19.82905982556428 - 59.224953039268215 - 0.7488656088074435'\n",
      "1096 - random_52 - lwr_k=700 - 13.064853289093861 - 20.780777267572258 - 57.935933198154245 - 73.9194816125738 - 130.76572164114378 - 59.27947734533838 - 0.7486344067936395'\n",
      "1097 - random_58 - lwr_k=1000 - 158.7073086338837 - 9.793412371668824 - 100.3720386968523 - 9.783573971023287 - 19.89986200998785 - 59.71927552815726 - 0.7467695095970712'\n",
      "1098 - random_82 - lwr_k=40 - 26.24854665439737 - 116.34614784682535 - 56.940276333762995 - 52.75122450553476 - 47.832216527252534 - 60.02737442034584 - 0.7454630631797043'\n",
      "1099 - random_63 - lwr_k=20 - 39.00407854645481 - 72.03438436085838 - 47.88102430673737 - 59.84567432198524 - 87.87334424572526 - 61.325798922947044 - 0.7399572918749293'\n",
      "1100 - random_52 - lwr_k=800 - 13.282049057322093 - 21.128655461062618 - 60.022474085325506 - 77.09479157205992 - 136.05420797068865 - 61.50192405785894 - 0.7392104600707659'\n",
      "1101 - random_5 - lwr_k=40 - 26.795293335732083 - 32.80722792215891 - 53.35032972720795 - 31.77997876273862 - 164.91467431944528 - 61.91897903838387 - 0.7374420019588869'\n",
      "1102 - random_79 - lwr_k=10 - 60.62867828910147 - 46.11302580580728 - 52.65181860140546 - 73.4022129815466 - 84.62217464500819 - 63.48027021586998 - 0.7308215845636664'\n",
      "1103 - random_39 - lwr_k=500 - 7.130272979580252 - 274.57857833444797 - 6.861961253732203 - 9.72446266819224 - 19.35021696155895 - 63.55442194604152 - 0.7305071554479496'\n",
      "1104 - random_52 - lwr_k=900 - 13.568386227691029 - 21.663208296383804 - 61.914307122698965 - 80.04703828907635 - 141.00472744836085 - 63.624461023521484 - 0.730210165409461'\n",
      "1105 - random_38 - lwr_k=40 - 15.096735943550005 - 19.748446463247063 - 37.459967991956994 - 14.869785433039644 - 232.65604682558748 - 63.950953824638795 - 0.7288257224233605'\n",
      "1106 - random_52 - lwr_k=1000 - 13.845637539254815 - 21.970478036513214 - 63.064739028742736 - 82.5472140816427 - 145.67143772027939 - 65.40434148318818 - 0.7226628534624586'\n",
      "1107 - random_55 - lwr_k=10 - 48.9901122024144 - 61.98516499453144 - 34.31578968471602 - 95.30319168240669 - 93.97481652402593 - 66.910073024359 - 0.7162780282412273'\n",
      "1108 - random_6 - lwr_k=10 - 75.6323322670404 - 43.40131476117241 - 56.64099294152729 - 73.5602452030674 - 85.48153298077781 - 66.94085151671129 - 0.7161475167331204'\n",
      "1109 - random_93 - lwr_k=10 - 27.35696406348775 - 55.196062987626455 - 94.4052761243504 - 72.33355243742781 - 85.87116469927025 - 67.02416912202419 - 0.7157942211201714'\n",
      "1110 - random_5 - lwr_k=20 - 72.38593247239385 - 58.31434488379899 - 64.04663129443534 - 77.89565569320162 - 69.15017043845711 - 68.35756172368241 - 0.7101401729182916'\n",
      "1111 - random_52 - lwr_k=20 - 51.945719539528795 - 47.62518574741851 - 57.77849472234121 - 89.78665737524436 - 94.83096021059816 - 68.38730954461143 - 0.7100140318153454'\n",
      "1112 - random_58 - lr - 187.00521566026998 - 12.633659067766668 - 115.11472866642809 - 10.34204468074679 - 20.38367012919203 - 69.10592539746271 - 0.7069668507633406'\n",
      "1113 - random_7 - lwr_k=10 - 32.09017414572732 - 80.0922728074822 - 48.64129188191188 - 69.93315585857961 - 115.0433765525733 - 69.15577429812579 - 0.7067554740939266'\n",
      "1114 - random_2 - lwr_k=50 - 229.90206576639193 - 41.21750011484456 - 16.587038381184904 - 32.03923349483116 - 29.282263171177647 - 69.82715421639013 - 0.7039085898267423'\n",
      "1115 - random_39 - lwr_k=40 - 21.266600934788723 - 279.3102568858288 - 10.817408616885555 - 19.99693493802759 - 23.17569164915971 - 70.93937338152408 - 0.6991923938894755'\n",
      "1116 - random_62 - deep - 25.067888250522646 - 69.37897890406232 - 69.15678503784754 - 133.57924768106554 - 58.58007232797234 - 71.14475877878441 - 0.6983214890417162'\n",
      "1117 - random_39 - lwr_k=300 - 7.066939156242489 - 316.56405637975314 - 6.893336834281277 - 9.274588493318047 - 22.212830000306823 - 72.4316323563714 - 0.6928646970333644'\n",
      "1118 - random_39 - lwr_k=400 - 7.086849477877558 - 324.8875644551871 - 6.90787202050054 - 9.576362537192129 - 19.39464534826874 - 73.60092454419254 - 0.6879064916377438'\n",
      "1119 - random_44 - lwr_k=10 - 76.20082079441671 - 78.02327603593933 - 56.02751586233048 - 77.58141948687128 - 84.39524890240905 - 74.44652944128245 - 0.6843208328888155'\n",
      "1120 - random_39 - lwr_k=50 - 12.096924332273156 - 333.1221883013915 - 8.968181721723687 - 17.22904430176651 - 21.63349922466289 - 78.64075178751158 - 0.6665358719662111'\n",
      "1121 - random_43 - lwr_k=100 - 51.547548524092846 - 95.48026985065346 - 56.4398998721268 - 105.47802428837127 - 95.1788012614175 - 80.82251445785147 - 0.6572844397265069'\n",
      "1122 - random_72 - lwr_k=10 - 97.79333282204887 - 122.13515559036681 - 44.448759357300254 - 95.17219021021533 - 54.71000487540407 - 82.86076767885653 - 0.6486415374448624'\n",
      "1123 - random_69 - lwr_k=10 - 35.37867631413262 - 100.67643680113173 - 33.15714493368266 - 78.40290614365362 - 175.08390860713033 - 84.53440691036299 - 0.64154472524137'\n",
      "1124 - random_51 - lwr_k=200 - 6.847778385102433 - 8.08983958107332 - 154.36769563065823 - 239.6079288831786 - 14.300191697476247 - 84.61741292757164 - 0.6411927508703106'\n",
      "1125 - random_51 - lwr_k=400 - 6.682791683379625 - 8.199216175297169 - 153.71372588999847 - 239.16242582018933 - 15.655016147334692 - 84.65733904655934 - 0.641023450244981'\n",
      "1126 - random_51 - lwr_k=500 - 6.641269681569888 - 8.310647273690895 - 154.1634584586808 - 239.14262580825522 - 15.927312335704828 - 84.8117274883172 - 0.640368789576828'\n",
      "1127 - random_51 - lwr_k=600 - 6.60412857736327 - 8.38412843846316 - 153.99646446144416 - 239.1541726842799 - 16.343232432456535 - 84.871076605256 - 0.6401171286875342'\n",
      "1128 - random_51 - lwr_k=700 - 6.606516305790937 - 8.413820299966082 - 154.02418307086293 - 239.1522139272845 - 16.52689209751581 - 84.91936586180447 - 0.6399123654513963'\n",
      "1129 - random_4 - lwr_k=20 - 48.46190121024867 - 80.52349220655091 - 64.24092008155844 - 83.69220503121406 - 147.75511800124673 - 84.92803268340373 - 0.6398756151146849'\n",
      "1130 - random_51 - lwr_k=800 - 6.596459797331832 - 8.408654910417667 - 154.25256910019746 - 239.14594849502166 - 16.62804451504902 - 84.98095341567172 - 0.6396512128113049'\n",
      "1131 - random_51 - lwr_k=1000 - 6.686962758794125 - 8.487153922258358 - 154.42424981930276 - 239.20711030373755 - 16.81419368135017 - 85.09854130987505 - 0.63915259925896'\n",
      "1132 - random_51 - lwr_k=900 - 6.670444796931431 - 8.48103289789172 - 154.56868154135498 - 239.2037869983669 - 16.69771538271635 - 85.09893569876715 - 0.6391509269129432'\n",
      "1133 - random_53 - lr - 14.616847457578265 - 10.918497840704903 - 123.58520349622755 - 81.60288127323768 - 194.8998153460343 - 85.10095267652318 - 0.6391423742260298'\n",
      "1134 - random_51 - lr - 7.2606677678731435 - 9.012009853242196 - 155.1860546680503 - 239.31472787508113 - 17.82384524064009 - 85.69405314835238 - 0.6366274220265726'\n",
      "1135 - random_51 - lwr_k=100 - 11.233836720988302 - 8.312945465711142 - 155.06046349402777 - 239.52894968753057 - 14.768007361331232 - 85.75594863517519 - 0.636364963643693'\n",
      "1136 - random_51 - lwr_k=40 - 10.414436527865124 - 11.18684406957949 - 155.84064702636212 - 239.25911987893102 - 14.496824319745516 - 86.21486863858678 - 0.6344189832798683'\n",
      "1137 - random_51 - lwr_k=50 - 10.535129129251281 - 11.6269127002353 - 153.942632907735 - 239.1511722165834 - 18.904886923583895 - 86.80733880892767 - 0.6319067037778541'\n",
      "1138 - random_51 - deep - 13.965525428909327 - 11.395110543933125 - 153.8179931656246 - 239.24597801564659 - 16.886806227163053 - 87.03792243329737 - 0.6309289498299653'\n",
      "1139 - random_52 - lr - 18.755562607649154 - 26.64021550744331 - 86.9917742216025 - 110.94662452203217 - 193.17648249315715 - 87.28097443045127 - 0.6298983240771632'\n",
      "1140 - random_40 - lwr_k=400 - 8.686829619447922 - 6.9137487957361135 - 7.7802774116565505 - 393.95602748117693 - 21.14761573939014 - 87.6707342255281 - 0.6282456070411016'\n",
      "1141 - random_44 - lwr_k=200 - 8.710697698341585 - 29.535801619082136 - 368.4499544960752 - 9.295444480960596 - 25.295167286202886 - 88.23477215872053 - 0.6258538901094246'\n",
      "1142 - random_2 - lwr_k=10 - 88.18872895620422 - 72.33406065030213 - 90.68998823578919 - 89.3593882160117 - 101.70576503250848 - 88.45290267749999 - 0.6249289408739582'\n",
      "1143 - random_43 - lwr_k=200 - 64.99164817294673 - 81.30341812474884 - 65.21302126733227 - 114.96209302524196 - 117.64056324963373 - 88.81701544044864 - 0.623384976170636'\n",
      "1144 - random_0 - lwr_k=10 - 63.86699817111563 - 64.57841084443639 - 61.24795555886032 - 191.6608693350751 - 66.40245841419896 - 89.54304351338409 - 0.6203063647285221'\n",
      "1145 - random_8 - lwr_k=40 - 9.31074836234991 - 69.97802044506982 - 107.69033824440461 - 250.29321226164174 - 22.31557747115539 - 91.90046024935815 - 0.6103100981821653'\n",
      "1146 - random_71 - lwr_k=10 - 157.84652807425317 - 57.402510685340296 - 39.40208923887034 - 27.639915790856016 - 180.5091906969125 - 92.56498040714742 - 0.6074923016842779'\n",
      "1147 - random_38 - lwr_k=50 - 11.038940760278138 - 16.400781641605214 - 10.034587539687065 - 12.64968668770277 - 413.19495485392497 - 92.63793670924952 - 0.6071829415991814'\n",
      "1148 - random_82 - lwr_k=10 - 51.38890942763219 - 66.04372082811568 - 69.30406956693581 - 205.45651576954756 - 71.45451093301767 - 92.71840620887254 - 0.6068417229445376'\n",
      "1149 - random_59 - lwr_k=50 - 90.35743619885808 - 44.914759931700154 - 91.09991900068553 - 226.8514758749038 - 17.851730871275553 - 94.2063599489871 - 0.6005322817804628'\n",
      "1150 - random_11 - lwr_k=20 - 256.8262550211098 - 203.23896022299994 - 8.171456876373144 - 9.86391926444708 - 16.045685084247612 - 98.87222347864905 - 0.5807473982680641'\n",
      "1151 - random_11 - lwr_k=30 - 257.0080116711832 - 203.02217697552274 - 8.182448251629287 - 9.708820978911335 - 17.825713520476757 - 99.19229187248514 - 0.5793901970023626'\n",
      "1152 - random_11 - lwr_k=100 - 257.06643759057755 - 202.6534166578204 - 7.702204641612319 - 9.238768126209559 - 19.804449724474114 - 99.33581509000093 - 0.5787816086624288'\n",
      "1153 - random_11 - lwr_k=50 - 257.00615691693883 - 202.72403680941335 - 7.874732200430654 - 9.54000623999382 - 19.514559247099996 - 99.37464699690024 - 0.5786169478767709'\n",
      "1154 - random_11 - lwr_k=40 - 256.79358301596466 - 202.94234725145523 - 7.9857547788773715 - 9.590999332322454 - 19.48161241000715 - 99.4016001816245 - 0.5785026570029235'\n",
      "1155 - random_11 - lwr_k=1000 - 257.93787502861426 - 202.90717035512824 - 7.60749426163908 - 9.747292476841118 - 20.265245625908193 - 99.73582855343894 - 0.5770854125076978'\n",
      "1156 - random_11 - lwr_k=10 - 258.17607514021637 - 203.5279212557859 - 9.16283000462205 - 10.557819242904587 - 17.13113372162963 - 99.75410358625423 - 0.577007920015014'\n",
      "1157 - random_11 - lwr_k=800 - 258.4118338148922 - 202.90444700582304 - 7.60199093097971 - 9.723425927896633 - 20.137962514524485 - 99.79880160108668 - 0.5768183848922928'\n",
      "1158 - random_11 - lwr_k=900 - 258.4410675894994 - 202.94987295688105 - 7.606723942085632 - 9.77650547631663 - 20.154851262814084 - 99.82867625011333 - 0.576691705994119'\n",
      "1159 - random_11 - lwr_k=200 - 259.1258985311438 - 202.70327195498533 - 7.664881350864403 - 9.48078248892341 - 20.033500287992553 - 99.84460549108724 - 0.5766241604743871'\n",
      "1160 - random_11 - lwr_k=300 - 258.09590095078113 - 203.4271693804626 - 7.640782232460515 - 10.084643778604574 - 20.277370580164888 - 99.94802793231892 - 0.5761856133674415'\n",
      "1161 - random_11 - lr - 258.12517623108977 - 202.78249556517736 - 7.981389218061967 - 10.653611148860618 - 20.20374005000618 - 99.99202177568502 - 0.5759990641765516'\n",
      "1162 - random_11 - lwr_k=700 - 259.59493752078185 - 202.8643040339266 - 7.610973592231237 - 9.733486021166547 - 20.150142507398908 - 100.0337485458239 - 0.5758221281643219'\n",
      "1163 - random_11 - lwr_k=400 - 259.6038910821257 - 202.96086391713638 - 7.638428052933402 - 9.60480112020583 - 20.214582242438926 - 100.04750586990656 - 0.5757637923273031'\n",
      "1164 - random_11 - lwr_k=500 - 259.9529062029303 - 202.8922722813409 - 7.62998760446467 - 9.643126123147237 - 20.232661540324045 - 100.11320774683631 - 0.5754851935270302'\n",
      "1165 - random_11 - lwr_k=600 - 260.81003382270256 - 202.80020964554132 - 7.6199167504748475 - 9.714004597618796 - 20.205506656236235 - 100.2730242527396 - 0.574807515979765'\n",
      "1166 - random_51 - lwr_k=300 - 6.664485899259373 - 87.9226897013105 - 153.75936178671398 - 239.42428687974535 - 14.820663849303882 - 100.50086690120244 - 0.5738413839378962'\n",
      "1167 - random_43 - lwr_k=300 - 81.42308747644894 - 72.01867198246111 - 68.32983101818688 - 114.54898909751039 - 166.37045825408845 - 100.53040755662298 - 0.5737161212887296'\n",
      "1168 - random_11 - deep - 257.88830931018884 - 202.84016755598853 - 9.062741467837522 - 14.999221349914576 - 24.7838303474595 - 101.9569198301936 - 0.5676672142229804'\n",
      "1169 - random_39 - lwr_k=10 - 72.43314129556113 - 99.263704269167 - 67.70260064746957 - 191.10014061254597 - 92.12205063861838 - 104.51821126384964 - 0.5568064471595535'\n",
      "1170 - random_43 - lwr_k=400 - 89.45928371685517 - 68.46185450731642 - 69.6035740967121 - 120.13963401418583 - 176.2104174063317 - 104.76649871728755 - 0.5557536220366945'\n",
      "1171 - random_14 - lwr_k=20 - 65.42047208055294 - 79.89790410327005 - 51.95879823806099 - 82.36459413470732 - 251.47636464786635 - 106.21263452084928 - 0.5496215034621397'\n",
      "1172 - random_99 - lwr_k=20 - 65.81042996484695 - 77.61823788960031 - 122.30262656971895 - 84.4747938001126 - 189.2447783858311 - 107.87832598637608 - 0.5425583925495425'\n",
      "1173 - random_43 - lwr_k=500 - 96.42003504263847 - 71.63863352976269 - 72.11438207369811 - 118.96384206129343 - 183.840845801808 - 108.58750243790837 - 0.539551237840858'\n",
      "1174 - random_51 - lwr_k=30 - 32.16795479662134 - 37.55830415342151 - 158.05858341897093 - 239.26649853655593 - 88.89544435740011 - 111.16436074423387 - 0.528624462744566'\n",
      "1175 - random_77 - lwr_k=20 - 222.65761473140174 - 43.97462416951592 - 68.5090403191928 - 137.67185507755417 - 84.2221456771307 - 111.41423106077059 - 0.5275649257318658'\n",
      "1176 - random_43 - lwr_k=600 - 104.18969170633812 - 73.23676084731798 - 73.8193177470794 - 124.22099936071145 - 188.79407732369964 - 112.84426406114585 - 0.5215010886411653'\n",
      "1177 - random_2 - lwr_k=30 - 84.70695216203433 - 69.96406816239207 - 48.7044963533919 - 54.875400306964714 - 311.0186080411364 - 113.84194548501262 - 0.5172705725473699'\n",
      "1178 - random_43 - lwr_k=700 - 108.42928397622705 - 75.65451662014577 - 76.5145135221448 - 127.82535441716198 - 195.10471157681798 - 116.69759880759061 - 0.505161609655544'\n",
      "1179 - random_98 - lwr_k=30 - 11.336349642364121 - 11.533213992363416 - 103.5067298001288 - 10.670679952235895 - 449.0099961731291 - 117.17675280770676 - 0.5031298301110865'\n",
      "1180 - random_51 - lwr_k=10 - 50.89992445161273 - 64.54306503830318 - 158.91391942902428 - 242.26677110791502 - 77.00829443231487 - 118.70641621158288 - 0.49664352547176904'\n",
      "1181 - random_93 - lwr_k=200 - 8.082918173412668 - 7.3817279145067785 - 541.3451784301172 - 19.698983467107666 - 22.57959388088553 - 119.78097319870807 - 0.4920870302629933'\n",
      "1182 - random_43 - lwr_k=800 - 114.60784615474189 - 77.4301701932688 - 78.7223070900291 - 129.77286695345154 - 200.86914107176108 - 120.27252083395761 - 0.4900026973967656'\n",
      "1183 - random_9 - lwr_k=10 - 128.45671008903298 - 408.96592402438404 - 22.085063441386332 - 15.248748243877282 - 31.453581773067782 - 121.29030068760736 - 0.4856869569732163'\n",
      "1184 - random_46 - lwr_k=600 - 8.765711259322922 - 8.973442390101724 - 8.686027939131032 - 10.812907039198949 - 574.0250423754867 - 122.21549404103597 - 0.4817638154994758'\n",
      "1185 - random_43 - lwr_k=900 - 119.08359481878087 - 78.80862755582373 - 80.00657379915751 - 132.5956278886517 - 203.8321619541611 - 122.85748382980394 - 0.47904155559920725'\n",
      "1186 - random_77 - lwr_k=10 - 83.25813343313976 - 114.11711261330143 - 109.13297023036947 - 153.3765439991322 - 158.84277854176415 - 123.73730148067246 - 0.4753108228797599'\n",
      "1187 - random_43 - lwr_k=1000 - 124.14740810213435 - 80.8837150974338 - 80.77093254272471 - 134.9936705822097 - 206.8231952476823 - 125.51624928161738 - 0.4677674657299885'\n",
      "1188 - random_72 - lwr_k=100 - 10.798714119880877 - 40.804649940712885 - 12.850076859361568 - 13.174769011933847 - 577.0845160982117 - 130.90811230502925 - 0.44490409196115843'\n",
      "1189 - random_22 - lwr_k=50 - 17.058088718546156 - 599.774843184106 - 17.60891156183946 - 18.09107442026702 - 25.678300049098876 - 135.69882594222082 - 0.4245898005871107'\n",
      "1190 - random_92 - lwr_k=10 - 86.01892609384078 - 61.324177123243935 - 120.13309624392849 - 70.8909533208752 - 359.6397518088503 - 139.57978935786946 - 0.40813316643860775'\n",
      "1191 - random_32 - lwr_k=100 - 7.819509954365282 - 202.76829266037768 - 153.7342936299666 - 17.673761884061093 - 325.5830884424479 - 141.50392689799781 - 0.3999741543176444'\n",
      "1192 - random_32 - lwr_k=600 - 7.7981714558819935 - 202.83608869620198 - 154.22907778865093 - 18.615130252662464 - 325.1035038467293 - 141.70447390226738 - 0.3991237652968319'\n",
      "1193 - random_32 - lwr_k=400 - 7.793162178072926 - 202.9944832861782 - 154.54451008784514 - 18.215581322718855 - 325.188708450393 - 141.73538355780963 - 0.39899269761119904'\n",
      "1194 - random_80 - lwr_k=30 - 14.407660904701725 - 8.798469252669522 - 12.859075106882319 - 168.06337727248177 - 504.77673458892286 - 141.73843103661144 - 0.39897977524158457'\n",
      "1195 - random_32 - lwr_k=500 - 7.802985017102482 - 202.96858441323474 - 154.4971345266114 - 18.402624263909086 - 325.1012651145647 - 141.74260815981353 - 0.398962062786978'\n",
      "1196 - random_32 - lwr_k=700 - 7.798744311511978 - 202.93081069745588 - 154.4258089205637 - 18.784786644953293 - 325.0891172490957 - 141.7939193659081 - 0.39874448543414553'\n",
      "1197 - random_32 - lwr_k=800 - 7.790073823249812 - 202.9520433757571 - 154.46625725025214 - 18.76562150904881 - 325.1053361226748 - 141.80393099526427 - 0.39870203264500537'\n",
      "1198 - random_32 - lwr_k=900 - 7.7952147558083045 - 202.93105087893585 - 154.42627204344458 - 18.942548757892375 - 325.10322744408586 - 141.82771696632042 - 0.39860117185823274'\n",
      "1199 - random_32 - lwr_k=200 - 9.310995321344835 - 202.76512686801266 - 154.04200618147954 - 17.706855155789977 - 325.480728762621 - 141.84941064800412 - 0.3985091831058103'\n",
      "1200 - random_32 - lwr_k=1000 - 7.777225577748385 - 202.9353523793036 - 154.43454379812266 - 19.14605146741943 - 325.22587679503727 - 141.8918409447331 - 0.3983292638962487'\n",
      "1201 - random_32 - lwr_k=50 - 7.980937469586302 - 202.86772999757247 - 153.68673668899837 - 18.083273603409463 - 327.34370533960896 - 141.98050080752589 - 0.39795331525428157'\n",
      "1202 - random_32 - lwr_k=40 - 8.004912525991687 - 203.12077375628115 - 153.7285221248652 - 18.194945477660067 - 328.4684209560569 - 142.29148265383432 - 0.3966346441091507'\n",
      "1203 - random_32 - lwr_k=20 - 8.392840480208516 - 203.41807894380747 - 153.85614518910748 - 19.85227977794264 - 328.1241177503396 - 142.71663307582781 - 0.39483185851096203'\n",
      "1204 - random_32 - deep - 7.676251802428849 - 202.9640411152192 - 154.23861844455678 - 23.408693682258974 - 325.7158038087012 - 142.78841079436532 - 0.39452749731735215'\n",
      "1205 - random_32 - lr - 7.775056003510953 - 202.82202175106715 - 155.1860546680503 - 22.714421513383385 - 325.6910373613933 - 142.82542443947702 - 0.39437054530639204'\n",
      "1206 - random_46 - lwr_k=700 - 8.882795745578175 - 9.074061919902972 - 8.779204444790318 - 10.812661685699025 - 681.5674359135901 - 143.77907120730944 - 0.3903267514633889'\n",
      "1207 - random_32 - lwr_k=30 - 8.099286930448093 - 203.06402806817758 - 154.66697420139062 - 18.866031011250765 - 335.14295036741015 - 143.95528290348184 - 0.3895795526093869'\n",
      "1208 - random_32 - lwr_k=10 - 8.953168290321367 - 203.77976920544637 - 154.05597525023143 - 21.973298140982727 - 333.6620973638047 - 144.4723781417564 - 0.387386889024385'\n",
      "1209 - random_32 - lwr_k=300 - 9.344336403144677 - 203.52347572336492 - 155.38612961493422 - 33.94411684518084 - 325.2238696093164 - 145.47159687888822 - 0.3831498541878793'\n",
      "1210 - random_22 - lwr_k=200 - 7.87202929588427 - 694.5896578383695 - 6.684374063769479 - 11.40105293402143 - 19.880550505880606 - 148.15206160308512 - 0.37178375185972734'\n",
      "1211 - random_17 - lwr_k=10 - 54.17886413450611 - 89.34723509399795 - 512.9275953030246 - 58.65902872311965 - 50.76875249104658 - 153.14963287779952 - 0.3505923121859017'\n",
      "1212 - random_2 - lwr_k=40 - 403.11404604111954 - 143.82532068530006 - 26.559751907309074 - 39.080369918332366 - 153.4786714041996 - 153.25101566617334 - 0.35016241391618463'\n",
      "1213 - random_40 - lwr_k=200 - 11.316340873590681 - 8.640869469124738 - 10.065302244114271 - 734.7993967574546 - 32.78166179302794 - 159.4717402274441 - 0.32378437906260704'\n",
      "1214 - random_91 - lwr_k=300 - 7.851810145929572 - 42.27765181699271 - 6.497593870814615 - 657.3855999437706 - 96.59139983370765 - 162.07592620980137 - 0.3127417251191271'\n",
      "1215 - random_43 - lwr_k=50 - 387.2288580649211 - 131.52074291354748 - 56.49305272251586 - 101.40855923826842 - 139.66363529922464 - 163.29444560343202 - 0.30757477925641175'\n",
      "1216 - random_74 - lwr_k=1000 - 9.902989676887364 - 7.867870099067156 - 6.248826116261181 - 10.276924889726075 - 801.8192041994977 - 167.17130849060322 - 0.29113553277425386'\n",
      "1217 - random_53 - deep - 27.428854993829557 - 11.783475916436378 - 239.38039515039944 - 236.69417682619587 - 324.28489830648186 - 167.8657905690866 - 0.28819068882951027'\n",
      "1218 - random_38 - lwr_k=30 - 25.319953032220972 - 27.889012264650997 - 32.48160572554088 - 22.364354678959515 - 770.1561281141873 - 175.5934019443418 - 0.2554229284827979'\n",
      "1219 - random_57 - lwr_k=300 - 778.5939681339756 - 9.833957852766343 - 71.15998352197148 - 10.857007842639305 - 15.271218145155188 - 177.21431625466238 - 0.24854968827562085'\n",
      "1220 - random_43 - lr - 227.7580400514823 - 134.43353718380774 - 111.44376796041901 - 178.28709805153082 - 243.78139802512777 - 179.14140851003702 - 0.24037814713476902'\n",
      "1221 - random_91 - lwr_k=200 - 7.972974028380569 - 64.06983768161649 - 6.564001356442262 - 798.3021715636494 - 44.861447521613066 - 184.30550850973276 - 0.21848056777119929'\n",
      "1222 - random_52 - deep - 43.871636071493896 - 32.754218767980895 - 197.39922203420127 - 239.45309620700246 - 411.2853799705912 - 184.90468609436533 - 0.21593984816782696'\n",
      "1223 - random_57 - lwr_k=200 - 926.7539196373924 - 11.514422377425685 - 14.488078356699 - 9.55510895112611 - 20.544474009937264 - 196.6604631424751 - 0.1660912647716989'\n",
      "1224 - random_35 - lwr_k=800 - 829.7954840584172 - 7.944235288317588 - 6.777899705136264 - 12.992028629420295 - 157.21186662089409 - 203.0150169776184 - 0.1391457472694574'\n",
      "1225 - random_4 - lwr_k=30 - 101.63108447709847 - 212.13352826921405 - 129.10008343571292 - 261.0047145785745 - 350.7192857361418 - 210.90004307110985 - 0.10571049530372822'\n",
      "1226 - random_83 - lwr_k=20 - 102.9868536308462 - 91.18984350219345 - 333.79214538747254 - 152.92066127495193 - 384.77009699211044 - 213.09391669539244 - 0.09640770840878032'\n",
      "1227 - random_74 - lwr_k=800 - 10.719674386161989 - 7.729444062577252 - 6.3034663259382535 - 10.195346546807778 - 1061.0231358143803 - 219.1254498266483 - 0.07083191099326602'\n",
      "1228 - random_36 - lwr_k=800 - 182.21940881513083 - 202.7587008658572 - 154.2525662254854 - 239.14594849502166 - 325.7929401930401 - 220.82463017514465 - 0.06362679557404294'\n",
      "1229 - random_36 - lwr_k=600 - 181.71397589386905 - 202.87196186715641 - 153.9964623313827 - 239.1541726842799 - 326.46838085407285 - 220.8316414476699 - 0.06359706534097809'\n",
      "1230 - random_36 - lwr_k=700 - 182.31446585522232 - 202.7807626725799 - 154.02418307086293 - 239.1522139272845 - 326.0572710614742 - 220.85650531550226 - 0.06349163389712986'\n",
      "1231 - random_36 - lwr_k=900 - 182.32857050337634 - 202.74434040972648 - 154.56868512800492 - 239.2037869983669 - 325.6835227673298 - 220.89649040462112 - 0.06332208321794253'\n",
      "1232 - random_36 - lwr_k=500 - 181.69731913404542 - 203.02911319063867 - 154.1634584586808 - 239.14262569280464 - 326.77274890158344 - 220.95168748298033 - 0.06308802841573513'\n",
      "1233 - random_36 - lwr_k=1000 - 182.37950643008546 - 202.8097922500784 - 154.42424981930276 - 239.20711030373755 - 326.103897421353 - 220.9756136317119 - 0.06298657322688428'\n",
      "1234 - random_36 - lr - 181.95914301451018 - 202.82202175106715 - 155.194072219849 - 239.31472787508113 - 325.6910373613933 - 220.98683230353794 - 0.06293900215802839'\n",
      "1235 - random_36 - lwr_k=400 - 182.2800257209088 - 203.1070479644621 - 153.71372588999847 - 239.16242582018933 - 327.44221689805363 - 221.1317720835412 - 0.06232440710066178'\n",
      "1236 - random_36 - lwr_k=300 - 182.97829050816662 - 203.22732665577945 - 153.75936282925255 - 239.42428687974535 - 326.46101921809844 - 221.16086538960647 - 0.06220104136828819'\n",
      "1237 - random_36 - deep - 185.23965469085644 - 202.82576644323072 - 153.8746289371175 - 239.42895270370246 - 325.7245770922262 - 221.4097471176209 - 0.061145698310914764'\n",
      "1238 - random_36 - lwr_k=200 - 184.20397075521976 - 203.2235867254859 - 154.36769563065823 - 239.6079288831786 - 327.431647843001 - 221.75777874392034 - 0.05966992122149928'\n",
      "1239 - random_36 - lwr_k=100 - 189.5091055177372 - 203.41863219256587 - 155.06045901961514 - 239.52894731370145 - 326.6448140663954 - 222.82375611430143 - 0.055149806570531146'\n",
      "1240 - random_36 - lwr_k=50 - 193.8665893162939 - 202.80897057295073 - 153.94263484521764 - 239.1511722165834 - 325.50734777992983 - 223.0472481163272 - 0.05420212278211278'\n",
      "1241 - random_36 - lwr_k=40 - 191.2177356463724 - 202.74159861860107 - 155.84064982696856 - 239.25911987893102 - 327.05681665865535 - 223.21458955672495 - 0.05349253689645217'\n",
      "1242 - random_36 - lwr_k=10 - 181.01367110824611 - 208.8717691306591 - 158.91391942902428 - 242.26677110791502 - 328.18428155036804 - 223.84061550524717 - 0.050837969228857305'\n",
      "1243 - random_36 - lwr_k=30 - 196.5928678035285 - 202.73170229454743 - 158.0585794299623 - 239.26649853655593 - 325.34222825702756 - 224.39027436692288 - 0.04850722455947787'\n",
      "1244 - random_36 - lwr_k=20 - 199.11178577379192 - 203.07085730665392 - 155.205723013861 - 245.37665170035297 - 327.457909235224 - 226.03641338571617 - 0.04152702281868681'\n",
      "1245 - random_58 - deep - 623.3850032225763 - 15.191739917386768 - 462.17170769068963 - 12.680299644485837 - 20.507616479703387 - 226.81756718130268 - 0.03821466119243133'\n",
      "1246 - random_60 - lwr_k=100 - 10.377016151189947 - 7.055132097146321 - 1002.7197927689273 - 70.377205492733 - 69.46504375829954 - 231.92571450234774 - 0.016554338593943174'\n",
      "1247 - random_74 - lwr_k=900 - 10.144531626347352 - 7.819372825615386 - 6.42879273929625 - 10.239536438392282 - 1139.7963646007772 - 234.8117377343713 - 0.004316596727583999'\n",
      "1248 - random_89 - lwr_k=100 - 257.06643759057755 - 202.76829266037768 - 153.7342936299666 - 239.38078651681073 - 325.5830884424479 - 235.7046838405291 - 0.0005301947934360784'\n",
      "1249 - random_13 - lwr_k=100 - 257.06643759057755 - 202.76829266037768 - 153.7342936299666 - 239.38078651681073 - 325.5830884424479 - 235.7046838405291 - 0.0005301947934360784'\n",
      "1250 - random_68 - lwr_k=100 - 257.06643759057755 - 202.76829266037768 - 153.7342936299666 - 239.38078651681073 - 325.5830884424479 - 235.7046838405291 - 0.0005301947934360784'\n",
      "1251 - random_30 - deep - 257.0104155345361 - 203.1262080290899 - 154.07742613246458 - 239.2928418683576 - 325.72690287815755 - 235.84486870526123 - -6.423564362578738e-05'\n",
      "1252 - random_68 - deep - 256.98296513534024 - 203.19908477982992 - 154.33951129335347 - 239.20229472692242 - 325.7218590673123 - 235.88724737305157 - -0.00024393592815608045'\n",
      "1253 - random_13 - deep - 257.8896043468419 - 202.80598166258167 - 153.7906374950862 - 239.58263992696976 - 325.61782553545476 - 235.93550832515012 - -0.00044857914315099556'\n",
      "1254 - random_13 - lwr_k=1000 - 257.93787502861426 - 202.9353523793036 - 154.43454379812266 - 239.16549902843036 - 325.22587679503727 - 235.93802714105738 - -0.0004592619255614494'\n",
      "1255 - random_68 - lwr_k=1000 - 257.93787502861426 - 202.9353523793036 - 154.43454379812266 - 239.16549902843036 - 325.22587679503727 - 235.93802714105738 - -0.0004592619255614494'\n",
      "1256 - random_13 - lwr_k=900 - 258.4410675894994 - 202.93105087893585 - 154.42627204344458 - 239.16404457014684 - 325.10322744408586 - 236.0113879257986 - -0.0007703371576714968'\n",
      "1257 - random_68 - lwr_k=900 - 258.4410675894994 - 202.93105087893585 - 154.42627204344458 - 239.16404457014684 - 325.10322744408586 - 236.0113879257986 - -0.0007703371576714968'\n",
      "1258 - random_13 - lwr_k=800 - 258.4118338148922 - 202.9520433757571 - 154.46625725025214 - 239.171417525146 - 325.1053361226748 - 236.019628988623 - -0.0008052821291570922'\n",
      "1259 - random_68 - lwr_k=800 - 258.4118338148922 - 202.9520433757571 - 154.46625725025214 - 239.171417525146 - 325.1053361226748 - 236.019628988623 - -0.0008052821291570922'\n",
      "1260 - random_89 - deep - 256.9932092145305 - 203.21792379026522 - 154.53123460686862 - 239.59503473755183 - 325.7874504045616 - 236.02303668299916 - -0.0008197298099854589'\n",
      "1261 - random_97 - lwr_k=50 - 258.74011714913854 - 202.80897057295073 - 153.942632907735 - 239.1511722165834 - 325.50734777992983 - 236.0283269841952 - -0.0008421646543517181'\n",
      "1262 - random_30 - lwr_k=50 - 258.74011714913854 - 202.80897057295073 - 153.942632907735 - 239.1511722165834 - 325.50734777992983 - 236.0283269841952 - -0.0008421646543517181'\n",
      "1263 - random_47 - deep - 256.8611107684196 - 203.3518000476294 - 154.6518052684587 - 239.88153429835864 - 325.6312650044759 - 236.07354856385138 - -0.0010339177454392612'\n",
      "1264 - random_97 - lwr_k=800 - 258.4357009150421 - 202.7587008658572 - 154.25256910019746 - 239.14594849502166 - 325.7929401930401 - 236.07537726145787 - -0.0010416741875711732'\n",
      "1265 - random_30 - lwr_k=800 - 258.4357009150421 - 202.7587008658572 - 154.25256910019746 - 239.14594849502166 - 325.7929401930401 - 236.07537726145787 - -0.0010416741875711732'\n",
      "1266 - random_13 - lwr_k=50 - 257.00615691693883 - 202.86772999757247 - 153.68673668899837 - 239.58563244079997 - 327.34370533960896 - 236.09597457617147 - -0.0011290139628743479'\n",
      "1267 - random_68 - lwr_k=50 - 257.00615691693883 - 202.86772999757247 - 153.68673668899837 - 239.58563244079997 - 327.34370533960896 - 236.09597457617147 - -0.0011290139628743479'\n",
      "1268 - random_97 - lwr_k=900 - 258.30562004211873 - 202.74434040972648 - 154.56868154135498 - 239.2037869983669 - 325.6835227673298 - 236.0993641817693 - -0.0011433870690484849'\n",
      "1269 - random_30 - lwr_k=900 - 258.30562004211873 - 202.74434040972648 - 154.56868154135498 - 239.2037869983669 - 325.6835227673298 - 236.0993641817693 - -0.0011433870690484849'\n",
      "1270 - random_68 - lwr_k=200 - 259.1258985311438 - 202.76512686801266 - 154.04200618147954 - 239.15469200026217 - 325.480728762621 - 236.11199792645095 - -0.0011969585387803239'\n",
      "1271 - random_13 - lwr_k=200 - 259.1258985311438 - 202.76512686801266 - 154.04200618147954 - 239.15469200026217 - 325.480728762621 - 236.11199792645095 - -0.0011969585387803239'\n",
      "1272 - random_97 - lwr_k=700 - 258.58556663516737 - 202.7807626725799 - 154.02418307086293 - 239.1522139272845 - 326.0572710614742 - 236.1182189479065 - -0.0012233378325690047'\n",
      "1273 - random_30 - lwr_k=700 - 258.58556663516737 - 202.7807626725799 - 154.02418307086293 - 239.1522139272845 - 326.0572710614742 - 236.1182189479065 - -0.0012233378325690047'\n",
      "1274 - random_68 - lr - 258.12517623108977 - 202.82202175106715 - 155.1860546680503 - 239.31472787508113 - 325.6910373613933 - 236.2259191153791 - -0.0016800239859817712'\n",
      "1275 - random_13 - lr - 258.12517623108977 - 202.82202175106715 - 155.1860546680503 - 239.31472787508113 - 325.6910373613933 - 236.2259191153791 - -0.0016800239859817712'\n",
      "1276 - random_30 - lr - 258.12517623108977 - 202.82202175106715 - 155.1860546680503 - 239.31472787508113 - 325.6910373613933 - 236.2259191153791 - -0.0016800239859817712'\n",
      "1277 - random_97 - lr - 258.12517623108977 - 202.82202175106715 - 155.1860546680503 - 239.31472787508113 - 325.6910373613933 - 236.2259191153791 - -0.0016800239859817712'\n",
      "1278 - random_68 - lwr_k=700 - 259.59493752078185 - 202.93081069745588 - 154.4258089205637 - 239.16396425841356 - 325.0891172490957 - 236.23929745134492 - -0.001736752781625217'\n",
      "1279 - random_13 - lwr_k=700 - 259.59493752078185 - 202.93081069745588 - 154.4258089205637 - 239.16396425841356 - 325.0891172490957 - 236.23929745134492 - -0.001736752781625217'\n",
      "1280 - random_30 - lwr_k=1000 - 258.7442374328373 - 202.8097922500784 - 154.42424981930276 - 239.20711030373755 - 326.103897421353 - 236.25606250765142 - -0.0018078424489491596'\n",
      "1281 - random_97 - lwr_k=1000 - 258.7442374328373 - 202.8097922500784 - 154.42424981930276 - 239.20711030373755 - 326.103897421353 - 236.25606250765142 - -0.0018078424489491596'\n",
      "1282 - random_97 - deep - 257.31881903665544 - 203.05891169971022 - 154.4450619624932 - 240.8455825324531 - 325.715822133932 - 236.2748457100667 - -0.0018874876270833507'\n",
      "1283 - random_97 - lwr_k=600 - 259.0192013561998 - 202.87196186715641 - 153.99646446144416 - 239.1541726842799 - 326.46838085407285 - 236.30028204301513 - -0.0019953486523736075'\n",
      "1284 - random_30 - lwr_k=600 - 259.0192013561998 - 202.87196186715641 - 153.99646446144416 - 239.1541726842799 - 326.46838085407285 - 236.30028204301513 - -0.0019953486523736075'\n",
      "1285 - random_13 - lwr_k=400 - 259.6038910821257 - 202.9944832861782 - 154.54451008784514 - 239.18817623158188 - 325.188708450393 - 236.30231480137837 - -0.00200396825454785'\n",
      "1286 - random_68 - lwr_k=400 - 259.6038910821257 - 202.9944832861782 - 154.54451008784514 - 239.18817623158188 - 325.188708450393 - 236.30231480137837 - -0.00200396825454785'\n",
      "1287 - random_68 - lwr_k=500 - 259.9529062029303 - 202.96858441323474 - 154.4971345266114 - 239.17767814423362 - 325.1012651145647 - 236.33791591762554 - -0.0021549293646154766'\n",
      "1288 - random_13 - lwr_k=500 - 259.9529062029303 - 202.96858441323474 - 154.4971345266114 - 239.17767814423362 - 325.1012651145647 - 236.33791591762554 - -0.0021549293646154766'\n",
      "1289 - random_13 - lwr_k=300 - 258.09590095078113 - 203.52347572336492 - 155.38612961493422 - 239.49298593082753 - 325.2238696093164 - 236.3426597625932 - -0.002175044916536306'\n",
      "1290 - random_68 - lwr_k=300 - 258.09590095078113 - 203.52347572336492 - 155.38612961493422 - 239.49298593082753 - 325.2238696093164 - 236.3426597625932 - -0.002175044916536306'\n",
      "1291 - random_89 - lwr_k=300 - 258.09590095078113 - 203.52347572336492 - 155.38612961493422 - 239.49298593082753 - 325.2238710389594 - 236.34266004842817 - -0.002175046128575886'\n",
      "1292 - random_13 - lwr_k=40 - 256.79358301596466 - 203.12077375628115 - 153.7285221248652 - 239.9901680911497 - 328.4684209560569 - 236.4181769636129 - -0.0024952641033724188'\n",
      "1293 - random_89 - lwr_k=40 - 256.79358301596466 - 203.12077375628115 - 153.7285221248652 - 239.9901680911497 - 328.4684209560569 - 236.4181769636129 - -0.0024952641033724188'\n",
      "1294 - random_68 - lwr_k=40 - 256.79358301596466 - 203.12077375628115 - 153.7285221248652 - 239.9901680911497 - 328.4684209560569 - 236.4181769636129 - -0.0024952641033724188'\n",
      "1295 - random_68 - lwr_k=600 - 260.81003382270256 - 202.83608869620198 - 154.22907778865093 - 239.14239230379587 - 325.1035038467293 - 236.42271244438115 - -0.0025144961186471804'\n",
      "1296 - random_13 - lwr_k=600 - 260.81003382270256 - 202.83608869620198 - 154.22907778865093 - 239.14239230379587 - 325.1035038467293 - 236.42271244438115 - -0.0025144961186471804'\n",
      "1297 - random_89 - lwr_k=20 - 256.8262550211098 - 203.41807894380747 - 153.85614518910748 - 240.40914257409528 - 328.1241177503396 - 236.52465044001377 - -0.0029467486601666426'\n",
      "1298 - random_68 - lwr_k=20 - 256.8262550211098 - 203.41807894380747 - 153.85614518910748 - 240.40915116330638 - 328.1241177503396 - 236.5246521572934 - -0.002946755942029089'\n",
      "1299 - random_13 - lwr_k=20 - 256.8262550211098 - 203.41807894380747 - 153.85614518910748 - 240.40915116330638 - 328.1241177503396 - 236.5246521572934 - -0.002946755942029089'\n",
      "1300 - random_30 - lwr_k=500 - 259.558909858602 - 203.02911319063867 - 154.1634584586808 - 239.14262580825522 - 326.77274890158344 - 236.53165538970595 - -0.0029764520822210816'\n",
      "1301 - random_97 - lwr_k=500 - 259.558909858602 - 203.02911319063867 - 154.1634584586808 - 239.14262580825522 - 326.77274890158344 - 236.53165538970595 - -0.0029764520822210816'\n",
      "1302 - random_47 - lwr_k=20 - 259.6835614600812 - 203.07085730665392 - 155.205723013861 - 238.55079305017065 - 326.2393209693289 - 236.54835709033622 - -0.0030472730994515196'\n",
      "1303 - random_97 - lwr_k=300 - 260.1124541800321 - 203.22732665577945 - 153.75936178671398 - 239.42428687974535 - 326.46101921809844 - 236.59527618608266 - -0.00324622637743488'\n",
      "1304 - random_30 - lwr_k=300 - 260.1124541800321 - 203.22732665577945 - 153.75936178671398 - 239.42428687974535 - 326.46101921809844 - 236.59527618608266 - -0.00324622637743488'\n",
      "1305 - random_97 - lwr_k=40 - 258.2741094452383 - 202.74159861860107 - 155.84064702636212 - 239.25911987893102 - 327.05681665865535 - 236.63245190528784 - -0.0034038643514753275'\n",
      "1306 - random_30 - lwr_k=40 - 258.2741094452383 - 202.74159861860107 - 155.84064702636212 - 239.25911987893102 - 327.05681665865535 - 236.63245190528784 - -0.0034038643514753275'\n",
      "1307 - random_30 - lwr_k=400 - 259.7875721523742 - 203.1070479644621 - 153.71372588999847 - 239.16242582018933 - 327.44221689805363 - 236.64089632445337 - -0.003439671624546614'\n",
      "1308 - random_97 - lwr_k=400 - 259.7875721523742 - 203.1070479644621 - 153.71372588999847 - 239.16242582018933 - 327.44221689805363 - 236.64089632445337 - -0.003439671624546614'\n",
      "1309 - random_30 - lwr_k=30 - 258.0686211336244 - 202.73170229454743 - 158.05858341897093 - 239.26649853655593 - 325.34222825702756 - 236.69146569490007 - -0.0036541033363235442'\n",
      "1310 - random_97 - lwr_k=30 - 258.0686211336244 - 202.73170229454743 - 158.05858341897093 - 239.26649853655593 - 325.34222825702756 - 236.69146569490007 - -0.0036541033363235442'\n",
      "1311 - random_47 - lwr_k=30 - 258.06861681116345 - 202.73170224489297 - 158.05858341897093 - 239.51476073571797 - 325.50781982395 - 236.77420846641868 - -0.004004961454136424'\n",
      "1312 - random_97 - lwr_k=200 - 260.1027536847559 - 203.2235867254859 - 154.36769563065823 - 239.6079288831786 - 327.431647843001 - 236.94499222679127 - -0.004729143973256411'\n",
      "1313 - random_30 - lwr_k=200 - 260.1027536847559 - 203.2235867254859 - 154.36769563065823 - 239.6079288831786 - 327.431647843001 - 236.94499222679127 - -0.004729143973256411'\n",
      "1314 - random_47 - lwr_k=40 - 258.2741094452383 - 202.74159861860107 - 155.84064702636212 - 242.53030436727352 - 325.77107890269014 - 237.02941120776575 - -0.005087109801917622'\n",
      "1315 - random_30 - lwr_k=100 - 260.58330807810245 - 203.41863219256587 - 155.06046349402777 - 239.52894968753057 - 326.6448140663954 - 237.04558088758935 - -0.005155674865964022'\n",
      "1316 - random_97 - lwr_k=100 - 260.58330807810245 - 203.41863219256587 - 155.06046349402777 - 239.52894968753057 - 326.6448140663954 - 237.04558088758935 - -0.005155674865964022'\n",
      "1317 - random_47 - lwr_k=100 - 260.58330807810245 - 203.41863535462764 - 155.06046349402777 - 240.11847600424184 - 327.2914470359708 - 237.29273241085704 - -0.006203683249980552'\n",
      "1318 - random_68 - lwr_k=30 - 257.0080116711832 - 203.06402806817758 - 154.66697420139062 - 239.21977292456668 - 335.14295036741015 - 237.81779813343712 - -0.008430144459477873'\n",
      "1319 - random_13 - lwr_k=30 - 257.0080116711832 - 203.06402806817758 - 154.66697420139062 - 239.21977292456668 - 335.14295036741015 - 237.81779813343712 - -0.008430144459477873'\n",
      "1320 - random_89 - lwr_k=30 - 257.0080116711832 - 203.06402806817758 - 154.66697420139062 - 239.2197718587523 - 335.142962466401 - 237.81780033934976 - -0.008430153813314467'\n",
      "1321 - random_89 - lwr_k=10 - 258.17607514021637 - 203.77976920544637 - 154.05597525023143 - 240.88691530195547 - 333.66208619079424 - 238.10982780310792 - -0.009668451786700505'\n",
      "1322 - random_68 - lwr_k=10 - 258.17607514021637 - 203.77976920544637 - 154.05597525023143 - 240.88691026250652 - 333.6620973638047 - 238.10982902941845 - -0.009668456986683394'\n",
      "1323 - random_13 - lwr_k=10 - 258.17607514021637 - 203.77976920544637 - 154.05597525023143 - 240.88691026250652 - 333.6620973638047 - 238.10982902941845 - -0.009668456986683394'\n",
      "1324 - random_97 - lwr_k=20 - 259.68355496515034 - 203.07085730665392 - 155.205723013861 - 245.37664693772996 - 327.457909235224 - 238.15671732163844 - -0.009867279647005622'\n",
      "1325 - random_30 - lwr_k=20 - 259.68355496515034 - 203.07085730665392 - 155.205723013861 - 245.37664693772996 - 327.457909235224 - 238.15671732163844 - -0.009867279647005622'\n",
      "1326 - random_38 - lwr_k=20 - 269.01467673227614 - 149.97963750421906 - 167.95618718331102 - 426.1438117743607 - 185.44061767062252 - 239.69709266417956 - -0.016399006630404056'\n",
      "1327 - random_97 - lwr_k=10 - 269.73058314292217 - 208.8717691306591 - 158.91391942902428 - 242.26677110791502 - 328.18428155036804 - 241.59271416356944 - -0.024437100824673097'\n",
      "1328 - random_30 - lwr_k=10 - 269.73058314292217 - 208.8717691306591 - 158.91391942902428 - 242.26677110791502 - 328.18428155036804 - 241.59271416356944 - -0.024437100824673097'\n",
      "1329 - random_47 - lwr_k=10 - 269.73058314292217 - 208.87176440434544 - 158.91391942902428 - 238.968772245952 - 334.7116281727983 - 242.23837125215223 - -0.027174911351139697'\n",
      "1330 - random_57 - lwr_k=1000 - 1206.6206447904958 - 8.660504786011856 - 6.6160096915761155 - 10.180274544088874 - 20.65548698931396 - 250.66352995712973 - -0.06290051337352232'\n",
      "1331 - random_29 - lwr_k=10 - 374.6665700970755 - 467.6280522749774 - 248.54979017336416 - 109.98788125070638 - 129.11172777334838 - 266.03961764511166 - -0.12810047086246157'\n",
      "1332 - random_35 - lwr_k=900 - 8.504316590270966 - 7.844226109892719 - 6.745998058157239 - 12.661801223852889 - 1318.2077871257964 - 270.7068200726267 - -0.14789103176724216'\n",
      "1333 - random_16 - lwr_k=50 - 19.959654024741983 - 1291.1852035872448 - 26.632775553380437 - 35.950775339407485 - 84.25776462444041 - 291.716433968951 - -0.2369790989463878'\n",
      "1334 - random_22 - lwr_k=40 - 42.15990263430129 - 1341.30122621074 - 19.187167836471218 - 23.360315919724247 - 52.34260185569127 - 295.7999498904133 - -0.2542946261394883'\n",
      "1335 - random_74 - lwr_k=700 - 10.584664155485353 - 7.750752986462226 - 6.353377139887278 - 10.122699798197207 - 1454.613357595717 - 297.79041744117103 - -0.2627349005659898'\n",
      "1336 - random_89 - lwr_k=50 - 257.00615691693883 - 202.86772999757247 - 153.68673668899837 - 561.3625262341201 - 327.34371107472714 - 300.4302785428037 - -0.2739288294181048'\n",
      "1337 - random_89 - lwr_k=500 - 259.9529062029303 - 202.96858441323474 - 154.4971345266114 - 561.3625262341201 - 325.1012651145647 - 300.7537828764844 - -0.27530060026329584'\n",
      "1338 - random_25 - lwr_k=30 - 1347.070054088189 - 8.831074809045727 - 130.13941521958213 - 15.71558440351963 - 21.012549362476506 - 304.6760204394648 - -0.2919322511459661'\n",
      "1339 - random_47 - lwr_k=800 - 258.4357009150421 - 202.7587008658572 - 154.25256910019746 - 238.50728699409848 - 673.6287355224155 - 305.4920630981273 - -0.2953925557262749'\n",
      "1340 - random_47 - lwr_k=1000 - 258.7442374328373 - 202.8097922500784 - 154.42424981930276 - 238.58692648111398 - 673.6287355224155 - 305.6142715913216 - -0.2959107622247512'\n",
      "1341 - random_47 - lwr_k=500 - 259.5589162123648 - 203.02911319063867 - 154.1634584586808 - 238.4766991783689 - 673.6287355224155 - 305.7469936921855 - -0.29647355007493603'\n",
      "1342 - random_47 - lwr_k=200 - 260.1027536847559 - 203.2235867254859 - 154.36769563065823 - 238.62655187835873 - 673.6287355224155 - 305.96552321314795 - -0.29740019121837613'\n",
      "1343 - random_89 - lwr_k=400 - 259.6038910821257 - 202.9944832861782 - 154.54451008784514 - 239.18817705388446 - 673.6287355224155 - 305.96749804339095 - -0.2974085651851812'\n",
      "1344 - random_47 - lwr_k=700 - 609.2242796649789 - 202.7807626725799 - 154.02418307086293 - 238.47634269919467 - 326.0414048554324 - 306.1421088963845 - -0.2981489759075515'\n",
      "1345 - random_47 - lwr_k=600 - 609.2242796649789 - 202.8719604376457 - 153.99646446144416 - 238.50566033765344 - 326.41495818808704 - 306.23536330976907 - -0.29854440704138185'\n",
      "1346 - random_47 - lr - 609.2242796649789 - 202.82122827141663 - 155.1860546680503 - 238.6588137317869 - 325.62015643648897 - 306.3347643725937 - -0.2989659021057982'\n",
      "1347 - random_87 - lwr_k=20 - 212.8574442276803 - 107.204286921085 - 72.08289615399516 - 225.5612533826912 - 975.879829018885 - 318.6651732896911 - -0.35125112273733494'\n",
      "1348 - random_54 - lwr_k=10 - 73.46689158466512 - 149.27489213290482 - 126.60723884913374 - 1073.961014344518 - 207.19661192550114 - 326.0310070432874 - -0.38248481867810646'\n",
      "1349 - random_53 - lwr_k=1000 - 41.05728854974685 - 26.10949126830616 - 79.28327227004331 - 51.17431284006002 - 1487.1323125582467 - 336.851984606307 - -0.428369525595399'\n",
      "1350 - random_43 - deep - 275.1415472452067 - 280.4297133225661 - 455.92521381300065 - 386.12649023132576 - 330.8625879131508 - 345.6748692824943 - -0.46578162089348063'\n",
      "1351 - random_75 - lwr_k=10 - 1185.7057483613548 - 62.63106597248916 - 76.72943127906329 - 70.6642552776049 - 338.1745094988336 - 346.8718445226338 - -0.4708572151720041'\n",
      "1352 - random_57 - lwr_k=900 - 1702.311286082786 - 8.956563278706184 - 6.5943740487868014 - 9.909059179810974 - 20.628206902132828 - 349.84559432116674 - -0.48346694817961655'\n",
      "1353 - random_89 - lwr_k=1000 - 257.93787502861426 - 202.9353523793036 - 154.43454379812266 - 561.3625262341201 - 673.6287355224155 - 370.0140809685137 - -0.5689883433945426'\n",
      "1354 - random_89 - lwr_k=900 - 258.4410675894994 - 202.93105087893585 - 154.42627204344458 - 561.3625262341201 - 673.6287355224155 - 370.1122543864751 - -0.5694046328179325'\n",
      "1355 - random_89 - lwr_k=700 - 259.59493752078185 - 202.93081069745588 - 154.4258089205637 - 561.3625262341201 - 673.6287355224155 - 370.3430010839077 - -0.5703830790370628'\n",
      "1356 - random_89 - lwr_k=600 - 260.81003382270256 - 202.83608869620198 - 154.22907778865093 - 561.3625262341201 - 673.6287355224155 - 370.5278526777098 - -0.5711669140607847'\n",
      "1357 - random_57 - lwr_k=800 - 1828.5756551318123 - 9.077178611872425 - 7.361289902740595 - 9.253925304796553 - 20.80366185635441 - 375.19243683147124 - -0.5909463725745738'\n",
      "1358 - random_47 - lwr_k=400 - 609.2242796649789 - 203.1070479644621 - 153.71372588999847 - 238.48227536445225 - 673.6287355224155 - 375.64121270144597 - -0.5928493382858897'\n",
      "1359 - random_47 - lwr_k=50 - 609.2242796649789 - 202.80897057295073 - 153.942632907735 - 240.9189116017542 - 673.6287355224155 - 376.11450199929675 - -0.5948562494538041'\n",
      "1360 - random_53 - lwr_k=900 - 212.97380147862526 - 45.078629304487215 - 76.51005022444825 - 50.147359873314 - 1503.147389677402 - 377.4900492782772 - -0.6006890481431384'\n",
      "1361 - random_22 - lwr_k=10 - 128.48027551238198 - 1278.8958595679187 - 88.44518675897169 - 274.8329801493241 - 117.20275681596398 - 377.6782126961599 - -0.6014869264523597'\n",
      "1362 - random_57 - lwr_k=700 - 2070.3821172481025 - 9.3024150828941 - 24.602579563040482 - 12.771340678489281 - 18.00876916741818 - 427.21414152231915 - -0.8115364864691832'\n",
      "1363 - random_79 - lwr_k=20 - 219.4578479579192 - 382.841699299587 - 950.8237076111185 - 292.5712707211218 - 408.4157912321347 - 450.7730467310913 - -0.9114344351065646'\n",
      "1364 - random_0 - lwr_k=100 - 7.808718870000669 - 8.841756420392068 - 6.990555433580104 - 2236.287931267467 - 17.918126495459067 - 455.42294834088653 - -0.9311516345293647'\n",
      "1365 - random_39 - lwr_k=30 - 34.75552404783395 - 75.09549527426006 - 22.515202909168604 - 40.38530672007467 - 2149.320460070767 - 464.28029304328 - -0.9687098554357338'\n",
      "1366 - random_99 - lwr_k=50 - 263.8741775575069 - 655.855517735882 - 603.1233455451472 - 201.96989498551824 - 654.8872328170343 - 475.93676856166087 - -1.0181373641552192'\n",
      "1367 - random_31 - lwr_k=20 - 624.1540835067973 - 276.32591764638505 - 180.2941198231286 - 548.8887619827584 - 753.9416971793555 - 476.71224371216147 - -1.0214256483971127'\n",
      "1368 - random_53 - lwr_k=500 - 2057.9886130855266 - 50.01850914480003 - 67.60489667193771 - 43.94054661594579 - 171.26853996134776 - 478.3528044723711 - -1.0283821963821564'\n",
      "1369 - random_74 - lwr_k=50 - 10.234051633058034 - 24.669933930804437 - 120.23326014377861 - 2262.2323656259846 - 87.68114593836766 - 500.8517895226245 - -1.1237857150527328'\n",
      "1370 - random_59 - lwr_k=40 - 151.64710729418132 - 285.68827542152917 - 320.0829662575428 - 1792.5946791376423 - 44.80645503929877 - 518.8655517126416 - -1.2001703294512946'\n",
      "1371 - random_57 - lwr_k=500 - 2515.2200844436607 - 9.87484445640735 - 36.07156356296424 - 16.78557346811164 - 16.44313538967975 - 519.122586497728 - -1.201260246301464'\n",
      "1372 - random_61 - lwr_k=30 - 245.5267974390297 - 32.40823919030177 - 26.064452035280382 - 2264.5515624319714 - 30.52277678287787 - 519.6900407528169 - -1.2036664496257097'\n",
      "1373 - random_67 - lwr_k=20 - 295.1693427267858 - 892.7698350339219 - 1171.15007697898 - 222.00695605641167 - 181.3670700794715 - 552.5062395819277 - -1.342818541628862'\n",
      "1374 - random_41 - lwr_k=30 - 9.77372897786353 - 8.30985630250639 - 8.329635577374482 - 2785.0517070230135 - 76.47300644687085 - 577.4013920745563 - -1.4483826433131104'\n",
      "1375 - random_54 - lwr_k=500 - 8.840179608627226 - 15.087876669102684 - 8.584479071780597 - 10.935483471608038 - 2881.5168643700486 - 584.8053136454386 - -1.4797778448408874'\n",
      "1376 - random_57 - lwr_k=400 - 3039.875796565997 - 9.643813010194583 - 19.425655837024603 - 10.573748023323175 - 16.53714099740862 - 619.507791742824 - -1.626928416730435'\n",
      "1377 - random_95 - lwr_k=10 - 105.64975042962578 - 279.71769874377054 - 2805.3947702699106 - 31.13934163820415 - 177.48658747757528 - 679.7180770983817 - -1.8822409595073357'\n",
      "1378 - random_16 - lwr_k=40 - 48.40051903819347 - 2995.4543594315064 - 59.90503636387015 - 152.93463012721213 - 152.71928472740882 - 682.1578747220816 - -1.89258654965841'\n",
      "1379 - random_74 - lwr_k=40 - 14.278585825687005 - 43.782879392163125 - 60.19157788299058 - 3242.1773887743675 - 87.93172859428799 - 689.4560765348735 - -1.923533462657069'\n",
      "1380 - random_39 - lwr_k=20 - 1083.570999994839 - 1839.3383916169867 - 340.3691487289574 - 140.96063741714326 - 82.18232351948788 - 697.5345607419903 - -1.9577890442827366'\n",
      "1381 - random_57 - lwr_k=600 - 3457.480551862232 - 10.32393031989354 - 36.6747350276724 - 10.90118728612688 - 16.866052659114274 - 706.7857751217427 - -1.9970174095548967'\n",
      "1382 - random_61 - lwr_k=10 - 44.25451377095406 - 114.72375963182996 - 41.3187435284829 - 3337.761176926656 - 62.02992478444567 - 719.8078551065556 - -2.052235555981766'\n",
      "1383 - random_74 - lwr_k=600 - 9.37223294442105 - 7.793388370808795 - 6.448220652294669 - 10.009088700686869 - 3768.0780586333262 - 760.0940025476488 - -2.2230628271220976'\n",
      "1384 - random_74 - lwr_k=500 - 9.47908602372485 - 7.775399261139409 - 6.539649418857538 - 9.851559630592668 - 4138.363655679492 - 834.1314345590539 - -2.5370072789026925'\n",
      "1385 - random_96 - lwr_k=900 - 9.347915239395126 - 8.058993275172341 - 6.290446917343844 - 4359.214764731331 - 21.286161921084673 - 880.554037880052 - -2.733855256390535'\n",
      "1386 - random_75 - lwr_k=20 - 32.6195208510396 - 46.41240198987334 - 37.98155931461214 - 82.97112583024698 - 4440.380100681302 - 927.7819455060322 - -2.9341180041057573'\n",
      "1387 - random_7 - lwr_k=20 - 289.9286673034434 - 923.7499855390976 - 1402.3905641987396 - 837.8596390520378 - 1327.7553247657886 - 956.2223781696107 - -3.0547153262764484'\n",
      "1388 - random_53 - lwr_k=400 - 4571.7014125650485 - 10.64256768828869 - 63.36637798944668 - 41.25864537057426 - 114.13861308597396 - 960.6574003553524 - -3.073521362235704'\n",
      "1389 - random_78 - lwr_k=20 - 61.20688643994671 - 50.454226934992356 - 243.26673076084163 - 4364.557200207538 - 179.56761375361853 - 979.5079346077837 - -3.1534541810933225'\n",
      "1390 - random_54 - lwr_k=200 - 9.597855872853273 - 14.701682016762295 - 524.2377385935141 - 11.598686859081589 - 4755.671371445369 - 1062.817267940646 - -3.5067147179709828'\n",
      "1391 - random_59 - lwr_k=30 - 65.32905149287448 - 323.71510928471884 - 572.0271453930031 - 4432.028165329426 - 47.06338882528752 - 1087.7399535971801 - -3.6123955698424446'\n",
      "1392 - random_49 - lwr_k=20 - 1349.2490549677586 - 2119.7737025382376 - 339.3220957926073 - 1133.3902416948254 - 523.8075948694421 - 1093.3185929392616 - -3.63605094013759'\n",
      "1393 - random_99 - lwr_k=100 - 8.508415027369201 - 5376.9182627487535 - 128.68109463731935 - 11.65851402650937 - 78.73012586027663 - 1121.4140406249146 - -3.7551854060634744'\n",
      "1394 - random_53 - lwr_k=600 - 4971.175703768266 - 63.00551182636558 - 68.96378425664582 - 45.770163799911934 - 640.7545195333258 - 1158.379050988028 - -3.9119299013577455'\n",
      "1395 - random_24 - lwr_k=50 - 5882.351746150881 - 16.71550018209476 - 18.508528023478373 - 20.27835969069793 - 25.198815555021987 - 1193.1859699869965 - -4.05952333898021'\n",
      "1396 - random_24 - lwr_k=100 - 12.203332799083832 - 10.24513869478239 - 86.45470873978232 - 5977.649271307107 - 21.816509482934116 - 1221.2773783987914 - -4.17864067698063'\n",
      "1397 - random_74 - lwr_k=10 - 202.60028990595094 - 5153.004854125482 - 377.61573702121746 - 330.27570559859794 - 201.74771722392495 - 1253.5154577002813 - -4.315341341196084'\n",
      "1398 - random_71 - lwr_k=400 - 6277.42862743069 - 7.515575675904603 - 5.953282228296679 - 24.561851627741245 - 18.485449990737106 - 1267.403230567082 - -4.374230326411691'\n",
      "1399 - random_46 - lwr_k=500 - 8.715107929850939 - 8.863771804480026 - 8.646678364698078 - 10.761962119640714 - 6680.124668783533 - 1342.9853547816315 - -4.694724810165454'\n",
      "1400 - random_24 - lwr_k=200 - 10.061973826707321 - 8.63064487010438 - 9.317636065552183 - 6986.1508466577025 - 21.35427998763364 - 1406.645320667677 - -4.96466519771681'\n",
      "1401 - random_12 - lwr_k=30 - 9.750309533374123 - 38.38917252170549 - 10.452852435507033 - 65.4174505569056 - 6988.292479052351 - 1422.0024895935753 - -5.029784933077168'\n",
      "1402 - random_40 - lwr_k=100 - 62.24522073459101 - 16.39886276615236 - 18.633178697089612 - 7107.220861072426 - 28.951049469635635 - 1446.2289313900299 - -5.132513468923883'\n",
      "1403 - random_46 - lwr_k=300 - 8.527621636726668 - 8.788325072482348 - 8.67745044155857 - 10.868663938282483 - 7436.3766477692025 - 1494.1610904638342 - -5.335762487619826'\n",
      "1404 - random_52 - lwr_k=10 - 3056.8087835506985 - 773.0019200474516 - 801.2077966016362 - 2209.4383166910084 - 757.5344438132878 - 1519.727712464923 - -5.444173853464836'\n",
      "1405 - random_2 - lwr_k=20 - 1102.8928420305715 - 1309.5990505961956 - 478.8218503409509 - 2952.548867672125 - 1838.0157022778415 - 1536.2675473989063 - -5.514308503868311'\n",
      "1406 - random_35 - lwr_k=1000 - 8.311991955125105 - 7.899588700646616 - 6.695837189470143 - 12.947840321879276 - 8555.556085689002 - 1717.7221978760415 - -6.283739306901946'\n",
      "1407 - random_56 - lwr_k=200 - 8.42905464530597 - 9013.643283536432 - 7.197621528674107 - 99.94002209519047 - 56.516311241390724 - 1838.0209392743673 - -6.793847793814014'\n",
      "1408 - random_94 - lwr_k=10 - 782.5555412810986 - 6175.678175190288 - 845.9308913200027 - 1018.4079924888282 - 425.346412496318 - 1850.117463290752 - -6.8451412611534765'\n",
      "1409 - random_82 - lwr_k=30 - 446.6561939846385 - 258.32318546987096 - 7488.917938162282 - 1171.4338128015256 - 136.21685305236917 - 1899.8026964462363 - -7.055823923434181'\n",
      "1410 - random_72 - lwr_k=40 - 707.4411438655516 - 977.3632108185859 - 798.5616288777777 - 7438.499853046536 - 201.9582292787958 - 2024.3775976425436 - -7.584064814550878'\n",
      "1411 - random_81 - lwr_k=10 - 1239.9672168389843 - 32.52076743134149 - 8730.602444173106 - 85.53413266471318 - 85.77605453556468 - 2034.4220795292676 - -7.626656909842108'\n",
      "1412 - random_87 - lwr_k=50 - 9893.897037151137 - 114.64960305791888 - 14.494919309660679 - 18.392508016526303 - 226.02902581887372 - 2054.45897881539 - -7.711620328897425'\n",
      "1413 - random_46 - lwr_k=400 - 8.644063360089572 - 8.831818781380695 - 8.647777118862514 - 10.740119831840463 - 10524.789674788566 - 2111.641778789999 - -7.954095281115414'\n",
      "1414 - random_33 - lwr_k=10 - 17.041154855615776 - 27.930794520861152 - 6.4025203604772765 - 21.122324668213512 - 10858.260216997809 - 2185.442816910498 - -8.267037340613232'\n",
      "1415 - random_8 - lwr_k=30 - 5632.905084512449 - 86.66353397964066 - 4633.564810340803 - 902.5439085218713 - 22.857535758564296 - 2255.904805745844 - -8.565820670279171'\n",
      "1416 - random_82 - lwr_k=20 - 8488.079009516387 - 196.9076341883993 - 331.62838822580756 - 1551.5632472446998 - 928.3547504025719 - 2299.9757353460413 - -8.752696733601622'\n",
      "1417 - random_54 - lwr_k=400 - 8.983680795887805 - 14.687633044602865 - 83.2666660494652 - 11.088120951142724 - 11408.665726420399 - 2304.5872592762616 - -8.772251198320538'\n",
      "1418 - random_6 - lwr_k=20 - 214.9019314362685 - 303.5047452456722 - 356.899387519496 - 9509.467819454485 - 1267.3181673423876 - 2329.7401016776844 - -8.87890799480724'\n",
      "1419 - random_53 - lwr_k=700 - 9894.11151483832 - 43.57542616856599 - 70.99431647972402 - 47.310027269555874 - 1818.9253978705538 - 2375.832807667959 - -9.074357092919824'\n",
      "1420 - random_93 - lwr_k=50 - 5048.659243484498 - 6280.502940570451 - 77.637352138786 - 590.7085151920631 - 28.037666878975305 - 2406.176598032642 - -9.203025313469642'\n",
      "1421 - random_78 - lwr_k=100 - 10.125179041746549 - 8.548251233197963 - 12133.658780711796 - 13.758480171594103 - 12.828683136116002 - 2434.9892303002775 - -9.325200891361447'\n",
      "1422 - random_53 - lwr_k=800 - 10799.406848552713 - 14.9448659852907 - 73.45553478448377 - 49.069413980186425 - 1425.6721905774189 - 2473.4708534963374 - -9.488376352337099'\n",
      "1423 - random_5 - lwr_k=30 - 754.6727498675123 - 851.6309880425416 - 499.06115434364335 - 671.4593344559148 - 11699.65661957547 - 2894.6110065583744 - -11.274157016035563'\n",
      "1424 - random_34 - lwr_k=30 - 11.593505153408433 - 9.504593909285493 - 8.805252178902238 - 17.067652134013574 - 14575.610300013757 - 2923.561956695183 - -11.396919109780624'\n",
      "1425 - random_34 - lwr_k=40 - 9.546730154764047 - 8.461055071387758 - 8.190193925180134 - 17.44625397730832 - 14704.84336132567 - 2948.7344621932107 - -11.503659284633054'\n",
      "1426 - random_59 - lwr_k=20 - 630.6481778952358 - 537.0085889881226 - 921.6083014066982 - 12556.11980949273 - 362.08360298981654 - 3000.701927589689 - -11.724019405062556'\n",
      "1427 - random_6 - lwr_k=100 - 11.146806256969867 - 7.80847194608895 - 15209.335723556636 - 13.04308096059344 - 20.498436390000045 - 3051.3699788950166 - -11.938869557987791'\n",
      "1428 - random_93 - lwr_k=100 - 26.033487041771632 - 8.7852701063091 - 16508.91119891323 - 72.41095399575396 - 20.4704635013224 - 3326.2383012726273 - -13.104406806327221'\n",
      "1429 - random_8 - lwr_k=50 - 8.088067250895318 - 25.393661942914072 - 16913.738447272426 - 480.0483980009967 - 20.374616495348725 - 3488.391324364583 - -13.791991998792138'\n",
      "1430 - random_60 - lwr_k=50 - 200.2572081529245 - 8.041597147538141 - 2427.1433972530413 - 14764.337176439352 - 131.81069016258934 - 3505.203827287391 - -13.863282856264714'\n",
      "1431 - random_54 - lwr_k=300 - 9.187268173394886 - 14.997749864549345 - 9.991555655566124 - 11.116832311807265 - 18289.313111173327 - 3665.7243724403406 - -14.543945774716585'\n",
      "1432 - random_35 - lwr_k=600 - 8.262242385129984 - 7.56783542139115 - 6.769694177641328 - 12.962133017504156 - 19056.015265271242 - 3817.067554442611 - -15.185693482782881'\n",
      "1433 - random_34 - lwr_k=20 - 25.373115505898955 - 10.13958483240136 - 26.505055392682547 - 17.986689981366535 - 19886.63176575903 - 3992.0252704940053 - -15.927575025109721'\n",
      "1434 - random_53 - lwr_k=200 - 19586.172987904993 - 121.30655154839113 - 56.82467806653383 - 36.17217858753401 - 202.00900958625655 - 4002.41397738521 - -15.971626754094139'\n",
      "1435 - random_48 - lwr_k=50 - 17.389716036358458 - 425.82438512112327 - 288.7947294734886 - 19408.50004531754 - 1020.6848013830268 - 4230.925281500406 - -16.940594128395595'\n",
      "1436 - random_99 - lwr_k=30 - 190.31853263494523 - 21961.642167598668 - 2031.9208715134441 - 700.4181669621645 - 975.5975230423201 - 5173.912971557043 - -20.939189775865863'\n",
      "1437 - random_1 - lwr_k=100 - 11.151237705091782 - 4266.119411819766 - 24832.711022622516 - 19.690950424594035 - 19.22223919016628 - 5828.57014849535 - -23.715163805954695'\n",
      "1438 - random_55 - lwr_k=20 - 19434.416736175826 - 690.415160119582 - 598.9285554699809 - 1437.675710009265 - 7811.434432375523 - 5995.906308720103 - -24.424727301846037'\n",
      "1439 - random_0 - lwr_k=50 - 11.602976692199674 - 70.75942142071919 - 11.487977658554376 - 30165.09850323619 - 375.94811416868976 - 6124.986342077356 - -24.972071519591477'\n",
      "1440 - random_35 - lwr_k=400 - 30753.826593927668 - 7.5725529211457845 - 6.868765234874564 - 12.785777605091438 - 78.08451697167541 - 6174.843482803217 - -25.183483129736427'\n",
      "1441 - random_40 - lwr_k=10 - 162.7725295621529 - 110.52984652656689 - 31777.09417076208 - 233.35892935457147 - 95.5195905697073 - 6473.778968218185 - -26.45107351660872'\n",
      "1442 - random_60 - lwr_k=40 - 154.96409363429174 - 12.186166617372896 - 1558.6122244325022 - 30489.36648624401 - 168.35556926504998 - 6474.603205147565 - -26.454568567745714'\n",
      "1443 - random_57 - lwr_k=100 - 34862.3229495972 - 168.3651725253376 - 42.15678477642162 - 109.17915143838167 - 76.5524011988748 - 7055.142057510371 - -28.91625513345886'\n",
      "1444 - random_8 - lwr_k=20 - 15204.3788632619 - 169.78809993153985 - 19483.686997656223 - 1313.5460430616429 - 39.09815663201222 - 7242.245361222628 - -29.709638190035353'\n",
      "1445 - random_25 - lwr_k=40 - 37885.51373309955 - 8.582415822493088 - 70.44706198398896 - 12.162561373783793 - 22.83038517174495 - 7603.623341149053 - -31.242006462561378'\n",
      "1446 - random_93 - lwr_k=20 - 398.57841336021124 - 95.01775601242699 - 34832.68927483205 - 2457.432366075582 - 265.36283246372216 - 7607.40479138035 - -31.25804112621121'\n",
      "1447 - random_74 - lwr_k=30 - 23.52749963149107 - 87.62873247524576 - 417.3877664470195 - 37175.05386265267 - 356.64346785956906 - 7609.573574426346 - -31.267237520331165'\n",
      "1448 - random_57 - lwr_k=50 - 31865.796354809518 - 2915.4250113221997 - 3426.613846569328 - 17.1217417040614 - 84.76190621782035 - 7665.129842833915 - -31.50281541322058'\n",
      "1449 - random_46 - lwr_k=50 - 332.60632426557976 - 233.19608204687012 - 19957.783572478293 - 17.096045521158814 - 18895.561632867237 - 7884.758360540699 - -32.43411668493702'\n",
      "1450 - random_72 - lwr_k=50 - 68.2784248218854 - 63.64317275056026 - 51.50772484944215 - 2517.7153397255192 - 37938.99983061992 - 8125.388627356556 - -33.454472674400364'\n",
      "1451 - random_64 - lwr_k=10 - 1344.2516659648693 - 2167.6286089191112 - 810.2556289288544 - 36507.44425691911 - 639.998593346323 - 8291.77460930153 - -34.160007084043905'\n",
      "1452 - random_84 - lwr_k=50 - 140.38401560069357 - 29714.28228023428 - 9.479138787579537 - 9.789081518943567 - 16997.501017624523 - 9376.105690686778 - -38.7579478505701'\n",
      "1453 - random_61 - lwr_k=50 - 352.8799910601974 - 26.053147513093375 - 16.200960166293136 - 47674.54947459338 - 30.401816620334163 - 9616.9286383284 - -39.779120873722626'\n",
      "1454 - random_16 - lwr_k=20 - 58.79685076667721 - 5425.861738862146 - 42.20604494557596 - 335.3716697465976 - 43929.676773345156 - 9956.019408288848 - -41.2169836275617'\n",
      "1455 - random_16 - lwr_k=30 - 177.08380728915077 - 38719.4948906508 - 199.06735275409747 - 12522.635356284029 - 495.6970838885965 - 10425.751484533512 - -43.208810939152876'\n",
      "1456 - random_55 - lwr_k=40 - 40.367691636573355 - 1496.2203675967207 - 1237.3068932368867 - 67.37032842831289 - 51086.22668766206 - 10782.21782899487 - -44.72035216971466'\n",
      "1457 - random_53 - lwr_k=300 - 53729.8984503509 - 116.28330785110636 - 60.67939804088071 - 39.37619649622989 - 95.01777229954631 - 10813.528522913117 - -44.85312039748787'\n",
      "1458 - random_43 - lwr_k=40 - 814.1744273126187 - 113.7915586826473 - 58.2504333270887 - 52774.703623403315 - 375.4247481456732 - 10823.875054141137 - -44.89699328699748'\n",
      "1459 - random_72 - lwr_k=20 - 1592.8055166345825 - 1026.0053186574523 - 8833.659769698472 - 768.0493844345382 - 42074.58582634598 - 10855.893730455497 - -45.03276360626928'\n",
      "1460 - random_57 - lwr_k=10 - 52457.026523879074 - 158.82868313953045 - 190.2866621743242 - 301.1224449363389 - 1674.4774831924824 - 10961.375894831646 - -45.480044655436394'\n",
      "1461 - random_17 - lwr_k=30 - 38057.505058726405 - 1848.3771240489295 - 16884.136371082197 - 199.2265265968581 - 502.0582613217075 - 11501.02951735572 - -47.76836363236463'\n",
      "1462 - random_71 - lwr_k=200 - 58871.6129946356 - 7.555211594914433 - 6.280009630697875 - 54.12415738380134 - 18.101521603856714 - 11797.31439230665 - -49.02471450935105'\n",
      "1463 - random_56 - lwr_k=400 - 61198.247802225385 - 12.899883062350746 - 6.72261477964562 - 13.160493235937158 - 28.74515882078637 - 12257.965872806426 - -50.97803693799963'\n",
      "1464 - random_22 - lwr_k=30 - 58.27331791901278 - 62121.877830430865 - 591.0535168388068 - 248.235556476387 - 236.6062563947912 - 12657.247895997943 - -52.671131531791026'\n",
      "1465 - random_44 - lwr_k=100 - 13.273237118038388 - 67696.10005884242 - 302.7801480601556 - 11.110693193243641 - 29.399625241177688 - 13617.162572008112 - -56.7415034845856'\n",
      "1466 - random_54 - lwr_k=100 - 39166.608897914484 - 23143.319546206665 - 783.4928089603984 - 11.98285611398087 - 7325.966465622211 - 14091.863992188732 - -58.75440254212068'\n",
      "1467 - random_46 - lwr_k=40 - 787.8481695806091 - 1331.4538773416295 - 43319.104252709156 - 224.015521783828 - 26554.48924070186 - 14438.999133599456 - -60.22637622763669'\n",
      "1468 - random_48 - lwr_k=40 - 162.33894585439515 - 1977.9369112964844 - 1325.3397874628529 - 2870.0445431599387 - 66959.52536195025 - 14654.586840031123 - -61.140543054707344'\n",
      "1469 - random_70 - lwr_k=40 - 11.188065464031688 - 143.56180888920548 - 75252.00069646933 - 14.266092650145133 - 110.05836536227486 - 15101.293165233645 - -63.03473317668545'\n",
      "1470 - random_28 - lwr_k=50 - 10.190324791217671 - 78932.42371537995 - 28.513011261622037 - 16.29850634734257 - 25.017707326696364 - 15810.24002631989 - -66.04091434205878'\n",
      "1471 - random_61 - lwr_k=40 - 171.58865078385847 - 25.055030248828327 - 18.493747274307523 - 79491.99449322409 - 22.89557131885251 - 15940.815492459602 - -66.59459971470802'\n",
      "1472 - random_88 - lwr_k=10 - 48352.303573858095 - 25274.155951731584 - 7028.5777894526655 - 1033.8315578593188 - 883.8958735600676 - 16521.200624685665 - -69.05563445359863'\n",
      "1473 - random_76 - lwr_k=100 - 8.434737436053153 - 10.754838629871445 - 82718.03655155074 - 9.625262968280431 - 19.501293644265665 - 16547.852598169306 - -69.16864808706588'\n",
      "1474 - random_92 - lwr_k=50 - 113.00209015595138 - 393.9152034150409 - 13275.600085561182 - 17.29429526240714 - 70291.95392102287 - 16812.92823142268 - -70.29266092896596'\n",
      "1475 - random_84 - lwr_k=40 - 263.3525118799989 - 10464.55787251669 - 20.413678332298137 - 13.238706915747654 - 75842.07467327495 - 17316.811728866767 - -72.4293021396174'\n",
      "1476 - random_17 - lwr_k=20 - 12675.311516991032 - 620.90560504706 - 1152.9694587302845 - 56258.64152229281 - 16140.443744872016 - 17366.14313794801 - -72.63848446481191'\n",
      "1477 - random_35 - lr - 9.107085792280913 - 9.216578532201554 - 6.8909126333430475 - 13.190005965008847 - 88080.99697310278 - 17618.111613488065 - -73.70691840113791'\n",
      "1478 - random_35 - lwr_k=700 - 8.260707211880042 - 7.826426937344036 - 6.736046508907053 - 12.908092462731748 - 88182.00669448519 - 17637.77208868203 - -73.79028565117387'\n",
      "1479 - random_14 - lwr_k=30 - 19291.815082625548 - 16288.386601987158 - 33388.71241823781 - 4493.965158227015 - 32842.3159427413 - 21259.902332658818 - -89.14937716515334'\n",
      "1480 - random_88 - lwr_k=900 - 48167.18481703394 - 11.055837543213352 - 7.718577482955611 - 57460.65968201481 - 714.3061990833004 - 21273.107549340224 - -89.20537187484356'\n",
      "1481 - random_60 - lwr_k=30 - 267.9348736278418 - 12.856523142905406 - 68284.05363542972 - 40389.68989138224 - 135.64320058959845 - 21810.93634861817 - -91.48595296678525'\n",
      "1482 - random_48 - lwr_k=30 - 169.67107627599165 - 5148.433685461329 - 2922.0709061747293 - 69149.79203516098 - 31997.751673929008 - 21871.249953181792 - -91.74170361892459'\n",
      "1483 - random_57 - lwr_k=40 - 114835.3854244178 - 315.48673810198363 - 3675.3624712469555 - 20.70616348349374 - 97.83810730727174 - 23800.020618082082 - -99.9203617996856'\n",
      "1484 - random_76 - lwr_k=30 - 31279.096374660036 - 15.78894934671746 - 35.43833758275225 - 84661.54650600636 - 10442.43771736851 - 25283.70473438113 - -106.21169827439377'\n",
      "1485 - random_1 - lwr_k=10 - 45.38939969990349 - 65.51900876060057 - 52.802076765279416 - 130204.72389313068 - 73.80054846113995 - 26079.92136139721 - -109.58793358775056'\n",
      "1486 - random_88 - lwr_k=700 - 8496.785036414392 - 11.048293792773258 - 7.699467066257807 - 122838.51220410317 - 696.0726033681633 - 26402.76755014305 - -110.956913669605'\n",
      "1487 - random_60 - lwr_k=20 - 631.7779541166198 - 23.372900135609044 - 81675.58058747527 - 66061.16447084588 - 136.4194320157787 - 29696.041949663933 - -124.92154207218633'\n",
      "1488 - random_40 - lwr_k=50 - 61.268543703713156 - 99.84482759070116 - 3382.1177243166558 - 143071.1908654694 - 2579.8769699757636 - 29829.1141490406 - -125.48581446178123'\n",
      "1489 - random_88 - lwr_k=1000 - 8.00759880720391 - 11.060355559483739 - 7.744280754983567 - 152058.76409818066 - 209.488310581719 - 30449.04093632501 - -128.1145195652771'\n",
      "1490 - random_21 - lwr_k=10 - 19.45853012489683 - 162225.49177007598 - 26.859285086586393 - 24.045408982984153 - 55.00462250942661 - 32486.105214080308 - -136.7523803141913'\n",
      "1491 - random_54 - lwr_k=40 - 194.5955328071583 - 23444.62934990187 - 44995.9365955074 - 8388.375113168471 - 89130.47210372186 - 33223.78972353169 - -139.88041909964073'\n",
      "1492 - random_74 - lwr_k=20 - 42.81258892419844 - 134.29792690875593 - 134.93378999553872 - 31049.040569773642 - 136347.9682883382 - 33530.85492446159 - -141.18248230669505'\n",
      "1493 - random_95 - lwr_k=200 - 170517.19833411643 - 8.190253047445271 - 9.901574547432505 - 12.946488685176764 - 25.738304922861055 - 34131.5455721356 - -143.7296194905549'\n",
      "1494 - random_88 - lwr_k=800 - 7.583618194726902 - 11.096436170069994 - 7.65789751767619 - 173531.92434209955 - 940.1484057416891 - 34888.25579571261 - -146.93833391842782'\n",
      "1495 - random_81 - lwr_k=200 - 177568.28047403437 - 14.486882608402988 - 21.259233560364763 - 14.681113173698085 - 22.21083604226741 - 35545.627035203426 - -149.72581651731758'\n",
      "1496 - random_55 - lwr_k=50 - 18.194692978142886 - 174.29082824563528 - 16.675120312042782 - 2693.365147946691 - 175383.93044401403 - 35645.645250797534 - -150.14992852403776'\n",
      "1497 - random_63 - lwr_k=30 - 10600.352259673815 - 4044.6458894119837 - 116825.23764984624 - 46403.96158448412 - 2362.1920308094873 - 36037.87071768767 - -151.81310086581'\n",
      "1498 - random_61 - lwr_k=20 - 4129.438151992189 - 224.03826438238676 - 4306.751375459118 - 185281.78836665986 - 704.5509217490307 - 38917.277203437465 - -164.02278542757267'\n",
      "1499 - random_24 - lwr_k=40 - 99642.94367275087 - 24.565476340394813 - 3082.461075000317 - 98745.77094072998 - 30.969098671097385 - 40308.462541489054 - -169.92189492413823'\n",
      "1500 - random_57 - lwr_k=30 - 19501.20867144908 - 414.81535294066646 - 209962.86846124005 - 30.132524230096763 - 95.68085220273615 - 45989.13736394349 - -194.00993112999365'\n",
      "1501 - random_95 - lwr_k=20 - 175217.92728933427 - 17.861553211332343 - 1590.9516338490814 - 1012.9910113730754 - 56211.99012754395 - 46823.708511730496 - -197.54880294585473'\n",
      "1502 - random_58 - lwr_k=20 - 155.99215717920902 - 249796.4027022541 - 655.7556929690119 - 10.453426215840897 - 18.091394513798964 - 50151.851554847475 - -211.66128651980966'\n",
      "1503 - random_53 - lwr_k=100 - 257901.25662586838 - 50.04461557097489 - 52.90140608304329 - 36.904933006176535 - 1418.9146847536847 - 51917.24881073376 - -219.14718464751363'\n",
      "1504 - random_12 - lwr_k=20 - 13.390701795387082 - 17.79649744731181 - 210.5555461591009 - 60713.26926836047 - 205519.2199074755 - 53277.39779974301 - -224.91468923395928'\n",
      "1505 - random_76 - lwr_k=50 - 50.75056783068809 - 11.985045876345495 - 217751.15375072538 - 50663.737026640825 - 19.398032662396204 - 53681.82897609217 - -226.6296180273075'\n",
      "1506 - random_89 - lr - 258.12517623108977 - 202.82202175106715 - 155.1860546680503 - 281086.6849708664 - 673.6287355224155 - 56456.8696060561 - -238.3967550764247'\n",
      "1507 - random_93 - lwr_k=40 - 2240.084659991504 - 4509.311686757578 - 254482.27595440272 - 23032.9166778415 - 75.5613614222722 - 56850.511374159854 - -240.06593302773777'\n",
      "1508 - random_22 - lwr_k=20 - 14120.578807049766 - 198095.71484132187 - 5127.016134031195 - 3952.794256275933 - 66249.92114693571 - 57525.12085234707 - -242.92651175179256'\n",
      "1509 - random_62 - lwr_k=50 - 11616.809499713358 - 16.30150117284617 - 276676.96513419185 - 13.487789845874465 - 35.46157165383744 - 57654.82286594438 - -243.47649338199514'\n",
      "1510 - random_8 - lwr_k=10 - 243009.06251895629 - 32119.153497078853 - 16043.187838775504 - 5705.248179329425 - 1539.941716101741 - 59708.82416654024 - -252.18617299612413'\n",
      "1511 - random_54 - lwr_k=50 - 156572.4726280488 - 6816.066843606347 - 116709.6540532719 - 1577.1753449379319 - 26452.596259439666 - 61632.16537782099 - -260.34180840591983'\n",
      "1512 - random_3 - lwr_k=30 - 11.28305960692354 - 18.005130420905754 - 9.921808959750368 - 27535.94727313571 - 288227.593718901 - 63139.870353402745 - -266.7350016751431'\n",
      "1513 - random_46 - lwr_k=100 - 13.21179161705015 - 327456.2029794944 - 13.457496661128927 - 11.19235512129503 - 21.92764734524795 - 65535.368589607504 - -276.89274702199464'\n",
      "1514 - random_78 - lwr_k=50 - 736.5658582277817 - 24.53268332576501 - 288095.61169998936 - 51544.754029179276 - 90.81405860431194 - 68076.27852278519 - -287.667088518232'\n",
      "1515 - random_26 - lwr_k=40 - 46.47449320269316 - 23.031187220478305 - 202889.93166550208 - 20.57749105069438 - 193015.89395326073 - 79173.25595849998 - -334.72213085060844'\n",
      "1516 - random_76 - lwr_k=40 - 218.94014137106112 - 12.76796956592074 - 201466.5309616241 - 194593.04743754576 - 22.781824348093387 - 79236.89358841992 - -334.9919765258981'\n",
      "1517 - random_87 - lwr_k=40 - 430316.15775468526 - 72.16230101867049 - 47.55461452370085 - 43.00202757811671 - 411.0296917385842 - 86220.2331785966 - -364.60376423482364'\n",
      "1518 - random_81 - lwr_k=50 - 206313.14575593712 - 241083.06339750782 - 1084.544866720134 - 699.381180573846 - 134.2986719257428 - 89906.71687697349 - -380.2357367687542'\n",
      "1519 - random_26 - lwr_k=50 - 42.11530508101129 - 34.54738203694706 - 280073.4963678983 - 19.580075626999744 - 176923.39586259733 - 91388.70058775737 - -386.5198629329105'\n",
      "1520 - random_71 - lwr_k=30 - 253061.9546001791 - 28741.798505257095 - 118401.7720068807 - 86811.18474410956 - 275.33242483847187 - 97472.63588972164 - -412.31786377050776'\n",
      "1521 - random_81 - lwr_k=100 - 486944.0716786284 - 24.310745318279423 - 40.04530497144249 - 222.8623098337719 - 34.4065860908201 - 97500.96347819827 - -412.43798259409414'\n",
      "1522 - random_0 - lwr_k=40 - 38.562919876484116 - 77.33551996742165 - 42.47087690823 - 491803.3726008727 - 18.91169925870477 - 98363.92565329213 - -416.097242236237'\n",
      "1523 - random_80 - lwr_k=10 - 257.04054025595946 - 89.92354120693916 - 137.42580293665472 - 85009.67961378292 - 421953.0891490669 - 101456.2514610075 - -429.2097787472364'\n",
      "1524 - random_6 - lwr_k=50 - 14.28473609184473 - 57.30459596276981 - 704365.5937339514 - 20381.822568832864 - 20.883910274021748 - 144920.51362762653 - -613.5133612356896'\n",
      "1525 - random_99 - lwr_k=40 - 2301.6954246648916 - 6623.9843442608335 - 12207.994821411132 - 242.73536520858886 - 782603.2498524095 - 160744.73396189325 - -680.6135569438743'\n",
      "1526 - random_42 - lwr_k=40 - 114.92939346608956 - 838130.753247877 - 21.138284865259042 - 37.92046333313406 - 27.199168488731157 - 167748.7383500895 - -710.312970580782'\n",
      "1527 - random_40 - lwr_k=40 - 1635.284336947769 - 128.11245509890887 - 1038.9053441110584 - 887954.24192296 - 322.16280036107713 - 178157.66570692905 - -754.4504413693911'\n",
      "1528 - random_71 - lwr_k=20 - 348579.3826997478 - 1745.5605948010655 - 51.07773301352298 - 5091.440098170186 - 594133.447345723 - 189915.34864695292 - -804.3071047426861'\n",
      "1529 - random_89 - lwr_k=800 - 258.4118338148922 - 202.9520433757571 - 154.46625725025214 - 561.3625262341201 - 1011014.7620852619 - 202372.1693316958 - -857.1283552176938'\n",
      "1530 - random_62 - lwr_k=30 - 8208.996323799456 - 19.92065511104261 - 1038723.4012296085 - 17.331476986693225 - 29.60131211669542 - 209332.62066236706 - -886.6430887489227'\n",
      "1531 - random_40 - lwr_k=20 - 274.5592776562157 - 1925.1324556521643 - 521205.4831493851 - 532712.9518206486 - 9205.539683735951 - 212995.31625190043 - -902.1741914311189'\n",
      "1532 - random_92 - lwr_k=20 - 1789.2827538926742 - 62136.94715426961 - 866.87052236613 - 2028.2011670406398 - 1044740.0387827003 - 222249.93005308934 - -941.4169714320218'\n",
      "1533 - random_56 - lwr_k=50 - 167196.7402790781 - 958749.0002472692 - 37.836420493811616 - 2006.672009470033 - 4515.906598194885 - 226611.42323114097 - -959.9112188353797'\n",
      "1534 - random_88 - lwr_k=600 - 7.4957220442404395 - 11.082880117034401 - 7.715664183974833 - 4199.754952601414 - 1186915.7445798395 - 238150.34367233008 - -1008.8402531580582'\n",
      "1535 - random_62 - lwr_k=40 - 13336.877361778712 - 19.171966420425242 - 1196188.690278428 - 14.26195682769224 - 33.34941816151488 - 241841.43058750313 - -1024.491744931571'\n",
      "1536 - random_95 - lwr_k=100 - 32772.90954724641 - 8.915435596735977 - 10.209649044116464 - 21.11294972467039 - 1434367.5941152214 - 293345.41795264714 - -1242.8865573742014'\n",
      "1537 - random_10 - lwr_k=30 - 1666.9006707776969 - 19.105414176614822 - 1403425.101223031 - 25.830103962207964 - 97585.35075451223 - 300446.30749260396 - -1272.9967977379258'\n",
      "1538 - random_72 - lwr_k=30 - 1256959.5781056522 - 32834.17358731353 - 20473.405285459663 - 196168.3309016378 - 209.67037226942904 - 301441.5477284741 - -1277.2169623459054'\n",
      "1539 - random_88 - lwr_k=400 - 7.43259674775439 - 10.880883430608854 - 7.602320341412614 - 10.451950306763516 - 1606426.0263902978 - 321187.26077612466 - -1360.945650515515'\n",
      "1540 - random_91 - lwr_k=40 - 25169.66580335409 - 672876.7181457571 - 18.873081106864046 - 853483.2174246892 - 56765.20777368336 - 321671.6968044962 - -1362.9998276961394'\n",
      "1541 - random_98 - lwr_k=20 - 14.923389913182637 - 1644344.6230815197 - 9.710147021098347 - 12.695672578613486 - 33.6982368301633 - 329044.6813232342 - -1394.2638453671661'\n",
      "1542 - random_1 - lwr_k=40 - 7115.855526486206 - 40051.133000780144 - 158490.5119971238 - 667118.9103284839 - 808373.0941504365 - 336127.51147823303 - -1424.2975076600944'\n",
      "1543 - random_88 - lwr_k=500 - 7.312562649442789 - 10.995602988862977 - 7.615646932923032 - 2476.154459644736 - 1701230.2667327116 - 340634.87990161823 - -1443.4103168190459'\n",
      "1544 - random_0 - lwr_k=30 - 56.51572018382265 - 297.72258887961794 - 44.5136757866645 - 1846337.0778127327 - 744.4323785501253 - 369375.1030547515 - -1565.2788548914152'\n",
      "1545 - random_60 - lwr_k=10 - 4621.624852339029 - 199873.47900658398 - 93029.07526919738 - 1675110.9610483458 - 22879.23325437165 - 399005.6566338484 - -1690.9227035045344'\n",
      "1546 - random_10 - lwr_k=40 - 8.828852042445233 - 17.60427982625875 - 1142.1591751594965 - 23.107499396860057 - 2055537.7702170468 - 411211.1854153907 - -1742.6783889451544'\n",
      "1547 - random_28 - lwr_k=30 - 20.404667994372613 - 2079768.8825081405 - 24.263131084027563 - 18.220556769353262 - 37.0892068649805 - 416178.101757604 - -1763.7398410475994'\n",
      "1548 - random_78 - lwr_k=30 - 268.0508421524719 - 328.21841181574973 - 13217.515572939916 - 2370496.6007712204 - 252.36504665659305 - 476756.46222474816 - -2020.6131502636072'\n",
      "1549 - random_51 - lwr_k=20 - 2509972.2398212627 - 482.7166118469849 - 155.205723013861 - 245.37664693772996 - 274.63009291689184 - 502472.6365119408 - -2129.6586697954467'\n",
      "1550 - random_46 - lwr_k=200 - 8.716644993576068 - 9.101908033518402 - 2583041.382054069 - 10.875643400648872 - 13215.176560296426 - 519087.00040945347 - -2200.109348915225'\n",
      "1551 - random_90 - lwr_k=50 - 10.232264833246237 - 17.026098006387674 - 2689788.419237058 - 14.270228863450015 - 18.648723478507296 - 537793.5424346892 - -2279.4315906688103'\n",
      "1552 - random_1 - lwr_k=50 - 788.5794141838793 - 7310.45851831958 - 49991.706224517315 - 127710.24020733131 - 2768256.003954835 - 590619.2372319729 - -2503.430902132064'\n",
      "1553 - random_90 - lwr_k=40 - 11.641275581445402 - 20.96462255379151 - 747994.7565471585 - 2335004.722059782 - 21.910474943536634 - 616408.8685895697 - -2612.7879051806995'\n",
      "1554 - random_80 - lwr_k=20 - 20.560758503063077 - 10.690381977233002 - 24.56512144702074 - 3091021.4763571536 - 119594.78852913372 - 641924.1267943524 - -2720.9814706720967'\n",
      "1555 - random_35 - lwr_k=200 - 3491813.8843788626 - 3492.4364562321707 - 5343.543989800611 - 290.8453985242514 - 9941.70142287023 - 702518.8686656223 - -2977.9242427368004'\n",
      "1556 - random_41 - lwr_k=20 - 11.532913350608627 - 8.991619997576443 - 42.74287686658428 - 188558.50100534633 - 3606070.5655429303 - 758689.9230862702 - -3216.1090420598202'\n",
      "1557 - random_96 - lwr_k=200 - 3550235.2212575804 - 8.451021928167343 - 7.090089662013813 - 414860.7043198358 - 281.48879571562276 - 793400.203425434 - -3363.292698167056'\n",
      "1558 - random_56 - lwr_k=100 - 4088134.5838346872 - 1022.3868010190305 - 221.5429117486868 - 30378.679177396632 - 1328.9039738067302 - 824616.8791615681 - -3495.662256161498'\n",
      "1559 - random_80 - lwr_k=40 - 12.73430485312267 - 8.19348829053126 - 10.188293292322706 - 4487997.842424491 - 6795.42544672935 - 898670.4754773361 - -3809.6752502123163'\n",
      "1560 - random_57 - lwr_k=20 - 166510.4103240144 - 23262.859852616883 - 883490.6803574804 - 3463465.666174035 - 212.43474146565953 - 907122.3216129282 - -3845.514016218729'\n",
      "1561 - random_26 - lwr_k=100 - 38.61859552363462 - 121.91891904862199 - 15.132009038047693 - 16.30757532650986 - 4597345.306106029 - 919206.350635464 - -3896.7545004402286'\n",
      "1562 - random_46 - lwr_k=20 - 62953.586699191925 - 3246348.1539955945 - 853742.8600203558 - 425376.5411026488 - 171857.77563761658 - 952285.8784925081 - -4037.0231990714947'\n",
      "1563 - random_78 - lwr_k=40 - 1720.9839166498532 - 310.2137823499007 - 1423738.6723984606 - 442706.81088065583 - 3071358.2770573352 - 987643.7718943813 - -4186.952959715513'\n",
      "1564 - random_40 - lwr_k=30 - 1485.6265878788765 - 888.646880342417 - 414.69678692994626 - 5018384.429270875 - 6489.445414276408 - 1005203.6528484653 - -4261.412959876151'\n",
      "1565 - random_96 - lwr_k=500 - 9.027398281179222 - 8.182657308498019 - 6.484964469762199 - 58.23123039043939 - 5423052.077629678 - 1084271.5958650224 - -4596.688527239542'\n",
      "1566 - random_90 - lwr_k=30 - 16.681202190855426 - 39.35395715818292 - 5761251.805659275 - 18.70897247042831 - 52.11651236662635 - 1151898.3801525608 - -4883.449603927956'\n",
      "1567 - random_84 - lwr_k=30 - 130796.77956793153 - 5321523.50203602 - 85.29527205327012 - 37.27289206241758 - 437506.14749643183 - 1178496.8144656229 - -4996.236212698369'\n",
      "1568 - random_28 - lwr_k=40 - 11.43516614441183 - 6768091.285281132 - 16.423284437312038 - 15.829711864213639 - 32.84227379685768 - 1354298.51084188 - -5741.696525023093'\n",
      "1569 - random_26 - lwr_k=30 - 53.52708478726066 - 1542627.3870776058 - 4276.194521771221 - 24.021519971224677 - 5246740.623075092 - 1358551.9799685767 - -5759.732712892775'\n",
      "1570 - random_53 - lwr_k=50 - 6703456.750397835 - 365215.52127989545 - 75.3505246742646 - 45.977040220655425 - 1292.7208807632453 - 1414711.653699416 - -5997.869254134365'\n",
      "1571 - random_81 - lwr_k=40 - 4854155.397052656 - 2347182.413366933 - 5095.7106847744335 - 16962.366512001972 - 4413.537713292454 - 1446267.6676171492 - -6131.677723994009'\n",
      "1572 - random_71 - lwr_k=50 - 145632.60767210214 - 24.767363542828335 - 30.020819455329068 - 4901530.467708265 - 2645476.490761117 - 1538058.86098006 - -6520.904296225288'\n",
      "1573 - random_71 - lwr_k=40 - 6227175.1355029605 - 37.92979453294002 - 10402.256317234342 - 654011.2845747174 - 1077354.509226992 - 1594293.9503829654 - -6759.360626135654'\n",
      "1574 - random_1 - lwr_k=20 - 597425.9026830338 - 45013.83674531263 - 7307.534808537528 - 7376665.5771064665 - 2447.2238978068463 - 1605351.3331601243 - -6806.247773350168'\n",
      "1575 - random_84 - lwr_k=20 - 1030764.6305215585 - 7174648.883815641 - 1378.561162013831 - 639.9432854237104 - 145143.94235993648 - 1671311.7180402593 - -7085.942737206303'\n",
      "1576 - random_53 - lwr_k=40 - 8164125.36354634 - 282493.2327909529 - 478.2713410586586 - 55.387964503968085 - 125.75508213845507 - 1690285.421596783 - -7166.397836734643'\n",
      "1577 - random_46 - lwr_k=30 - 6059.270779445238 - 44198.789306329345 - 934.1003946757145 - 8333928.488865035 - 337432.63129725924 - 1743947.570611573 - -7393.9440048840415'\n",
      "1578 - random_43 - lwr_k=30 - 1080.5484976056987 - 306.6918555634195 - 675.9164736578161 - 8985979.66527372 - 1503.2380631446338 - 1797320.6364336365 - -7620.264359792841'\n",
      "1579 - random_80 - lwr_k=50 - 12.050752337782015 - 7.584269143894845 - 7.645085337626196 - 9274563.39524951 - 20657.81539041811 - 1858440.8755259207 - -7879.435422770419'\n",
      "1580 - random_55 - lwr_k=30 - 7004842.891860643 - 62837.234179480045 - 11032.907229115062 - 71268.45226718286 - 2224154.374956563 - 1875370.4872749264 - -7951.222862380442'\n",
      "1581 - random_53 - lwr_k=30 - 10761464.240795048 - 119831.95015115773 - 14617.591133900118 - 79.28999564826367 - 8193.226665515886 - 2181904.82511119 - -9251.035025356447'\n",
      "1582 - random_6 - lwr_k=40 - 176.36440266481642 - 78.53007196683335 - 8736999.626679013 - 2387982.3551939824 - 409.6604551472409 - 2224400.6347356574 - -9431.232032371368'\n",
      "1583 - random_70 - lwr_k=100 - 9.312981509702745 - 142.59132929828357 - 11312667.624230675 - 12.552881485765685 - 19.901899798692536 - 2261829.445479188 - -9589.943202524693'\n",
      "1584 - random_28 - lwr_k=10 - 57.297985513121915 - 11583670.53043068 - 103.90479590138004 - 168.95705844085083 - 106.32288949698217 - 2317959.454870938 - -9827.95395665536'\n",
      "1585 - random_54 - lwr_k=30 - 31569.982415979193 - 1888743.004802364 - 1053804.80522221 - 46791.97024798557 - 9187860.013267502 - 2441268.7423333144 - -10350.828205533522'\n",
      "1586 - random_64 - lwr_k=100 - 9.206676614675532 - 142.40160598469927 - 15.30865859793903 - 15529333.736020744 - 73.69118715395739 - 3104897.7281503715 - -13164.845824431406'\n",
      "1587 - random_6 - lwr_k=30 - 101501.11747874616 - 502.58088984521214 - 15511170.01033574 - 1505.5161714012347 - 94299.39090749237 - 3140783.509672455 - -13317.014014232116'\n",
      "1588 - random_0 - lwr_k=20 - 14950877.544590224 - 60055.80320489015 - 7039.469907954662 - 775155.7355172646 - 730741.9709750011 - 3306149.802421861 - -14018.224587178569'\n",
      "1589 - random_47 - lwr_k=900 - 16624033.739832658 - 202.7443408414415 - 154.56868154135498 - 238.69898176410254 - 673.6287355224155 - 3326693.9027612554 - -14105.33871503464'\n",
      "1590 - random_24 - lwr_k=10 - 13064.98122522125 - 17382883.76980603 - 46.55913551547095 - 29.976923572919173 - 46.24853137982835 - 3480923.414728206 - -14759.325465620403'\n",
      "1591 - random_24 - lwr_k=20 - 16248520.33332828 - 4291796.675103656 - 237.31721524568837 - 255.85499684313498 - 296.19934758779505 - 4110239.267558243 - -17427.84347125745'\n",
      "1592 - random_20 - lwr_k=10 - 31.276238123281644 - 8.808852080695038 - 8.203929123112898 - 25995144.09187847 - 18.34609705715748 - 5197339.501841462 - -22037.52640880217'\n",
      "1593 - random_73 - lwr_k=100 - 7.733927291779434 - 9.971422334596607 - 7.677530331247105 - 28808022.233929608 - 16.06568987104756 - 5759725.851324244 - -24422.240051356166'\n",
      "1594 - random_64 - lwr_k=50 - 23.130505384958223 - 37048719.849816 - 566.4069109821601 - 24.760770560031197 - 714.8893795557136 - 7413649.683580211 - -31435.45210771805'\n",
      "1595 - random_24 - lwr_k=30 - 43440288.68242964 - 12315.9098174096 - 81.23347240858148 - 223.92551523373737 - 1210.3355599333795 - 8695093.045778368 - -36869.21747347533'\n",
      "1596 - random_81 - lwr_k=20 - 45305443.61253821 - 1842.004624680598 - 296003.5686743729 - 6320.355518713742 - 1087.6626997693725 - 9126570.91399529 - -38698.83364346829'\n",
      "1597 - random_42 - lwr_k=30 - 45274097.12479594 - 907818.0124513673 - 22.75993540527168 - 11960.570545593782 - 77.32326676142834 - 9243331.644988084 - -39193.93973622671'\n",
      "1598 - random_71 - lwr_k=100 - 58735155.22471405 - 9.060128976195548 - 8.855128379803928 - 447981.7906995317 - 18.549032548715218 - 11842375.959227579 - -50214.791219321036'\n",
      "1599 - random_28 - lwr_k=20 - 112338.48270465524 - 62796367.311898686 - 1322.520890698102 - 689.6468041849724 - 726.9693095310515 - 12588469.455917422 - -53378.48703415261'\n",
      "1600 - random_89 - lwr_k=200 - 259.1258985311438 - 202.76512686801266 - 154.04200618147954 - 66815324.20341213 - 673.6287355224155 - 13358946.433465669 - -56645.57728505019'\n",
      "1601 - random_47 - lwr_k=300 - 66801890.25538565 - 203.22732665577945 - 153.75936178671398 - 238.56761058356673 - 673.6287355224155 - 13367194.98408282 - -56680.554007380146'\n",
      "1602 - random_62 - lwr_k=20 - 11788.609949172558 - 59.49858094133919 - 66886380.63094742 - 18.477951514013295 - 29.284663965488146 - 13375275.496506317 - -56714.81818934841'\n",
      "1603 - random_44 - lwr_k=30 - 34896605.74812405 - 39609556.06354459 - 260452.31356762318 - 3461.2044348056884 - 328.91824881025434 - 14961383.616918594 - -63440.46805050073'\n",
      "1604 - random_96 - lwr_k=100 - 76045665.92651774 - 8.899471516301231 - 8.047327275016956 - 615769.8357814149 - 15.147103947243446 - 15339724.566439902 - -65044.765211500584'\n",
      "1605 - random_35 - lwr_k=300 - 79508122.35866413 - 7.840412692278193 - 4904.210703720785 - 12.782407467357036 - 10245.735565727346 - 15912469.100210646 - -67473.40115658968'\n",
      "1606 - random_42 - lwr_k=20 - 62373911.15177611 - 21441362.647752192 - 529.9011380864445 - 42.69427650713438 - 2600.4855538086817 - 16771923.843930902 - -71117.78807030547'\n",
      "1607 - random_81 - lwr_k=30 - 78446446.18281795 - 80512.44068638941 - 15310.47918220765 - 6592445.345030292 - 10062.608937719137 - 17036237.06317319 - -72238.56801173565'\n",
      "1608 - random_95 - lwr_k=40 - 10662259.524469648 - 10.100228971510017 - 105.0410615278637 - 29399.434285279087 - 81423870.23465504 - 18418841.329997372 - -78101.29078297876'\n",
      "1609 - random_54 - lwr_k=20 - 758669.0188990363 - 8262854.94476211 - 3309030.6123075644 - 2197249.927078966 - 79043551.18959713 - 18709619.588125587 - -79334.2916901917'\n",
      "1610 - random_87 - lwr_k=30 - 4032821.870371635 - 86610703.85912558 - 1673.9309796724003 - 8447.17960107979 - 2854270.1922390508 - 18710301.32998012 - -79337.18251267113'\n",
      "1611 - random_96 - lwr_k=50 - 90393747.092603 - 1949495.2071270845 - 840207.4111207904 - 446149.4522146095 - 729530.2091029629 - 18880766.367400922 - -80060.01353566755'\n",
      "1612 - random_90 - lwr_k=20 - 12553.796440309767 - 73045.53753972818 - 97079388.06258681 - 1517028.0846757658 - 1344.4256822422628 - 19730222.37525614 - -83662.00233312824'\n",
      "1613 - random_26 - lwr_k=20 - 241.8432286776132 - 9199281.118396992 - 150442.4957645983 - 37.99808321010318 - 103097282.23912752 - 22483598.388090625 - -95337.2738736377'\n",
      "1614 - random_88 - lwr_k=300 - 7.5022025204284954 - 10.760778300432905 - 7.592504097770006 - 738141.9420606301 - 112148101.96676366 - 22569860.062303018 - -95703.05336226332'\n",
      "1615 - random_93 - lwr_k=30 - 30780.988210720567 - 113768465.83595875 - 117295.37744111689 - 45223.114999549325 - 15251.941678144258 - 22806572.346355677 - -96706.79574267431'\n",
      "1616 - random_95 - lwr_k=30 - 6217383.631682694 - 11.4354709990448 - 2831.8716561557776 - 97267.65036996348 - 108732019.29685675 - 23003385.27098703 - -97541.35096762062'\n",
      "1617 - random_43 - lwr_k=10 - 9311.438200553102 - 5070.256219310878 - 1292.164778600341 - 116091121.35109185 - 9369.608036643625 - 23215629.87143795 - -98441.34186305972'\n",
      "1618 - random_44 - lwr_k=40 - 1366533.8974614276 - 116360897.99936579 - 5697.3595453710695 - 301.22559606414717 - 501865.69021972344 - 23658592.4444308 - -100319.65717410587'\n",
      "1619 - random_43 - lwr_k=20 - 24933.23474960625 - 373.84267571467984 - 9987947.702528184 - 112701752.78760396 - 21320.594579638055 - 24539230.717708666 - -104053.86961786865'\n",
      "1620 - random_1 - lwr_k=30 - 141169.0900386067 - 37563493.52694785 - 3896790.662391554 - 97230916.33212987 - 7797087.0571130775 - 29322461.31570381 - -124336.43030414998'\n",
      "1621 - random_44 - lwr_k=50 - 70469.90238459554 - 150829363.60032088 - 72.60992224003502 - 73.10531341772436 - 121.49160606978236 - 30194845.71800554 - -128035.643465411'\n",
      "1622 - random_56 - lwr_k=40 - 479438.7108842437 - 150858737.14751196 - 55.08459189587151 - 32519.797128685655 - 62339.310768268275 - 30301480.453248568 - -128487.81181575957'\n",
      "1623 - random_46 - lwr_k=10 - 910.7602951696301 - 153228323.56153488 - 1331.139888867201 - 128331.36806300376 - 43692.26785231664 - 30695560.917395324 - -130158.84999081319'\n",
      "1624 - random_41 - lwr_k=10 - 605.8056682656314 - 64.82200701834296 - 53.49127575885449 - 357559.0917892896 - 155525385.7812146 - 31166523.744850926 - -132155.896115425'\n",
      "1625 - random_91 - lwr_k=50 - 2046937.4486604927 - 8.28245518122979 - 7.900116761034604 - 157233163.01858053 - 169.13084565039793 - 31845959.699347533 - -135036.94077701733'\n",
      "1626 - random_3 - lwr_k=20 - 12.25905991379456 - 525.8755543807829 - 14.169319304574115 - 210406.91455010962 - 165955102.94505635 - 33222328.87503143 - -140873.22459411714'\n",
      "1627 - random_76 - lwr_k=10 - 322220.55982435547 - 78174.6414419008 - 230.33220695047876 - 1384.457779680192 - 172287098.20285907 - 34526576.30591321 - -146403.68713921666'\n",
      "1628 - random_71 - lwr_k=300 - 174565333.08968592 - 7.58255620325461 - 6.025367535813003 - 70.86162017453421 - 18.209645539471282 - 34930237.827815466 - -148115.35233012986'\n",
      "1629 - random_96 - lwr_k=40 - 13577791.23802611 - 2111603.3573968504 - 57250485.83119859 - 104296602.04558165 - 11076982.13573775 - 37652927.73553158 - -159660.50411652378'\n",
      "1630 - random_88 - lwr_k=200 - 7.43824617830549 - 10.901269894074423 - 7.697525598790903 - 10.948821314559678 - 209626645.16759205 - 41911606.177780926 - -177718.7812952332'\n",
      "1631 - random_88 - lwr_k=100 - 118053339.41190095 - 11.695850491208052 - 2036.6678272138417 - 9881189.289207121 - 82535006.06032696 - 42099861.85473562 - -178517.0508149948'\n",
      "1632 - random_44 - lwr_k=20 - 28372.837812001522 - 238096813.70398238 - 410716.3713409181 - 192602.71048090234 - 111572.69707796375 - 47791364.1421499 - -202650.99923658546'\n",
      "1633 - random_48 - lwr_k=20 - 26901.766314581895 - 111849.53746778765 - 52380420.86649078 - 495849.07078008336 - 207435845.2925734 - 52073136.85751791 - -220807.20416233497'\n",
      "1634 - random_17 - lr - 9.057900942034616 - 9.351250444806213 - 5.922920917757451 - 1326940.4907544781 - 282996865.8009965 - 56846143.312608056 - -241046.41092814616'\n",
      "1635 - random_56 - lwr_k=30 - 50325268.16113783 - 123820706.98476623 - 24137.312122863288 - 111724987.91134454 - 15062.175915618398 - 57191821.588822894 - -242512.20699873392'\n",
      "1636 - random_10 - lwr_k=20 - 56.8232861841127 - 23.757034543207137 - 292943894.1081931 - 33.679217085922694 - 3842170.453600006 - 59337796.6958286 - -251611.88752790095'\n",
      "1637 - random_17 - lwr_k=1000 - 8.164223036681783 - 9.7511791165132 - 6.002840570789983 - 299689151.3082403 - 19.13769943215903 - 59918209.65066026 - -254073.04024431217'\n",
      "1638 - random_64 - lwr_k=40 - 191.35512811823608 - 284181328.58973914 - 622.5813188804021 - 16304439.708903002 - 399.0617623159026 - 60124248.51537589 - -254946.7166627309'\n",
      "1639 - random_95 - lwr_k=50 - 271797118.8465137 - 9.92773637100875 - 18.948569935905798 - 29.367040167094892 - 61686647.29799323 - 66719427.985319 - -282912.57051307743'\n",
      "1640 - random_70 - lwr_k=20 - 124679482.4508701 - 145.27265382194125 - 64056038.277606964 - 808.2496554944571 - 186685953.48474714 - 75080311.7558228 - -318365.6244672953'\n",
      "1641 - random_70 - lwr_k=30 - 165185816.4902368 - 146.10519778062846 - 222438162.592503 - 18.664596278059058 - 175357.60745686703 - 77561548.59100115 - -328886.93128190615'\n",
      "1642 - random_17 - lwr_k=900 - 7.931274435953061 - 10.086849430389249 - 7.094885703471662 - 397016992.8710692 - 19.4725486021719 - 79377403.43090883 - -336586.7871080799'\n",
      "1643 - random_12 - lwr_k=10 - 549.7919871745709 - 1125.8265318324077 - 276460.9553016762 - 143502909.95597947 - 304404199.3180871 - 89607693.9341557 - -379966.77553208824'\n",
      "1644 - random_17 - lwr_k=40 - 562464.2003799883 - 93193983.31018119 - 135.1516365742652 - 393134287.894794 - 163.26374921336713 - 97361668.37115364 - -412846.3229125142'\n",
      "1645 - random_73 - lwr_k=600 - 7.428541253449307 - 9.353811982607434 - 7.065891035354761 - 515422221.48904186 - 16.542041375749708 - 103050692.93784852 - -436969.76493684365'\n",
      "1646 - random_58 - lwr_k=10 - 186.97615601223103 - 183494147.95284247 - 101824.65101760307 - 313863217.458671 - 33654150.21793764 - 106217964.8133728 - -450400.0988313038'\n",
      "1647 - random_53 - lwr_k=10 - 593270977.0156999 - 368587.99662137654 - 564.7731689324505 - 1022.2995235632272 - 3884.016338317448 - 118787330.70980243 - -503698.58013163344'\n",
      "1648 - random_56 - lwr_k=20 - 71722964.98159508 - 451300658.5209435 - 101983.45843277591 - 66424197.788539 - 9056645.638631113 - 119767725.48672397 - -507855.7948324591'\n",
      "1649 - random_17 - lwr_k=50 - 3086440.2915566238 - 1369159.9601546738 - 235.3166812383363 - 607136395.417746 - 208365.98604141545 - 122320776.89590359 - -518681.62040808273'\n",
      "1650 - random_73 - lwr_k=40 - 11.440118533561211 - 15.367739719487444 - 12.473604222749513 - 686352139.5209601 - 18.32122761454515 - 137225484.3155043 - -581882.7616777569'\n",
      "1651 - random_76 - lwr_k=20 - 732052082.1220102 - 2283.820582006962 - 29547.71549988884 - 16693.933618636456 - 8437150.65560609 - 148178918.81216908 - -628329.132044038'\n",
      "1652 - random_53 - lwr_k=20 - 795875493.137343 - 643035.3518855511 - 170100.48381676569 - 120371.37404787303 - 11612.641108433549 - 159442359.09400418 - -676090.0347168841'\n",
      "1653 - random_90 - lwr_k=10 - 41.185720509209155 - 55.548235196318515 - 518962686.98410326 - 376812539.6832233 - 66.16840703728415 - 179096405.88854176 - -759430.026104359'\n",
      "1654 - random_92 - lwr_k=40 - 36133.549754376014 - 326.9635300913781 - 14486959.675139379 - 922605328.619261 - 15387567.091795795 - 190440880.65724233 - -807534.5431741817'\n",
      "1655 - random_42 - lwr_k=10 - 1064844089.2250252 - 37970122.4896047 - 1466170.3500887097 - 15406617.367219139 - 731155.6201365861 - 224190827.17050877 - -950646.049987379'\n",
      "1656 - random_64 - lwr_k=20 - 183692.68346571506 - 871576869.9853191 - 10990437.773012921 - 304693009.06457347 - 1911.2252457687543 - 237554155.85000384 - -1007311.2095191372'\n",
      "1657 - random_34 - lwr_k=10 - 41.17822322403356 - 10637.217383270015 - 23.524337377895108 - 1235043959.0245426 - 2411.1460171313242 - 246930521.64743748 - -1047070.2603127434'\n",
      "1658 - random_17 - lwr_k=200 - 9.456452938744327 - 12.73034331752534 - 17.587493057467675 - 1251803160.79038 - 54207824.06847798 - 261116663.02591446 - -1107224.4317494552'\n",
      "1659 - random_62 - lwr_k=10 - 486464.09996295685 - 100.89633306514885 - 1313659428.5816357 - 37104.712440849275 - 2268.404454057931 - 262751075.69991484 - -1114154.9097115118'\n",
      "1660 - random_64 - lwr_k=30 - 426.1900974319932 - 1048555881.6637654 - 21196.48179953717 - 305072091.6491771 - 254.0194179930679 - 270813005.2665879 - -1148340.2939062668'\n",
      "1661 - random_91 - lwr_k=30 - 48132.61123797186 - 3464.8523975961116 - 5260.082058818295 - 1457415116.138711 - 376476.18859748717 - 291474211.37913865 - -1235950.990953828'\n",
      "1662 - random_88 - lwr_k=40 - 353203036.5985206 - 18.780602829187377 - 9998.37087223061 - 712533338.5838274 - 435826383.7541427 - 300274040.06202483 - -1273265.323941313'\n",
      "1663 - random_56 - lwr_k=10 - 480.7048961383861 - 6065248.638374907 - 2478.9225117245437 - 1502387841.3341076 - 2080397.1797365688 - 302009344.56316817 - -1280623.6183267792'\n",
      "1664 - random_88 - lwr_k=50 - 473176601.60359263 - 7856.741043570749 - 3598.133240642599 - 399495848.3874806 - 643546179.0273368 - 303224188.1269566 - -1285774.9773930144'\n",
      "1665 - random_92 - lwr_k=30 - 345.74414335434614 - 5457.525363703048 - 7756.062143523083 - 1554367007.1782525 - 33115685.248330094 - 317395272.506971 - -1345865.3019214887'\n",
      "1666 - random_35 - lwr_k=100 - 1181424.556404001 - 317535453.36311036 - 29223.926910154765 - 10343269.720806586 - 1582287528.9685297 - 382202376.36025596 - -1620669.9532710034'\n",
      "1667 - random_91 - lwr_k=20 - 3920162.6525491965 - 121969720.69670497 - 567954.9744296191 - 1862310029.999423 - 1961816.7483872734 - 398036161.02106494 - -1687810.705048887'\n",
      "1668 - random_96 - lwr_k=30 - 14.596900116312245 - 73719.07836851556 - 125146305.65289219 - 2097974879.3725014 - 6279110.5436173985 - 445748790.47665644 - -1890128.8418413524'\n",
      "1669 - random_84 - lwr_k=10 - 52966123.88044237 - 7417926.582396096 - 783132.8422664641 - 260318059.79733363 - 1985839212.9948332 - 461323652.35716915 - -1956171.671012991'\n",
      "1670 - random_73 - lwr_k=20 - 540.0248208042692 - 434089158.3272497 - 2117.3102527501896 - 1845368422.649636 - 51599299.07742505 - 466130307.00029314 - -1976553.557794285'\n",
      "1671 - random_10 - lwr_k=10 - 15309755.029560061 - 87.65890533476396 - 453657890.14944714 - 6021.666679596407 - 1879280841.7434287 - 469499618.77542514 - -1990840.6111048157'\n",
      "1672 - random_25 - lwr_k=10 - 39752920.559276655 - 191572.89622907137 - 2809249770.1384773 - 13200.396353622364 - 1038126.5851128435 - 569868971.7678206 - -2416441.5624710997'\n",
      "1673 - random_73 - lwr_k=30 - 17.756076348269772 - 2094.8407769738933 - 17.466213311993357 - 3361931581.364639 - 659.1596858303939 - 672166672.4414282 - -2850219.378420251'\n",
      "1674 - random_17 - lwr_k=100 - 59.45847073101715 - 16.333533301262328 - 110.25091858005032 - 1637737482.6683867 - 1990246458.2598429 - 725359197.4959176 - -3075774.1777072484'\n",
      "1675 - random_98 - lwr_k=10 - 104.63472286837356 - 4214355385.8224645 - 56.45149756323206 - 17.886859388790437 - 365.3450107165191 - 843285237.6276529 - -3575821.5863492843'\n",
      "1676 - random_25 - lwr_k=20 - 14281.049509536462 - 14.990475331063664 - 4737336364.185906 - 15.310933941044306 - 15971891.28148361 - 950353179.6923519 - -4029826.884231583'\n",
      "1677 - random_73 - lwr_k=400 - 7.479549171751293 - 9.15039179608295 - 7.064491460393697 - 5746045967.845682 - 16.240111943213524 - 1148832843.5428667 - -4871449.662930171'\n",
      "1678 - random_17 - lwr_k=300 - 8.796300400516294 - 10.753104070792709 - 29.352910555576962 - 3387241517.489547 - 7204058838.110143 - 2117566365.4685605 - -8979216.588390213'\n",
      "1679 - random_88 - lwr_k=20 - 299950617.0871575 - 3759.9274589263564 - 151998.69495291827 - 10966279584.264576 - 764217729.1924696 - 2405381866.521934 - -10199654.37556575'\n",
      "1680 - random_88 - lwr_k=30 - 78963818.75800607 - 31.732944819216403 - 77696.09198324021 - 12570090062.311243 - 541705269.4928899 - 2637316324.2772384 - -11183137.111404046'\n",
      "1681 - random_17 - lwr_k=400 - 8.496410976797936 - 9.31554978890398 - 9.390525231176813 - 441487090.490417 - 12754401199.948599 - 2638313351.2130485 - -11187364.852240939'\n",
      "1682 - random_35 - lwr_k=50 - 8921859429.844137 - 4002156162.569849 - 54716695.62125712 - 12.051044672541494 - 216240410.0919314 - 2640246552.271168 - -11195562.296829233'\n",
      "1683 - random_17 - lwr_k=600 - 8.011445172466104 - 9.38507494942548 - 20.947233724949655 - 4227028658.84913 - 10384207950.375942 - 2921290313.852781 - -12387285.554363394'\n",
      "1684 - random_3 - lwr_k=10 - 127.84896975517483 - 48773248.01204594 - 19823217.167602338 - 3643975119.7053733 - 12361138600.193533 - 3213697243.4027133 - -13627192.594632689'\n",
      "1685 - random_96 - lwr_k=20 - 32389909.403343525 - 264449799.40939555 - 96949.29082670333 - 13268254712.86294 - 3525607237.965576 - 3417088904.7347116 - -14489643.950370988'\n",
      "1686 - random_17 - lwr_k=700 - 8.390298726538138 - 10.155882985727883 - 6.9014417540325255 - 3570093618.9661264 - 13792712327.942947 - 3471423954.8495283 - -14720043.453127103'\n",
      "1687 - random_91 - lwr_k=10 - 3334949320.7451444 - 111835756.1348358 - 1158537.8416478331 - 14442771608.407534 - 30603435.401966494 - 3583654309.3406568 - -15195939.174475614'\n",
      "1688 - random_17 - lwr_k=500 - 8.084602348673071 - 9.691281714868541 - 7.056685210872355 - 491221110.16387707 - 17627266216.338974 - 3622510734.5666156 - -15360703.926362716'\n",
      "1689 - random_73 - lwr_k=200 - 7.258414525584464 - 8.77622914594347 - 7.060869255628296 - 18529089829.785595 - 16.518253756079773 - 3704604344.285681 - -15708809.372456145'\n",
      "1690 - random_17 - lwr_k=800 - 8.009190982604064 - 9.397433105592942 - 7.223276003302672 - 3559870351.3716273 - 15976299997.281916 - 3905954482.6914163 - -16562603.961225754'\n",
      "1691 - random_70 - lwr_k=10 - 4983435605.252873 - 165.0519554028383 - 14826277375.689295 - 2274111733.3768663 - 3023932643.521219 - 5020723001.673767 - -21289610.045124874'\n",
      "1692 - random_73 - lwr_k=10 - 31.862939871696252 - 55.1827963836133 - 66.59628545661235 - 28363716106.530094 - 66.03357875742593 - 5670885481.313935 - -24046524.99603108'\n",
      "1693 - random_35 - lwr_k=40 - 67233.35663855524 - 28754041222.9836 - 22620227.46161209 - 1265745.5805521766 - 135019548.57467756 - 5785417418.624846 - -24532180.22870993'\n",
      "1694 - random_73 - lwr_k=50 - 22.39137851032815 - 13.16789132714036 - 10.010992896588546 - 46815404408.30094 - 18.064537090531633 - 9360014550.559868 - -39689714.822143055'\n",
      "1695 - random_48 - lwr_k=10 - 25797.127447413488 - 90813.45928813465 - 174659137.7275276 - 95379.5646392084 - 50819609459.417465 - 10195556069.051819 - -43232701.34715479'\n",
      "1696 - random_96 - lwr_k=10 - 496253005.16160583 - 80996988157.32797 - 41090886.158082314 - 10577621.490286853 - 29982476865.210926 - 22311516653.155964 - -94608586.49111712'\n",
      "1697 - random_26 - lwr_k=10 - 266645.202772927 - 73406167.42956008 - 288738720.0333737 - 345.5131841246328 - 226869242502.27386 - 45431459582.66669 - -192645182.45362645'\n",
      "1698 - random_73 - lwr_k=300 - 7.477757510560483 - 9.108102062571287 - 7.013145410788494 - 1125535470528.502 - 16.473226092467453 - 225033373106.9667 - -954219736.6797413'\n",
      "1699 - random_73 - lwr_k=500 - 7.4520457221033345 - 9.25592791472473 - 7.012317938721096 - 1484563033158.3635 - 16.461449371427697 - 296815369827.3196 - -1258600358.6955903'\n"
     ]
    }
   ],
   "source": [
    "scores_df = pd.DataFrame(all_scores)\n",
    "scores_df.to_csv(log_dir/f\"scores.csv\",index=False)\n",
    "\n",
    "scores_df_final = pd.DataFrame(all_scores_final)\n",
    "scores_df_final.to_csv(log_dir/f\"test_scores.csv\",index=False)\n",
    "\n",
    "scores_df_sorted = pd.DataFrame(scores_df).sort_values(by='MSE')\n",
    "\n",
    "best_5 = []\n",
    "summary_logger.info(f\"Rank - \" +\" - \".join(list(scores_df_sorted.columns)))\n",
    "for i,(index,row) in enumerate(scores_df_sorted.iterrows()):\n",
    "    if i < 5:\n",
    "        best_5.append((row[\"model_num\"],row[\"predictor\"],row[\"MSE\"],row[\"R2\"]))\n",
    "    s = f\"{i} - \" + \" - \".join([f\"{i}\" for i in row.tolist()])\n",
    "    summary_logger.info(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% save scores\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      " Best 5 on Test Sest \n",
      " ---------------------'\n",
      "Rank -  Deep Model - Predictor - Val Set - Test Set'\n",
      "0 - random_25 - lwr_k=200 - 8.435853172927498 - 0.964229049715442 - 9.072328483810395 - 0.961807446453495'\n",
      "1 - random_5 - lwr_k=500 - 8.893078295009875 - 0.9622902562377001 - 7.848591842301141 - 0.9669591147700761'\n",
      "2 - random_5 - lwr_k=400 - 8.894780973216314 - 0.9622830362901472 - 7.593511474609604 - 0.9680329483089638'\n",
      "3 - random_5 - lwr_k=700 - 8.900058910474058 - 0.9622606560012326 - 7.762968117092229 - 0.9673195722552441'\n",
      "4 - random_5 - lwr_k=800 - 8.921254621916066 - 0.9621707788157605 - 7.8332767520804785 - 0.9670235880091532'\n"
     ]
    }
   ],
   "source": [
    "summary_logger.info(\"-----------------------\\n Best 5 on Test Sest \\n ---------------------\")\n",
    "summary_logger.info(f\"Rank -  Deep Model - Predictor - Val Set - Test Set\")\n",
    "for i, (j,k,v,x) in enumerate(best_5):\n",
    "\n",
    "    row = scores_df_final.loc[(scores_df_final['model_num']==j) & (scores_df_final['predictor'] == k)].iloc[0]\n",
    "    #print(row)\n",
    "    s = f\"{i} - {j} - {k} - {v} - {x} - {row['MSE']} - {row['R2']}\"\n",
    "    summary_logger.info(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote Summary Graph'\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#take 1 is a scatter plot - lets, for each dataset\n",
    "#graph our deep models by rank - plot - then overlay our knn moels\n",
    "#plot points\n",
    "\n",
    "deep_set = scores_df[scores_df[\"predictor\"]==\"deep\"].sort_values(\"R2\")\n",
    "deep_set[\"order\"] = [i for i in range(0,100)]\n",
    "deep_ordering = {row[\"model_num\"]:row[\"order\"] for index, row in deep_set.iterrows()}\n",
    "\n",
    "def order_models(x):\n",
    "    x = [deep_ordering[i] for i in x]\n",
    "    return x\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "set_deep = False\n",
    "knn_models = scores_df[\"predictor\"].unique()\n",
    "for knn_model in knn_models:\n",
    "    subset = scores_df[scores_df[\"predictor\"]==knn_model]\n",
    "    s=3\n",
    "    if knn_model == \"deep\":\n",
    "        s=10\n",
    "    ax.scatter(x=order_models(subset[\"model_num\"].tolist()), y=subset[\"R2\"], s=s, label=knn_model)\n",
    "\n",
    "#ax.set_ylim(0,scores_db[\"deep_mean\"].max())\n",
    "ax.set_ylim(0,1)\n",
    "# plot residuals\n",
    "ax.legend(loc='upper right',bbox_to_anchor=(1.4, 1))\n",
    "ax.set_ylabel(\"R2 Score\")\n",
    "ax.set_xlabel(\"Deep Model Rank\")\n",
    "#ax.set_yscale(\"symlog\")\n",
    "ax.set_title(\"Summary of LWR improvements over Deep Models\")\n",
    "plt.savefig(log_dir/f\"summary_plot.png\", bbox_inches='tight')\n",
    "logging.getLogger().info(\"Wrote Summary Graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df[\"n_features\"] = [deep_models[i].n_features for i in scores_df[\"model_num\"]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import Colormap\n",
    "import seaborn as sns #heatmap of features - pls model - score\n",
    "class nlcmap(Colormap):\n",
    "    def __init__(self, cmap, levels):\n",
    "        self.cmap = cmap\n",
    "        self.N = cmap.N\n",
    "        self.monochrome = self.cmap.monochrome\n",
    "        self.levels = np.asarray(levels, dtype='float64')\n",
    "        self._x = self.levels\n",
    "        self.levmax = self.levels.max()\n",
    "        self.levmin = self.levels.min()\n",
    "        self.transformed_levels = np.linspace(self.levmin, self.levmax, #uniform spacing along levels (colour segments)\n",
    "             len(self.levels))\n",
    "\n",
    "    def __call__(self, xi, alpha=1.0, **kw):\n",
    "        yi = np.interp(xi, self._x, self.transformed_levels)\n",
    "        return self.cmap((yi-self.levmin) / (self.levmax-self.levmin), alpha)\n",
    "    \n",
    "levels = np.concatenate((\n",
    "    [0, 1],\n",
    "    [0.6,0.8,0.9,0.95,0.98]\n",
    "    ))\n",
    "\n",
    "levels = levels[levels <= 1]\n",
    "levels.sort()\n",
    "cmap_nonlin = nlcmap(plt.cm.YlGnBu, levels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = scores_df[[\"predictor\",\"n_features\",\"R2\"]]\n",
    "subset = subset[np.logical_not(subset[\"predictor\"]==\"deep\")]\n",
    "subset = subset[np.logical_not(subset[\"predictor\"]==\"lr\")]\n",
    "trans = subset[\"predictor\"].transform(lambda x: int(x.replace(\"lwr_k=\",\"\"))).tolist()\n",
    "subset.loc[:,\"predictor\"]=trans\n",
    "subset=subset.sort_values(\"predictor\",ascending=False)\n",
    "\n",
    "def rand_jitter(arr):\n",
    "    stdev = .01 * (max(arr) - min(arr))\n",
    "    return arr + np.random.randn(len(arr)) * stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huonf\\AppData\\Local\\Temp\\ipykernel_16624\\3702862414.py:6: MatplotlibDeprecationWarning: Auto-removal of grids by pcolor() and pcolormesh() is deprecated since 3.5 and will be removed two minor releases later; please call grid(False) first.\n",
      "  cbar = fig.colorbar(sc,label=\"R2 Score\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sc = ax.scatter(x=rand_jitter(subset[\"n_features\"]), y=rand_jitter(subset[\"predictor\"]), s=20,c=subset[\"R2\"],cmap=cmap_nonlin,vmin=0)\n",
    "ax.set_xlabel(\"Number of Features\")\n",
    "ax.set_ylabel(\"Number of Neighbours\")\n",
    "\n",
    "cbar = fig.colorbar(sc,label=\"R2 Score\")\n",
    "\n",
    "ax.set_title(\"LWR performance as a function of the number of components\")\n",
    "plt.savefig(log_dir/f\"heat_scatter.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (lazydeep)",
   "language": "python",
   "name": "pycharm-12fcba0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
