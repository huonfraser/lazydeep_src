{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU detected is GeForce GTX 970\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "# set seed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import utils as ut\n",
    "import experiment as ex\n",
    "from evaluation import *\n",
    "from sk_models import CustomWrapper\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import random\n",
    "#define fixed_hyperparams and create a config gen\n",
    "from configurations import RandomConfigGen, Configuration\n",
    "from torch import nn\n",
    "from deep_net import RandomNet\n",
    "from experiment import run_experiment\n",
    "import regex as re\n",
    "from pathlib import *\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from sk_models import PLSRegression\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed + 1)\n",
    "np.random.seed(seed + 2)\n",
    "random_state = np.random.RandomState(seed)\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"GPU detected is {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory is D:\\workspace\\lazydeep\\experiments\\1.01\\A_C_OF_SIWARE\n"
     ]
    }
   ],
   "source": [
    "#setup input and outpu t formats, load data\n",
    "\n",
    "file_name = \"A_C_OF_SIWARE.csv\" # \"PLN7.csv\"\n",
    "id_cols = []#[\"sample_id\"]\n",
    "\n",
    "data_path = Path('D:/workspace/lazydeep/data/soil_data/')\n",
    "log_path = Path(\"D:/workspace/lazydeep/experiments/1.01\") #1.01/\")\n",
    "if not log_path.exists():\n",
    "    log_path.mkdir()\n",
    "\n",
    "data_file = data_path / file_name\n",
    "log_dir = log_path / re.sub(r'\\.(?=csv$)[^.]+$', '',file_name)\n",
    "if not log_dir.exists():\n",
    "    log_dir.mkdir()\n",
    "print(f\"Output directory is {log_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape is (13916, 247)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(data_file)\n",
    "data = data.sample(frac=1)\n",
    "nrow, ncol = data.shape\n",
    "data = ut.sample_data(data,random_state)\n",
    "n_features = ncol - 1-len(id_cols)\n",
    "dataset = TabularDataset(data,id_cols = id_cols, cat_cols=None, output_cols=None, ignore_cols= None)\n",
    "eval = CrossValEvaluation(preprocessing=None,tensorboard=None,time=True,random_state=random_state)\n",
    "print(f\"Dataset shape is {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% setup logging and tensorboard outputs\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Cross Evaluation with 5 folds'\n",
      "-----------------------------------Fold 0 - Train 6956 - Val 2320 - Test 2320-----------------------------------'\n",
      "Finished training SKLearn with a train loss of 1:303.3696,2:297.698,3:268.5639,4:228.6555,5:221.7038,6:203.9539,7:201.006,8:197.5122,9:195.7092,10:193.1037,11:189.55,12:188.3762,13:185.5406,14:184.8236,15:183.7554,16:182.9346,17:182.4316,18:181.8299,19:181.5873,20:181.364,21:181.0507,22:180.8183,23:180.5975,24:180.4706,25:180.3172,26:180.2479,27:180.0848,28:179.929,29:179.7862,30:179.6789,31:179.5678,32:179.4196,33:179.3174,34:179.1482,35:179.0141,36:178.8126,37:178.6545,38:178.4367,39:178.257,40:178.0996,41:177.9869,42:177.7648,43:177.6419,44:177.5362,45:177.3944,46:177.2747,47:177.157,48:176.984,49:176.8333,50:176.6422,51:176.5596,52:176.4384,53:176.3011,54:176.1157,55:175.9549,56:175.8083,57:175.7221,58:175.6319,59:175.5258,60:175.3723,61:175.2605,62:175.1153,63:174.9779,64:174.8611,65:174.7812,66:174.7251,67:174.6276,68:174.562,69:174.4816,70:174.3758,71:174.2915,72:174.2021,73:174.1082,74:174.029,75:173.8919,76:173.8307,77:173.785,78:173.6528,79:173.6001,80:173.5248,81:173.4423,82:173.4019,83:173.3028,84:173.2774,85:173.1665,86:173.1327,87:173.0593,88:172.9391,89:172.8457,90:172.6916,91:172.5586,92:172.4458,93:172.3678,94:172.2892,95:172.2115,96:172.1589,97:172.1061,98:172.0362,99:171.9842,100:171.9373'\n",
      "Tested (test) on 2320 instances with mean losses of: 1:308.8228,2:303.1223,3:278.1292,4:230.7213,5:225.3088,6:207.0001,7:203.2342,8:201.1395,9:198.3576,10:197.2269,11:195.925,12:194.3736,13:192.5995,14:192.8874,15:192.4262,16:191.5681,17:190.9623,18:189.8192,19:189.6776,20:190.2121,21:189.6719,22:190.2758,23:190.021,24:189.654,25:189.5501,26:189.2177,27:189.9125,28:189.8046,29:189.846,30:189.5617,31:189.6366,32:189.5435,33:189.1028,34:188.8031,35:188.4524,36:188.2979,37:187.9215,38:187.9337,39:188.1641,40:188.5723,41:188.3565,42:188.3303,43:188.503,44:188.5709,45:188.7363,46:188.4702,47:188.4399,48:188.6012,49:188.5457,50:189.2318,51:189.2776,52:189.2172,53:189.1124,54:188.7148,55:189.3936,56:189.0034,57:188.9793,58:189.0229,59:188.6508,60:188.3574,61:188.705,62:188.6776,63:187.9494,64:187.9829,65:187.7792,66:187.7899,67:188.1357,68:187.9958,69:188.3034,70:188.4355,71:188.1742,72:188.203,73:188.521,74:188.3047,75:188.305,76:188.3349,77:188.3861,78:188.0854,79:188.4578,80:188.5347,81:188.7976,82:188.8268,83:189.3305,84:189.4975,85:189.3401,86:189.5503,87:189.6369,88:190.2694,89:190.5939,90:190.5826,91:190.5094,92:190.5583,93:191.01,94:191.3408,95:191.216,96:191.1994,97:191.3668,98:191.3076,99:191.4162,100:191.4488'\n",
      "-----------------------------------Fold 1 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training SKLearn with a train loss of 1:287.3363,2:282.3603,3:255.0793,4:212.5245,5:204.1394,6:186.5409,7:184.0635,8:180.6042,9:178.9411,10:176.2792,11:172.5709,12:171.7058,13:170.0871,14:168.6007,15:167.5279,16:166.7839,17:166.3299,18:165.9701,19:165.6743,20:165.405,21:165.0367,22:164.8739,23:164.6783,24:164.4623,25:164.3007,26:164.2362,27:164.0973,28:163.9785,29:163.8469,30:163.7611,31:163.6671,32:163.4919,33:163.3697,34:163.1608,35:162.9735,36:162.7882,37:162.6646,38:162.5045,39:162.3741,40:162.2718,41:162.13,42:161.9288,43:161.7587,44:161.6071,45:161.4075,46:161.2866,47:161.1728,48:161.0027,49:160.8551,50:160.6501,51:160.5397,52:160.3761,53:160.2104,54:160.0595,55:159.9873,56:159.922,57:159.8567,58:159.7847,59:159.6827,60:159.5845,61:159.5234,62:159.4435,63:159.294,64:159.148,65:159.0813,66:159.0251,67:158.9737,68:158.8937,69:158.8376,70:158.787,71:158.7451,72:158.6894,73:158.5736,74:158.501,75:158.431,76:158.3541,77:158.3021,78:158.2487,79:158.1987,80:158.1613,81:158.0898,82:158.0564,83:157.9355,84:157.903,85:157.8582,86:157.8001,87:157.7267,88:157.643,89:157.5543,90:157.4647,91:157.4157,92:157.3254,93:157.2678,94:157.1951,95:157.1384,96:157.0779,97:157.027,98:156.9805,99:156.9149,100:156.8648'\n",
      "Tested (test) on 2319 instances with mean losses of: 1:386.0849,2:380.0947,3:350.7851,4:305.0481,5:298.5796,6:273.079,7:272.2839,8:268.6442,9:266.2281,10:261.2579,11:259.912,12:259.1782,13:257.8042,14:255.5293,15:254.373,16:252.3866,17:252.177,18:252.0728,19:252.6594,20:252.8588,21:253.6789,22:253.8897,23:253.9974,24:253.971,25:254.0548,26:254.3199,27:254.3922,28:254.6023,29:254.9155,30:254.9724,31:255.0846,32:255.2431,33:254.9616,34:254.2948,35:254.0222,36:253.8315,37:254.471,38:254.5804,39:254.8642,40:254.9633,41:255.3693,42:255.6661,43:255.2557,44:256.1145,45:256.1369,46:256.3139,47:256.06,48:255.7205,49:255.9625,50:254.8649,51:254.9795,52:255.5787,53:255.7504,54:255.2539,55:255.3112,56:255.5491,57:255.4399,58:255.425,59:254.8512,60:254.9353,61:255.0131,62:255.0479,63:254.3284,64:254.7205,65:254.9596,66:255.4262,67:255.545,68:255.5584,69:255.5596,70:255.3406,71:255.3185,72:255.1924,73:254.8559,74:254.5524,75:254.4155,76:254.2988,77:254.3397,78:254.1493,79:254.2449,80:254.366,81:254.5887,82:254.2778,83:254.572,84:254.4819,85:254.7446,86:254.6212,87:254.6599,88:254.7546,89:254.7607,90:255.0012,91:254.7625,92:254.8637,93:255.262,94:255.1546,95:255.0309,96:255.4208,97:255.451,98:255.3763,99:254.8742,100:254.8473'\n",
      "-----------------------------------Fold 2 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training SKLearn with a train loss of 1:303.8762,2:298.9427,3:251.9374,4:222.5432,5:216.4273,6:198.1864,7:194.8889,8:191.7749,9:189.8551,10:187.5196,11:183.8106,12:182.8946,13:179.9304,14:179.3932,15:178.5333,16:177.9169,17:177.436,18:176.9097,19:176.4488,20:176.1979,21:175.7824,22:175.6076,23:175.419,24:175.236,25:175.1165,26:175.023,27:174.8035,28:174.6524,29:174.5658,30:174.466,31:174.364,32:174.1689,33:174.0222,34:173.8173,35:173.6272,36:173.4673,37:173.3321,38:173.1474,39:173.0228,40:172.877,41:172.7519,42:172.5694,43:172.4211,44:172.3247,45:172.1955,46:172.0619,47:171.9135,48:171.7053,49:171.5481,50:171.3578,51:171.2079,52:171.078,53:170.952,54:170.8066,55:170.6963,56:170.635,57:170.5863,58:170.4924,59:170.4099,60:170.304,61:170.2192,62:170.1522,63:170.0487,64:169.9634,65:169.9069,66:169.8702,67:169.8144,68:169.759,69:169.7105,70:169.6418,71:169.5927,72:169.5527,73:169.4892,74:169.4224,75:169.3255,76:169.2632,77:169.2299,78:169.1153,79:169.0633,80:169.0392,81:168.8884,82:168.8648,83:168.8204,84:168.7359,85:168.7197,86:168.6493,87:168.5912,88:168.5386,89:168.488,90:168.43,91:168.3589,92:168.2859,93:168.2271,94:168.1348,95:168.0888,96:168.0392,97:167.9916,98:167.9551,99:167.9162,100:167.8798'\n",
      "Tested (test) on 2319 instances with mean losses of: 1:259.7737,2:253.4685,3:215.1176,4:196.2625,5:189.6925,6:175.1067,7:171.9233,8:170.2049,9:168.4735,10:165.4608,11:160.8602,12:159.9479,13:157.4207,14:157.4428,15:157.2967,16:157.1157,17:156.9703,18:157.8014,19:157.7704,20:157.1447,21:157.1556,22:157.2291,23:157.5909,24:157.8957,25:158.2263,26:158.4763,27:158.0941,28:158.1241,29:158.1669,30:158.3496,31:158.0734,32:158.2929,33:158.0938,34:157.8825,35:157.379,36:157.5652,37:157.5727,38:157.6466,39:157.489,40:157.2561,41:157.4838,42:157.8358,43:157.7564,44:157.7675,45:157.7004,46:157.9911,47:157.6405,48:158.0445,49:157.8981,50:157.9443,51:157.7522,52:157.7205,53:157.2511,54:157.0052,55:157.0364,56:157.3261,57:157.2154,58:156.889,59:157.1479,60:157.0799,61:157.0407,62:156.7899,63:156.7815,64:156.5355,65:156.7728,66:156.9365,67:156.7725,68:156.638,69:156.2939,70:156.4451,71:156.6976,72:156.8011,73:156.7369,74:156.8673,75:157.2588,76:157.4063,77:157.2038,78:156.9026,79:157.0892,80:157.0715,81:157.0451,82:156.9181,83:156.9591,84:156.9542,85:157.0993,86:157.033,87:157.0804,88:156.9344,89:156.9893,90:157.1487,91:157.2677,92:157.3016,93:157.479,94:157.6883,95:157.8486,96:158.0466,97:158.1734,98:158.2209,99:158.0282,100:158.1618'\n",
      "-----------------------------------Fold 3 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training SKLearn with a train loss of 1:344.0943,2:339.0651,3:294.6693,4:262.3322,5:254.9081,6:235.5024,7:231.0765,8:226.9568,9:225.1702,10:222.0107,11:218.975,12:217.7647,13:214.5351,14:213.9134,15:213.0314,16:212.4371,17:212.0051,18:211.7207,19:211.1123,20:210.8597,21:210.5618,22:210.3629,23:210.0809,24:209.9316,25:209.7832,26:209.6996,27:209.4889,28:209.3263,29:209.1981,30:209.0903,31:208.9784,32:208.7887,33:208.6605,34:208.4949,35:208.3463,36:208.1969,37:208.071,38:207.8748,39:207.7534,40:207.5627,41:207.442,42:207.1703,43:207.0796,44:206.9652,45:206.8585,46:206.673,47:206.472,48:206.2472,49:206.0989,50:205.9462,51:205.8383,52:205.7403,53:205.5878,54:205.3787,55:205.2199,56:205.1142,57:204.9972,58:204.8785,59:204.7322,60:204.5861,61:204.4864,62:204.3804,63:204.2337,64:204.1587,65:204.1018,66:204.0582,67:203.993,68:203.9232,69:203.8408,70:203.7499,71:203.69,72:203.6269,73:203.5566,74:203.4876,75:203.3019,76:203.2261,77:203.1672,78:203.0304,79:202.991,80:202.9169,81:202.8761,82:202.8194,83:202.748,84:202.7311,85:202.6291,86:202.5963,87:202.5029,88:202.4051,89:202.3222,90:202.21,91:202.1332,92:202.0271,93:201.9433,94:201.8548,95:201.767,96:201.691,97:201.6184,98:201.573,99:201.5057,100:201.4491'\n",
      "Tested (test) on 2319 instances with mean losses of: 1:264.9055,2:258.5976,3:220.6457,4:189.917,5:180.366,6:164.5668,7:161.0683,8:159.5841,9:159.2898,10:157.8162,11:152.8036,12:152.9209,13:149.5497,14:149.158,15:149.2217,16:149.0285,17:148.7124,18:149.0323,19:148.5498,20:148.211,21:148.0519,22:147.9224,23:147.4473,24:147.8091,25:147.899,26:148.1998,27:148.5449,28:148.5922,29:148.1099,30:147.7322,31:147.5609,32:147.2813,33:147.4615,34:147.5017,35:147.8213,36:147.93,37:147.8286,38:148.4169,39:148.2033,40:148.0253,41:148.0987,42:148.3403,43:148.1559,44:148.2972,45:147.9329,46:147.0437,47:147.5129,48:147.4962,49:147.253,50:147.1963,51:147.253,52:147.4277,53:147.3025,54:147.6746,55:147.7638,56:147.6539,57:147.5574,58:147.7797,59:147.4024,60:147.4351,61:147.4071,62:147.6157,63:147.5439,64:147.6312,65:147.5161,66:147.6083,67:147.8218,68:147.6809,69:147.7324,70:147.9006,71:147.9091,72:148.1659,73:148.2856,74:148.2138,75:148.2293,76:148.4645,77:148.5829,78:148.9963,79:149.1975,80:149.4684,81:149.3133,82:149.2238,83:149.2968,84:149.2948,85:149.3974,86:149.3215,87:149.5109,88:149.8486,89:150.0735,90:150.0668,91:149.8984,92:149.9172,93:149.6126,94:149.951,95:149.7355,96:149.9404,97:149.8794,98:150.1892,99:149.9554,100:150.1675'\n",
      "-----------------------------------Fold 4 - Train 6958 - Val 2319 - Test 2319-----------------------------------'\n",
      "Finished training SKLearn with a train loss of 1:317.956,2:312.2633,3:278.2331,4:242.3414,5:236.6157,6:218.4477,7:215.0315,8:211.5665,9:209.5644,10:206.2972,11:203.4633,12:202.3263,13:199.4274,14:198.757,15:197.6793,16:196.956,17:196.5768,18:196.142,19:195.5033,20:195.1389,21:194.8394,22:194.6837,23:194.3925,24:194.2317,25:194.0754,26:193.994,27:193.7688,28:193.6031,29:193.443,30:193.307,31:193.1843,32:193.0341,33:192.9354,34:192.7894,35:192.6737,36:192.5366,37:192.3984,38:192.1755,39:192.0536,40:191.8828,41:191.7413,42:191.5089,43:191.3763,44:191.2832,45:191.1996,46:191.0058,47:190.8355,48:190.6194,49:190.4714,50:190.2987,51:190.1906,52:190.0186,53:189.7917,54:189.5566,55:189.3538,56:189.2046,57:189.0885,58:188.9674,59:188.8516,60:188.7363,61:188.6454,62:188.5049,63:188.3994,64:188.3055,65:188.2342,66:188.1463,67:188.0523,68:187.9777,69:187.8917,70:187.8099,71:187.7131,72:187.6289,73:187.5764,74:187.4176,75:187.2965,76:187.2531,77:187.1485,78:187.0814,79:187.0501,80:186.9846,81:186.9323,82:186.8686,83:186.8104,84:186.765,85:186.6998,86:186.6847,87:186.6258,88:186.5431,89:186.4752,90:186.3746,91:186.2787,92:186.1955,93:186.1104,94:186.0263,95:185.9685,96:185.9164,97:185.8579,98:185.8086,99:185.7773,100:185.7355'\n",
      "Tested (test) on 2319 instances with mean losses of: 1:338.1115,2:336.6922,3:292.7239,4:250.7147,5:243.6813,6:227.3116,7:222.5075,8:215.6731,9:214.1745,10:212.379,11:209.8957,12:208.038,13:206.0912,14:205.9134,15:206.0452,16:205.9808,17:205.988,18:205.9849,19:205.2806,20:205.632,21:206.7071,22:207.0977,23:207.3384,24:207.6841,25:208.02,26:208.0691,27:207.4435,28:207.9496,29:207.9803,30:208.106,31:208.0572,32:207.975,33:208.2828,34:207.9678,35:207.7599,36:207.4969,37:207.4202,38:207.8299,39:208.5441,40:207.9891,41:207.5334,42:207.5275,43:207.2393,44:207.0869,45:207.1996,46:207.5424,47:208.4212,48:208.6667,49:208.8614,50:208.5246,51:208.4288,52:208.3556,53:208.4729,54:207.7119,55:207.8096,56:207.7794,57:207.4933,58:207.1264,59:206.9653,60:206.8089,61:207.2579,62:207.4417,63:206.6454,64:206.7914,65:207.4103,66:207.4004,67:207.727,68:207.8671,69:207.7024,70:208.1746,71:208.6894,72:208.6852,73:208.6797,74:208.6309,75:208.2913,76:208.5961,77:208.6486,78:208.615,79:208.5707,80:208.7836,81:208.5338,82:208.7326,83:209.0713,84:209.046,85:209.0194,86:209.2852,87:209.3594,88:209.3106,89:209.0857,90:209.0841,91:208.7765,92:208.7919,93:209.1852,94:209.0643,95:209.3,96:209.5439,97:209.3673,98:209.3184,99:209.0412,100:209.0534'\n",
      "Train times: {'fold_0': 75, 'fold_1': 85, 'fold_2': 85, 'fold_3': 86, 'fold_4': 87, 'mean': 83.6}'\n",
      "Test times: {'fold_0': 0, 'fold_1': 0, 'fold_2': 0, 'fold_3': 0, 'fold_4': 0, 'mean': 0.0}'\n",
      "Scores: {'fold_0': {1: 308.8227954890182, 2: 303.12226125911786, 3: 278.12916308597516, 4: 230.72132566505425, 5: 225.3088228502973, 6: 207.00014984543859, 7: 203.2342446157878, 8: 201.1395082768244, 9: 198.35756243791437, 10: 197.22688342690014, 11: 195.9249957132016, 12: 194.37358003967935, 13: 192.59948361452746, 14: 192.88741799122963, 15: 192.4261903070467, 16: 191.5680677323913, 17: 190.96230844830262, 18: 189.81924924995425, 19: 189.67757208689898, 20: 190.21214986863444, 21: 189.67191514424624, 22: 190.27575376791884, 23: 190.0209706422549, 24: 189.65395105978297, 25: 189.55010250626404, 26: 189.21765125535154, 27: 189.912458403723, 28: 189.80462252593003, 29: 189.84604875851616, 30: 189.5617333768669, 31: 189.63662808582114, 32: 189.5435319470453, 33: 189.10281072108916, 34: 188.80307968991963, 35: 188.4523770312968, 36: 188.29789130741742, 37: 187.9215422336651, 38: 187.93369219279072, 39: 188.16408370144993, 40: 188.57228867029278, 41: 188.3564762024678, 42: 188.33031691205787, 43: 188.50299282181925, 44: 188.57089339518163, 45: 188.73631799792892, 46: 188.4702053108762, 47: 188.43986942199092, 48: 188.60117387559652, 49: 188.54568412783445, 50: 189.23179060540193, 51: 189.27757858950986, 52: 189.2172279916751, 53: 189.11242764220185, 54: 188.71477692673892, 55: 189.39363862350567, 56: 189.0034032581459, 57: 188.9793281521621, 58: 189.02285131511974, 59: 188.65076685235454, 60: 188.35742511107674, 61: 188.70495557064513, 62: 188.6776066842113, 63: 187.94939414313552, 64: 187.9829363045283, 65: 187.7791830915386, 66: 187.78985334498446, 67: 188.13566313615243, 68: 187.9957900069388, 69: 188.3033795722962, 70: 188.43553870923603, 71: 188.17420466106364, 72: 188.20300612583478, 73: 188.52096591358202, 74: 188.30466187234342, 75: 188.3049864944645, 76: 188.33488586822023, 77: 188.3860774365694, 78: 188.0854108883303, 79: 188.45782436847483, 80: 188.53465030530313, 81: 188.797620051855, 82: 188.82680201580123, 83: 189.3305095748718, 84: 189.4974562113366, 85: 189.34012071955604, 86: 189.55025915386744, 87: 189.63686326738795, 88: 190.26937766800296, 89: 190.59391359186267, 90: 190.58262642396366, 91: 190.50941842268884, 92: 190.5583228743941, 93: 191.00999562189145, 94: 191.34084869639048, 95: 191.21599677906147, 96: 191.19943506067779, 97: 191.36678842424823, 98: 191.30759006927565, 99: 191.4162199768153, 100: 191.4488480223964}, 'fold_1': {1: 386.08485055834694, 2: 380.0947278597024, 3: 350.7850612658948, 4: 305.04810647145763, 5: 298.57960720634605, 6: 273.0789541138656, 7: 272.2838812758872, 8: 268.64417888242076, 9: 266.22811254191106, 10: 261.257936214973, 11: 259.9119988316014, 12: 259.1782039012275, 13: 257.80416389195136, 14: 255.52930537919514, 15: 254.3729959438235, 16: 252.38656131263463, 17: 252.17696327331785, 18: 252.07275361840314, 19: 252.6593566124174, 20: 252.858815799251, 21: 253.6789431844844, 22: 253.88965208747004, 23: 253.99735034537244, 24: 253.97100995303032, 25: 254.0547701928822, 26: 254.3199256777722, 27: 254.39216850222164, 28: 254.60225151106746, 29: 254.91552338608548, 30: 254.9723694521277, 31: 255.0846114947094, 32: 255.24309560921697, 33: 254.96160646464412, 34: 254.29480870333404, 35: 254.02216224807174, 36: 253.83149360101535, 37: 254.4710236723512, 38: 254.58037430639956, 39: 254.86418909337223, 40: 254.96330703947274, 41: 255.36934385755657, 42: 255.66611344433198, 43: 255.25566488095225, 44: 256.1144908621283, 45: 256.13688984691413, 46: 256.31387517098415, 47: 256.0599779522972, 48: 255.7204795608474, 49: 255.96248711803264, 50: 254.86486751351723, 51: 254.97954379406258, 52: 255.57869877432822, 53: 255.7503525612597, 54: 255.25390633944272, 55: 255.31115458601897, 56: 255.5491374408896, 57: 255.43990363359157, 58: 255.42503046050027, 59: 254.85121242587675, 60: 254.93527688540584, 61: 255.01312955436714, 62: 255.04788529060778, 63: 254.32839502238642, 64: 254.72054388343858, 65: 254.9596255295683, 66: 255.42622241266668, 67: 255.5449519109853, 68: 255.55838069336272, 69: 255.55957967614694, 70: 255.34063794799695, 71: 255.3184871379109, 72: 255.19238933292277, 73: 254.8558717498208, 74: 254.55241631304085, 75: 254.41545801520283, 76: 254.2987765439433, 77: 254.33970938584798, 78: 254.14933816372167, 79: 254.24493679949643, 80: 254.3659794144234, 81: 254.58866408746428, 82: 254.27775620029936, 83: 254.57204855903268, 84: 254.48192560222532, 85: 254.74463898515603, 86: 254.62121371655542, 87: 254.65987627726716, 88: 254.754600964188, 89: 254.76074009431932, 90: 255.0012234287873, 91: 254.76254878475018, 92: 254.86373631199046, 93: 255.26202951986744, 94: 255.1545602276504, 95: 255.0308733608458, 96: 255.42080414839145, 97: 255.45095753056032, 98: 255.37630406527092, 99: 254.87417627921246, 100: 254.84729046099244}, 'fold_2': {1: 259.7737103222799, 2: 253.4684540511573, 3: 215.11759829804697, 4: 196.26249624173272, 5: 189.69250761891485, 6: 175.10669023458726, 7: 171.92329100851674, 8: 170.2048516462407, 9: 168.47353043389114, 10: 165.46083121211615, 11: 160.86020760685435, 12: 159.94787853705228, 13: 157.42068040736262, 14: 157.44282699082456, 15: 157.2966589837402, 16: 157.11572408675613, 17: 156.97025792057434, 18: 157.8013700281075, 19: 157.77040044847823, 20: 157.14465897574152, 21: 157.15564161728145, 22: 157.22907810786936, 23: 157.5909067931682, 24: 157.89571549368762, 25: 158.22625403153563, 26: 158.47629097047948, 27: 158.09412759046927, 28: 158.1240650996796, 29: 158.16687543961754, 30: 158.34955988210024, 31: 158.07341984069862, 32: 158.29289751347602, 33: 158.09379209787807, 34: 157.88246558176033, 35: 157.37895438374557, 36: 157.56519524008365, 37: 157.5727225237133, 38: 157.64656704572081, 39: 157.48900931656576, 40: 157.25611258117783, 41: 157.48383665126377, 42: 157.8357889433787, 43: 157.75643123818205, 44: 157.7675334950645, 45: 157.70044414742256, 46: 157.99109780134947, 47: 157.64046931742405, 48: 158.0445262317179, 49: 157.8981127668679, 50: 157.9443253002708, 51: 157.75222362370818, 52: 157.72054204487026, 53: 157.2510615578744, 54: 157.00518012819117, 55: 157.03638983183063, 56: 157.32610497098818, 57: 157.21542377004513, 58: 156.88900598809795, 59: 157.1478883975762, 60: 157.07990853469354, 61: 157.04068086858624, 62: 156.7898516506162, 63: 156.78146571981645, 64: 156.53552638447633, 65: 156.77278765502993, 66: 156.93649684190976, 67: 156.7724799990524, 68: 156.6380354656061, 69: 156.29386137375366, 70: 156.44514442226006, 71: 156.6975868643569, 72: 156.80110338766815, 73: 156.73690350242296, 74: 156.86726188484798, 75: 157.25878061567673, 76: 157.40631458340508, 77: 157.2037708061701, 78: 156.90263683579857, 79: 157.08921255938432, 80: 157.0715062233186, 81: 157.04510012105874, 82: 156.91807630625726, 83: 156.95907727845378, 84: 156.95419652752423, 85: 157.09929013000288, 86: 157.03300351526912, 87: 157.08037615668798, 88: 156.93435975304024, 89: 156.989272972314, 90: 157.14870279609997, 91: 157.2676813183485, 92: 157.30156343167303, 93: 157.4790277261396, 94: 157.6883263254537, 95: 157.84857361735538, 96: 158.04655795303844, 97: 158.1734184956863, 98: 158.22087369555226, 99: 158.02824643260197, 100: 158.16182276724973}, 'fold_3': {1: 264.9055241280742, 2: 258.597579553004, 3: 220.64572067240007, 4: 189.9169876802666, 5: 180.36596711199635, 6: 164.56675053879627, 7: 161.06825803705576, 8: 159.5840507156863, 9: 159.28983322532054, 10: 157.81624535445454, 11: 152.80356447587172, 12: 152.9208653586382, 13: 149.54971284542785, 14: 149.15804876012322, 15: 149.22173084111742, 16: 149.02853354867437, 17: 148.71243849853516, 18: 149.03225286902514, 19: 148.54980722134573, 20: 148.2110086128159, 21: 148.05194069091817, 22: 147.92241264184267, 23: 147.4472995981367, 24: 147.8091468744208, 25: 147.8989726760256, 26: 148.1998409351344, 27: 148.54490864720174, 28: 148.59215720197813, 29: 148.1098982128694, 30: 147.73220939451292, 31: 147.56090173621183, 32: 147.28134101215286, 33: 147.4614666743049, 34: 147.50174977883864, 35: 147.8213224290277, 36: 147.93003586820294, 37: 147.8285795981481, 38: 148.4169294459518, 39: 148.20329064937286, 40: 148.02534004363207, 41: 148.09865793528755, 42: 148.34034761454498, 43: 148.1559022416577, 44: 148.29715295124495, 45: 147.93288680328698, 46: 147.04370448536304, 47: 147.5129174043192, 48: 147.49618830872072, 49: 147.25295558490185, 50: 147.19632905343627, 51: 147.25301946030072, 52: 147.4276580839574, 53: 147.30251720695622, 54: 147.67462944955463, 55: 147.76377997396406, 56: 147.65387264910598, 57: 147.55739775841525, 58: 147.7796865198015, 59: 147.40244784829397, 60: 147.43512841066598, 61: 147.40706063161943, 62: 147.61566090184306, 63: 147.54392250651, 64: 147.63118227461308, 65: 147.51607360297726, 66: 147.60826914671307, 67: 147.82183522142296, 68: 147.6809121541969, 69: 147.73241761754966, 70: 147.90061895023038, 71: 147.9091035640675, 72: 148.16592008473089, 73: 148.2855811213014, 74: 148.21380597133688, 75: 148.22930974579472, 76: 148.46450787082094, 77: 148.58287684290175, 78: 148.99631677702146, 79: 149.19745599490074, 80: 149.4683526766222, 81: 149.31330741863826, 82: 149.2237892430968, 83: 149.29677046533797, 84: 149.29480591008848, 85: 149.3973981795968, 86: 149.32149041370747, 87: 149.51094307882417, 88: 149.8486347924689, 89: 150.07353212701364, 90: 150.0667792050176, 91: 149.8984257311492, 92: 149.91723654897422, 93: 149.6126094570475, 94: 149.95104242904688, 95: 149.73549347716946, 96: 149.94039866597336, 97: 149.8793835119344, 98: 150.1891692357678, 99: 149.95541990054735, 100: 150.1675025574258}, 'fold_4': {1: 338.11145694047343, 2: 336.6922334064203, 3: 292.72390497599474, 4: 250.71465055880526, 5: 243.68133135576392, 6: 227.31164908663052, 7: 222.5075061971951, 8: 215.67313223719762, 9: 214.17450818655988, 10: 212.37896918234128, 11: 209.8956800555641, 12: 208.03796822518242, 13: 206.09118177104037, 14: 205.9134382984265, 15: 206.04519034347476, 16: 205.98083879699956, 17: 205.98804360203616, 18: 205.98494632163, 19: 205.28059826422538, 20: 205.63201295647215, 21: 206.70713006120494, 22: 207.09765508794686, 23: 207.33840493267957, 24: 207.68405365661698, 25: 208.020038383136, 26: 208.0691464307565, 27: 207.44346426336213, 28: 207.94957254781215, 29: 207.98027936085566, 30: 208.1060343334492, 31: 208.05720467455754, 32: 207.97502056622096, 33: 208.28277763287045, 34: 207.96777149189793, 35: 207.75987699212106, 36: 207.4968847615193, 37: 207.42021791506923, 38: 207.82985048639082, 39: 208.54411938694952, 40: 207.98909398845953, 41: 207.53341658560095, 42: 207.5275086238272, 43: 207.23929877040743, 44: 207.0868946077878, 45: 207.19957070941743, 46: 207.54243068569144, 47: 208.42117024401966, 48: 208.66671041919898, 49: 208.86143638826243, 50: 208.524591574775, 51: 208.4287979696155, 52: 208.35562126021236, 53: 208.47292614702772, 54: 207.71193541343405, 55: 207.80960574413888, 56: 207.77935509637982, 57: 207.4932855961213, 58: 207.12638833498113, 59: 206.96533271966004, 60: 206.80893171667768, 61: 207.25794971624285, 62: 207.44165388471245, 63: 206.64540183323743, 64: 206.7913621165812, 65: 207.41025099934902, 66: 207.4004296954392, 67: 207.7269539314024, 68: 207.86710835530212, 69: 207.7023541782709, 70: 208.17458882807966, 71: 208.68939728343778, 72: 208.6851780244332, 73: 208.6796544960434, 74: 208.63085468245123, 75: 208.2913286135425, 76: 208.5961265637461, 77: 208.64863868397148, 78: 208.6149519014198, 79: 208.57073533324743, 80: 208.7835732404074, 81: 208.53379703493397, 82: 208.73262044522954, 83: 209.07129467552954, 84: 209.04595451648737, 85: 209.01942808392474, 86: 209.28522410263778, 87: 209.3593998137943, 88: 209.31059177502067, 89: 209.0857013026242, 90: 209.08407920756292, 91: 208.77653175303084, 92: 208.7918984581844, 93: 209.18515355349248, 94: 209.0642775007366, 95: 209.30001542712913, 96: 209.54385037646057, 97: 209.36727771497422, 98: 209.31841018272667, 99: 209.04122507815887, 100: 209.0534302649678}, 'MSE': {1: 311.5394331937442, 2: 306.39476899149207, 3: 271.48086303612195, 4: 234.53238464222335, 5: 227.52545605719268, 6: 209.41263070169407, 7: 206.2031801738003, 8: 203.04897967108803, 9: 201.3044552130905, 10: 198.82803498832854, 11: 195.87929327818267, 12: 194.89165453150284, 13: 192.69303643768563, 14: 192.18626795399317, 15: 191.87260102763346, 16: 191.21597546136195, 17: 190.96200237495023, 18: 190.9420175853123, 19: 190.7874512061799, 20: 190.81167753687637, 21: 191.05299502967574, 22: 191.28282348481767, 23: 191.2788779752734, 24: 191.4026245947837, 25: 191.54985509116537, 26: 191.65636072966646, 27: 191.6772732765768, 28: 191.81436044931388, 29: 191.80355620817792, 30: 191.74419306360383, 31: 191.68237673270875, 32: 191.6669941935943, 33: 191.58027705137596, 34: 191.28976058766693, 35: 191.08671142113113, 36: 191.02406503932764, 37: 191.0425480203456, 38: 191.28119399326872, 39: 191.4526548097828, 40: 191.3609879558286, 41: 191.36808651290272, 42: 191.53973831406174, 43: 191.38180970971646, 44: 191.5671346542384, 45: 191.54098001552464, 46: 191.47200380353135, 47: 191.61460706571233, 48: 191.70554794535948, 49: 191.70386282299313, 50: 191.5521806895937, 51: 191.53803773624085, 52: 191.65973897891837, 53: 191.5776444127345, 54: 191.27186511777748, 55: 191.4627353045712, 56: 191.46216262968477, 57: 191.3368644585391, 58: 191.24840058327166, 59: 191.00332675440978, 60: 190.92311285634864, 61: 191.08455004237825, 62: 191.11432153019067, 63: 190.64948297836472, 64: 190.73207309597962, 65: 190.88731611764814, 66: 191.03197467460137, 67: 191.2001125492112, 68: 191.14777349519449, 69: 191.11807573274874, 70: 191.25906225939585, 71: 191.3574813634263, 72: 191.40924287220923, 73: 191.4155457162889, 74: 191.31354064685024, 75: 191.29971441940936, 76: 191.41985622562544, 77: 191.43195194247582, 78: 191.34944940928935, 79: 191.51176962643, 80: 191.6445441621092, 81: 191.6554512717922, 82: 191.59557005231048, 83: 191.8457231883844, 84: 191.85466445829766, 85: 191.91995272443344, 86: 191.96203017945655, 87: 192.0492836618372, 88: 192.2233444724929, 89: 192.30048483597568, 90: 192.3765274989551, 91: 192.24277171054996, 92: 192.28640248842282, 93: 192.5096338407831, 94: 192.63969901771665, 95: 192.62606892195063, 96: 192.83006860843332, 97: 192.84743743828244, 98: 192.88233363742302, 99: 192.66295001039398, 100: 192.73566783402757}, 'R2': {1: 0.10317680522664485, 2: 0.11798666136139146, 3: 0.21849272044970924, 4: 0.3248556681367305, 5: 0.34502639264102786, 6: 0.39716747069065006, 7: 0.4064064605879666, 8: 0.41548640319041386, 9: 0.4205083356685686, 10: 0.4276371638711648, 11: 0.43612565578997065, 12: 0.4389687544211751, 13: 0.4452978784192836, 14: 0.4467567041155892, 15: 0.44765965168825717, 16: 0.4495498683843481, 17: 0.4502809763291987, 18: 0.45033850619855176, 19: 0.45078345376912743, 20: 0.45071371384866177, 21: 0.45001903734293525, 22: 0.4493574341318194, 23: 0.449368792002116, 24: 0.44901256474217033, 25: 0.448588734850784, 26: 0.44828213890560065, 27: 0.4482219383173838, 28: 0.44782730783688207, 29: 0.4478584098195535, 30: 0.44802929748029985, 31: 0.4482072470862106, 32: 0.44825152853633377, 33: 0.44850115967846105, 34: 0.4493374644134567, 35: 0.4499219785481905, 36: 0.45010231760733965, 37: 0.4500491109685666, 38: 0.44936212492104566, 39: 0.44886854362487316, 40: 0.4491324234168317, 41: 0.44911198892297566, 42: 0.4486178578425988, 43: 0.4490724841929232, 44: 0.4485389924704998, 45: 0.4486142833780069, 46: 0.4488128440101845, 47: 0.44840233445810884, 48: 0.4481405445158708, 49: 0.448145395448572, 50: 0.44858204019068626, 51: 0.44862275326665046, 52: 0.4482724139977269, 53: 0.4485087382099263, 54: 0.4493889797940738, 55: 0.4488395250783337, 56: 0.4488411736276552, 57: 0.44920186731254175, 58: 0.4494565267449804, 59: 0.4501620165505632, 60: 0.4503929269158642, 61: 0.44992820047193993, 62: 0.4498424978032942, 63: 0.45118062052769137, 64: 0.45094286978013176, 65: 0.45049597437035416, 66: 0.450079548276655, 67: 0.4495955326760481, 68: 0.4497462002084164, 69: 0.4498316906451202, 70: 0.4494258351617503, 71: 0.44914251778391945, 72: 0.4489935128210578, 73: 0.44897536893247925, 74: 0.44926900916710133, 75: 0.4493088104896438, 76: 0.44896296034357386, 77: 0.448928140612031, 78: 0.44916563923180663, 79: 0.44869837082144826, 80: 0.44831615505488653, 81: 0.4482847569463023, 82: 0.44845713598035264, 83: 0.44773702446067587, 84: 0.4477112853811419, 85: 0.44752334117511694, 86: 0.4474022135308412, 87: 0.44715103843553616, 88: 0.4466499725810572, 89: 0.4464279099467532, 90: 0.4462090072439615, 91: 0.44659404772582667, 92: 0.44646844855784573, 93: 0.4458258363127602, 94: 0.44545141993050474, 95: 0.44549065665217635, 96: 0.4449034062720604, 97: 0.4448534069210469, 98: 0.4447529518342611, 99: 0.44538448769877803, 100: 0.4451751561536844}}'\n",
      "1: {'fold_0': 308.8227954890182, 'fold_1': 386.08485055834694, 'fold_2': 259.7737103222799, 'fold_3': 264.9055241280742, 'fold_4': 338.11145694047343, 'MSE': 311.5394331937442, 'R2': 0.10317680522664485}'\n",
      "2: {'fold_0': 303.12226125911786, 'fold_1': 380.0947278597024, 'fold_2': 253.4684540511573, 'fold_3': 258.597579553004, 'fold_4': 336.6922334064203, 'MSE': 306.39476899149207, 'R2': 0.11798666136139146}'\n",
      "3: {'fold_0': 278.12916308597516, 'fold_1': 350.7850612658948, 'fold_2': 215.11759829804697, 'fold_3': 220.64572067240007, 'fold_4': 292.72390497599474, 'MSE': 271.48086303612195, 'R2': 0.21849272044970924}'\n",
      "4: {'fold_0': 230.72132566505425, 'fold_1': 305.04810647145763, 'fold_2': 196.26249624173272, 'fold_3': 189.9169876802666, 'fold_4': 250.71465055880526, 'MSE': 234.53238464222335, 'R2': 0.3248556681367305}'\n",
      "5: {'fold_0': 225.3088228502973, 'fold_1': 298.57960720634605, 'fold_2': 189.69250761891485, 'fold_3': 180.36596711199635, 'fold_4': 243.68133135576392, 'MSE': 227.52545605719268, 'R2': 0.34502639264102786}'\n",
      "6: {'fold_0': 207.00014984543859, 'fold_1': 273.0789541138656, 'fold_2': 175.10669023458726, 'fold_3': 164.56675053879627, 'fold_4': 227.31164908663052, 'MSE': 209.41263070169407, 'R2': 0.39716747069065006}'\n",
      "7: {'fold_0': 203.2342446157878, 'fold_1': 272.2838812758872, 'fold_2': 171.92329100851674, 'fold_3': 161.06825803705576, 'fold_4': 222.5075061971951, 'MSE': 206.2031801738003, 'R2': 0.4064064605879666}'\n",
      "8: {'fold_0': 201.1395082768244, 'fold_1': 268.64417888242076, 'fold_2': 170.2048516462407, 'fold_3': 159.5840507156863, 'fold_4': 215.67313223719762, 'MSE': 203.04897967108803, 'R2': 0.41548640319041386}'\n",
      "9: {'fold_0': 198.35756243791437, 'fold_1': 266.22811254191106, 'fold_2': 168.47353043389114, 'fold_3': 159.28983322532054, 'fold_4': 214.17450818655988, 'MSE': 201.3044552130905, 'R2': 0.4205083356685686}'\n",
      "10: {'fold_0': 197.22688342690014, 'fold_1': 261.257936214973, 'fold_2': 165.46083121211615, 'fold_3': 157.81624535445454, 'fold_4': 212.37896918234128, 'MSE': 198.82803498832854, 'R2': 0.4276371638711648}'\n",
      "11: {'fold_0': 195.9249957132016, 'fold_1': 259.9119988316014, 'fold_2': 160.86020760685435, 'fold_3': 152.80356447587172, 'fold_4': 209.8956800555641, 'MSE': 195.87929327818267, 'R2': 0.43612565578997065}'\n",
      "12: {'fold_0': 194.37358003967935, 'fold_1': 259.1782039012275, 'fold_2': 159.94787853705228, 'fold_3': 152.9208653586382, 'fold_4': 208.03796822518242, 'MSE': 194.89165453150284, 'R2': 0.4389687544211751}'\n",
      "13: {'fold_0': 192.59948361452746, 'fold_1': 257.80416389195136, 'fold_2': 157.42068040736262, 'fold_3': 149.54971284542785, 'fold_4': 206.09118177104037, 'MSE': 192.69303643768563, 'R2': 0.4452978784192836}'\n",
      "14: {'fold_0': 192.88741799122963, 'fold_1': 255.52930537919514, 'fold_2': 157.44282699082456, 'fold_3': 149.15804876012322, 'fold_4': 205.9134382984265, 'MSE': 192.18626795399317, 'R2': 0.4467567041155892}'\n",
      "15: {'fold_0': 192.4261903070467, 'fold_1': 254.3729959438235, 'fold_2': 157.2966589837402, 'fold_3': 149.22173084111742, 'fold_4': 206.04519034347476, 'MSE': 191.87260102763346, 'R2': 0.44765965168825717}'\n",
      "16: {'fold_0': 191.5680677323913, 'fold_1': 252.38656131263463, 'fold_2': 157.11572408675613, 'fold_3': 149.02853354867437, 'fold_4': 205.98083879699956, 'MSE': 191.21597546136195, 'R2': 0.4495498683843481}'\n",
      "17: {'fold_0': 190.96230844830262, 'fold_1': 252.17696327331785, 'fold_2': 156.97025792057434, 'fold_3': 148.71243849853516, 'fold_4': 205.98804360203616, 'MSE': 190.96200237495023, 'R2': 0.4502809763291987}'\n",
      "18: {'fold_0': 189.81924924995425, 'fold_1': 252.07275361840314, 'fold_2': 157.8013700281075, 'fold_3': 149.03225286902514, 'fold_4': 205.98494632163, 'MSE': 190.9420175853123, 'R2': 0.45033850619855176}'\n",
      "19: {'fold_0': 189.67757208689898, 'fold_1': 252.6593566124174, 'fold_2': 157.77040044847823, 'fold_3': 148.54980722134573, 'fold_4': 205.28059826422538, 'MSE': 190.7874512061799, 'R2': 0.45078345376912743}'\n",
      "20: {'fold_0': 190.21214986863444, 'fold_1': 252.858815799251, 'fold_2': 157.14465897574152, 'fold_3': 148.2110086128159, 'fold_4': 205.63201295647215, 'MSE': 190.81167753687637, 'R2': 0.45071371384866177}'\n",
      "21: {'fold_0': 189.67191514424624, 'fold_1': 253.6789431844844, 'fold_2': 157.15564161728145, 'fold_3': 148.05194069091817, 'fold_4': 206.70713006120494, 'MSE': 191.05299502967574, 'R2': 0.45001903734293525}'\n",
      "22: {'fold_0': 190.27575376791884, 'fold_1': 253.88965208747004, 'fold_2': 157.22907810786936, 'fold_3': 147.92241264184267, 'fold_4': 207.09765508794686, 'MSE': 191.28282348481767, 'R2': 0.4493574341318194}'\n",
      "23: {'fold_0': 190.0209706422549, 'fold_1': 253.99735034537244, 'fold_2': 157.5909067931682, 'fold_3': 147.4472995981367, 'fold_4': 207.33840493267957, 'MSE': 191.2788779752734, 'R2': 0.449368792002116}'\n",
      "24: {'fold_0': 189.65395105978297, 'fold_1': 253.97100995303032, 'fold_2': 157.89571549368762, 'fold_3': 147.8091468744208, 'fold_4': 207.68405365661698, 'MSE': 191.4026245947837, 'R2': 0.44901256474217033}'\n",
      "25: {'fold_0': 189.55010250626404, 'fold_1': 254.0547701928822, 'fold_2': 158.22625403153563, 'fold_3': 147.8989726760256, 'fold_4': 208.020038383136, 'MSE': 191.54985509116537, 'R2': 0.448588734850784}'\n",
      "26: {'fold_0': 189.21765125535154, 'fold_1': 254.3199256777722, 'fold_2': 158.47629097047948, 'fold_3': 148.1998409351344, 'fold_4': 208.0691464307565, 'MSE': 191.65636072966646, 'R2': 0.44828213890560065}'\n",
      "27: {'fold_0': 189.912458403723, 'fold_1': 254.39216850222164, 'fold_2': 158.09412759046927, 'fold_3': 148.54490864720174, 'fold_4': 207.44346426336213, 'MSE': 191.6772732765768, 'R2': 0.4482219383173838}'\n",
      "28: {'fold_0': 189.80462252593003, 'fold_1': 254.60225151106746, 'fold_2': 158.1240650996796, 'fold_3': 148.59215720197813, 'fold_4': 207.94957254781215, 'MSE': 191.81436044931388, 'R2': 0.44782730783688207}'\n",
      "29: {'fold_0': 189.84604875851616, 'fold_1': 254.91552338608548, 'fold_2': 158.16687543961754, 'fold_3': 148.1098982128694, 'fold_4': 207.98027936085566, 'MSE': 191.80355620817792, 'R2': 0.4478584098195535}'\n",
      "30: {'fold_0': 189.5617333768669, 'fold_1': 254.9723694521277, 'fold_2': 158.34955988210024, 'fold_3': 147.73220939451292, 'fold_4': 208.1060343334492, 'MSE': 191.74419306360383, 'R2': 0.44802929748029985}'\n",
      "31: {'fold_0': 189.63662808582114, 'fold_1': 255.0846114947094, 'fold_2': 158.07341984069862, 'fold_3': 147.56090173621183, 'fold_4': 208.05720467455754, 'MSE': 191.68237673270875, 'R2': 0.4482072470862106}'\n",
      "32: {'fold_0': 189.5435319470453, 'fold_1': 255.24309560921697, 'fold_2': 158.29289751347602, 'fold_3': 147.28134101215286, 'fold_4': 207.97502056622096, 'MSE': 191.6669941935943, 'R2': 0.44825152853633377}'\n",
      "33: {'fold_0': 189.10281072108916, 'fold_1': 254.96160646464412, 'fold_2': 158.09379209787807, 'fold_3': 147.4614666743049, 'fold_4': 208.28277763287045, 'MSE': 191.58027705137596, 'R2': 0.44850115967846105}'\n",
      "34: {'fold_0': 188.80307968991963, 'fold_1': 254.29480870333404, 'fold_2': 157.88246558176033, 'fold_3': 147.50174977883864, 'fold_4': 207.96777149189793, 'MSE': 191.28976058766693, 'R2': 0.4493374644134567}'\n",
      "35: {'fold_0': 188.4523770312968, 'fold_1': 254.02216224807174, 'fold_2': 157.37895438374557, 'fold_3': 147.8213224290277, 'fold_4': 207.75987699212106, 'MSE': 191.08671142113113, 'R2': 0.4499219785481905}'\n",
      "36: {'fold_0': 188.29789130741742, 'fold_1': 253.83149360101535, 'fold_2': 157.56519524008365, 'fold_3': 147.93003586820294, 'fold_4': 207.4968847615193, 'MSE': 191.02406503932764, 'R2': 0.45010231760733965}'\n",
      "37: {'fold_0': 187.9215422336651, 'fold_1': 254.4710236723512, 'fold_2': 157.5727225237133, 'fold_3': 147.8285795981481, 'fold_4': 207.42021791506923, 'MSE': 191.0425480203456, 'R2': 0.4500491109685666}'\n",
      "38: {'fold_0': 187.93369219279072, 'fold_1': 254.58037430639956, 'fold_2': 157.64656704572081, 'fold_3': 148.4169294459518, 'fold_4': 207.82985048639082, 'MSE': 191.28119399326872, 'R2': 0.44936212492104566}'\n",
      "39: {'fold_0': 188.16408370144993, 'fold_1': 254.86418909337223, 'fold_2': 157.48900931656576, 'fold_3': 148.20329064937286, 'fold_4': 208.54411938694952, 'MSE': 191.4526548097828, 'R2': 0.44886854362487316}'\n",
      "40: {'fold_0': 188.57228867029278, 'fold_1': 254.96330703947274, 'fold_2': 157.25611258117783, 'fold_3': 148.02534004363207, 'fold_4': 207.98909398845953, 'MSE': 191.3609879558286, 'R2': 0.4491324234168317}'\n",
      "41: {'fold_0': 188.3564762024678, 'fold_1': 255.36934385755657, 'fold_2': 157.48383665126377, 'fold_3': 148.09865793528755, 'fold_4': 207.53341658560095, 'MSE': 191.36808651290272, 'R2': 0.44911198892297566}'\n",
      "42: {'fold_0': 188.33031691205787, 'fold_1': 255.66611344433198, 'fold_2': 157.8357889433787, 'fold_3': 148.34034761454498, 'fold_4': 207.5275086238272, 'MSE': 191.53973831406174, 'R2': 0.4486178578425988}'\n",
      "43: {'fold_0': 188.50299282181925, 'fold_1': 255.25566488095225, 'fold_2': 157.75643123818205, 'fold_3': 148.1559022416577, 'fold_4': 207.23929877040743, 'MSE': 191.38180970971646, 'R2': 0.4490724841929232}'\n",
      "44: {'fold_0': 188.57089339518163, 'fold_1': 256.1144908621283, 'fold_2': 157.7675334950645, 'fold_3': 148.29715295124495, 'fold_4': 207.0868946077878, 'MSE': 191.5671346542384, 'R2': 0.4485389924704998}'\n",
      "45: {'fold_0': 188.73631799792892, 'fold_1': 256.13688984691413, 'fold_2': 157.70044414742256, 'fold_3': 147.93288680328698, 'fold_4': 207.19957070941743, 'MSE': 191.54098001552464, 'R2': 0.4486142833780069}'\n",
      "46: {'fold_0': 188.4702053108762, 'fold_1': 256.31387517098415, 'fold_2': 157.99109780134947, 'fold_3': 147.04370448536304, 'fold_4': 207.54243068569144, 'MSE': 191.47200380353135, 'R2': 0.4488128440101845}'\n",
      "47: {'fold_0': 188.43986942199092, 'fold_1': 256.0599779522972, 'fold_2': 157.64046931742405, 'fold_3': 147.5129174043192, 'fold_4': 208.42117024401966, 'MSE': 191.61460706571233, 'R2': 0.44840233445810884}'\n",
      "48: {'fold_0': 188.60117387559652, 'fold_1': 255.7204795608474, 'fold_2': 158.0445262317179, 'fold_3': 147.49618830872072, 'fold_4': 208.66671041919898, 'MSE': 191.70554794535948, 'R2': 0.4481405445158708}'\n",
      "49: {'fold_0': 188.54568412783445, 'fold_1': 255.96248711803264, 'fold_2': 157.8981127668679, 'fold_3': 147.25295558490185, 'fold_4': 208.86143638826243, 'MSE': 191.70386282299313, 'R2': 0.448145395448572}'\n",
      "50: {'fold_0': 189.23179060540193, 'fold_1': 254.86486751351723, 'fold_2': 157.9443253002708, 'fold_3': 147.19632905343627, 'fold_4': 208.524591574775, 'MSE': 191.5521806895937, 'R2': 0.44858204019068626}'\n",
      "51: {'fold_0': 189.27757858950986, 'fold_1': 254.97954379406258, 'fold_2': 157.75222362370818, 'fold_3': 147.25301946030072, 'fold_4': 208.4287979696155, 'MSE': 191.53803773624085, 'R2': 0.44862275326665046}'\n",
      "52: {'fold_0': 189.2172279916751, 'fold_1': 255.57869877432822, 'fold_2': 157.72054204487026, 'fold_3': 147.4276580839574, 'fold_4': 208.35562126021236, 'MSE': 191.65973897891837, 'R2': 0.4482724139977269}'\n",
      "53: {'fold_0': 189.11242764220185, 'fold_1': 255.7503525612597, 'fold_2': 157.2510615578744, 'fold_3': 147.30251720695622, 'fold_4': 208.47292614702772, 'MSE': 191.5776444127345, 'R2': 0.4485087382099263}'\n",
      "54: {'fold_0': 188.71477692673892, 'fold_1': 255.25390633944272, 'fold_2': 157.00518012819117, 'fold_3': 147.67462944955463, 'fold_4': 207.71193541343405, 'MSE': 191.27186511777748, 'R2': 0.4493889797940738}'\n",
      "55: {'fold_0': 189.39363862350567, 'fold_1': 255.31115458601897, 'fold_2': 157.03638983183063, 'fold_3': 147.76377997396406, 'fold_4': 207.80960574413888, 'MSE': 191.4627353045712, 'R2': 0.4488395250783337}'\n",
      "56: {'fold_0': 189.0034032581459, 'fold_1': 255.5491374408896, 'fold_2': 157.32610497098818, 'fold_3': 147.65387264910598, 'fold_4': 207.77935509637982, 'MSE': 191.46216262968477, 'R2': 0.4488411736276552}'\n",
      "57: {'fold_0': 188.9793281521621, 'fold_1': 255.43990363359157, 'fold_2': 157.21542377004513, 'fold_3': 147.55739775841525, 'fold_4': 207.4932855961213, 'MSE': 191.3368644585391, 'R2': 0.44920186731254175}'\n",
      "58: {'fold_0': 189.02285131511974, 'fold_1': 255.42503046050027, 'fold_2': 156.88900598809795, 'fold_3': 147.7796865198015, 'fold_4': 207.12638833498113, 'MSE': 191.24840058327166, 'R2': 0.4494565267449804}'\n",
      "59: {'fold_0': 188.65076685235454, 'fold_1': 254.85121242587675, 'fold_2': 157.1478883975762, 'fold_3': 147.40244784829397, 'fold_4': 206.96533271966004, 'MSE': 191.00332675440978, 'R2': 0.4501620165505632}'\n",
      "60: {'fold_0': 188.35742511107674, 'fold_1': 254.93527688540584, 'fold_2': 157.07990853469354, 'fold_3': 147.43512841066598, 'fold_4': 206.80893171667768, 'MSE': 190.92311285634864, 'R2': 0.4503929269158642}'\n",
      "61: {'fold_0': 188.70495557064513, 'fold_1': 255.01312955436714, 'fold_2': 157.04068086858624, 'fold_3': 147.40706063161943, 'fold_4': 207.25794971624285, 'MSE': 191.08455004237825, 'R2': 0.44992820047193993}'\n",
      "62: {'fold_0': 188.6776066842113, 'fold_1': 255.04788529060778, 'fold_2': 156.7898516506162, 'fold_3': 147.61566090184306, 'fold_4': 207.44165388471245, 'MSE': 191.11432153019067, 'R2': 0.4498424978032942}'\n",
      "63: {'fold_0': 187.94939414313552, 'fold_1': 254.32839502238642, 'fold_2': 156.78146571981645, 'fold_3': 147.54392250651, 'fold_4': 206.64540183323743, 'MSE': 190.64948297836472, 'R2': 0.45118062052769137}'\n",
      "64: {'fold_0': 187.9829363045283, 'fold_1': 254.72054388343858, 'fold_2': 156.53552638447633, 'fold_3': 147.63118227461308, 'fold_4': 206.7913621165812, 'MSE': 190.73207309597962, 'R2': 0.45094286978013176}'\n",
      "65: {'fold_0': 187.7791830915386, 'fold_1': 254.9596255295683, 'fold_2': 156.77278765502993, 'fold_3': 147.51607360297726, 'fold_4': 207.41025099934902, 'MSE': 190.88731611764814, 'R2': 0.45049597437035416}'\n",
      "66: {'fold_0': 187.78985334498446, 'fold_1': 255.42622241266668, 'fold_2': 156.93649684190976, 'fold_3': 147.60826914671307, 'fold_4': 207.4004296954392, 'MSE': 191.03197467460137, 'R2': 0.450079548276655}'\n",
      "67: {'fold_0': 188.13566313615243, 'fold_1': 255.5449519109853, 'fold_2': 156.7724799990524, 'fold_3': 147.82183522142296, 'fold_4': 207.7269539314024, 'MSE': 191.2001125492112, 'R2': 0.4495955326760481}'\n",
      "68: {'fold_0': 187.9957900069388, 'fold_1': 255.55838069336272, 'fold_2': 156.6380354656061, 'fold_3': 147.6809121541969, 'fold_4': 207.86710835530212, 'MSE': 191.14777349519449, 'R2': 0.4497462002084164}'\n",
      "69: {'fold_0': 188.3033795722962, 'fold_1': 255.55957967614694, 'fold_2': 156.29386137375366, 'fold_3': 147.73241761754966, 'fold_4': 207.7023541782709, 'MSE': 191.11807573274874, 'R2': 0.4498316906451202}'\n",
      "70: {'fold_0': 188.43553870923603, 'fold_1': 255.34063794799695, 'fold_2': 156.44514442226006, 'fold_3': 147.90061895023038, 'fold_4': 208.17458882807966, 'MSE': 191.25906225939585, 'R2': 0.4494258351617503}'\n",
      "71: {'fold_0': 188.17420466106364, 'fold_1': 255.3184871379109, 'fold_2': 156.6975868643569, 'fold_3': 147.9091035640675, 'fold_4': 208.68939728343778, 'MSE': 191.3574813634263, 'R2': 0.44914251778391945}'\n",
      "72: {'fold_0': 188.20300612583478, 'fold_1': 255.19238933292277, 'fold_2': 156.80110338766815, 'fold_3': 148.16592008473089, 'fold_4': 208.6851780244332, 'MSE': 191.40924287220923, 'R2': 0.4489935128210578}'\n",
      "73: {'fold_0': 188.52096591358202, 'fold_1': 254.8558717498208, 'fold_2': 156.73690350242296, 'fold_3': 148.2855811213014, 'fold_4': 208.6796544960434, 'MSE': 191.4155457162889, 'R2': 0.44897536893247925}'\n",
      "74: {'fold_0': 188.30466187234342, 'fold_1': 254.55241631304085, 'fold_2': 156.86726188484798, 'fold_3': 148.21380597133688, 'fold_4': 208.63085468245123, 'MSE': 191.31354064685024, 'R2': 0.44926900916710133}'\n",
      "75: {'fold_0': 188.3049864944645, 'fold_1': 254.41545801520283, 'fold_2': 157.25878061567673, 'fold_3': 148.22930974579472, 'fold_4': 208.2913286135425, 'MSE': 191.29971441940936, 'R2': 0.4493088104896438}'\n",
      "76: {'fold_0': 188.33488586822023, 'fold_1': 254.2987765439433, 'fold_2': 157.40631458340508, 'fold_3': 148.46450787082094, 'fold_4': 208.5961265637461, 'MSE': 191.41985622562544, 'R2': 0.44896296034357386}'\n",
      "77: {'fold_0': 188.3860774365694, 'fold_1': 254.33970938584798, 'fold_2': 157.2037708061701, 'fold_3': 148.58287684290175, 'fold_4': 208.64863868397148, 'MSE': 191.43195194247582, 'R2': 0.448928140612031}'\n",
      "78: {'fold_0': 188.0854108883303, 'fold_1': 254.14933816372167, 'fold_2': 156.90263683579857, 'fold_3': 148.99631677702146, 'fold_4': 208.6149519014198, 'MSE': 191.34944940928935, 'R2': 0.44916563923180663}'\n",
      "79: {'fold_0': 188.45782436847483, 'fold_1': 254.24493679949643, 'fold_2': 157.08921255938432, 'fold_3': 149.19745599490074, 'fold_4': 208.57073533324743, 'MSE': 191.51176962643, 'R2': 0.44869837082144826}'\n",
      "80: {'fold_0': 188.53465030530313, 'fold_1': 254.3659794144234, 'fold_2': 157.0715062233186, 'fold_3': 149.4683526766222, 'fold_4': 208.7835732404074, 'MSE': 191.6445441621092, 'R2': 0.44831615505488653}'\n",
      "81: {'fold_0': 188.797620051855, 'fold_1': 254.58866408746428, 'fold_2': 157.04510012105874, 'fold_3': 149.31330741863826, 'fold_4': 208.53379703493397, 'MSE': 191.6554512717922, 'R2': 0.4482847569463023}'\n",
      "82: {'fold_0': 188.82680201580123, 'fold_1': 254.27775620029936, 'fold_2': 156.91807630625726, 'fold_3': 149.2237892430968, 'fold_4': 208.73262044522954, 'MSE': 191.59557005231048, 'R2': 0.44845713598035264}'\n",
      "83: {'fold_0': 189.3305095748718, 'fold_1': 254.57204855903268, 'fold_2': 156.95907727845378, 'fold_3': 149.29677046533797, 'fold_4': 209.07129467552954, 'MSE': 191.8457231883844, 'R2': 0.44773702446067587}'\n",
      "84: {'fold_0': 189.4974562113366, 'fold_1': 254.48192560222532, 'fold_2': 156.95419652752423, 'fold_3': 149.29480591008848, 'fold_4': 209.04595451648737, 'MSE': 191.85466445829766, 'R2': 0.4477112853811419}'\n",
      "85: {'fold_0': 189.34012071955604, 'fold_1': 254.74463898515603, 'fold_2': 157.09929013000288, 'fold_3': 149.3973981795968, 'fold_4': 209.01942808392474, 'MSE': 191.91995272443344, 'R2': 0.44752334117511694}'\n",
      "86: {'fold_0': 189.55025915386744, 'fold_1': 254.62121371655542, 'fold_2': 157.03300351526912, 'fold_3': 149.32149041370747, 'fold_4': 209.28522410263778, 'MSE': 191.96203017945655, 'R2': 0.4474022135308412}'\n",
      "87: {'fold_0': 189.63686326738795, 'fold_1': 254.65987627726716, 'fold_2': 157.08037615668798, 'fold_3': 149.51094307882417, 'fold_4': 209.3593998137943, 'MSE': 192.0492836618372, 'R2': 0.44715103843553616}'\n",
      "88: {'fold_0': 190.26937766800296, 'fold_1': 254.754600964188, 'fold_2': 156.93435975304024, 'fold_3': 149.8486347924689, 'fold_4': 209.31059177502067, 'MSE': 192.2233444724929, 'R2': 0.4466499725810572}'\n",
      "89: {'fold_0': 190.59391359186267, 'fold_1': 254.76074009431932, 'fold_2': 156.989272972314, 'fold_3': 150.07353212701364, 'fold_4': 209.0857013026242, 'MSE': 192.30048483597568, 'R2': 0.4464279099467532}'\n",
      "90: {'fold_0': 190.58262642396366, 'fold_1': 255.0012234287873, 'fold_2': 157.14870279609997, 'fold_3': 150.0667792050176, 'fold_4': 209.08407920756292, 'MSE': 192.3765274989551, 'R2': 0.4462090072439615}'\n",
      "91: {'fold_0': 190.50941842268884, 'fold_1': 254.76254878475018, 'fold_2': 157.2676813183485, 'fold_3': 149.8984257311492, 'fold_4': 208.77653175303084, 'MSE': 192.24277171054996, 'R2': 0.44659404772582667}'\n",
      "92: {'fold_0': 190.5583228743941, 'fold_1': 254.86373631199046, 'fold_2': 157.30156343167303, 'fold_3': 149.91723654897422, 'fold_4': 208.7918984581844, 'MSE': 192.28640248842282, 'R2': 0.44646844855784573}'\n",
      "93: {'fold_0': 191.00999562189145, 'fold_1': 255.26202951986744, 'fold_2': 157.4790277261396, 'fold_3': 149.6126094570475, 'fold_4': 209.18515355349248, 'MSE': 192.5096338407831, 'R2': 0.4458258363127602}'\n",
      "94: {'fold_0': 191.34084869639048, 'fold_1': 255.1545602276504, 'fold_2': 157.6883263254537, 'fold_3': 149.95104242904688, 'fold_4': 209.0642775007366, 'MSE': 192.63969901771665, 'R2': 0.44545141993050474}'\n",
      "95: {'fold_0': 191.21599677906147, 'fold_1': 255.0308733608458, 'fold_2': 157.84857361735538, 'fold_3': 149.73549347716946, 'fold_4': 209.30001542712913, 'MSE': 192.62606892195063, 'R2': 0.44549065665217635}'\n",
      "96: {'fold_0': 191.19943506067779, 'fold_1': 255.42080414839145, 'fold_2': 158.04655795303844, 'fold_3': 149.94039866597336, 'fold_4': 209.54385037646057, 'MSE': 192.83006860843332, 'R2': 0.4449034062720604}'\n",
      "97: {'fold_0': 191.36678842424823, 'fold_1': 255.45095753056032, 'fold_2': 158.1734184956863, 'fold_3': 149.8793835119344, 'fold_4': 209.36727771497422, 'MSE': 192.84743743828244, 'R2': 0.4448534069210469}'\n",
      "98: {'fold_0': 191.30759006927565, 'fold_1': 255.37630406527092, 'fold_2': 158.22087369555226, 'fold_3': 150.1891692357678, 'fold_4': 209.31841018272667, 'MSE': 192.88233363742302, 'R2': 0.4447529518342611}'\n",
      "99: {'fold_0': 191.4162199768153, 'fold_1': 254.87417627921246, 'fold_2': 158.02824643260197, 'fold_3': 149.95541990054735, 'fold_4': 209.04122507815887, 'MSE': 192.66295001039398, 'R2': 0.44538448769877803}'\n",
      "100: {'fold_0': 191.4488480223964, 'fold_1': 254.84729046099244, 'fold_2': 158.16182276724973, 'fold_3': 150.1675025574258, 'fold_4': 209.0534302649678, 'MSE': 192.73566783402757, 'R2': 0.4451751561536844}'\n",
      "Selected pls preprocessing with 63 components'\n"
     ]
    }
   ],
   "source": [
    "# set logging, in this case the root logger\n",
    "ut.setup_logger(logger_name=\"log\",file_name=log_dir/\"log.txt\")\n",
    "ut.setup_logger(logger_name=\"test_log\",file_name=log_dir/\"test_log.txt\")\n",
    "ut.setup_logger(logger_name=\"summary\",file_name=log_dir/\"summary.txt\")\n",
    "tb = SummaryWriter(log_dir/\"tb\")\n",
    "summary_logger = logging.getLogger(\"summary\")\n",
    "#step 1, run pls, set up pls - that runs best\n",
    "n_comps = [i for i in range(1,101)]\n",
    "pls_models = {i:PLSRegression(n_components=i) for i in n_comps}\n",
    "\n",
    "pls_scheme = SKLearnScheme(logger=\"log\")\n",
    "scores_pls, preds_pls, model_states_pls , train_time_pls, test_time_pls,_ = eval.evaluate(pls_models,dataset,pls_scheme,logger_name=\"log\")\n",
    "summary_logger.info(f\"Train times: {train_time_pls}\")\n",
    "summary_logger.info(f\"Test times: {test_time_pls}\")\n",
    "from collections import defaultdict\n",
    "summary_logger.info(f\"Scores: {scores_pls}\")\n",
    "for key,value in flip_dicts(scores_pls).items():\n",
    "    summary_logger.info(f\"{key}: {value}\")\n",
    "\n",
    "selected_comps =  min(scores_pls[\"MSE\"],key=scores_pls[\"MSE\"].get)\n",
    "summary_logger.info(f\"Selected pls preprocessing with {selected_comps} components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = CrossValEvaluation(preprocessing=PLSRegression(n_components=selected_comps),tensorboard=None,time=True,random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the deep learners\n",
    "The following cells setup our models and run a train-test evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% setup experiment\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_models = 100\n",
    "epochs = 100\n",
    "bs = 32\n",
    "fixed_hyperparams = {'bs': bs,'loss': nn.MSELoss(),'epochs': epochs}\n",
    "device = \"cpu\" #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#setup models\n",
    "config_gen = RandomConfigGen(lr= (0,1),\n",
    "                             allow_increase_size=False,\n",
    "                             n_features=selected_comps,\n",
    "                             opt=[torch.optim.SGD,\n",
    "                                  torch.optim.Adam],\n",
    "                             lr_update = [None,\n",
    "                                          torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                                          torch.optim.lr_scheduler.ExponentialLR,\n",
    "                                          torch.optim.lr_scheduler.CosineAnnealingLR],\n",
    "                            dropout = [True,False],\n",
    "                            batch_norm = [True,False])\n",
    "configs = {f\"random_{i}\":config_gen.sample() for i in range(n_models)}\n",
    "config_gen.save(log_dir/'config_gen.txt')\n",
    "\n",
    "deep_models = {name:RandomNet(input_size=selected_comps,\n",
    "                             n_layers=config.n_layers,\n",
    "                             act_function=config.act_function,\n",
    "                             n_features = config.n_features,\n",
    "                             dropout=config.dropout,\n",
    "                             batch_norm=config.batch_norm,\n",
    "                             device=device,dtype=torch.float)\n",
    "              for name, config in configs.items()}\n",
    "\n",
    "ex.write_summary_head(seed,fixed_hyperparams)\n",
    "ex.save_models(deep_models,configs,log_dir)\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "deep_scheme = DeepScheme(configs,fixed_hyperparams=fixed_hyperparams,logger=\"log\",device=device,adaptive_lr=True)\n",
    "scores_deep, preds_deep, model_states_deep , train_time_deep, test_time_deep,pp_state = eval.evaluate(deep_models,dataset,deep_scheme,logger_name=\"log\")\n",
    "\n",
    "scores_final, preds_final, model_states_ls_final , train_time_deep_final, test_time_deep_final,pp_state_final = eval.build(deep_models,dataset,deep_scheme,logger_name=\"test_log\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% log results\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model, state_dict in model_states_ls_final.items():\n",
    "     torch.save(state_dict.state(), log_dir / \"models\" / f\"{model}\" / f\"_final\")\n",
    "        \n",
    "\n",
    "ex.save_pp(pp_state,log_dir)\n",
    "PLSRegression(n_components=selected_comps).save_state(pp_state_final.state(),log_dir / \"preprocessing\"   / f\"_final\")\n",
    "        \n",
    "summary_logger.info(f\"Train times: {train_time_deep}\")\n",
    "summary_logger.info(f\"Test times: {test_time_deep}\")\n",
    "ex.save_results(model_states_deep, preds_deep, configs, scores_deep, log_dir,tb)\n",
    "\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "diff = end - start\n",
    "ex.write_summary(diff, deep_models, scores_deep)\n",
    "ex.save_pred_plots(preds_deep, deep_models, log_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(scores_deep)\n",
    "scores_df.to_csv(log_dir / f\"scores.csv\", index=False)\n",
    "scores_df_final = pd.DataFrame(scores_final)\n",
    "scores_df_final.to_csv(log_dir / f\"scores_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting deep results as a function of number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "if True:\n",
    "    # plot deep results as a function of number of features\n",
    "\n",
    "    n_features_dict = {name:config.n_features for name,config in configs.items()}\n",
    "    to_plot = pd.DataFrame([[name, scores_deep[\"R2\"][name],n_features_dict[name]] for name in scores_deep[\"R2\"].keys()]\n",
    "                           ,columns = [\"name\",\"score\",\"n_features\"])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(to_plot[\"score\"],bins=100,density=True)\n",
    "    #ax.set_xscale('log')\n",
    "    #ax.legend(loc='upper right',bbox_to_anchor=(1.4, 1))\n",
    "    ax.set_xlabel(\"Number of Models\")\n",
    "    ax.set_ylabel(\"R2\")\n",
    "    ax.set_title(\"Distribution of R2 Scoes\")\n",
    "    plt.savefig(log_dir / f\"dist_plot.png\",bbox_inches='tight')\n",
    "    #plt.savefig(log_dir / f\"pp_deep_pls_compressed.png\",bbox_inches='tight')\n",
    "\n",
    "    n_features_dict = {name:config.n_features for name,config in configs.items()}\n",
    "    to_plot = pd.DataFrame([[name, scores_deep[\"R2\"][name],n_features_dict[name]] for name in scores_deep[\"R2\"].keys()]\n",
    "                           ,columns = [\"name\",\"score\",\"n_features\"])\n",
    "    to_plot = to_plot[to_plot[\"score\"]>=0]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(to_plot[\"score\"],bins=100,density=True)\n",
    "    #ax.set_xscale('log')\n",
    "    #ax.legend(loc='upper right',bbox_to_anchor=(1.4, 1))\n",
    "    ax.set_xlabel(\"Number of Models\")\n",
    "    ax.set_ylabel(\"R2\")\n",
    "    ax.set_title(\"Distribution of R2 Scoes\")\n",
    "    plt.savefig(log_dir / f\"dist_plot_compressed.png\",bbox_inches='tight')\n",
    "    #plt.savefig(log_dir / f\"pp_deep_pls_compressed.png\",bbox_inches='tight')\n",
    "    pass\n",
    "\n",
    "    #plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% Return our best models\n"
    }
   },
   "outputs": [],
   "source": [
    "summary_logger.info(\"------------------\\n Top 5 performance on Test Set\")\n",
    "summary_logger.info(f\"Index - Model - Val - Score - Test Score\")\n",
    "for i,key in enumerate(sorted(scores_deep['MSE'],key=scores_deep['MSE'].get)):\n",
    "    if i <5:\n",
    "        summary_logger.info(f\"{i} - {key} - {scores_deep['MSE'][key]} - {scores_deep['R2'][key]} - {scores_final['MSE'][key]} - {scores_final['R2'][key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    model_path = Path('D:/workspace/lazydeep/experiments/1.01/')\n",
    "    log_path = Path(\"D:/workspace/lazydeep/experiments/1.02\")\n",
    "\n",
    "    log_dir = log_path / re.sub(r'\\.(?=csv$)[^.]+$', '',file_name)\n",
    "    model_dir = model_path / re.sub(r'\\.(?=csv$)[^.]+$', '',file_name)\n",
    "\n",
    "    if not log_dir.exists():\n",
    "        log_dir.mkdir()\n",
    "\n",
    "    ut.setup_logger(logger_name=\"\",file_name=log_dir/\"log.txt\")\n",
    "    ut.setup_logger(logger_name=\"summary2\",file_name=log_dir/\"summary.txt\")\n",
    "    summary_logger = logging.getLogger(\"summary2\")\n",
    "    tb = SummaryWriter(log_dir/\"tb\")\n",
    "\n",
    "\n",
    "\n",
    "    deep_scores_dict={}\n",
    "    deep_preds_dict={}\n",
    "    actual_y = None\n",
    "    preprocessing=PLSRegression(n_components=selected_comps)\n",
    "\n",
    "    load_fun_cv = lambda name,model, fold : model.load_state(model_dir/'models'/name/f\"_fold_{fold}\")\n",
    "    load_fun_pp_cv = lambda fold : preprocessing.from_state(preprocessing.load_state(model_dir/'preprocessing'/f\"_fold_{fold}\"))\n",
    "    load_fun_build = lambda name,model : model.load_state(model_dir/'models'/name/f\"_final\")\n",
    "    load_fun_pp_build = lambda : preprocessing.from_state(preprocessing.load_state(model_dir/'preprocessing'/f\"_final\"))\n",
    "\n",
    "    deep_scheme = DeepScheme(configs, fixed_hyperparams=fixed_hyperparams,loss_eval=loss_target,device=device,tensorboard=tb,adaptive_lr=False,update=False)\n",
    "    deep_scores, deep_preds, _ , _, _,_ = eval.evaluate(deep_models,dataset,deep_scheme,logger_name=\"log\",load_fun=load_fun_cv,load_fun_pp=load_fun_pp_cv)\n",
    "    deep_scores_final, deep_preds_final, _ ,_, _,_ = eval.build(deep_models,dataset,deep_scheme,logger_name=\"test_log\",load_fun=load_fun_build,load_fun_pp=load_fun_pp_build)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    all_scores = []\n",
    "    for k,v in ut.flip_dicts(deep_scores).items():\n",
    "        dict1 = {'model_num':k,\"predictor\":\"deep\"}\n",
    "        all_scores.append({**dict1,**v})\n",
    "\n",
    "    all_scores_final = []\n",
    "    for k,v in ut.flip_dicts(deep_scores_final).items():\n",
    "        dict1 = {'model_num':k,\"predictor\":\"deep\"}\n",
    "        all_scores_final.append({**dict1,**v})  \n",
    "\n",
    "    scores_df_sorted = pd.DataFrame(all_scores).sort_values(by='MSE')\n",
    "    for i,(index,row) in enumerate(scores_df_sorted.iterrows()):\n",
    "        s = f\"{i} - \" + \" - \".join([f\"{i}\" for i in row.tolist()])\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model_names = deep_models.keys()\n",
    "    for name in model_names:\n",
    "            sub_path = log_dir / name\n",
    "            if not sub_path.exists():\n",
    "                sub_path.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    from sk_models import setup_pls_models_exh, StandardScaler, PLSRegression\n",
    "    from plot import plot_preds_and_res\n",
    "\n",
    "    for deep_name,deep_model in tqdm(deep_models.items()):\n",
    "        if int(deep_name.replace(\"random_\",\"\"))>24:\n",
    "            logging.getLogger().info(f\"Running model {deep_name}\")\n",
    "            temp_dict = {deep_name:deep_model}\n",
    "\n",
    "            lwr_scheme = DeepLWRScheme_1_to_n(lwr_models = setup_pls_models_exh(nrow),n_neighbours=500,loss_fun_sk = mean_squared_error)\n",
    "            lwr_scores, lwr_preds, _ , _, _,_= eval.evaluate(temp_dict,dataset,lwr_scheme,logger_name=\"log\",load_fun=load_fun_cv,load_fun_pp = load_fun_pp_cv)\n",
    "            lwr_scores_final, lwr_preds_final, _ , _, _,_= eval.build(temp_dict,dataset,lwr_scheme,logger_name=\"test_log\",load_fun=load_fun_build,load_fun_pp = load_fun_pp_build)\n",
    "\n",
    "            #scores\n",
    "            for k,v in ut.flip_dicts(lwr_scores).items():\n",
    "                dict1 = {'model_num':deep_name,\"predictor\":k}\n",
    "                all_scores.append({**dict1,**v})\n",
    "\n",
    "            for k,v in ut.flip_dicts(lwr_scores_final).items():\n",
    "                dict1 = {'model_num':deep_name,\"predictor\":k}\n",
    "                all_scores_final.append({**dict1,**v})\n",
    "\n",
    "            lwr_preds['deep'] = deep_preds[deep_name]\n",
    "            lwr_preds_final['deep'] = deep_preds_final[deep_name]\n",
    "\n",
    "            lwr_preds.to_csv(log_dir/deep_name/ f\"predictions.csv\",index=False)\n",
    "            lwr_preds_final.to_csv(log_dir/deep_name/ f\"predictions_test.csv\",index=False)\n",
    "\n",
    "            #preds\n",
    "            # todo save predictions - appending solns\n",
    "            plot_preds_and_res(lwr_preds,name_lambda=lambda x:f\"{deep_name} with {x} predictor\",save_lambda= lambda x:f\"deep_lwr{x}\",save_loc=log_dir/deep_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    scores_df = pd.DataFrame(all_scores)\n",
    "    scores_df.to_csv(log_dir/f\"scores.csv\",index=False)\n",
    "    scores_df_final = pd.DataFrame(all_scores_final)\n",
    "    scores_df_final.to_csv(log_dir/f\"test_scores.csv\",index=False)\n",
    "\n",
    "    scores_df_sorted = pd.DataFrame(scores_df).sort_values(by='MSE')\n",
    "\n",
    "    best_5 = []\n",
    "    summary_logger.info(f\"Rank - \" +\" - \".join(list(scores_df_sorted.columns)))\n",
    "    for i,(index,row) in enumerate(scores_df_sorted.iterrows()):\n",
    "        if i < 5:\n",
    "            best_5.append((row[\"model_num\"],row[\"predictor\"],row[\"MSE\"]))\n",
    "        s = f\"{i} - \" + \" - \".join([f\"{i}\" for i in row.tolist()])\n",
    "        summary_logger.info(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    summary_logger.info(\"-----------------------\\n Best 5 on Test Sest \\n ---------------------\")\n",
    "    summary_logger.info(f\"Rank -  Deep Model - Predictor - Val Set - Test Set\")\n",
    "    for i, (j,k,v) in enumerate(best_5):\n",
    "\n",
    "        row = scores_df_final.loc[(scores_df_final['model_num']==j) & (scores_df_final['predictor'] == k)].iloc[0]\n",
    "        #print(row)\n",
    "        s = f\"{i} - {j} - {k} - {v} - {row['MSE']} - {row['R2']}\"\n",
    "        summary_logger.info(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    scores_df_original =  pd.read_csv(model_path/re.sub(r'\\.(?=csv$)[^.]+$', '',file_name)/\"scores.csv\")\n",
    "    scores_df_original[\"model_num\"] = [f\"random_{i}\" for i in range(0,100)]\n",
    "    print(scores_df_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    #take 1 is a scatter plot - lets, for each dataset\n",
    "    #graph our deep models by rank - plot - then overlay our knn moels\n",
    "    #plot points\n",
    "\n",
    "\n",
    "    deep_set = scores_df_original.sort_values(\"R2\")\n",
    "    deep_set[\"order\"] = [i for i in range(0,100)]\n",
    "    deep_ordering = {row[\"model_num\"]:row[\"order\"] for index, row in deep_set.iterrows()}\n",
    "    print(deep_ordering)\n",
    "    def order_models(x):\n",
    "        x = [deep_ordering[i] for i in x]\n",
    "        return x\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    set_deep = False\n",
    "    knn_models = scores_df[\"predictor\"].unique()\n",
    "    for knn_model in knn_models:\n",
    "        subset = scores_df[scores_df[\"predictor\"]==knn_model]\n",
    "        ax.scatter(x=order_models(subset[\"model_num\"].tolist()), y=subset[\"R2\"], s=3, label=knn_model)\n",
    "\n",
    "    ax.scatter(x=order_models(deep_set[\"model_num\"]),y=deep_set[\"R2\"],s=10,label=\"deep\")\n",
    "    #ax.set_ylim(0,scores_db[\"deep_mean\"].max())\n",
    "    ax.set_ylim(0,1)\n",
    "    # plot residuals\n",
    "    ax.legend(loc='upper right',bbox_to_anchor=(1.4, 1))\n",
    "    ax.set_ylabel(\"R^2 Score\")\n",
    "    ax.set_xlabel(\"Deep Model Rank\")\n",
    "    #ax.set_ylim(0,200)\n",
    "    #ax.set_yscale(\"symlog\")\n",
    "    ax.set_title(\"Summary of LWR improvements over Deep Models\")\n",
    "    plt.savefig(log_dir/f\"summary_plot.png\", bbox_inches='tight')\n",
    "    logging.getLogger().info(\"Wrote Summary Graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    scores_df[\"n_features\"] = [deep_models[i].n_features for i in scores_df[\"model_num\"]] \n",
    "    from matplotlib.colors import Colormap\n",
    "    import seaborn as sns #heatmap of features - pls model - score\n",
    "    class nlcmap(Colormap):\n",
    "        def __init__(self, cmap, levels):\n",
    "            self.cmap = cmap\n",
    "            self.N = cmap.N\n",
    "            self.monochrome = self.cmap.monochrome\n",
    "            self.levels = np.asarray(levels, dtype='float64')\n",
    "            self._x = self.levels\n",
    "            self.levmax = self.levels.max()\n",
    "            self.levmin = self.levels.min()\n",
    "            self.transformed_levels = np.linspace(self.levmin, self.levmax, #uniform spacing along levels (colour segments)\n",
    "                 len(self.levels))\n",
    "\n",
    "        def __call__(self, xi, alpha=1.0, **kw):\n",
    "            yi = np.interp(xi, self._x, self.transformed_levels)\n",
    "            return self.cmap((yi-self.levmin) / (self.levmax-self.levmin), alpha)\n",
    "\n",
    "    levels = np.concatenate((\n",
    "        [0, 1],\n",
    "        [0.6,0.8,0.9,0.95,0.98]\n",
    "        ))\n",
    "\n",
    "    levels = levels[levels <= 1]\n",
    "    levels.sort()\n",
    "    cmap_nonlin = nlcmap(plt.cm.YlGnBu, levels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    subset = scores_df[[\"predictor\",\"n_features\",\"R2\"]]\n",
    "    subset = subset[np.logical_not(subset[\"predictor\"]==\"deep\")]\n",
    "    subset = subset[np.logical_not(subset[\"predictor\"]==\"lr\")]\n",
    "    trans = subset[\"predictor\"].transform(lambda x: int(x.replace(\"lwr_k=\",\"\"))).tolist()\n",
    "    subset.loc[:,\"predictor\"]=trans\n",
    "    subset=subset.sort_values(\"predictor\",ascending=False)\n",
    "\n",
    "    def rand_jitter(arr):\n",
    "        stdev = .01 * (max(arr) - min(arr))\n",
    "        return arr + np.random.randn(len(arr)) * stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    fig, ax = plt.subplots()\n",
    "    sc = ax.scatter(x=rand_jitter(subset[\"n_features\"]), y=rand_jitter(subset[\"predictor\"]), s=20,c=subset[\"R2\"],cmap=cmap_nonlin,vmin=0)\n",
    "    ax.set_xlabel(\"Number of Features\")\n",
    "    ax.set_ylabel(\"Number of Neighbours\")\n",
    "\n",
    "    cbar = fig.colorbar(sc,label=\"R2 Score\")\n",
    "\n",
    "    ax.set_title(\"LWR performance as a function of the number of components\")\n",
    "    plt.savefig(log_dir/f\"heat_scatter.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (lazydeep)",
   "language": "python",
   "name": "pycharm-12fcba0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
