{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 1455] The paging file is too small for this operation to complete. Error loading \"C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\torch\\lib\\caffe2_detectron_ops_gpu.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2e03d936a45f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKNeighborsRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mut\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexperiment\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\jupyter_root\\utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_decomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPLSRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\lazydeep\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    121\u001b[0m                 \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWinError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                 \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrerror\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34mf' Error loading \"{dll}\" or one of its dependencies.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m                 \u001b[0mis_loaded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"C:\\Users\\huonf\\.conda\\envs\\lazydeep\\lib\\site-packages\\torch\\lib\\caffe2_detectron_ops_gpu.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import logging\n",
    "# set seed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import utils as ut\n",
    "import experiment as ex\n",
    "from evaluation import *\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import random\n",
    "#define fixed_hyperparams and create a config gen\n",
    "from configurations import RandomConfigGen, Configuration\n",
    "from torch import nn\n",
    "from deep_net import RandomNet\n",
    "from experiment import run_experiment\n",
    "import regex as re\n",
    "from pathlib import *\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed + 1)\n",
    "np.random.seed(seed + 2)\n",
    "random_state = np.random.RandomState(seed)\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"GPU detected is {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#setup input and outpu t formats, load data\n",
    "\n",
    "#we need to set parametesr\n",
    "file_name = \"mango_513_1050.csv\" #\"mango_684_990.csv\" #\"mango_729_975.csv\" #fitlered=513-1050\n",
    "id_cols =['Set','Season','Region','Date','Type','Cultivar','Pop','Temp',\"FruitID\"]#\n",
    "output_cols = ['DM']\n",
    "n_comps = [i for i in range(1,101)]\n",
    "data_path = Path('D:/workspace/lazydeep/data/soil_data/')\n",
    "log_path = Path(\"D:/workspace/lazydeep/experiments/1.03\") #1.01/\")\n",
    "if not log_path.exists():\n",
    "    log_path.mkdir()\n",
    "\n",
    "data_file = data_path / file_name\n",
    "log_dir = log_path / re.sub(r'\\.(?=csv$)[^.]+$', '',file_name)\n",
    "if not log_dir.exists():\n",
    "    log_dir.mkdir()\n",
    "print(f\"Output directory is {log_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_file)\n",
    "data = ut.sample_data(data,random_state)\n",
    "nrow, ncol = data.shape\n",
    "n_features = ncol - 1-len(id_cols)\n",
    "dataset = ut.TabularDataset(data,id_cols = id_cols, cat_cols=None, output_cols=output_cols, ignore_cols= None)\n",
    "eval = MangoesSplitter(preprocessing=None,tensorboard=None,time=True,random_state=random_state)\n",
    "print(f\"Dataset shape is {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% setup logging and tensorboard outputs\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set logging, in this case the root logger\n",
    "ut.setup_logger(logger_name=\"log\",file_name=log_dir/\"log.txt\")\n",
    "ut.setup_logger(logger_name=\"test_log\",file_name=log_dir/\"test_log.txt\")\n",
    "ut.setup_logger(logger_name=\"summary\",file_name=log_dir/\"summary.txt\")\n",
    "tb = SummaryWriter(log_dir/\"tb\")\n",
    "summary_logger = logging.getLogger(\"summary\")\n",
    "#step 1, run pls, set up pls - that runs best\n",
    "n_comps = [i for i in range(1,min(101,n_features))]\n",
    "pls_models = {i:PLSRegression(n_components=i) for i in n_comps}\n",
    "\n",
    "pls_scheme = SKLearnScheme(logger=\"log\")\n",
    "scores_pls, preds_pls, model_states_pls , train_time_pls, test_time_pls = eval.evaluate(pls_models,dataset,pls_scheme,logger_name=\"log\")\n",
    "summary_logger.info(f\"Train times: {train_time_pls}\")\n",
    "summary_logger.info(f\"Test times: {test_time_pls}\")\n",
    "from collections import defaultdict\n",
    "summary_logger.info(f\"Scores: {scores_pls}\")\n",
    "last_r2 = -math.inf\n",
    "selected_comps = -1\n",
    "\n",
    "for i,(key,value) in enumerate(flip_dicts(scores_pls).items()):\n",
    "    summary_logger.info(f\"{key}: {value}\")\n",
    "    \n",
    "for i,(key,value) in enumerate(flip_dicts(scores_pls).items()):\n",
    "    new_r2  = value[\"R2\"]\n",
    "    if new_r2 - last_r2 <= 0.01:\n",
    "        selected_comps = i-1\n",
    "        break\n",
    "    if i == 99:\n",
    "        selected_comps =  min(scores_pls[\"MSE\"],key=scores_pls[\"MSE\"].get)\n",
    "    last_r2 = new_r2\n",
    "\n",
    "summary_logger.info(f\"Selected pls preprocessing with {selected_comps} components\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = MangoesSplitter(preprocessing=Preprocess_PLS(n_components=selected_comps),tensorboard=None,time=True,random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the deep learners\n",
    "The following cells setup our models and run a train-test evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% setup experiment\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_models = 100\n",
    "epochs = 100\n",
    "bs = 32\n",
    "fixed_hyperparams = {'bs': bs,'loss': nn.MSELoss(),'epochs': epochs}\n",
    "device = \"cpu\" #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#setup models\n",
    "config_gen = RandomConfigGen(lr= (0,1),\n",
    "                             allow_increase_size=False,\n",
    "                             n_features=selected_comps,\n",
    "                             opt=[torch.optim.SGD,\n",
    "                                  torch.optim.Adam],\n",
    "                             lr_update = [None,\n",
    "                                          torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                                          torch.optim.lr_scheduler.ExponentialLR,\n",
    "                                          torch.optim.lr_scheduler.CosineAnnealingLR],\n",
    "                            dropout = [True,False],\n",
    "                            batch_norm = [True,False])\n",
    "configs = {f\"random_{i}\":config_gen.sample() for i in range(n_models)}\n",
    "config_gen.save(log_dir/'config_gen.txt')\n",
    "\n",
    "deep_models = {name:RandomNet(input_size=selected_comps,\n",
    "                             n_layers=config.n_layers,\n",
    "                             act_function=config.act_function,\n",
    "                             n_features = config.n_features,\n",
    "                             dropout=config.dropout,\n",
    "                             batch_norm=config.batch_norm,\n",
    "                             device=device,dtype=torch.float)\n",
    "              for name, config in configs.items()}\n",
    "\n",
    "ex.write_summary_head(seed,fixed_hyperparams)\n",
    "ex.save_models(deep_models,configs,log_dir)\n",
    "start = datetime.datetime.now()\n",
    "deep_scheme = DeepScheme(configs,fixed_hyperparams=fixed_hyperparams,logger=\"log\",device=device,adaptive_lr=True)\n",
    "scores_deep, preds_deep, model_states_deep , train_time_deep, test_time_deep = eval.evaluate(deep_models,dataset,deep_scheme,logger_name=\"log\")\n",
    "\n",
    "scores_final, preds_final, model_states_ls_final , train_time_deep_final, test_time_deep_final = eval.build(deep_models,dataset,deep_scheme,logger_name=\"test_log\")\n",
    "for model, state_dict in model_states_ls_final.items():\n",
    "    torch.save(state_dict, log_dir / \"models\" / f\"{model}\" / f\"_final\")\n",
    "\n",
    "summary_logger.info(f\"Train times: {train_time_deep}\")\n",
    "summary_logger.info(f\"Test times: {test_time_deep}\")\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% log results\n"
    }
   },
   "outputs": [],
   "source": [
    "ex.save_results(model_states_deep, preds_deep,configs, scores_deep, log_dir,tb,prefix=\"\")\n",
    "\n",
    "#summary_logger.info(f\"Scores: {scores_deep}\")\n",
    "#for key,value in flip_dicts(scores_deep).items():\n",
    "#    summary_logger.info(f\"{key}: {value}\")\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "diff = end - start\n",
    "ex.write_summary(diff, deep_models, scores_deep,prefix=\"\")\n",
    "ex.save_pred_plots(preds_deep, deep_models,log_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting deep results as a function of number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(scores_deep)\n",
    "scores_df.to_csv(log_dir / f\"scores.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "if True:\n",
    "    # plot deep results as a function of number of features\n",
    "\n",
    "    n_features_dict = {name:config.n_features for name,config in configs.items()}\n",
    "    to_plot = pd.DataFrame([[name, scores_deep[\"R2\"][name],n_features_dict[name]] for name in scores_deep[\"R2\"].keys()]\n",
    "                           ,columns = [\"name\",\"score\",\"n_features\"])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(to_plot[\"score\"],bins=100,density=True)\n",
    "    #ax.set_xscale('log')\n",
    "    #ax.legend(loc='upper right',bbox_to_anchor=(1.4, 1))\n",
    "    ax.set_xlabel(\"Number of Models\")\n",
    "    ax.set_ylabel(\"R2\")\n",
    "    ax.set_title(\"Distribution of R2 Scoes\")\n",
    "    plt.savefig(log_dir / f\"dist_plot.png\",bbox_inches='tight')\n",
    "    #plt.savefig(log_dir / f\"pp_deep_pls_compressed.png\",bbox_inches='tight')\n",
    "\n",
    "    n_features_dict = {name:config.n_features for name,config in configs.items()}\n",
    "    to_plot = pd.DataFrame([[name, scores_deep[\"R2\"][name],n_features_dict[name]] for name in scores_deep[\"R2\"].keys()]\n",
    "                           ,columns = [\"name\",\"score\",\"n_features\"])\n",
    "    to_plot = to_plot[to_plot[\"score\"]>=0]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(to_plot[\"score\"],bins=100,density=True)\n",
    "    #ax.set_xscale('log')\n",
    "    #ax.legend(loc='upper right',bbox_to_anchor=(1.4, 1))\n",
    "    ax.set_xlabel(\"Number of Models\")\n",
    "    ax.set_ylabel(\"R2\")\n",
    "    ax.set_title(\"Distribution of R2 Scoes\")\n",
    "    plt.savefig(log_dir / f\"dist_plot_compressed.png\",bbox_inches='tight')\n",
    "    #plt.savefig(log_dir / f\"pp_deep_pls_compressed.png\",bbox_inches='tight')\n",
    "    pass\n",
    "\n",
    "    #plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% Return our best models\n"
    }
   },
   "outputs": [],
   "source": [
    "summary_logger.info(\"------------------\\n Top 5 performance on Test Set\")\n",
    "summary_logger.info(f\"Index - Model - Val MSE - Val R2 - Test MSE - Test R2\")\n",
    "for i,key in enumerate(sorted(scores_deep['MSE'],key=scores_deep['MSE'].get)):\n",
    "    if i <5:\n",
    "        summary_logger.info(f\"{i} - {key} - {scores_deep['MSE'][key]} -{scores_deep['R2'][key]} - {scores_deep['MSE'][key]} - {scores_final['MSE'][key]} - {scores_final['R2'][key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (lazydeep)",
   "language": "python",
   "name": "pycharm-12fcba0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
